[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 512/612: Linear Models",
    "section": "",
    "text": "BSTA 512/612: Linear Models\n\nWinter 2024\n \nWelcome to BSTA 512/612! In this course, we will focus on linear models, and build our understanding of regression analysis. We will build some theoretical understanding in order to interpret and apply regression models appropriately. We will learn how to build a regression model, interpret the model, and diagnose potential issues with our model.  \n\n\n\n\n\n\n\nInstructor\n Dr. Nicky Wakim\n Vanport 622A\n wakim@ohsu.edu\n\n\nOffice Hours\nOH with Nicky\n Thursdays, 11:30am - 1pm\nOH with Antara\n Wednesdays, 4:30 - 6 pm\nOH with Ariel\n Tuesdays, 2:30 - 4 pm\n\n\nCourse details\n Mondays, Wednesdays\n Jan 8 - March 20\n 1:00 PM - 2:50 PM\n In-person\n\n\nContacting me\nE-mail or Slack is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Weekly Pages",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n1/8/24\n\n\nWeek 1\n\n\nIntroduction; Review; and Data Wrangling\n\n\n\n\n1/15/24\n\n\nWeek 2\n\n\nSimple Linear Regression\n\n\n\n\n1/22/24\n\n\nWeek 3\n\n\nSLR: Hypothesis Testing and Evaluation\n\n\n\n\n1/29/24\n\n\nWeek 4\n\n\nSLR: Diagnostics\n\n\n\n\n2/5/24\n\n\nWeek 5\n\n\nMultiple Linear Regression\n\n\n\n\n2/12/24\n\n\nWeek 6\n\n\nCategorical Covariates + Interactions\n\n\n\n\n2/19/24\n\n\nWeek 7\n\n\nInteractions continued\n\n\n\n\n2/26/24\n\n\nWeek 8\n\n\nInteractions continued\n\n\n\n\n3/4/24\n\n\nWeek 9\n\n\nModel Selection\n\n\n\n\n3/11/24\n\n\nWeek 10\n\n\nModel Diagnostics\n\n\n\n\n3/18/24\n\n\nWeek 11\n\n\nSpill-Over Week\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course is designed to introduce history, concepts and distributions in probability, Monte Carlo simulation techniques, and Markov chains. Students will also learn how to write R codes for various statistical computations and plots. Previous experience in R is not required. R is free software available from http://www.r-project.org.\n\n\nAt the end of this course, students should be able to…\n\nUnderstand basic concepts in probability\nCompute probabilities for random variables\nCompute probabilities using basic distributions\nPerform statistical computations and simulations using R"
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\n\nIntroduction to Probability\n\nAuthors: Mark Daniel Ward and Ellen Gundlach\nPublisher: W. H. Freeman\nEdition: 1st\nISBN-13: 978-0716771098\nTextbook in Sakai\n\n\n\nSupplemental Readings (Optional)\n\nStatistical Inference, Casella and Berger, 2nd ed. (This will be the textbook for BSTA 551-552 Math Stat.)\nIntroduction to Probability, Charles M. Grinstead and J. Laurie Snell, http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/pdf.html\nProbability With Applications and R, Robert P. Dobrow, Wiley 2013 (eBook available from OHSU library)\nAn Introduction to R (free pdf available from http://cran.r-project.org/manuals.html)"
  },
  {
    "objectID": "syllabus.html#online-resources",
    "href": "syllabus.html#online-resources",
    "title": "Syllabus",
    "section": "Online Resources",
    "text": "Online Resources\n\nSlack\nWe will use Slack as our main form of communication for the class. I will try to mirror the Slack Workspace that you had for 512/612. If you are unsure how to do a homework problem or have other questions, please ask myself or the TA’s either by email or posting your question(s) on Slack. Please know that Slack is not guarded by the OSHU firewall, so if you have a question about accommodations or any sensitive topics, you may wish to message me via email. You can still message me regarding sensitive information on Slack, but I will not initiate those conversations on Slack. Please continue to use the tips on asking for help that Meike laid out in the 512/612 syllabus. I will also add her tips to the Slack workspace.\n**Please use this invitation link for our Slack workspace!**\n\n\nSakai\nCourse materials will be delivered online through Sakai, OHSU’s course management system. This is where you’ll find all course materials (syllabus, videos, lectures, homework assignments, announcements, etc.). This is also where you’ll submit all course assignments.\nI will try to keep the structure of the Sakai page similar to that of BSTA 512/612. It’s always tough for you, the students, to spend two quarters with one faculty member then change to a new one. That is why I will try my best to keep the Sakai navigation similar. However, certain aspects will certainly be different. I hope we can work together to identify the things that are and are not working for our classroom. \n\n\nExplain Everything\nI plan to use Explain Everything on my iPad to deliver lectures. This application allows me to take real time notes, access Poll Everywhere, and record the lecture. I hope to use these features to allow you to follow me during lecture and have access to a recording for asynchronous viewing. While I will try to always make a recorded lecture available to you after class, I want you to try to attend class in person. I understand that life events get in the way on in-person attendance, but your attendance in-person brings me joy while I teach, and then further motivates me to be a great teacher. \nNote: I am new to this application so I ask you for some grace as I learn to navigate it. \n\n\nWebex\nWebex software will be used for office hours. To give everyone the best possible experience with Webex, we recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 414 Website\nPennState has a class offered to advanced undergraduates that has some overlap with our class. They have all their course notes posted on thispage. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#types-of-assessments",
    "href": "syllabus.html#types-of-assessments",
    "title": "Syllabus",
    "section": "Types of assessments",
    "text": "Types of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Grading of these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom.\n\nStudents will always have the opportunity to redo formative assessments until they are satisfied with their grade. That means students can redo homework and readings until they are satisfied with their grade. This is meant to emphasize the learning process, and reduce pressure for correctness. However, there are some limitations on “redoing” your work. In the homework grading scale (shown in Criteria), you might notice that certain grades correspond to a certain percentage of the homework completed. You can only redo problems that you have completely worked through. Thus, you can only increase your grade by increasing the amount of correct problems."
  },
  {
    "objectID": "syllabus.html#breakdown",
    "href": "syllabus.html#breakdown",
    "title": "Syllabus",
    "section": "Breakdown",
    "text": "Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade\n\n\nHomework\nFormative\nApprox. weekly\n35%\n\n\nPost-class survey\nN/A\nTwice Weekly\n5%\n\n\nMidterm Exam\nSummative\n5/12\n30%\n\n\nFinal Exam\nSummative\nPaper: 6/11\n30%\n\n\n\n\n\nCriteria\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn something in (whether it is incomplete, complete, correct, or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nAfter the due date, either the TA or myself will post the solutions.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points.\n\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nEach of you will receive one “no questions asked” extension on any homework assignment. To request the extension, you can write an email with subject “Extension on Homework Assignment __” with body saying “I would like an extension on Homework Assignment __ for __ days.” I will grant you that extension with no questions asked.\n\nIf you have already used a “no questions asked” extension on homework, then homework grades will go down one point for every day it is late. For example, if your work is on par of a 5/5, but you turn it in homework 2 days late, then you will receive a 3/5. \n\nThe class will end on June 16, 2023. All coursework is expected to be completed by then. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor non-homework assignments, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class and participate in class polls and the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. Make sure to complete the exit ticket to demonstrate attendance."
  },
  {
    "objectID": "syllabus.html#ongoing-course-feedback",
    "href": "syllabus.html#ongoing-course-feedback",
    "title": "Syllabus",
    "section": "Ongoing Course Feedback",
    "text": "Ongoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 513/613 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith for my race. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material."
  },
  {
    "objectID": "syllabus.html#midterm-feedback",
    "href": "syllabus.html#midterm-feedback",
    "title": "Syllabus",
    "section": "Midterm Feedback",
    "text": "Midterm Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your midterm exam grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback."
  },
  {
    "objectID": "syllabus.html#final-course-feedback",
    "href": "syllabus.html#final-course-feedback",
    "title": "Syllabus",
    "section": "Final Course Feedback",
    "text": "Final Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement."
  },
  {
    "objectID": "syllabus.html#instructor-expectations",
    "href": "syllabus.html#instructor-expectations",
    "title": "Syllabus",
    "section": "Instructor Expectations",
    "text": "Instructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai."
  },
  {
    "objectID": "syllabus.html#student-expectations-and-resources",
    "href": "syllabus.html#student-expectations-and-resources",
    "title": "Syllabus",
    "section": "Student Expectations and Resources",
    "text": "Student Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the semester so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact the instructor or any of the TAs.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of papers or homework, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "readings/Chapter_01.html",
    "href": "readings/Chapter_01.html",
    "title": "Outcomes, Events, and Sample Space",
    "section": "",
    "text": "Note that I will keep the example and definition numbering that is presented in the textbook, so it may seem like numbers are being skipped. This just means I may not have presented an example from the textbook here. Feel free to consult the textbook for more examples!"
  },
  {
    "objectID": "readings/Chapter_01.html#introduction",
    "href": "readings/Chapter_01.html#introduction",
    "title": "Outcomes, Events, and Sample Space",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThe introduction in this chapter starts with the following quote:\n\n\n\n\n\n\n“Probability theory is the study of randomness and all things associated with randomness.”\n\n\n\nI couldn’t say it better myself. There are examples with obvious hints at randomness (i.e. dice rolls, coins flips) and there are examples with not-so-obvious randomness involved (i.e. time until any given traffic light turns green).\n\n\n\n\n\n\nDefinition 1.1\n\n\n\nWhen something happens at random there are several potential outcomes. Exactly one of the outcomes occurs. An event is defined to be a collection of some outcomes.\n\n\nOur goal in probability and statistics is to characterize randomness. Then as statisticians, we can use what we know about randomness to see if certain features of the world is within randomness or outside the randomness. For example, according to a New York Times article published on July 25, 2023, the Education Department is investigating Harvard University’s legacy admissions policy. Here is an excerpt from that article:\n\nHarvard gives preference to applicants who are recruited athletes, legacies, relatives of donors and children of faculty and staff. As a group, they make up less than 5 percent of applicants, but around 30 percent of those admitted each year. About 67.8 percent of these applicants are white, according to court papers.\n\nUsing probability, we can start to see why this practice might be inequitable. If these applicants make up 5% of the total applicant pool, and we completely, randomly chose applicants to admit, then we would expect 5% of admitted students to be this group of recruited athletes, legacies, relatives of donors, and children of faculty and staff. We could argue that recruited athletes are more likely to be admitted since they are, in fact, recruited, or that children of faculty and staff might “game” high school succesfully because they live in a home that subscribes to the academic world. If we want to compare the acceptance rate of 30% for this group to the average acceptance rate, the article actually fails to present a crucial peice of information: the average acceptance rate. I looked it up, and it is ~4%. So now we can start to think about questions like: With an average acceptance rate of 4%, does an acceptance of 30% of this group of recruited athletes, legacies, relatives of donors and children of faculty and staff make sense? We will explore this example further as we progress through our class.\nBaaack to our definitions: When we have several potential outcomes, and an event is a collection of outcomes, then we can start thinking of the set of potential outcomes. When there are no outcomes, then we have the empty set \\(\\emptyset\\). When we look at all outcomes, we call it the sample space \\(S\\). For now, when we look at a single event, we will focus on the sample space. When we start thinking of multiple events, the empty set might come into play. And just to reiterate: The empty set is an event. It is an impossible event, but it is still considered an event. So when you consider all possible events, the empty set is included. When you consider all possible outcomes, this is the sample space.\nNow let’s look at at an example with an obvious hint at randomness:\n\n\n\n\n\n\nExample 1.2\n\n\n\nYou roll a 6-sided die.\n\n\n\n\n\n\n\n\nExample 1.2 Explanation\n\n\n\n\n\nA 6-sided die has sides labelled 1-6. Thus, the sample space is \\(S=\\{1,2,3,4,5,6\\}\\) since we can land on any of the 6 sides. For every roll, only one of these outcomes occur, and that outcome is random.\nAn event can be any subgroup (more formally, subset) of the sample space, but not necessarily. Events can technically be outside of the sample space, but we often only consider events within the sample space.\nIf we can say our event is if we roll a 6, then our event is defined mathematically as \\(\\{6\\}\\). We can define a different event as rolling an odd numbered side, then our event is defined mathematically as \\(\\{1,3,5\\}\\). See TB pg 4 for more event examples.\n\n\n\nWe introduced subset, but here’s the definition:\n\n\n\n\n\n\nDefinition 1.3\n\n\n\nEvent A is a subset of event B, written \\(A \\subset B\\). if every outcome in A is also an outcome in B.\n\n\n\nLet’s work on defining different types of events within sample spaces:\n\n\n\n\n\n\nExample 1.4\n\n\n\nA student buys a book and opens it to a random page. They note the number of typographical errors on the page. Let’s define the sample space and discuss one potential event.\n\n\n\n\n\n\n\n\nExample 1.4 Explanation\n\n\n\n\n\nSince there must be 0 or more errors, and errors are counted with whole numbers, the sample space will be the set of nonnegative integers: \\(S=\\mathbb{Z}^{&gt;=0}\\). More plainly, \\(S=\\{0, 1, 2, 3, 4, ...\\}\\).\nHere are a few possible events we can consider: (I invite you to think of others if you’d like)\n\nevent that a page contains 4 errors\nevent that a page contains at most 3 errors\nevent that a page contains more than 3 errors\nevent that a page contains an odd number of errors.\n\n\n\n\nDefinitions:\n\n\n\n\n\n\nDefinition 1.11\n\n\n\nThe union of events \\(A\\) and \\(B\\), denoted by \\(A \\cup B\\), contains all outcomes that are in \\(A\\) or \\(B\\).\n\n\n\n\n\n\n\n\nDefinition 1.12\n\n\n\nThe intersection of events \\(A\\) and \\(B\\), denoted by \\(A \\cap B\\), contains all outcomes that are both in \\(A\\) and \\(B\\)."
  },
  {
    "objectID": "readings/Chapter_01.html#complements-and-demorgans-laws",
    "href": "readings/Chapter_01.html#complements-and-demorgans-laws",
    "title": "Outcomes, Events, and Sample Space",
    "section": "1.2 Complements and DeMorgan’s Laws",
    "text": "1.2 Complements and DeMorgan’s Laws"
  },
  {
    "objectID": "readings/Chapter_01.html#notes",
    "href": "readings/Chapter_01.html#notes",
    "title": "Outcomes, Events, and Sample Space",
    "section": "Notes",
    "text": "Notes\nThere are a few things I’d like to address from the book:\n\nThere is a set of examples, 1.5 and 1.6, that talk about the birth of a baby. While this example is helpful for considering potential sample spaces, there is an oversight on the biology of births. The examples refer to the event of the sex assigned at birth (SAB). There are two issues in this example. First, SAB is considered binary in this example (I will get back to this). Second, the binary SAB is written as “boy” or “girl.” These labels have gender identity inherently attached to their meaning, and thus, the more accurate binary versions of these are “male” or “female,” respectively (“Assigned Sex at Birth,” n.d.). These are often denoted as “assigned male at birth” (AMAB) or “assigned female at birth” (AFAB). Going back the first issue, a binary representation of SAB does not cover all possible events. People can also be intersex, meaning their genitals, chromosomes, and/or reproductive organs are not exclusively AMAB or AFAB (“Intersex: What Is Intersex, Gender Identity, Intersex Surgery,” n.d.).\n\n\n\n\n\n\nExtension to Example 1.5 and 1.6\n\n\n\nYou can go back to these examples (TB pg. 5) and work through them with the more accurate representation of sex at birth. Please use the possible outcomes of AFAB, AMAB, or intersex."
  },
  {
    "objectID": "homework/HW5.html",
    "href": "homework/HW5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Important\n\n\n\nThis assignment is NO LONGER under construction !!! (3/1/2024)"
  },
  {
    "objectID": "homework/HW8.html",
    "href": "homework/HW8.html",
    "title": "Homework 7",
    "section": "",
    "text": "Complete all of the problems listed below. Only turn in the ones listed in the “Turn In” column. Please submit problems in the order they are listed.\nYou must show all of your work to receive credit. Don’t forget to define every r.v. you use! In particular, if a similar problem was done in class or an example in the book, make sure to still show every step in the solution and not just cite the examples’ results."
  },
  {
    "objectID": "homework/HW2.html",
    "href": "homework/HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "Important\n\n\n\nThis page is no longer under construction. You may start this homework! (1/25/24)"
  },
  {
    "objectID": "homework/HW7.html",
    "href": "homework/HW7.html",
    "title": "Homework 7",
    "section": "",
    "text": "Please turn in this homework on Sakai. Please submit your homework in pdf format. You can type your work on your computer or submit a photo of your written work or any other method that can be turned into a pdf. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in homework.\nTry to complete all of the problems listed below at some point this quarter! You may want to save some of them for studying later! Only turn in the ones listed in the “Turn In” column. Please submit problems in the order they are listed.\nThe more work you include that shows your thought process, the more I can give you feedback.\n\n\n\n\n\n\n\n\n\nChapter\nTurn In\nExtra Problems\n\n\n\n\n28\nTB # 18\nTB # 1, 10\n\n\n291\nTB # 26, NTB # 1, 3\nTB # 10, 14, 23, 11, 13, 32\n\n\n30\n\nTB # 4, 7-12\n\n\n31\nTB # 18\nTB # 13, 14, 17\n\n\n32\nTB # 8\nTB # 3, 5, 102, 15\n\n\n33\nNTB # 4\nTB # 3, 9, 10\n\n\n35\nTB # 10, NTB # 5\nTB # 6, 9, 24\n\n\n43\nTB # 93, 104, 11, 125, NTB # 6, 7, 8\nTB # 1-4, NTB # 9\n\n\n36\nTB # 126, 14\nTB # 4, 11, 13, 15, 16\n\n\n37\nTB # 24, 30\nTB # 2, 4, 13, 20, 29"
  },
  {
    "objectID": "class_slides.html",
    "href": "class_slides.html",
    "title": "Class Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n11/28/22\n\n\nNHANES\n\n\nundefined\n\n\n\n\n1/8/23\n\n\nWelcome to BSTA 512/612!\n\n\nWeek 1\n\n\n\n\n1/8/23\n\n\nReview\n\n\nWeek 1\n\n\n\n\n1/10/23\n\n\nData Management with the tidyverse\n\n\nWeek 1\n\n\n\n\n1/17/23\n\n\nSLR: Inference and Prediction\n\n\nWeek 1\n\n\n\n\n1/17/23\n\n\nSimple Linear Regression (SLR)\n\n\nWeek 1\n\n\n\n\n1/22/23\n\n\nSLR: Inference and Prediction\n\n\nWeek 1\n\n\n\n\n1/24/23\n\n\nSLR: More inference + Evaluation\n\n\nWeek 1\n\n\n\n\n1/29/23\n\n\nSLR: Model Evaluation and Diagnostics\n\n\nWeek 4\n\n\n\n\n1/31/23\n\n\nSLR: Model Evaluation and Diagnostics\n\n\nWeek 4\n\n\n\n\n2/5/24\n\n\nIntroduction to Multiple Linear Regression (MLR)\n\n\nWeek 5\n\n\n\n\n2/5/24\n\n\nA word on Quiz 1 and Lab 1\n\n\nWeek 5\n\n\n\n\n2/7/24\n\n\nMLR: Inference / F-test\n\n\nWeek 5\n\n\n\n\n2/12/24\n\n\nLesson 10: Categorical Covariates\n\n\nWeek 6\n\n\n\n\n2/14/24\n\n\nLesson 11: Interactions\n\n\nWeek 6\n\n\n\n\n2/26/24\n\n\nLesson 11: Interactions Continued\n\n\nWeek 8\n\n\n\n\n2/28/24\n\n\nLesson 12: In-class exercise!!\n\n\nWeek 8\n\n\n\n\n3/4/24\n\n\nLesson 12: Model/Variable Selection\n\n\nWeek 9\n\n\n\n\n3/4/24\n\n\nLesson 12: Model/Variable Selection\n\n\nWeek 9\n\n\n\n\n3/4/24\n\n\nLesson 13: Purposeful model selection\n\n\nWeek 9\n\n\n\n\n3/4/24\n\n\nSome words on Quiz 2, Lab 2, and Mid-term Feedback\n\n\nWeek 9\n\n\n\n\n3/11/24\n\n\nSome words on Lab 3\n\n\nWeek 10\n\n\n\n\n3/13/24\n\n\nLesson 14: MLR Model Diagnostics\n\n\nWeek 10\n\n\n\n\nInvalid Date\n\n\nPrediction Bands\n\n\nundefined\n\n\n\n\n3/18/24\n\n\nSome words on Lab 4 + help with coefficients\n\n\nWeek 10\n\n\n\n\nundefined\n\n\nAdditional Example on Model Diagnostics\n\n\nundefined\n\n\n\n\nundefined\n\n\nUntitled\n\n\nundefined\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/12_Variance.html",
    "href": "slides/12_Variance.html",
    "title": "Chapter 12: Variance of Discrete r.v.’s - or, Expected Values of Functions of r.v.’s",
    "section": "",
    "text": "Question: What is \\(\\mathbb{E}[g(X)]\\) for a function \\(g\\) and discrete r.v. \\(X\\)?\n\n\nExample 1. Let \\(g(x) = ax+b\\), for real-valued constants \\(a\\) and \\(b\\). What is \\(\\mathbb{E}[g(X)]\\)?\n\nSolution:\n\n\nDefinition 2 (Expected value of a function of a r.v.).   For any function \\(g\\) and discrete r.v. \\(X\\), the expected value of \\(g(X)\\) is \\[\\mathbb{E}[g(X)] = \\sum_{\\{all\\ x\\}}\\ g(x) p_X(x).\\]\n\n\nExample 3. Suppose you draw 2 cards from a standard deck of cards with* replacement. Let \\(X\\) be the number of hearts you draw.*\n\nFind \\(\\mathbb{E}[X^2]\\).\nSolution:\nFind \\(\\mathbb{E}\\big[\\big(X-\\frac{1}{2}\\big)^2\\big]\\).\nSolution:\n\n\n\n\n\n\nDefinition 4 (Variance of a r.v.).   The variance of a r.v. \\(X\\), with (finite) expected value \\(\\mu_X=\\mathbb{E}[X]\\) is \\[\\sigma_X^2=Var(X)=\\mathbb{E}[(X-\\mu_X)^2] = \\mathbb{E}[(X-\\mathbb{E}[X])^2].\\]\n\n\nDefinition 5 (Standard deviation of a r.v.).   The standard deviation of a r.v. \\(X\\) is \\[\\sigma_X = SD(X) = \\sqrt{\\sigma_X^2}=\\sqrt{Var(X)}.\\]\n\n\nQuestions: Why do we square the difference in the variance definition? \\((X-\\mu_X)^2\\)\n\nWhy not define the measure of spread as \\(\\mathbb{E}[X-\\mu_X] = \\mathbb{E}[X-\\mathbb{E}[X]]\\)?\nWhy not use \\(\\mathbb{E}[|X-\\mu_X|]\\)?\n\n\nLemma 6 (\"Computation formula\" for Variance).   The variance of a r.v. \\(X\\), can be computed as \\[\\sigma_X^2=Var(X)=\\mathbb{E}[X^2]-\\mu_X^2 = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2.\\]\n\n\nProof. Proof. ◻\n\n\n\n\n\nLemma 7.   For a r.v. \\(X\\) and constants \\(a\\) and \\(b\\), \\[Var(aX+b) = a^2Var(X).\\]\n\n\nProof. Proof. See homework. ◻\n\n\n\n\n\nTheorem 8.   For independent r.v.’s \\(X\\) and \\(Y\\), and functions \\(g\\) and \\(h\\), \\[\\mathbb{E}[g(X)h(Y)] = \\mathbb{E}[g(X)]\\mathbb{E}[h(Y)].\\]\n\n\nCorollary 1.   For independent r.v.’s \\(X\\) and \\(Y\\), \\[\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y].\\]\n\n\nProof. Proof of Theorem 8. ◻\n\n\nTheorem 9 (Variance of sum of independent discrete r.v.’s).   For independent discrete r.v.’s \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Big(\\sum_{i=1}^n a_iX_i\\Big) = \\sum_{i=1}^n a_i^2Var(X_i).\\]\n\n\nCorollary 2.   For independent discrete r.v.’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Big(\\sum_{i=1}^n X_i\\Big) = \\sum_{i=1}^n Var(X_i).\\]\n\n\nCorollary 3.   For independent identically (i.i.d.) discrete r.v.’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Big(\\sum_{i=1}^n X_i\\Big) = n Var(X_1).\\]\n\n\nProof. Proof to Theorem 9. ◻\n\n\nExample 10. A tour group is planning a visit to the city of Landport and needs to book 30 hotel rooms. The average price of a room is $200 with standard deviation $10. In addition, there is a 10% tourism tax for each room. What is the standard deviation of the cost for the 30 hotel rooms?\n\nSolution:"
  },
  {
    "objectID": "slides/12_Variance.html#expected-values-of-functions-of-r.v.s",
    "href": "slides/12_Variance.html#expected-values-of-functions-of-r.v.s",
    "title": "Chapter 12: Variance of Discrete r.v.’s - or, Expected Values of Functions of r.v.’s",
    "section": "",
    "text": "Question: What is \\(\\mathbb{E}[g(X)]\\) for a function \\(g\\) and discrete r.v. \\(X\\)?\n\n\nExample 1. Let \\(g(x) = ax+b\\), for real-valued constants \\(a\\) and \\(b\\). What is \\(\\mathbb{E}[g(X)]\\)?\n\nSolution:\n\n\nDefinition 2 (Expected value of a function of a r.v.).   For any function \\(g\\) and discrete r.v. \\(X\\), the expected value of \\(g(X)\\) is \\[\\mathbb{E}[g(X)] = \\sum_{\\{all\\ x\\}}\\ g(x) p_X(x).\\]\n\n\nExample 3. Suppose you draw 2 cards from a standard deck of cards with* replacement. Let \\(X\\) be the number of hearts you draw.*\n\nFind \\(\\mathbb{E}[X^2]\\).\nSolution:\nFind \\(\\mathbb{E}\\big[\\big(X-\\frac{1}{2}\\big)^2\\big]\\).\nSolution:"
  },
  {
    "objectID": "slides/12_Variance.html#variance-of-a-r.v.",
    "href": "slides/12_Variance.html#variance-of-a-r.v.",
    "title": "Chapter 12: Variance of Discrete r.v.’s - or, Expected Values of Functions of r.v.’s",
    "section": "",
    "text": "Definition 4 (Variance of a r.v.).   The variance of a r.v. \\(X\\), with (finite) expected value \\(\\mu_X=\\mathbb{E}[X]\\) is \\[\\sigma_X^2=Var(X)=\\mathbb{E}[(X-\\mu_X)^2] = \\mathbb{E}[(X-\\mathbb{E}[X])^2].\\]\n\n\nDefinition 5 (Standard deviation of a r.v.).   The standard deviation of a r.v. \\(X\\) is \\[\\sigma_X = SD(X) = \\sqrt{\\sigma_X^2}=\\sqrt{Var(X)}.\\]\n\n\nQuestions: Why do we square the difference in the variance definition? \\((X-\\mu_X)^2\\)\n\nWhy not define the measure of spread as \\(\\mathbb{E}[X-\\mu_X] = \\mathbb{E}[X-\\mathbb{E}[X]]\\)?\nWhy not use \\(\\mathbb{E}[|X-\\mu_X|]\\)?\n\n\nLemma 6 (\"Computation formula\" for Variance).   The variance of a r.v. \\(X\\), can be computed as \\[\\sigma_X^2=Var(X)=\\mathbb{E}[X^2]-\\mu_X^2 = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2.\\]\n\n\nProof. Proof. ◻"
  },
  {
    "objectID": "slides/12_Variance.html#some-important-variance-and-expected-values-results",
    "href": "slides/12_Variance.html#some-important-variance-and-expected-values-results",
    "title": "Chapter 12: Variance of Discrete r.v.’s - or, Expected Values of Functions of r.v.’s",
    "section": "",
    "text": "Lemma 7.   For a r.v. \\(X\\) and constants \\(a\\) and \\(b\\), \\[Var(aX+b) = a^2Var(X).\\]\n\n\nProof. Proof. See homework. ◻\n\n\n\n\n\nTheorem 8.   For independent r.v.’s \\(X\\) and \\(Y\\), and functions \\(g\\) and \\(h\\), \\[\\mathbb{E}[g(X)h(Y)] = \\mathbb{E}[g(X)]\\mathbb{E}[h(Y)].\\]\n\n\nCorollary 1.   For independent r.v.’s \\(X\\) and \\(Y\\), \\[\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y].\\]\n\n\nProof. Proof of Theorem 8. ◻\n\n\nTheorem 9 (Variance of sum of independent discrete r.v.’s).   For independent discrete r.v.’s \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Big(\\sum_{i=1}^n a_iX_i\\Big) = \\sum_{i=1}^n a_i^2Var(X_i).\\]\n\n\nCorollary 2.   For independent discrete r.v.’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Big(\\sum_{i=1}^n X_i\\Big) = \\sum_{i=1}^n Var(X_i).\\]\n\n\nCorollary 3.   For independent identically (i.i.d.) discrete r.v.’s \\(X_i\\), \\(i=1,2,\\dots, n\\), \\[Var\\Big(\\sum_{i=1}^n X_i\\Big) = n Var(X_1).\\]\n\n\nProof. Proof to Theorem 9. ◻\n\n\nExample 10. A tour group is planning a visit to the city of Landport and needs to book 30 hotel rooms. The average price of a room is $200 with standard deviation $10. In addition, there is a 10% tourism tax for each room. What is the standard deviation of the cost for the 30 hotel rooms?\n\nSolution:"
  },
  {
    "objectID": "slides/37_Central_Limit_Theorem.html",
    "href": "slides/37_Central_Limit_Theorem.html",
    "title": "Ch 37: The Central Limit Theorem (CLT)",
    "section": "",
    "text": "Ch 37: The Central Limit Theorem (CLT)\n\nTheorem 1 (Central Limit Theorem (CLT)).  Let \\(X_i\\) be iid rv’s with common mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,2,\\ldots,n\\). Then \\[\\hspace{-12cm} \\sum_{i=1}^n X_i \\rightarrow\\]\n\n\n\nCorollary 1.   Let \\(X_i\\) be iid rv’s with common mean \\(\\mu\\) and variance \\(\\sigma^2\\), for \\(i=1,2,\\ldots,n\\). Then \\[\\hspace{-12cm} \\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n}  \\rightarrow\\]\n\n\nExample 2.   According to a large US study, the mean resting heart rate of adult women is about 74 beats per minutes (bpm), with standard deviation 13 bpm (NHANES 2003-2004).\n\nFind the probability that the average resting heart rate for a random sample of 36 adult women is more than 3 bpm away from the mean.\nRepeat the previous question for a single adult woman.\n\n\n\nExample 3.   Let \\(X_i \\sim Exp(\\lambda)\\) be iid r.v.’s for \\(i=1,2,\\ldots,n\\). Then \\[\\hspace{-12cm} \\sum_{i=1}^n X_i \\rightarrow\\]\n\n\nCLT for Discrete rv’s\n\nBinomial rv’s: Let \\(X \\sim Bin(n,p)\\).\nPoisson rv’s: Let \\(X \\sim Poisson(\\lambda)\\).\n\n\nExample 4.   Suppose that the probability of developing a specific type of breast cancer in women aged 40-49 is 0.001. Assume the occurrences of cancer are independent. Suppose you have data from a random sample of 20,000 women aged 40-49.\n\nHow many of the 20,000 women would you expect to develop this type of breast cancer, and what is the standard deviation?\nFind the exact probability that more than 15 of the 20,000 women will develop this type of breast cancer.\nUse the CLT to find the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer.\nUse the CLT to approximate the following probabilities, where \\(X\\) is the number of women that will develop this type of breast cancer.\n\n\\(\\mathbb{P}(15 \\leq X \\leq 22)\\)\n\\(\\mathbb{P}(X &gt; 20)\\)\n\\(\\mathbb{P}(X &lt; 20)\\)\n\nFind the approximate probability that more than 15 of the 20,000 women will develop this type of breast cancer - not using the CLT!\nUse the CLT to approximate the approximate probability in the previous question!"
  },
  {
    "objectID": "slides/43_Moment_Generating_Functions_Part2.html",
    "href": "slides/43_Moment_Generating_Functions_Part2.html",
    "title": "Chapter 43: Moment Generating Functions Part 2",
    "section": "",
    "text": "Chapter 43: Moment Generating Functions Part 2\nRecap: What is an mgf?\n\nExample 1.   Let \\(X\\) be a random variable with mgf \\[M_X(t)= \\frac{1}{5}e^t + \\frac{3}{10}e^{2t} + \\frac{1}{2}e^{3t}.\\] Find the pmf or pdf of \\(X\\).\n\n\nExample 2.   Let \\(X\\) be a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), i.e. \\(X \\sim N(\\mu,\\sigma^2)\\).\n\nFind the mgf of \\(X\\).\nFind \\(\\mathbb{E}[X]\\).\nFind \\(Var(X)\\).\n\n\n\n\nTheorem 3.   Let \\(X\\) have mgf \\(M_X(t)\\), and let \\(Y=aX+b\\), where \\(a\\) and \\(b\\) are constants. Then \\[M_Y(t)=\\]\n\n\nProof. Proof. ◻\n\nQuestion: Do linear transformations always preserve the distribution type?\nI.e., if \\(X\\) has a certain probability distribution, does \\(aX+b\\) always have the same distribution type?\n\n\nExample 4.   Let \\(X \\sim U[0,1]\\), and \\(Y = 2X+3\\). Is \\(Y\\) also a uniform rv? If so, what are its parameters?\n\n\nExample 5.   Let \\(X \\sim Exp(\\lambda=5)\\), and \\(Y = 2X+3\\). Is \\(Y\\) also an exponential rv? If so, what is its parameter?\n\nMgf’s of Sums of Independent rv’s\n\n\nTheorem 6.   Let \\(X_1, X_2, \\ldots, X_n\\) be independent rv’s with respective mgf’s \\(M_{X_i}(t)\\), for \\(i=1,2,\\ldots,n\\). Let \\(Y=\\sum_{i=1}^n a_iX_i\\), where \\(a_i\\) are constants. Then \\[M_Y(t)= %\\Pi_{i=1}^n M_{X_i}(a_it).\\]\n\n\nProof. Proof. ◻\n\n\n\nExample 7.   Let \\(X_i \\sim N(\\mu_i, \\sigma_i^2)\\) be independent normal rv’s. What is the distribution of  \\(Y=\\sum_{i=1}^n X_i\\)?\n\n\n\nExample 8.   Let \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). What is the distribution of  \\(\\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n}\\)?\n\n\nExample 9.   Let \\(Z\\) be a standard normal random variable, i.e. \\(Z \\sim N(0,1)\\). Show that \\(Z^2 \\sim \\chi_1^2\\), i.e. is a chi-squared rv with 1 degree of freedom."
  },
  {
    "objectID": "slides/20_Discrete_Uniform_rv.html",
    "href": "slides/20_Discrete_Uniform_rv.html",
    "title": "Chapter 20: Discrete Uniform r.v.’s",
    "section": "",
    "text": "Chapter 20: Discrete Uniform r.v.’s\nScenario: There are \\(N\\) possible outcomes, which are all equally likely.\n\n\nExample 1. Examples of discrete uniform r.v.’s.\n\n\nProperties of discrete uniform r.v.’s"
  },
  {
    "objectID": "slides/24_Calculus_review.html",
    "href": "slides/24_Calculus_review.html",
    "title": "Calculus Review",
    "section": "",
    "text": "Example 1. Find the derivatives of the following functions.\n\n\\(f(x) = 2\\)\n\\(f(x) = 2x\\)\n\\(f(x) = 2x+2\\)\n\\(f(x) = x^2\\)\n\\(f(x) = 3\\sqrt{x}+\\frac2x+5\\)\n\\(f(x) = e^x\\)\n\\(f(x) = \\ln(x)\\)\n\\(f(x) = x^2 e^x\\)\n\\(f(x) = \\frac{x^5}{2x+7}\\)\n\\(f(x) = e^{-2x+7}\\)\n\\(f(x) = \\ln(x^2)\\)\n\n\n\n\n\n\n\n\nExample 2. Find the antiderivatives of the following functions.\n\n\\(f(x) = 2\\)\n\\(f(x) = x\\)\n\\(f(x) = \\frac1x\\)\n\\(f(x) = x^{3/2}\\)\n\\(f(x) = e^x\\)\n\\(f(x) = e^{-x}\\)\n\\(f(x) = e^{-2x}\\)\n\n\n\n\n\n\nExample 3. Solve the following integrals.\n\n\\(\\displaystyle\\int_0^1 (2x+x^5)dx\\)\n\\(\\displaystyle\\int_2^3 e^{-x}dx\\)\n\\(\\displaystyle\\int_2^3 x e^{x^2}dx\\)\n\\(\\displaystyle\\int_0^{\\infty} x e^{-x}dx\\)\n\\(\\displaystyle\\int_1^2 x^2 \\ln(x)dx\\)\n\\(\\displaystyle\\int_1^2 \\ln(x)dx\\)\n\\(\\displaystyle\\int_1^2 x^2 e^{x}dx\\)"
  },
  {
    "objectID": "slides/24_Calculus_review.html#differentiation",
    "href": "slides/24_Calculus_review.html#differentiation",
    "title": "Calculus Review",
    "section": "",
    "text": "Example 1. Find the derivatives of the following functions.\n\n\\(f(x) = 2\\)\n\\(f(x) = 2x\\)\n\\(f(x) = 2x+2\\)\n\\(f(x) = x^2\\)\n\\(f(x) = 3\\sqrt{x}+\\frac2x+5\\)\n\\(f(x) = e^x\\)\n\\(f(x) = \\ln(x)\\)\n\\(f(x) = x^2 e^x\\)\n\\(f(x) = \\frac{x^5}{2x+7}\\)\n\\(f(x) = e^{-2x+7}\\)\n\\(f(x) = \\ln(x^2)\\)"
  },
  {
    "objectID": "slides/24_Calculus_review.html#integration",
    "href": "slides/24_Calculus_review.html#integration",
    "title": "Calculus Review",
    "section": "",
    "text": "Example 2. Find the antiderivatives of the following functions.\n\n\\(f(x) = 2\\)\n\\(f(x) = x\\)\n\\(f(x) = \\frac1x\\)\n\\(f(x) = x^{3/2}\\)\n\\(f(x) = e^x\\)\n\\(f(x) = e^{-x}\\)\n\\(f(x) = e^{-2x}\\)\n\n\n\n\n\n\nExample 3. Solve the following integrals.\n\n\\(\\displaystyle\\int_0^1 (2x+x^5)dx\\)\n\\(\\displaystyle\\int_2^3 e^{-x}dx\\)\n\\(\\displaystyle\\int_2^3 x e^{x^2}dx\\)\n\\(\\displaystyle\\int_0^{\\infty} x e^{-x}dx\\)\n\\(\\displaystyle\\int_1^2 x^2 \\ln(x)dx\\)\n\\(\\displaystyle\\int_1^2 \\ln(x)dx\\)\n\\(\\displaystyle\\int_1^2 x^2 e^{x}dx\\)"
  },
  {
    "objectID": "slides/4_Conditional_Probability.html",
    "href": "slides/4_Conditional_Probability.html",
    "title": "Chapter 4: Conditional Probability",
    "section": "",
    "text": "Use set process to calculate probability of event of interest\nCalculate the probability of an event occurring, given that another event occurred.\nDefine keys facts for conditional probabilities using notation."
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#example",
    "href": "slides/4_Conditional_Probability.html#example",
    "title": "Chapter 4: Conditional Probability",
    "section": "Example",
    "text": "Example\n\n\n\n\nExample 4.2\n\n\nTwo dice (red and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?\n\n\n\n\n\n\nChapter 4 Slides"
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#conditional-probability-facts-12",
    "href": "slides/4_Conditional_Probability.html#conditional-probability-facts-12",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional Probability facts (1/2)",
    "text": "Conditional Probability facts (1/2)\n\n\n\n\nFact 1: General Multiplication Rule\n\n\n\\[\\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\cdot\\mathbb{P}(B|A)\\]\n\n\n\n\n\nFact 2: Conditional Probability Definition\n\n\n\\[\\mathbb{P}(A|B)=\\frac{\\mathbb{P}(A\\cap B)}{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#conditional-probability-facts-22",
    "href": "slides/4_Conditional_Probability.html#conditional-probability-facts-22",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional Probability facts (2/2)",
    "text": "Conditional Probability facts (2/2)\n\n\n\n\nFact 3\n\n\nIf \\(A\\) and \\(B\\) are independent events (\\(A \\unicode{x2AEB}B\\)), then \\[\\mathbb{P}(A|B) = \\mathbb{P}(A)\\]\n\n\n\n\n\nFact 4\n\n\n\\(\\mathbb{P}(A|B)\\) is a probability, meaning that it satisfies the probability axioms. In particular, \\[\\mathbb{P}(A|B) + \\mathbb{P}(A^C|B) = 1\\]"
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#example-1",
    "href": "slides/4_Conditional_Probability.html#example-1",
    "title": "Chapter 4: Conditional Probability",
    "section": "Example",
    "text": "Example\n\n\nExample 4.2\n\n\nTwo dice (red and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?\n\n\nSolution:\n\n\nChapter 4 Slides"
  },
  {
    "objectID": "slides/14_Bernoulli_rv.html",
    "href": "slides/14_Bernoulli_rv.html",
    "title": "Chapter 14: Bernoulli r.v.’s",
    "section": "",
    "text": "Chapter 14: Bernoulli r.v.’s\nScenario: One trial, with outcome success or failure.\n\nProperties of Bernoulli r.v.’s\n\nExample 1.  \n\nWe roll a fair 6-sided die.\nWe get $1 if we roll a 5, and nothing otherwise.\nLet \\(X\\) be how much money we get.\nFind the mean and variance of \\(X\\).\n\n\nSolution:\n\n\nExample 2.  \n\nSuppose we roll a fair 6-sided die 50 times.\nWe get $1 every time we roll a 5, and nothing otherwise.\nLet \\(X\\) be how much money we get on the 50 rolls.\nFind the mean and variance of \\(X\\).\n\n\nSolution:"
  },
  {
    "objectID": "slides/36_Sums_of_Independent_Normal_rv.html",
    "href": "slides/36_Sums_of_Independent_Normal_rv.html",
    "title": "Chapter 36: Sums of Independent Normal Random Variables",
    "section": "",
    "text": "Chapter 36: Sums of Independent Normal Random Variables\n\nTheorem 1.   Let \\(X\\sim N(\\mu, \\sigma^2)\\), and let \\(Y=aX+b\\), where \\(a\\) and \\(b\\) are constants. Then \\[\\hspace{-12cm} Y \\sim\\]\n\n\nProof. Proof. ◻\n\n\n\nTheorem 2.   Let \\(X_i \\sim N(\\mu_i, \\sigma_i^2)\\) be independent normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\hspace{-12cm} \\sum_{i=1}^n X_i \\sim\\]\n\n\nProof. Proof. ◻\n\n\nSpecial Cases\n\nLet \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\hspace{-12cm} \\sum_{i=1}^n X_i \\sim\\]\nLet \\(X_i \\sim N(\\mu, \\sigma^2)\\) be iid normal rv’s, for \\(i=1,2,\\ldots,n\\). Then \\[\\hspace{-12cm} \\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n} \\sim\\]\nLet \\(X\\sim N(\\mu_X,\\sigma_X^2)\\), and \\(Y\\sim N(\\mu_Y,\\sigma_Y^2)\\). Then \\[\\hspace{-12cm} X-Y \\sim\\]\n\nProof. Proof. ◻\n\n\n\nExample 3.   Glaucoma is an eye disease that is manifested by high intraocular pressure (IOP). The distribution of IOP in the general population is approximately normal with mean 16 mmHg and standard deviation 3 mmHg.\n\nSuppose a patient has 40 IOP readings. What is the probability that their average reading is greater than 20.32 mmHg, assuming their eyes are healthy?\nRepeat the previous question for a patient with 10 IOP readings."
  },
  {
    "objectID": "slides/27_Conditional_distributions.html",
    "href": "slides/27_Conditional_distributions.html",
    "title": "Chapter 27: Conditional Distributions",
    "section": "",
    "text": "Chapter 27: Conditional Distributions\nWhat do we know about conditional probabilities and distributions for events and discrete r.v.’s?\n\n\nExample 1.   Let \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(2&lt;X&lt;10|Y=4)\\).\n\n\n\n\nDefinition 2 (Conditional density). The conditional density of a r.v. \\(X\\) given \\(Y=y\\), is \\[f_{X|Y}(x|y)= \\frac{f_{X,Y}(x,y)}{f_Y(y)},\\] for \\(f_Y(y)&gt; 0\\).\n\nRemarks\n\nIt follows from the definition for the conditional density \\(f_{X|Y}(x|y)\\), that \\[f_{X,Y}(x,y)= f_{X|Y}(x|y)f_Y(y).\\]\nFor a fixed value of \\(Y=y\\), the conditional density \\(f_{X|Y}(x|y)\\) is an actual pdf, meaning\n\n\\(f_{X|Y}(x|y)\\geq 0\\) for all \\(x\\) and \\(y\\), and\n\\(\\displaystyle\\int_{-\\infty}^{\\infty} f_{X|Y}(x|y)dx =1\\).\n\n\nExample 1 cont’d Let \\(f_{X,Y}(x,y)= 5 e^{-x-3y}\\), for \\(0 &lt; y &lt; \\frac{x}{2}\\).\n\nFind \\(\\mathbb{P}(2&lt;X&lt;10|Y=4)\\).\nFind \\(\\mathbb{P}(X&gt;20 |Y=5)\\).\n\n\nExample 3.   Randomly choose a point \\(X\\) from the interval \\([0,1]\\), and given \\(X=x\\), randomly choose a point \\(Y\\) from \\([0,x]\\). Find \\(\\mathbb{P}(0 &lt; Y &lt; \\frac14)\\).\n\n\nQuestion What is \\(f_{X|Y}(x|y)\\) if \\(X\\) and \\(Y\\) are independent?\n\nRemark If \\(f_{X|Y}(x|y)\\) does not depend on \\(y\\), then \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "slides/7_Random_Variables.html",
    "href": "slides/7_Random_Variables.html",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "",
    "text": "Map the sample space to the set of real numbers using a discrete and continuous random variable\nDistinguish between discrete and continuous random variables from a written description"
  },
  {
    "objectID": "slides/7_Random_Variables.html#example",
    "href": "slides/7_Random_Variables.html#example",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Example",
    "text": "Example\n\nExample 1. Suppose we toss 3 fair coins.\n\nWhat is the sample space?\nWhat are the probabilities for each of the elements in the sample space?\n\n\nSolution:"
  },
  {
    "objectID": "slides/7_Random_Variables.html#definition",
    "href": "slides/7_Random_Variables.html#definition",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Definition",
    "text": "Definition\n\n\n\n\nExample 2\n\n\nWhat are some other random variables we could consider in Example 1?"
  },
  {
    "objectID": "slides/7_Random_Variables.html#slide-3",
    "href": "slides/7_Random_Variables.html#slide-3",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Slide 3",
    "text": "Slide 3\nRemarks:\n\nA random variable’s value is completely determined by the outcome \\(\\omega\\), where \\(\\omega \\in S\\). What is random is the outcome \\(\\omega\\).\nWe typically write \\(X\\) instead of \\(X(\\omega)\\)."
  },
  {
    "objectID": "slides/7_Random_Variables.html#slide-4",
    "href": "slides/7_Random_Variables.html#slide-4",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Slide 4",
    "text": "Slide 4\n\nExample 4. Let \\(X =\\) how many hours you slept last night.\n\nWhat is the sample space \\(S\\)?\nWhat is the range of possible values for \\(X\\)?\nWhat is \\(X(\\omega)\\)?\n\n\nSolution:"
  },
  {
    "objectID": "slides/7_Random_Variables.html#discrete-vs.-continuous-r.v.s",
    "href": "slides/7_Random_Variables.html#discrete-vs.-continuous-r.v.s",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Discrete vs. Continuous r.v.’s",
    "text": "Discrete vs. Continuous r.v.’s\n\nFor a discrete r.v., the set of possible values is either finite or can be put into a countably infinite list\n\nYou could theoretically list the specific possible outcomes that the variable can take\nIf you sum the rolls of three dice, you must get a whole number. For example, you can’t get any number between 3 and 4.\n\n\n   \n\nContinuous r.v.’s take on values from continuous intervals, or unions of continuous intervals\n\nVariable takes on a range of values, but there are infinitely possible values within the range\nIf you keep track of the time you sleep, you can sleep for 8 hours or 7.9 hours or 7.99 hours or 7.999 hours …\n\n\n\n\nChapter 7 Slides"
  },
  {
    "objectID": "slides/16_Geometric_rv.html",
    "href": "slides/16_Geometric_rv.html",
    "title": "Chapter 16: Geometric r.v.’s",
    "section": "",
    "text": "Chapter 16: Geometric r.v.’s\nScenario: There are repeated independent trials, each resulting in a success or failure, with constant probability of success for each trial. We are counting the number of trials until the first success.\n\n\nExample 1.   We throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nWhat is the pmf for the number of throws until we hit the bullseye?\nSolution:\nWhat are the mean and variance for the number of throws until we hit the bullseye?\nSolution:\nFind the probability that our first bullseye:\n\nis on the fourth try\nSolution:\nis on one of the first four tries\nSolution:\nis after the fifth try\nSolution:\nis on one of the first fifty tries\nSolution:\nis after the \\(50^{th}\\) try, given that it did not happen on the first 20 tries.\nSolution:\n\nFind the expected number of misses until we hit the bullseye.\nSolution:"
  },
  {
    "objectID": "slides/9_joint_distributions.html",
    "href": "slides/9_joint_distributions.html",
    "title": "Chapter 9: Independence and Conditioning - or, Joint Distributions",
    "section": "",
    "text": "Definition 1. The joint pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[p_{X,Y}(x,y) = \\mathbb{P}(X=x\\ and\\ Y=y) = \\mathbb{P}(X=x, Y=y)\\]\n\n\nExample 2. Let \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)\n\n\nSolution:\nRemarks: Some properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)\n\n\n\nDefinition 3. The joint cdf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]\n\n\n\nExample 4. Find the joint cdf \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\) in Example 2.\n\nSolution:\n\nExample 5. Find the marginal cdfs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\) for Example 4.\n\nSolution:\n\nRemark: Some properties of joint cdf’s:\nIndependence and Conditioning\nRecall that for events \\(A\\) and \\(B\\),\n\n\\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\)\n\\(A\\) and \\(B\\) are independent if and only if\n\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\cdot\\mathbb{P}(B)\\)\n\n\nIndependence and conditioning are defined similarly for r.v.’s, since \\[p_X(x) = \\mathbb{P}(X=x)\\ \\mathrm{and}\\ \\ p_{X,Y}(x,y) = \\mathbb{P}(X = x ,Y = y).\\]\n\nDefinition 6. The conditional pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is defined as \\[p_{X|Y}(x|y) = \\mathbb{P}(X = x |Y = y) = \\frac{\\mathbb{P}(X = x\\ and\\ Y = y)}{\\mathbb{P}(Y = y)}\n=\\frac{p_{X,Y}(x,y) }{p_{Y}(y) }\\] if \\(p_{Y}(y) &gt; 0\\).\n\nRemark: The following properties follow from the conditional pmf definition:\n\nExample 7. Using \\(X\\) and \\(Y\\) from Example 2:\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\nSolution:\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counterexample.\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\).\n\n\nExample 8. Hypothetical 4-sided die\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\).\n\n\nSolution:\nExample 8 cont’d."
  },
  {
    "objectID": "slides/25_Joint_densities.html",
    "href": "slides/25_Joint_densities.html",
    "title": "Chapter 25: Joint densities",
    "section": "",
    "text": "Chapter 25: Joint densities\nRecall from Chapter 24, that the probability distribution, or probability density function (pdf), of a continuous random variable \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\), \\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx.\\]\n\nHow to define the joint pdf for continuous r.v.’s?\n\nRemarks:\n\nNote that \\(f_{X,Y}(x,y)\\neq \\mathbb{P}(X=x, Y=y)\\)!!!\nIn order for \\(f_{X,Y}(x,y)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_{X,Y}(x,y)\\geq 0\\) for all \\(x,y\\)\n\\(\\displaystyle\\int_{-\\infty}^{\\infty}\\displaystyle\\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dxdy=1\\)\n\n\nDouble Integrals Mini Lesson\n\nExample 1.  Solve the following integrals.\n\n\\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} xy dydx\\)\n\\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} (x+y) dydx\\)\n\\(\\displaystyle\\int_{2}^{3}\\displaystyle\\int_{0}^{1} e^{x+y} dydx\\)\n\n\n\nDefinition 2 (Joint cumulative distribution function).   The joint cumulative distribution function (cdf) of continuous random variables \\(X\\) and \\(Y\\), is the function \\(F_{X,Y}(x,y)\\), such that for all real values of \\(x\\) and \\(y\\), \\[F_{X,Y}(x,y)= \\mathbb{P}(X \\leq x, Y \\leq y) = \\int_{-\\infty}^x\\int_{-\\infty}^y f_{X,Y}(s,t)dtds\\]\n\nRemarks:\n\nThe definition above for \\(F_{X,Y}(x,y)\\) is a function of \\(x\\) and \\(y\\).\nThe joint cdf at the point \\((a,b)\\), is \\[F_{X,Y}(a,b) = \\mathbb{P}(X \\leq a, Y \\leq b) = \\int_{-\\infty}^a\\int_{-\\infty}^b f_{X,Y}(s,t)dtds\\]\n\n\n\nDefinition 3 (Marginal pdf’s).   Suppose \\(X\\) and \\(Y\\) are continuous r.v.’s, with joint pdf \\(f_{X,Y}(x,y)\\). Then the marginal probability density functions are \\[\\begin{aligned}\nf_X(x)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dy\\\\\nf_Y(y)&=& \\int_{-\\infty}^{\\infty} f_{X,Y}(x,y)dx\n\\end{aligned}\\]\n\n\nExample 4.   Let \\(f_{X,Y}(x,y)= \\frac32 y^2\\), for \\(0 \\leq x \\leq 2, \\ 0 \\leq y \\leq 1\\).\n\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\).\nFind \\(f_X(x)\\) and \\(f_Y(y)\\).\n\n\n\nExample 5.   Let \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\).\n\nFind \\(f_X(x)\\) and \\(f_Y(y)\\).\nFind \\(\\mathbb{P}(Y &lt; 3)\\).\n\n\n\nExample 6.   Let \\(X\\) and \\(Y\\) have constant density on the square \\(0 \\leq X \\leq 4, 0 \\leq Y \\leq 4\\).\n\nFind \\(\\mathbb{P}(|X-Y| &lt; 2)\\).\nExample 6 continued.\nLet \\(M = \\max(X,Y)\\). Find the pdf for \\(M\\), that is \\(f_M(m)\\).\nLet \\(Z = \\min(X,Y)\\). Find the pdf for \\(Z\\), that is \\(f_Z(z)\\).\n\n\n\nExample 7.   Let \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)= \\frac85(x+y)\\) in the region \\(0 &lt; x &lt; 1,\\ \\frac12 &lt; y &lt;1\\). Find the pdf of the r.v. \\(Z\\), where \\(Z=XY\\).\n\nExample 7 solution continued."
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html",
    "href": "slides/22_Counting_Intro-solutions.html",
    "title": "Chapter 22: Counting",
    "section": "",
    "text": "Example 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them if without replacements and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?\n\n\n\n\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n\\(10!\\)\n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\n\n\n\nneed 10 total?\n\n\\(10^{10}\\)\n\nneed 6 total?\n\n\\(10^6\\)\n\n\n\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.3\n\n\nHow many ways to order them if without replacements and only need 6?\n\n\n\\[\\frac{10!}{4!}=151,200\\]\n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?\n\n\n\\[\\frac{\\frac{10!}{4!}}{6!} = \\frac{10!}{4!6!}=210\\]\nThere are \\(6!\\) ways to order the 6 subjects."
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#basic-counting-examples-13",
    "href": "slides/22_Counting_Intro-solutions.html#basic-counting-examples-13",
    "title": "Chapter 22: Counting",
    "section": "Basic Counting Examples (1/3)",
    "text": "Basic Counting Examples (1/3)\n\n\nExample 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them if without replacements and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#basic-counting-examples-23",
    "href": "slides/22_Counting_Intro-solutions.html#basic-counting-examples-23",
    "title": "Chapter 22: Counting",
    "section": "Basic Counting Examples (2/3)",
    "text": "Basic Counting Examples (2/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n\\(10!\\)\n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\n\n\n\nneed 10 total?\n\n\\(10^{10}\\)\n\nneed 6 total?\n\n\\(10^6\\)"
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#basic-counting-examples-33",
    "href": "slides/22_Counting_Intro-solutions.html#basic-counting-examples-33",
    "title": "Chapter 22: Counting",
    "section": "Basic Counting Examples (3/3)",
    "text": "Basic Counting Examples (3/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.3\n\n\nHow many ways to order them if without replacements and only need 6?\n\n\n\\[\\frac{10!}{4!}=151,200\\]\n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?\n\n\n\\[\\frac{\\frac{10!}{4!}}{6!} = \\frac{10!}{4!6!}=210\\]\nThere are \\(6!\\) ways to order the 6 subjects."
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#permutations-and-combinations-1",
    "href": "slides/22_Counting_Intro-solutions.html#permutations-and-combinations-1",
    "title": "Chapter 22: Counting",
    "section": "Permutations and Combinations",
    "text": "Permutations and Combinations\n\n\nDefinition: Permutations\n\n\nPermutations are the number of ways to arrange in order \\(r\\) distinct objects when there are \\(n\\) total.\n\\[nPr = \\frac{n!}{(n-r)!}\\]\n\n\n\n\nDefinition: Combinations\n\n\nCombinations are the number of ways to choose (order doesn’t matter) \\(r\\) objects from \\(n\\) without replacement.\n\\[nCr = \\textrm{\"n choose r\"} = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#some-combinations-properties",
    "href": "slides/22_Counting_Intro-solutions.html#some-combinations-properties",
    "title": "Chapter 22: Counting",
    "section": "Some combinations properties",
    "text": "Some combinations properties\n\n\\[\\binom{n}{r} = \\binom{n}{n-r}\\]\n\\[\\binom{n}{1} = n\\]\n\\[\\binom{n}{0} = 1\\]"
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-12",
    "href": "slides/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-12",
    "title": "Chapter 22: Counting",
    "section": "More examples: order matters vs. not (1/2)",
    "text": "More examples: order matters vs. not (1/2)\n\n\nExample 2\n\n\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\norder doesn’t matter?"
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-22",
    "href": "slides/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-22",
    "title": "Chapter 22: Counting",
    "section": "More examples: order matters vs. not (2/2)",
    "text": "More examples: order matters vs. not (2/2)\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\n\n\\[\\frac{13P2}{52P2} = \\frac{\\frac{13!}{11!}}{\\frac{52!}{50!}} = \\frac{13\\cdot12}{52\\cdot51}\\]\n\norder doesn’t matter?\n\n\\[\\frac{\\binom{13}{2}}{\\binom{52}{2}} = \\frac{\\frac{13!}{2!11!}}{\\frac{52!}{2!50!}} = \\frac{13\\cdot12}{52\\cdot51}\\]"
  },
  {
    "objectID": "slides/22_Counting_Intro-solutions.html#table-of-different-cases",
    "href": "slides/22_Counting_Intro-solutions.html#table-of-different-cases",
    "title": "Chapter 22: Counting",
    "section": "Table of different cases",
    "text": "Table of different cases\nSee table on pg. 277 of textbook\n\n\\(n\\) = total number of objects\n\\(r\\) = number objects needed\n\n\n\n\n\n\n\n\n\nwith replacement\nwithout replacement\n\n\n\n\norder matters\n\\(n^r\\)\n\\(nPr = \\frac{n!}{(n-r)!}\\)\n\n\norder doesn’t matter\n\\(\\binom{n+r-1}{r}\\)\n\\(nCr = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)\n\n\n\n\n\n\nChapter 22 Slides"
  },
  {
    "objectID": "slides/15_Binomial_rv.html",
    "href": "slides/15_Binomial_rv.html",
    "title": "Chapter 15: Binomial r.v.’s",
    "section": "",
    "text": "Chapter 15: Binomial r.v.’s\nScenario: There are \\(n\\) independent trials, each resulting in a success or failure, with constant probability in each trial. We are counting the number of successes (or failures).\n\nProperties of Binomial r.v.’s"
  },
  {
    "objectID": "slides/18_Poisson_rv.html",
    "href": "slides/18_Poisson_rv.html",
    "title": "Chapter 18: Poisson r.v.’s",
    "section": "",
    "text": "Chapter 18: Poisson r.v.’s\nScenario: We are counting the number of successes in a fixed time period, which has a constant rate of successes.\n\n\nRecall that if \\(X\\sim Binomial(n,p)\\), then\n\n\\(X\\) models the number of successes\nin \\(n\\) independent (Bernoulli) trials\nthat each have the same probability of success \\(p\\).\n\nPoisson r.v.’s are similar,\n\nexcept that instead of having \\(n\\) discrete independent trials,\nthere is a fixed time period during which the successes happen.\n\n\n\n\nExample 1.   Some examples of Poisson r.v.’s:\n\nNumber of visitors to an emergency room in an hour during a weekend night\nNumber of study participants enrolled in a study per week\nNumber of gun shootings in a square mile\n\n\n\nProperties of Poisson r.v.’s\n\nExample 2.   Suppose an emergency room has an average of 50 visitors per day. Find the following probabilities.\n\nProbability of 30 visitors in a day.\nSolution:\nProbability of 8 visitors in an hour.\nSolution:\nProbability of at least 8 visitors in an hour.\nSolution:\n\n\n\nExample 3.   Suppose emergency room 1 has an average of 50 visitors per day, and emergency room 2 has an average of 70 visitors per day, independently of each other. What is the probability distribution to model of the total number of visitors to both?\n\nSolution:\n\nTheorem 4.   If \\(X\\sim Poiss(\\lambda_1)\\) and \\(Y\\sim Poiss(\\lambda_2)\\) are independent of each other, then \\(Z=X+Y\\sim Poiss(\\lambda_1 + \\lambda_2)\\).\n\n\nPoisson vs. Binomial r.v.’s\nBoth Poisson and Binomial r.v.’s are counting the number of successes\n\nIf for a Binomial r.v.\n\nthe number of trials \\(n\\) is very large, and\nthe probability of success \\(p\\) is close to 0 or 1,\n\nthen the Poisson distribution can be used to approximate Binomial probabilities.\n\n\nExample 5.   Suppose that in the long run, errors in a medical testing lab are made 0.1% of the time. Find the probability that fewer than 4 mistakes are made in the next 2,000 tests.\n\nFind the probability using the Binomial distribution.\nApproximate the probability in part (1) using the Poisson distribution.\n\nSolution:"
  },
  {
    "objectID": "slides/19_Hypergeometric_rv.html",
    "href": "slides/19_Hypergeometric_rv.html",
    "title": "Chapter 19: Hypergeometric r.v.’s",
    "section": "",
    "text": "Chapter 19: Hypergeometric r.v.’s\nScenario: There are a fixed number of successes and failures (which are known in advance), from which we make \\(n\\) draws without replacement. We are counting the number of successes from the \\(n\\) trials.\n\n\nExample 1.   A wildlife biologist is using mark-recapture to research a wolf population. Suppose a specific study region is known to have 24 wolves, of which 11 have already been tagged. If 5 wolves are randomly captured, what is the probability that 3 of them have already been tagged?\nSolution:\n\n\nProperties of Hypergeometric r.v.’s\n\nThere is a finite population of \\(N\\) items.\nEach item in the population is either a success or a failure, and there are \\(M\\) successes total.\nWe randomly select (sample) \\(n\\) items from the population.\n\nHypergeometric vs. Binomial r.v.’s\nSuppose a hypergeometric r.v. \\(X\\) has the following properties:\n\nthe population size \\(N\\) is really big,\nthe number of successes \\(M\\) in the population is relatively large,\n\n\\(\\frac{M}{N}\\) shouldn’t be close to 0 or 1\n\nand the number of items \\(n\\) selected is small.\n\nThen, in this case, making \\(n\\) draws from the population doesn’t change the probability of success much, and the hypergeometric r.v. can be approximated by a binomial r.v.\n\nExample 2.   Suppose a specific study region is known to have 2400 wolves, of which 1100 have already been tagged.\n\nIf 50 wolves are randomly captured, what is the probability that 20 of them have already been tagged?\nApproximate the probability in part (1) using the binomial distribution.\n\nSolution:"
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nThursdays, 11:30am - 1pm\nExceptions:\n\nJanuary 25th: 10:30am - 12pm\nFebruary 22nd: 11am - 12pm"
  },
  {
    "objectID": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "href": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nThursdays, 11:30am - 1pm\nExceptions:\n\nJanuary 25th: 10:30am - 12pm\nFebruary 22nd: 11am - 12pm"
  },
  {
    "objectID": "instructors.html#teaching-assistants",
    "href": "instructors.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\nAntara and Ariel were great students in my BSTA 513 class last year! They will have the following office hours and will help answer questions on Slack.\n\nAntara Vidyarthi\nLink to Zoom!!\n\nWednesdays, 4:30 - 6 pm\n\n\n\nAriel Weingarten\nLink to Webex!!\n\nTuesdays, 2:30 - 4 pm"
  },
  {
    "objectID": "schedule/week_02_sched.html",
    "href": "schedule/week_02_sched.html",
    "title": "Week 2",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #006a4e; background-color: #fff; padding: 10px; }\n.description{ font-size: 20px; color: #fff; background-color: #006a4e; padding: 10px; } ```"
  },
  {
    "objectID": "schedule/week_02_sched.html#resources",
    "href": "schedule/week_02_sched.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n3\nIndependent Events\n\n\n\n\n\n4\nConditional Probability\n\n\n\n\n\n6\nBayes’ Theorem\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_02_sched.html#post-class",
    "href": "schedule/week_02_sched.html#post-class",
    "title": "Week 2",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_02_sched.html#muddiest-points",
    "href": "schedule/week_02_sched.html#muddiest-points",
    "title": "Week 2",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_02_sched.html#clearest-points",
    "href": "schedule/week_02_sched.html#clearest-points",
    "title": "Week 2",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_02_sched.html#additional-information",
    "href": "schedule/week_02_sched.html#additional-information",
    "title": "Week 2",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "schedule/week_04_sched.html",
    "href": "schedule/week_04_sched.html",
    "title": "Week 4",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nExpected values of discrete random variables (Chapter 10 and 11)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_04_sched.html#resources",
    "href": "schedule/week_04_sched.html#resources",
    "title": "Week 4",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n10\nExpected Values of discrete RVs\n\n\n\n\n\n11\nExpected Values of sums of discrete RVs\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_04_sched.html#post-class",
    "href": "schedule/week_04_sched.html#post-class",
    "title": "Week 4",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_04_sched.html#muddiest-points",
    "href": "schedule/week_04_sched.html#muddiest-points",
    "title": "Week 4",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_04_sched.html#clearest-points",
    "href": "schedule/week_04_sched.html#clearest-points",
    "title": "Week 4",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_04_sched.html#additional-information",
    "href": "schedule/week_04_sched.html#additional-information",
    "title": "Week 4",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "schedule/week_03_sched.html",
    "href": "schedule/week_03_sched.html",
    "title": "Week 3",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nRandom Variables (Chapter 7)\nPMF and CDF (Chapter 8)\nIndependence and Conditioning (Chapter 9)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_03_sched.html#resources",
    "href": "schedule/week_03_sched.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n7\nRandom Variables\n\n\n\n\n\n8\nPMF and CDF\n\n\n\n\n\n9\nIndependence and Conditioning\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_03_sched.html#post-class",
    "href": "schedule/week_03_sched.html#post-class",
    "title": "Week 3",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_03_sched.html#muddiest-points",
    "href": "schedule/week_03_sched.html#muddiest-points",
    "title": "Week 3",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_03_sched.html#clearest-points",
    "href": "schedule/week_03_sched.html#clearest-points",
    "title": "Week 3",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_03_sched.html#additional-information",
    "href": "schedule/week_03_sched.html#additional-information",
    "title": "Week 3",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "schedule/week_11_sched.html",
    "href": "schedule/week_11_sched.html",
    "title": "Week 11",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nFinal Exam!! (12/6)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_11_sched.html#resources",
    "href": "schedule/week_11_sched.html#resources",
    "title": "Week 11",
    "section": "Resources",
    "text": "Resources\nFinal exam week!"
  },
  {
    "objectID": "schedule/week_11_sched.html#post-class",
    "href": "schedule/week_11_sched.html#post-class",
    "title": "Week 11",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_11_sched.html#muddiest-points",
    "href": "schedule/week_11_sched.html#muddiest-points",
    "title": "Week 11",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_11_sched.html#clearest-points",
    "href": "schedule/week_11_sched.html#clearest-points",
    "title": "Week 11",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_11_sched.html#additional-information",
    "href": "schedule/week_11_sched.html#additional-information",
    "title": "Week 11",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "schedule/week_06_sched.html",
    "href": "schedule/week_06_sched.html",
    "title": "Week 6",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nCommon families of discrete distributions (Chapter 17 and 18)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_06_sched.html#resources",
    "href": "schedule/week_06_sched.html#resources",
    "title": "Week 6",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n17\nNegative Binomial RV\n\n\n\n\n\n18\nPoisson RV\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_06_sched.html#post-class",
    "href": "schedule/week_06_sched.html#post-class",
    "title": "Week 6",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_06_sched.html#muddiest-points",
    "href": "schedule/week_06_sched.html#muddiest-points",
    "title": "Week 6",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_06_sched.html#clearest-points",
    "href": "schedule/week_06_sched.html#clearest-points",
    "title": "Week 6",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_06_sched.html#additional-information",
    "href": "schedule/week_06_sched.html#additional-information",
    "title": "Week 6",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "schedule/week_07_sched.html",
    "href": "schedule/week_07_sched.html",
    "title": "Week 7",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nContinuous random variables and PDFs (Chapter 24)\nJoint Densities (Chapter 25)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_07_sched.html#resources",
    "href": "schedule/week_07_sched.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n24\nContinuous RVs and PDFs\n\n\n\n\n\n25\nJoint Densities\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_07_sched.html#post-class",
    "href": "schedule/week_07_sched.html#post-class",
    "title": "Week 7",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_07_sched.html#muddiest-points",
    "href": "schedule/week_07_sched.html#muddiest-points",
    "title": "Week 7",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_07_sched.html#clearest-points",
    "href": "schedule/week_07_sched.html#clearest-points",
    "title": "Week 7",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_07_sched.html#additional-information",
    "href": "schedule/week_07_sched.html#additional-information",
    "title": "Week 7",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n1/11/24\n\n\nHomework 0\n\n\n4 min\n\n\n\n\n1/25/24\n\n\nHomework 1\n\n\n7 min\n\n\n\n\n2/1/24\n\n\nHomework 2\n\n\n7 min\n\n\n\n\n2/15/24\n\n\nHomework 3\n\n\n8 min\n\n\n\n\n2/25/24\n\n\nHomework 4\n\n\n6 min\n\n\n\n\n3/7/24\n\n\nHomework 5\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "612 Readings",
    "section": "",
    "text": "These reading assignments are for 612 students only. 612 students, you must complete all the readings by 3/22. I have included some recommended due dates that will keep you on track if you would like.\n\n\n\nReading\nRecommended Due Date\nLink to Sakai\nArticle\n\n\n\n\n1\n1/21 @ 11pm\n\n\n\n\n2\n2/4 @ 11pm\n\n\n\n\n3\n2/18 @ 11pm\n\n\n\n\n4\n2/25 @ 11pm\n\n\n\n\n5\n3/10 @ 11pm\n\n\n\n\n\n\nInstructions for each assignment\nFor each reading assignment, students will write a summary including:\n\nstudy background,\nwhy the author think the research important,\nstatistical methods used to address the research question,\nand a discussion/critique on whether you agree or disagree with the authors’  science and statistics.\n\nYou may write complete sentences under each bullet point. Please keep your summary to less than 1 page (margins are up to you). There are no requirements on formatting for references."
  },
  {
    "objectID": "schedule/week_01_sched.html#resources",
    "href": "schedule/week_01_sched.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording(s)\n\n\n\n\n\nIntro\n\n\n\n\n\n1\nOutcomes, Events, and Sample Space\n\n\n\n\n\n2\nProbability\n\n\n\n\n\n22\nIntroduction to Counting\n\n\n\n\n\n23\nCase Study on Counting\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_01_sched.html#homework",
    "href": "schedule/week_01_sched.html#homework",
    "title": "Week 1",
    "section": "Homework",
    "text": "Homework\nHomework 1 due 10/5"
  },
  {
    "objectID": "schedule/week_01_sched.html#post-class",
    "href": "schedule/week_01_sched.html#post-class",
    "title": "Week 1",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_01_sched.html#statistician-of-the-week-regina-nuzzo",
    "href": "schedule/week_01_sched.html#statistician-of-the-week-regina-nuzzo",
    "title": "Week 1",
    "section": "Statistician of the Week: Regina Nuzzo",
    "text": "Statistician of the Week: Regina Nuzzo\n\n\n\n\n\n\n\nRegina Nuzzo\n\n\n\n\n\n\nDr. Nuzzo received her PhD in Statistics from Stanford University and is now Professor of Science, Technology, & Mathematics at Gallaudet University. Gallaudet University, federally funded and located in Washington, DC, is the only higher education institution where all programs are designed for the education of the deaf and hard of hearing. Dr. Nuzzo teaches statistics using American Sign Language.\nShe is the Senior Advisor for Statistics Communication and Media Innovation at the American Statistical Association and a freelance writer.\n\n\n\nTopics covered\nDr. Nuzzo is a statistician and a science journalist. Her work has appeared in Nature, Los Angeles Times, New York Times, Reader’s Digest, New Scientist, and Scientific American. Most of her work is in the “Health” or “Science” sections of the aforementioned outlets. Primarily, she works to help lay-audiences understand science and statistics in particular. She earned the American Statistical Association’s 2014 Excellence in Statistical Reporting Award for her article on p-values in Nature. Her work led to the ASA’s statement on p-values.\n\n\nRelevant work\n\nNuzzo, R. “Scientific method: Statistical errors.” Nature 506, 150–152 (2014).\nNuzzo, R. “Tips for Communicating Statistical Significance.” Science, Health, and Public Trust, National Institutes of Health, 2018.\nNuzzo, R. “Vying for a soul mate? Psych out the competition with science.” Health: Features. Los Angeles Times, 2008.\n\n\n\nOutside links\n\nWikipedia\nacademic\nLinkedin\npersonal\n\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin. I also invite you to check out this youtube video of her Women Rise Keynote address where she discusses her hearing impairment, career growth, and her work with p-values."
  },
  {
    "objectID": "schedule/week_01_sched.html#muddiest-points",
    "href": "schedule/week_01_sched.html#muddiest-points",
    "title": "Week 1",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why is the number of possible events \\(2^{|S|}\\)?\nIn class, we were wondering why/if \\(2^{|S|}\\) is the general formula for calculating the total number of possible events. We were specifically wondering if the \\(2\\) came from the fact that we had two options (heads and tails) for our outcome. Let’s work through the example of a 6-sided die to explain this further. The sample space is \\(S=\\{1, 2, 3, 4, 5, 6\\}\\). So is the total number of possible events \\(2^6\\) or \\(6^6\\) or something else? We can actually think about an event by using an indicator variable for each outcome of the sample space. An indicator variable is just a way to give us a yes/no answer to a question. So in this case, we are wondering: is this outcome a part of our event? If our event is \\(\\{1\\}\\) then for the outcome \\(1\\), the answer is “yes, the outcome is part of the event. For outcomes \\(2-6\\), the answer is”no, the outcome is not apart of the event.”\n\nFor each outcome, we have a “yes” or “no” answer. We can look at another example of an event. Let’s say our event is rolling an even number:\n\nFor \\(2\\), \\(4\\), and \\(6\\), the answer is “yes.” We can define the indicator variable for whether an outcome is in an event or not. The indicator gives a 1 or 0 for yes and no respectively.\n\nAs stated above, the \\(2\\) in \\(2^6\\) comes from the \\(2\\) options from our indicator. Each side has two options, and there are \\(6\\) sides. Thus, \\(2^6\\) possible events.\n\n\n2. What is an event??\nI think this will become clearer when we start thinking about events in the context of probability. When we think of events outside of probability, we may think of something we actually do or something that happens, like going to a concert or coming to class or missing the streetcar. In this case, we think of the event as the single thing (out of all the options) that actually occured. For example, if I’m taking the streetcar to class, I can think of two definitive options of what might occur: I miss the streetcar or I get on the streetcar. Only one of these things can occur, which I may call an event colloquially.\nIt is important to make the distinction with events defined within probability. Events are not necessarily a single thing that occurred. Instead it can be a collection of things that may occur. In the example of the streetcar, I can define my event to include both options. Thus, my event is that I make the streetcar or I miss it. Both of these things cannot happen simultaneously, but if I want to calculate the probability that I miss or make the streetcar, then it is helpful to have the event defined."
  },
  {
    "objectID": "schedule/week_01_sched.html#clearest-points",
    "href": "schedule/week_01_sched.html#clearest-points",
    "title": "Week 1",
    "section": "Clearest Points",
    "text": "Clearest Points\nMostly: heads/tails example, sample space, how to draw a quarter, possible events for two coins."
  },
  {
    "objectID": "schedule/week_01_sched.html#additional-information",
    "href": "schedule/week_01_sched.html#additional-information",
    "title": "Week 1",
    "section": "Additional Information",
    "text": "Additional Information\nAs we start the course, here are some administrative items that we need to do:\n\nPlease join the Slack page\nPlease read the syllabus on your own time\n\n\nExtra Practice/Learning\n\nIf you would like a Calculus review, please see this page!\nCombinatorics practice problems\n\nhandout (& answers)\nTry to complete as many of these as you can before class on Wednesday.\nWe will discuss some of them Wednesday in class.\n\nPixar has a series of videos explaining how they use combinatorics in making animations\n\nThis is a great & fun introduction to the basic principles of counting\nI highly recommend looking at them, especially if you have not studied permutations and combinations before.\n\nThere is a table on p. 277 of the book with formulas for 4 different common counting cases (does order matter (y/n) vs. sampling with replacement (y/n).\n\nIn class we covered all cases except “order does not matter and sampling with replacement.”\n\nThis case is often referred to as the “stars and bars” problem.\nSee this page for a proof to the Stars and Bars Theorem.\n\nNote: Their notation is opposite of what our textbook uses. The website uses k instead of n and n instead of r."
  },
  {
    "objectID": "schedule/week_08_sched.html",
    "href": "schedule/week_08_sched.html",
    "title": "Week 8",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #006a4e; background-color: #fff; padding: 10px; }\n.description{ font-size: 20px; color: #fff; background-color: #006a4e; padding: 10px; } ```"
  },
  {
    "objectID": "schedule/week_08_sched.html#resources",
    "href": "schedule/week_08_sched.html#resources",
    "title": "Week 8",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n25\nJoint PDFs\n\n\n\n\n\n26\nIndependent continuous random variables\n\n\n\n\n\n27\nConditional distributions\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_08_sched.html#post-class",
    "href": "schedule/week_08_sched.html#post-class",
    "title": "Week 8",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_08_sched.html#muddiest-points",
    "href": "schedule/week_08_sched.html#muddiest-points",
    "title": "Week 8",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_08_sched.html#clearest-points",
    "href": "schedule/week_08_sched.html#clearest-points",
    "title": "Week 8",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_08_sched.html#additional-information",
    "href": "schedule/week_08_sched.html#additional-information",
    "title": "Week 8",
    "section": "Additional Information",
    "text": "Additional Information\n\nExtra Practice/Learning"
  },
  {
    "objectID": "schedule/week_05_sched.html",
    "href": "schedule/week_05_sched.html",
    "title": "Week 5",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nVariance of discrete random variables (Chapter 12-16)\nCommon families of discrete distributions (Chapter 19 and 20)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_05_sched.html#resources",
    "href": "schedule/week_05_sched.html#resources",
    "title": "Week 5",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n12\nVariance of discrete RVs\n\n\n\n\n\n13\n\n\n\n\n\n\n14\nBernoulli RV\n\n\n\n\n\n15\nBinomial RV\n\n\n\n\n\n16\nGeometric RV\n\n\n\n\n\n19\nHypergeometric RV\n\n\n\n\n\n20\nDiscrete Uniform RV\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_05_sched.html#post-class",
    "href": "schedule/week_05_sched.html#post-class",
    "title": "Week 5",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_05_sched.html#muddiest-points",
    "href": "schedule/week_05_sched.html#muddiest-points",
    "title": "Week 5",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_05_sched.html#clearest-points",
    "href": "schedule/week_05_sched.html#clearest-points",
    "title": "Week 5",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_05_sched.html#additional-information",
    "href": "schedule/week_05_sched.html#additional-information",
    "title": "Week 5",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "schedule/week_09_sched.html",
    "href": "schedule/week_09_sched.html",
    "title": "Week 9",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nExpect value and variance of continuous random variables (Chapter 28-30)\nCommon families of continuous distributions (Chapter 31-33, 35)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_09_sched.html#resources",
    "href": "schedule/week_09_sched.html#resources",
    "title": "Week 9",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n28\nExpected value of continuous RVs\n\n\n\n\n\n29\nVariance of continuous RVs\n\n\n\n\n\n31\nUniform RV\n\n\n\n\n\n32\nExponential RV\n\n\n\n\n\n33\nGamma RV\n\n\n\n\n\n35\nNormal RV\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_09_sched.html#post-class",
    "href": "schedule/week_09_sched.html#post-class",
    "title": "Week 9",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_09_sched.html#muddiest-points",
    "href": "schedule/week_09_sched.html#muddiest-points",
    "title": "Week 9",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_09_sched.html#clearest-points",
    "href": "schedule/week_09_sched.html#clearest-points",
    "title": "Week 9",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_09_sched.html#additional-information",
    "href": "schedule/week_09_sched.html#additional-information",
    "title": "Week 9",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "schedule/week_10_sched.html",
    "href": "schedule/week_10_sched.html",
    "title": "Week 10",
    "section": "",
    "text": "Resource\nInformation and Links\n\n\n\n\nReadings and Code\nMoment generating functions (Chapter 43)\nSums of independent normal random variables (Chapter 36)\nCentral limit theorem (Chapter 37)\n\n\nSlides\n\n\n\nClass Notes\n\n\n\nData\n\n\n\nHomework"
  },
  {
    "objectID": "schedule/week_10_sched.html#resources",
    "href": "schedule/week_10_sched.html#resources",
    "title": "Week 10",
    "section": "Resources",
    "text": "Resources\n\n\n\n\n\n\n\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n43\nMoment generating functions\n\n\n\n\n\n36\nSums of independent normal random variables\n\n\n\n\n\n37\nCentral limit theorem\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "schedule/week_10_sched.html#post-class",
    "href": "schedule/week_10_sched.html#post-class",
    "title": "Week 10",
    "section": "Post Class",
    "text": "Post Class\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_10_sched.html#muddiest-points",
    "href": "schedule/week_10_sched.html#muddiest-points",
    "title": "Week 10",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_10_sched.html#clearest-points",
    "href": "schedule/week_10_sched.html#clearest-points",
    "title": "Week 10",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "schedule/week_10_sched.html#additional-information",
    "href": "schedule/week_10_sched.html#additional-information",
    "title": "Week 10",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "slides/33_Gamma_rv.html",
    "href": "slides/33_Gamma_rv.html",
    "title": "Chapter 33: Gamma Random Variables",
    "section": "",
    "text": "Chapter 33: Gamma Random Variables\nScenario: Modeling the time until the \\(r^{th}\\) event.\n\nProperties of gamma r.v.’s\nRemarks\n\nThe parameter \\(r\\) in a Gamma(\\(r\\),\\(\\lambda\\)) distribution doesn’t have to be a positive integer (\\(\\mathbb{Z}^{+}\\)).\nWhen \\(r\\in \\mathbb{Z}^{+}\\), the distribution is sometimes called an Erlang(\\(r\\),\\(\\lambda\\)) distribution.\nWhen \\(r\\) is any positive real number, we have a general gamma distribution that is usually instead parameterized by \\(\\alpha&gt;0\\) and \\(\\beta&gt;0\\), where:\n\n= shape parameter\n= scale parameter"
  },
  {
    "objectID": "slides/43_Moment_Generating_Functions_Part1.html",
    "href": "slides/43_Moment_Generating_Functions_Part1.html",
    "title": "Chapter 43: Moment Generating Functions Part 1",
    "section": "",
    "text": "Chapter 43: Moment Generating Functions Part 1\n\nWhat are moments?\n\n\nDefinition 1.   The \\(j^{th}\\) moment of a r.v. \\(X\\) is \\(\\mathbb{E}[X^j]\\).\n\n\nExample 2.   \\(1^{st}-4^{th}\\) moments.\n\n\n\nWhat is a moment generating function (mgf)?\n\n\nDefinition 3.   If \\(X\\) is a r.v., then \\[M_X(t)= \\mathbb{E}[e^{tX}]\\] is the moment generating function (mgf) associated with \\(X\\).\n\nRemarks\n\nFor a discrete r.v., the mgf of \\(X\\) is \\[M_X(t)= \\mathbb{E}[e^{tX}]=\\sum_{all \\ x}e^{tx}p_X(x)\\]\nFor a continuous r.v., the mgf of \\(X\\) is \\[M_X(t)= \\mathbb{E}[e^{tX}]=\\int_{-\\infty}^{\\infty}e^{tx}f_X(x)dx\\]\nThe mgf \\(M_X(t)\\) is a function of \\(t\\), not of \\(X\\), and it might not be defined (i.e. finite) for all values of \\(t\\). We just need it to be defined for \\(t=0\\).\n\n\nExample 4.   What is \\(M_X(t)\\) for \\(t=0\\)?\n\n\n\nTheorem 5.   The moment generating function uniquely specifies a probability distribution.\n\n\nTheorem 6.   \\[\\mathbb{E}[X^r] = M_X^{(r)}(0)\\]\n\n\nProof. Proof. ◻\n\n\nExample 7.   Let \\(X \\sim Poisson(\\lambda)\\).\n\nFind the mgf of \\(X\\).\nFind \\(\\mathbb{E}[X]\\).\nFind \\(Var(X)\\).\n\n\nRemark\nFinding the mean and variance is sometimes easier with the following trick.\n\nTheorem 8. Let \\[R_X(t) = \\ln[M_X(t)]\\]\nThen,\n\\[\\mu = \\mathbb{E}[X] = R_X'(0)\\] and \\[\\sigma^2 = Var(X) = R_X''(0)\\]\n\n\nProof. Proof. ◻\n\n\n\nExample 9.   Let \\(X \\sim Poisson(\\lambda)\\).\n\nFind \\(\\mathbb{E}[X]\\) using \\(R_X(t)\\).\nFind \\(Var(X)\\) using \\(R_X(t)\\).\n\n\n\nExample 10.   Let \\(Z\\) be a standard normal random variable, i.e. \\(Z \\sim N(0,1)\\).\n\nFind the mgf of \\(Z\\).\nFind \\(\\mathbb{E}[Z]\\).\nFind \\(Var(Z)\\)."
  },
  {
    "objectID": "slides/31_Uniform_rv.html",
    "href": "slides/31_Uniform_rv.html",
    "title": "Chapter 31: Continuous Uniform Random Variables",
    "section": "",
    "text": "Chapter 31: Continuous Uniform Random Variables\nScenario: Events are equally likely to happen anywhere or anytime in an interval of values.\n\nProperties of continuous uniform r.v.’s"
  },
  {
    "objectID": "slides/10_Expected_Values.html",
    "href": "slides/10_Expected_Values.html",
    "title": "Chapter 10 Expected Values of Discrete r.v.’s",
    "section": "",
    "text": "Chapter 10 Expected Values of Discrete r.v.’s\n::: {#Die expected outcome .example} Example 1. Suppose you roll a fair 6-sided die. What value do you expect to get? :::\nSolution:\n\n::: {#Die expected outcome .example} Example 2. Suppose the die is 6-sided, but not fair. What value do you expect to get on a roll? :::\nSolution:\nRemark: Expected vs. actual outcomes\n\n\nDefinition 3 (Expected value). The expected value of a discrete r.v. \\(X\\) that takes on values \\(x_1, x_2, \\ldots, x_n\\) is \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i).\\]\n\nRemarks:\n\nThe definition holds if the r.v. \\(X\\) takes on countably infinitely many values \\(x_1, x_2, \\ldots\\), as well: \\[\\mathbb{E}[X] = \\sum_{i=1}^{\\infty} x_ip_X(x_i).\\]\nAnother way to define the expected value of a discrete r.v. is to do so at the \\(\\omega\\) level, where the \\(\\omega\\)’s are outcomes in the sample space:\n\nSuppose \\(\\omega_1, \\omega_2, \\ldots, \\omega_n\\) are the possible outcomes of a random phenomenon. If outcome \\(\\omega_i\\) causes the r.v. X to take on value \\(x_i\\) (meaning \\(X(\\omega_i)=x_i\\)), then \\[\\mathbb{E}[X] = \\sum_{i=1}^{\\infty} x_i\\mathbb{P}(\\{\\omega_i\\}).\\]\n\n\n\nExample 4. Suppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\quad\\mathrm{(success)}\\\\\n            0 & \\quad \\mathrm{with\\ probability}\\ 1-p \\quad\\mathrm{(failure)}\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\).\n\nSolution:\n\n\nExample 5. Suppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\\\\n            -1 & \\quad \\mathrm{with\\ probability}\\ 1-p\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\).\n\nSolution:\n\nExample 6. Suppose I throw darts at a dartboard until I hit the bullseye, and that my probability of hitting the bullseye is \\(p\\). Suppose further that all of my throws are independent, and that the probability of a bullseye never changes, no matter how many times I throw a dart. How many times should I expect to have to throw the dart until I hit the bullseye?\n\nSolution:\n\nExample 7. A ghost is trick-or-treating. It comes to a house where it is known that there are 30 candies in the bag and only one is a watermelon Jolly Rancher, which is the ghost’s favorite. The ghost takes pieces of candy without replacement until it gets the watermelon Jolly Rancher. How many pieces of candy do we expect the ghost to take?\n\nSolution:\nRemark: Both examples are repeated random processes. They are fundamentally different though:\n\nThe bullseye example (6) is \"with replacement\" since the probability of success remains constant.\nThe ghost trick-or-treating example (7) is without replacement, and thus the probability of success changes with each trial."
  },
  {
    "objectID": "slides/35_Normal_rv.html",
    "href": "slides/35_Normal_rv.html",
    "title": "Chapter 35: Normal Variables",
    "section": "",
    "text": "Chapter 35: Normal Variables\n\nI will not cover normal rv’s in detail here since I expect you have already learned about normal random variables and \\(z\\)-scores in statistics classes you have taken.\nMake sure you know how to use a normal distribution \\(z\\)-table if you will be taking the MS Biostatistics comprehensive exam, since that is what will be provided during the exam.\n\n\nSome remarks on normal rv’s"
  },
  {
    "objectID": "slides/29_Variance_and_Sums_of_rv.html",
    "href": "slides/29_Variance_and_Sums_of_rv.html",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "",
    "text": "How do we calculate the expected value of a function of a discrete r.v. or joint r.v.’s?\n\nHow do we calculate the expected value of a function of a continuous r.v. or joint r.v.’s?\n\n::: {#29_EaX+b .example} Example 1.   What is \\(\\mathbb{E}[aX+b]\\)? :::\n\nExample 2.   Let \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\n\nRemark\nIf you are given \\(f_{X,Y}(x,y)\\) and want to calculate \\(\\mathbb{E}[X]\\), you have two options:\n\nFind \\(f_X(x)\\) and use it to calculate \\(\\mathbb{E}[X]\\).\nOr, calculate \\(\\mathbb{E}[X]\\) using the joint density: \\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x f_{X,Y}(x,y)dydx.\\]\n\n\n\n\n\n\\(\\mathbb{E}[X+Y] =\\)\nIf \\(X_1, X_2, \\ldots X_n\\) are continuous r.v.’s and \\(a_1, a_2, \\ldots a_n\\) are constants, then \\(\\mathbb{E}[\\sum_{i=1}^{n} a_i X_i] =\\)\nIf \\(X\\) and \\(Y\\) are independent continuous r.v.’s, and \\(g\\) and \\(h\\) are functions, then \\(\\mathbb{E}[g(X)h(Y)] =\\)\nIf \\(X\\) and \\(Y\\) are independent continuous r.v.’s, then \\(\\mathbb{E}[XY] =\\)\n\n\n\n\nHow do we calculate the variance of a discrete r.v.?\n\nHow do we calculate the variance of a continuous r.v.?\n\nExample 3.   Let \\(f_X(x)= \\frac{1}{b-a}\\), for \\(a \\leq x \\leq b\\). Find \\(Var[X]\\).\n\n\n\nExample 4.   Let \\(f_X(x)= \\lambda e^{-\\lambda x}\\), for \\(x &gt; 0\\) and \\(\\lambda&gt; 0\\). Find \\(Var[X]\\).\n\n\n\n\n\n\\(Var[aX+b] =\\)\nIf \\(X_1, X_2, \\ldots X_n\\) are independent continuous r.v.’s and \\(a_1, a_2, \\ldots a_n\\) are constants, then \\(Var(\\sum_{i=1}^{n} a_i X_i) =\\)\nIf \\(X_1, X_2, \\ldots X_n\\) are independent continuous r.v.’s, then \\(Var(\\sum_{i=1}^{n} X_i) =\\)\n\n\nExample 5.   A machine manufactures cubes with a side length that varies uniformly from 1 to 2 inches. Assume the sides of the base and height are equal. The cost to make a cube is 10per cubic inch, and 5for the general cost per cube. Find the mean and standard deviation of the cost to make 10 cubes."
  },
  {
    "objectID": "slides/29_Variance_and_Sums_of_rv.html#expected-value-of-a-function-of-a-continuous-r.v.",
    "href": "slides/29_Variance_and_Sums_of_rv.html#expected-value-of-a-function-of-a-continuous-r.v.",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "",
    "text": "How do we calculate the expected value of a function of a discrete r.v. or joint r.v.’s?\n\nHow do we calculate the expected value of a function of a continuous r.v. or joint r.v.’s?\n\n::: {#29_EaX+b .example} Example 1.   What is \\(\\mathbb{E}[aX+b]\\)? :::\n\nExample 2.   Let \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\).\n\nRemark\nIf you are given \\(f_{X,Y}(x,y)\\) and want to calculate \\(\\mathbb{E}[X]\\), you have two options:\n\nFind \\(f_X(x)\\) and use it to calculate \\(\\mathbb{E}[X]\\).\nOr, calculate \\(\\mathbb{E}[X]\\) using the joint density: \\[\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x f_{X,Y}(x,y)dydx.\\]"
  },
  {
    "objectID": "slides/29_Variance_and_Sums_of_rv.html#important-properties-of-expected-values-of-functions-of-continuous-r.v.s",
    "href": "slides/29_Variance_and_Sums_of_rv.html#important-properties-of-expected-values-of-functions-of-continuous-r.v.s",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "",
    "text": "\\(\\mathbb{E}[X+Y] =\\)\nIf \\(X_1, X_2, \\ldots X_n\\) are continuous r.v.’s and \\(a_1, a_2, \\ldots a_n\\) are constants, then \\(\\mathbb{E}[\\sum_{i=1}^{n} a_i X_i] =\\)\nIf \\(X\\) and \\(Y\\) are independent continuous r.v.’s, and \\(g\\) and \\(h\\) are functions, then \\(\\mathbb{E}[g(X)h(Y)] =\\)\nIf \\(X\\) and \\(Y\\) are independent continuous r.v.’s, then \\(\\mathbb{E}[XY] =\\)"
  },
  {
    "objectID": "slides/29_Variance_and_Sums_of_rv.html#variance-of-continuous-r.v.s",
    "href": "slides/29_Variance_and_Sums_of_rv.html#variance-of-continuous-r.v.s",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "",
    "text": "How do we calculate the variance of a discrete r.v.?\n\nHow do we calculate the variance of a continuous r.v.?\n\nExample 3.   Let \\(f_X(x)= \\frac{1}{b-a}\\), for \\(a \\leq x \\leq b\\). Find \\(Var[X]\\).\n\n\n\nExample 4.   Let \\(f_X(x)= \\lambda e^{-\\lambda x}\\), for \\(x &gt; 0\\) and \\(\\lambda&gt; 0\\). Find \\(Var[X]\\)."
  },
  {
    "objectID": "slides/29_Variance_and_Sums_of_rv.html#important-properties-of-variances-of-continuous-r.v.s",
    "href": "slides/29_Variance_and_Sums_of_rv.html#important-properties-of-variances-of-continuous-r.v.s",
    "title": "Chapter 29: Variance of Continuous Random Variables",
    "section": "",
    "text": "\\(Var[aX+b] =\\)\nIf \\(X_1, X_2, \\ldots X_n\\) are independent continuous r.v.’s and \\(a_1, a_2, \\ldots a_n\\) are constants, then \\(Var(\\sum_{i=1}^{n} a_i X_i) =\\)\nIf \\(X_1, X_2, \\ldots X_n\\) are independent continuous r.v.’s, then \\(Var(\\sum_{i=1}^{n} X_i) =\\)\n\n\nExample 5.   A machine manufactures cubes with a side length that varies uniformly from 1 to 2 inches. Assume the sides of the base and height are equal. The cost to make a cube is 10per cubic inch, and 5for the general cost per cube. Find the mean and standard deviation of the cost to make 10 cubes."
  },
  {
    "objectID": "slides/26_Independent_rv.html",
    "href": "slides/26_Independent_rv.html",
    "title": "Chapter 26: Independent Continuous Random Variables",
    "section": "",
    "text": "Chapter 26: Independent Continuous Random Variables\nWhat do we know about independence for events and discrete r.v.’s?\n\nWhat does it mean for continuous r.v.’s to be independent?\n\nExample 1.   Let \\(X\\) and \\(Y\\) be independent r.v.’s with \\(f_X(x)= \\frac12\\), for \\(0 \\leq x \\leq 2\\) and \\(f_Y(y)= 3y^2\\), for \\(0 \\leq y \\leq 1\\).\n\nFind \\(f_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(0 \\leq X \\leq 1, 0 \\leq Y \\leq \\frac12)\\).\n\n\n\nExample 2.   Let \\(f_{X,Y}(x,y)= 18 x^2 y^5\\), for \\(0 \\leq x \\leq 1, \\ 0 \\leq y \\leq 1\\).\n\nAre \\(X\\) and \\(Y\\) independent?\nFind \\(F_{X,Y}(x,y)\\).\n\n\n\nExample 3.   Let \\(f_{X,Y}(x,y)= 2 e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Are \\(X\\) and \\(Y\\) independent?\n\n\nRemarks\n\nIf \\(f_{X,Y}(x,y)= g(x)h(y)\\), where \\(g(x)\\) and \\(h(y)\\) are pdf’s, then \\(X\\) and \\(Y\\) are independent.\nIf \\(F_{X,Y}(x,y)= G(x)H(y)\\), where \\(G(x)\\) and \\(H(y)\\) are cdf’s, then \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "slides/17_Negative_Binomial_rv.html",
    "href": "slides/17_Negative_Binomial_rv.html",
    "title": "Chapter 17: Negative Binomial r.v.’s",
    "section": "",
    "text": "Chapter 17: Negative Binomial r.v.’s\nScenario: There are repeated independent trials, each resulting in a success or failure, with constant probability of success for each trial. We are counting the number of trials until the \\(r^{th}\\) success.\n\n\nExample 1.   Consider again the bullseye example, where we throw darts at a dartboard until we hit the bullseye. Assume throws are independent and the probability of hitting the bullseye is 0.01 for each throw.\n\nWhat is the probability that the \\(5^{th}\\) bullseye is on the \\(20^{th}\\) throw?\nSolution:\n\n\n\nProperties of Negative Binomial r.v.’s"
  },
  {
    "objectID": "slides/28_Expected_Values.html",
    "href": "slides/28_Expected_Values.html",
    "title": "Chapter 28: Expected Values of Continuous Random Variables",
    "section": "",
    "text": "Chapter 28: Expected Values of Continuous Random Variables\nHow do we calculate expected values of discrete r.v.’s?\n\nHow do we calculate expected values of continuous r.v.’s?\n\n\nExample 1.   Let \\(f_X(x)= \\frac{1}{b-a}\\), for \\(a \\leq x \\leq b\\). Find \\(\\mathbb{E}[X]\\).\n\n\nExample 2.   Let \\(f_X(x)= \\lambda e^{-\\lambda x}\\), for \\(x &gt; 0\\) and \\(\\lambda&gt; 0\\). Find \\(\\mathbb{E}[X]\\).\n\n\nExample 3.   Let \\(f_{X,Y}(x,y)= 2e^{-(x+y)}\\), for \\(0 \\leq x \\leq y\\). Find \\(\\mathbb{E}[X]\\)."
  },
  {
    "objectID": "slides/32_Exponential_rv.html",
    "href": "slides/32_Exponential_rv.html",
    "title": "Chapter 32: Exponential Random Variables",
    "section": "",
    "text": "Chapter 32: Exponential Random Variables\nScenario: Modeling the time until the next (first) event.\n\nProperties of exponential r.v.’s\nMemoryless Property\n\nExample 1.   Let \\(X_i \\sim \\textrm{Exp}(\\lambda_i)\\) be independent r.v.’s, for \\(i=1 \\ldots n\\). Find the pdf for the first of the arrival times."
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html",
    "href": "slides/5_Bayes_Theorem.html",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "",
    "text": "So we learned about conditional probabilities\n\nWe learned how the occurrence of event A affects event B (B conditional on A)\n\nCan we figure out information on how the occurrence of event B affects event A?\nWe can use the conditional probability (\\(\\mathbb{P}(A|B)\\)) to get information on the flipped conditional probability (\\(\\mathbb{P}(B|A)\\))"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#introduction",
    "href": "slides/5_Bayes_Theorem.html#introduction",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Introduction",
    "text": "Introduction\n\nSo we learned about conditional probabilities\n\nWe learned how the occurrence of event A affects event B (B conditional on A)\n\nCan we figure out information on how the occurrence of event B affects event A?\nWe can use the conditional probability (\\(\\mathbb{P}(A|B)\\)) to get information on the flipped conditional probability (\\(\\mathbb{P}(B|A)\\))"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#bayes-rule-for-two-events",
    "href": "slides/5_Bayes_Theorem.html#bayes-rule-for-two-events",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Bayes’ Rule for two events",
    "text": "Bayes’ Rule for two events\n\n\n\n\nTheorem: Bayes’ Rule (for two events)\n\n\nFor any two events \\(A\\) and \\(B\\) with nonzero probabilties,\n\\[\\mathbb{P}(A| B) =\n\\frac{\\mathbb{P}(A) \\cdot \\mathbb{P}(B|A)}\n{\\mathbb{P}(B)}\\]"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#example",
    "href": "slides/5_Bayes_Theorem.html#example",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Example",
    "text": "Example\n\n\nExample 4\n\n\nRecall the color-blind example (Example 2), where\n\na person is AMAB with probability 0.5,\nAMAB people are color-blind with probability 0.05, and\nall people are color-blind with probability 0.03.\n\nAssuming people are AMAB or AFAB, find the probability that a color-blind person is AMAB."
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#example-1",
    "href": "slides/5_Bayes_Theorem.html#example-1",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Example",
    "text": "Example\n\n\nExample 5\n\n\nSuppose\n\n1% of women aged 40-50 years have breast cancer,\na woman with breast cancer has a 90% chance of a positive test from a mammogram, and\na woman has a 10% chance of a false-positive result from a mammogram.\n\nWhat is the probability that a woman has breast cancer given that she just had a positive test?"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#example-2",
    "href": "slides/5_Bayes_Theorem.html#example-2",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Example",
    "text": "Example\n\n\nExample 5.3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?\n\n\nSolution:\n\n\nLaw of Total Probability (general)\n\n\nIf \\(\\{A_i\\}_{i=1}^{n} = \\{A_1, A_2, \\ldots, A_n\\}\\) form a partition of the sample space, then for event \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=& \\sum_{i=1}^{n} \\mathbb{P}(B \\cap A_i)\\\\\n           &=& \\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#example-3",
    "href": "slides/5_Bayes_Theorem.html#example-3",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Example",
    "text": "Example\n\n\nExample 5.4\n\n\nRecall the color-blind example (Example 2), where\n\na person is male with probability 0.5,\nmales are color-blind with probability 0.05, and\nall people are color-blind with probability 0.03.\n\nFind the probability that a color-blind person is male.\n\n\nSolution:"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#example-4",
    "href": "slides/5_Bayes_Theorem.html#example-4",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Example",
    "text": "Example\n\n\nExample 5.5\n\n\nSuppose\n\n1% of women aged 40-50 years have breast cancer,\na woman with breast cancer has a 90% chance of a positive test from a mammogram, and\na woman has a 10% chance of a false-positive result from a mammogram.\n\nWhat is the probability that a woman has breast cancer given that she just had a positive test?\n\n\nSolution:"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#bayes-rule",
    "href": "slides/5_Bayes_Theorem.html#bayes-rule",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\nTheorem: Bayes’ Rule\n\n\nIf \\(\\{A_i\\}_{i=1}^{n}\\) form a partition of the sample space \\(S\\), with \\(\\mathbb{P}(A_i)&gt;0\\) for \\(i=1\\ldots n\\) and \\(\\mathbb{P}(B)&gt;0\\), then\n\\[\\mathbb{P}(A_j | B) =\n\\frac{\\mathbb{P}(B|A_j) \\cdot \\mathbb{P}(A_j)}\n{\\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)}\\]\n\n\n\n\nChapter 5 Slides"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Suppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?\n\n\n\n\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\nNote: When something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur.\n\n\n\n\n\n\n\n\nDefinition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all possible outcomes.\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some possible outcomes.\n\n\n\n\nWhat is the sample space?\n\n\\(S = \\{H, T\\}\\)\n\nWhat are the possible events?\n\n\\(\\{H\\}\\)\n\\(\\{T\\}\\)\n\\(\\{H, T\\}\\)\n\\(\\emptyset\\)\n\n \nWhen thinking about events, think about outcomes that you might be asking the probability of."
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#overview",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#overview",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Overview",
    "text": "Overview\n\nOutcomes, Events, and Sample Space\n\nTossing one coin\nTossing two coins\n\nSet Theory"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-13",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-13",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Suppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-23",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-23",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Suppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\nNote: When something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur."
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-33",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-33",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Definition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all possible outcomes.\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some possible outcomes.\n\n\n\n\nWhat is the sample space?\n\n\\(S = \\{H, T\\}\\)\n\nWhat are the possible events?\n\n\\(\\{H\\}\\)\n\\(\\{T\\}\\)\n\\(\\{H, T\\}\\)\n\\(\\emptyset\\)\n\n \nWhen thinking about events, think about outcomes that you might be asking the probability of."
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#tossing-two-coins",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#tossing-two-coins",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Tossing two coins",
    "text": "Tossing two coins"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-2-coins",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-2-coins",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Coin Toss Example: 2 coins",
    "text": "Coin Toss Example: 2 coins\nSuppose you toss two coins.\n\nWhat is the sample space? Assume the coins are distinguishable\n\n\\(S =\\) \\[S = \\{HH, TT, HT, TH\\}\\]\n\nWhat are some possible events?\n\n\\(A\\) = exactly one \\(H\\) = \\(\\{HT, TH\\}\\)\n\\(B\\) = at least one \\(H\\) = \\(\\{HH, HT, TH\\}\\)"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#more-info-on-events-and-sample-spaces",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#more-info-on-events-and-sample-spaces",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "More info on events and sample spaces",
    "text": "More info on events and sample spaces\n\nWe usually use capital letters from the beginning of the alphabet to denote events. However, other letters might be chosen to be more descriptive.\nWe use the notation \\(|S|\\) to denote the size of the sample space.\nThe total number of possible events is \\(2^{|S|}\\), which is the total number of possible subsets of \\(S\\). We will prove this later in the course.\nThe empty set, denoted by \\(\\emptyset\\), is the set containing no outcomes."
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#example-keep-sampling-until",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#example-keep-sampling-until",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Example: Keep sampling until…",
    "text": "Example: Keep sampling until…\nSuppose you keep sampling people until you have someone with high blood pressure (BP). What is the sample space?\n\nLet \\(H =\\) denote someone with high BP.\nLet \\(H^C =\\) denote someone with not high blood pressure, such as low or regular BP.\nThen, \\(S =\\)\n\n\\[S = \\{H, (H, H^C), (H, H, H^C), (H, H, H, H^C), \\ldots\\]"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#set-theory-12",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#set-theory-12",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Set Theory (1/2)",
    "text": "Set Theory (1/2)\n\n\n\n\nDefinition: Union\n\n\nThe union of events \\(A\\) and \\(B\\), denoted by \\(A \\cup B\\), contains all outcomes that are in \\(A\\) or \\(B\\).\n\n\n\n\nDefinition: Intersection\n\n\nThe intersection of events \\(A\\) and \\(B\\), denoted by \\(A \\cap B\\), contains all outcomes that are both in \\(A\\) and \\(B\\).\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#set-theory-22",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#set-theory-22",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Set Theory (2/2)",
    "text": "Set Theory (2/2)\n\n\n\n\nDefinition: Complement\n\n\nThe complement of event \\(A\\), denoted by \\(A^C\\) or \\(A'\\), contains all outcomes in the sample space \\(S\\) that are not in \\(A\\) .\n\n\n\n\nDefinition: Mutually Exclusive\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive, or disjoint, if they have no outcomes in common. In this case \\(A \\cap B = \\emptyset\\), where \\(\\emptyset\\) is the empty set.\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-13",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-13",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (1/3)",
    "text": "BP example variation (1/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\nEvent all \\(n\\) subjects have high BP\nEvent at least one subject has high BP\nEvent all of them do not have high BP\nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-23",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-23",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (2/3)",
    "text": "BP example variation (2/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\n\\[H_i^C\\]\nEvent all \\(n\\) subjects have high BP\n\\[H_1 \\textrm{ and } H_2 \\textrm{ and } \\ldots = \\bigcap\\limits_{i=1}^{n}H_i\\]\nEvent at least one subject has high BP\n\\[H_1 \\textrm{ or } H_2 \\textrm{ or } \\ldots = \\bigcup\\limits_{i=1}^{n}H_i\\]"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-33",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-33",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (3/3)",
    "text": "BP example variation (3/3)\n\nEvent all of them do not have high BP\n\\(H_1^C\\) and \\(H_2^C\\) and... \\[\\bigcap\\limits_{i=1}^{n}H_i^C = \\Big(\\bigcup\\limits_{i=1}^{n}H_i\\Big)^C\\] = complement of at least one person having high BP\nEvent at least one subject does not have high BP\n\\(H_1^C\\) or \\(H_2^C\\) or... \\[\\bigcup\\limits_{i=1}^{n}H_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}H_i\\Big)^C\\] = complement of all having high BP"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#de-morgans-laws",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#de-morgans-laws",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "De Morgan’s Laws",
    "text": "De Morgan’s Laws\n\n\nTheorem: De Morgan’s 1st Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcap\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcup\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“all not A = \\((\\)at least one event A\\()^C\\)”\n\n\nTheorem: De Morgan’s 2nd Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcup\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“at least one event not A = \\((\\)all A\\()^C\\)”"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample_space-solutions.html#remarks-on-de-morgans-laws",
    "href": "slides/1_Outcomes_Events_Sample_space-solutions.html#remarks-on-de-morgans-laws",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Remarks on De Morgan’s Laws",
    "text": "Remarks on De Morgan’s Laws\n\nThese laws also hold for infinite collections of events.\nDraw Venn diagrams to convince yourself that these are true!\nThese laws are very useful when calculating probabilities.\n\nThis is because calculating the probability of the intersection of events is often much easier than the union of events.\nThis is not obvious right now, but we will see in the coming chapters why."
  },
  {
    "objectID": "slides/2_Probability-solutions.html#what-do-you-know-about-probabilities",
    "href": "slides/2_Probability-solutions.html#what-do-you-know-about-probabilities",
    "title": "Chapter 2: Probability",
    "section": "What do you know about probabilities?",
    "text": "What do you know about probabilities?"
  },
  {
    "objectID": "slides/2_Probability-solutions.html#probabilities-of-equally-likely-events-12",
    "href": "slides/2_Probability-solutions.html#probabilities-of-equally-likely-events-12",
    "title": "Chapter 2: Probability",
    "section": "Probabilities of equally likely events (1/2)",
    "text": "Probabilities of equally likely events (1/2)\n\n\nExample 1\n\n\nSuppose you have a regular well-shuffled deck of cards. What’s the probability of drawing:\n\nany heart\nthe queen of hearts\nany queen\n\nSolution:\n\nany heart = 13/52 = 1/4\nthe queen of hearts = 1/52\nany queen = 4/52 = 1/13"
  },
  {
    "objectID": "slides/2_Probability-solutions.html#probabilities-of-equally-likely-events-22",
    "href": "slides/2_Probability-solutions.html#probabilities-of-equally-likely-events-22",
    "title": "Chapter 2: Probability",
    "section": "Probabilities of equally likely events (2/2)",
    "text": "Probabilities of equally likely events (2/2)\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}.\\]"
  },
  {
    "objectID": "slides/2_Probability-solutions.html#a-probability-is-a-function",
    "href": "slides/2_Probability-solutions.html#a-probability-is-a-function",
    "title": "Chapter 2: Probability",
    "section": "A probability is a function…",
    "text": "A probability is a function…\n\\(\\mathbb{P}(A)\\) is a function with\n\ninput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\noutput: a number between 0 and 1 (inclusive)\n\n\\[\\mathbb{P}(A): S \\rightarrow [0,1]\\]\nA function that follows some specific rules though!\nSee Probability Axioms on next slide."
  },
  {
    "objectID": "slides/2_Probability-solutions.html#probability-axioms-1",
    "href": "slides/2_Probability-solutions.html#probability-axioms-1",
    "title": "Chapter 2: Probability",
    "section": "Probability Axioms",
    "text": "Probability Axioms\n\n\nAxiom 1\n\n\nFor every event \\(A\\), \\(0\\leq\\mathbb{P}(A)\\leq 1\\).\n\n\n\n\nAxiom 2\n\n\nFor the sample space \\(S\\), \\(\\mathbb{P}(S)=1\\).\n\n\n\n\nAxiom 3\n\n\nIf \\(A_1, A_2, A_3, \\ldots\\), is a collection of disjoint events, then \\[\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i).\\]"
  },
  {
    "objectID": "slides/2_Probability-solutions.html#some-probability-properties-1",
    "href": "slides/2_Probability-solutions.html#some-probability-properties-1",
    "title": "Chapter 2: Probability",
    "section": "Some probability properties",
    "text": "Some probability properties\nUsing the Axioms, we can prove all other probability properties!\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\n\nProposition 4\n\n\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\n\n\nProposition 5\n\n\n\\(\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\\\ \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\\\ \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C)\\)"
  },
  {
    "objectID": "slides/2_Probability-solutions.html#proposition-1-proof",
    "href": "slides/2_Probability-solutions.html#proposition-1-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 1 Proof",
    "text": "Proposition 1 Proof\n\n\n\n−+\n01:00\n\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\nProposition 1 Proof\n\n\nSince \\(A\\) and \\(A^C\\) are disjoint, we know from Axiom 3 that \\(\\mathbb{P}(A \\cup A^C) = \\mathbb{P}(A) + \\mathbb{P}(A^C)\\).\nHowever, \\(S = A \\cup A^C\\), and by Axiom 2, \\(\\mathbb{P}(S) = 1\\), implying \\(\\mathbb{P}(A \\cup A^C) = \\mathbb{P}(S) = 1\\).\nThus, \\(\\mathbb{P}(A) + \\mathbb{P}(A^C) = 1\\), or \\(\\mathbb{P}(A) = 1 - \\mathbb{P}(A^C)\\)"
  },
  {
    "objectID": "slides/2_Probability-solutions.html#proposition-2-proof",
    "href": "slides/2_Probability-solutions.html#proposition-2-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 2 Proof",
    "text": "Proposition 2 Proof\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\nProposition 2 Proof (different from book)\n\n\nWe know \\(\\emptyset = S^C\\).\nThus by Prop 1,\n\\[\\mathbb{P}(\\emptyset) = \\mathbb{P}(S^C) = 1- \\mathbb{P}(S) = 1-1 =0\n.\\]"
  },
  {
    "objectID": "slides/2_Probability-solutions.html#proposition-3-proof",
    "href": "slides/2_Probability-solutions.html#proposition-3-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 3 Proof",
    "text": "Proposition 3 Proof\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\nProposition 3 Proof\n\n\nCreate a partition! Make a Venn diagram.\n\\[%\\left(\n\\begin{array}{ccl}\nB &=& A \\cup (B \\cap A^C) \\\\\n\\mathbb{P}(B) &=& \\mathbb{P}(A) + \\mathbb{P}(B \\cap A^C) \\\\\n\\mathbb{P}(B)  & \\geq &   \\mathbb{P}(A),\n\\end{array}\n%\\right)\\] since \\(\\mathbb{P}(B \\cap A^C) \\geq 0\\)."
  },
  {
    "objectID": "slides/2_Probability-solutions.html#partitions-1",
    "href": "slides/2_Probability-solutions.html#partitions-1",
    "title": "Chapter 2: Probability",
    "section": "Partitions",
    "text": "Partitions\n\n\nDefinition: Partition\n\n\nA set of events \\(\\{A_i\\}_{i=1}^{n}\\) create a partition of \\(A\\), if\n\nthe \\(A_i\\)’s are disjoint (mutually exclusive) and\n\\(\\bigcup \\limits_{i=1}^n A_i = A\\)\n\n\n\n\n\nExample 2\n\n\n\nIf \\(A \\subset B\\), then \\(\\{A, B \\cap A^C\\}\\) is a partition of \\(B\\).\nIf \\(S = \\bigcup \\limits_{i=1}^n A_i\\), then the \\(A_i\\)’s are a partition of the sample space.\n\n\n\nCreating partitions is sometimes used to help calculate probabilities, since by Axiom 3 we can add the probabilities of disjoint events."
  },
  {
    "objectID": "slides/2_Probability-solutions.html#venn-diagram-probabilities-1",
    "href": "slides/2_Probability-solutions.html#venn-diagram-probabilities-1",
    "title": "Chapter 2: Probability",
    "section": "Venn Diagram Probabilities",
    "text": "Venn Diagram Probabilities\n\n\n\nExample 3\n\n\nIf a subject has an\n\n80% chance of taking their medication this week,\n70% chance of taking their medication next week, and\n10% chance of not taking their medication either week,\n\nthen find the probability of them taking their medication exactly one of the two weeks.\n\nHint: Draw a Venn diagram labelling each of the parts to find the probability.Answer: \\(\\mathbb{P}(A \\cap B) = 0.6\\).\n\n\n\nChapter 2 Slides"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html",
    "title": "Chapter 11 Expected Values of Sums of Discrete r.v.’s",
    "section": "",
    "text": "Chapter 11 Expected Values of Sums of Discrete r.v.’s\n\nExample 1. Suppose you draw 2 cards from a standard deck of cards with* replacement. Let \\(X\\) be the number of hearts you draw. Find \\(\\mathbb{E}[X]\\).*\n\nSolution:\n\n\nExample 2. What is the expected number of hearts in Example 1 if you draw 200 cards?\n\nSolution:\n\nTheorem 3 (Sums of discrete r.v.’s). For discrete r.v.’s \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[\\mathbb{E}[\\sum_{i=1}^n a_iX_i] = \\sum_{i=1}^n a_i\\mathbb{E}[X_i] .\\]\n\nRemark: The theorem holds for infinitely r.v.’s \\(X_i\\) as well.\n\n\nCorollary 1. For a discrete r.v. \\(X\\), and constants \\(a\\) and \\(b\\), \\[\\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b.\\]\n\n\nCorollary 2. If \\(X_i\\), \\(i=1,2,\\dots, n\\), are i.i.d. r.v.’s, then \\[\\mathbb{E}[\\sum_{i=1}^n X_i] = n\\mathbb{E}[X_1] .\\]\n\n\nProof. Proof to Theorem 3. ◻\n\n\nProof. Proof to Corollary 1. ◻\n\n\nExample 4. The ghost is trick-or-treating at a different house now. In this case it is known that the bag of candy has 10 chocolates, 20 lollipops, and 30 [insert your favorite candy]. The ghost takes five pieces of candy without replacement. How many pieces of chocolate do we expect the ghost to take?\n\nSolution:\n\nExample 5. A tour group is planning a visit to the city of Landport and needs to book 30 hotel rooms. The average price of a room is $200. In addition, there is a 10% tourism tax for each room. What is the expected cost for the 30 hotel rooms?\n\nSolution:"
  },
  {
    "objectID": "slides/3_IndependentEvents.html",
    "href": "slides/3_IndependentEvents.html",
    "title": "Chapter 3: Independent Events",
    "section": "",
    "text": "Define independence of 2-3 events given probability notation\nCalculate whether two or more events are independent"
  },
  {
    "objectID": "slides/3_IndependentEvents.html#slide-1",
    "href": "slides/3_IndependentEvents.html#slide-1",
    "title": "Chapter 3: Independent Events",
    "section": "Slide 1",
    "text": "Slide 1\nQuestion: Which of the following sequences of coin tosses of heads (\\(H\\)) and tails (\\(T\\)) is more likely to happen, assuming the coin is fair?\n\\[HTTHHHTHTHHTTTH\\] or \\[HTTTTTTTTHTTTTT\\]"
  },
  {
    "objectID": "slides/3_IndependentEvents.html#slide-2",
    "href": "slides/3_IndependentEvents.html#slide-2",
    "title": "Chapter 3: Independent Events",
    "section": "Slide 2",
    "text": "Slide 2\n\n\nDefinition: Independence\n\n\nEvents \\(A\\) and \\(B\\) are independent if \\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B).\\]\n\n\nNotation: For shorthand, we sometimes write \\[A \\mathrel{\\unicode{x2AEB}} B,\\] to denote that \\(A\\) and \\(B\\) are independent events."
  },
  {
    "objectID": "slides/3_IndependentEvents.html#da",
    "href": "slides/3_IndependentEvents.html#da",
    "title": "Chapter 3: Independent Events",
    "section": "da",
    "text": "da\n\n\nExample 1\n\n\nTwo dice (red and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event first die is a six. Are events \\(A\\) and \\(B\\) independent?"
  },
  {
    "objectID": "slides/3_IndependentEvents.html#fsf",
    "href": "slides/3_IndependentEvents.html#fsf",
    "title": "Chapter 3: Independent Events",
    "section": "fsf",
    "text": "fsf\n\n\nDefinition: Independence of 3 Events\n\n\nEvents \\(A\\), \\(B\\), and \\(C\\) are independent if\n\n\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B)\\)\n\\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(C)\\)\n\\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\\(\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\n\nRemark:\nOn your homework you will show that \\((1) \\not \\Rightarrow (2)\\) and \\((2) \\not \\Rightarrow (1)\\)."
  },
  {
    "objectID": "slides/3_IndependentEvents.html#fafs",
    "href": "slides/3_IndependentEvents.html#fafs",
    "title": "Chapter 3: Independent Events",
    "section": "fafs",
    "text": "fafs\n\n\nExample 2\n\n\nSuppose you take a random sample of \\(n\\) people, of which people are smokers and non-smokers independently of each other. Let\n\n\\(A_i =\\) event person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\), and\n\\(p_i =\\) probability person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\).\n\nFind the probability that at least one person in the random sample is a smoker."
  },
  {
    "objectID": "slides/3_IndependentEvents.html#faf",
    "href": "slides/3_IndependentEvents.html#faf",
    "title": "Chapter 3: Independent Events",
    "section": "faf",
    "text": "faf\n\n\nExample 3\n\n\n\\(A, B,\\) and \\(C\\) toss a fair coin in order. The first to throw heads wins. What are their respective chances of winning?\n\n\nLet\n\n\\(A_H\\) and \\(A_T\\) be the events player A tosses heads and tails, respectively.\nSimilarly define \\(B_H\\), \\(B_T\\), \\(C_H\\), and \\(C_T\\).\n\n\n\nChapter 3 Slides"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html",
    "href": "slides/8_pmfs_and_cdfs.html",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "",
    "text": "Example 1. Suppose we toss 3 coins with probability of heads \\(p\\). If \\(X\\) is the random variable counting the number of heads, what are the probabilities of each value of \\(X\\)?\n\nSolution:\n\nDefinition 2. The probability distribution or probability mass function (pmf) of a discrete r.v. \\(X\\) is defined for every number \\(x\\) by \\[p_X(x) = \\mathbb{P}(X=x) = \\mathbb{P}(\\mathrm{all }\\ \\omega\\in S:X(\\omega) = x)\\]"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-1",
    "href": "slides/8_pmfs_and_cdfs.html#slide-1",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 1",
    "text": "Slide 1\n\n\n\n\nExample 1\n\n\nSuppose we toss 3 coins with probability of heads \\(p\\). If \\(X\\) is the random variable counting the number of heads, what are the probabilities of each value of \\(X\\)?\n\n\n\n\n\nDefinition 2. The probability distribution or probability mass function (pmf) of a discrete r.v. \\(X\\) is defined for every number \\(x\\) by \\[p_X(x) = \\mathbb{P}(X=x) = \\mathbb{P}(\\mathrm{all }\\ \\omega\\in S:X(\\omega) = x)\\]"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-2",
    "href": "slides/8_pmfs_and_cdfs.html#slide-2",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 2",
    "text": "Slide 2\nRemarks:\n\nA pmf \\(p_X(x)\\) must satisfy the following properties:\n\n\\(p_X(x)\\geq 0\\) for all \\(x\\).\n\\(\\sum \\limits_{\\{all\\ x\\}}p_X(x)=1\\).\n\nSome distributions depend on parameters.\n\nEach value of a parameter gives a different pmf.\nThe collection of all pmf’s for different values of the parameters is called a family of pmf’s."
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-3",
    "href": "slides/8_pmfs_and_cdfs.html#slide-3",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 3",
    "text": "Slide 3\n\n\n\n\nExample 2: Binomial family of RVs\n\n\nSuppose you toss \\(n\\) coins, each with probability of heads \\(p\\). If \\(X\\) is the number of heads, what is the pmf of \\(X\\)?"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-4",
    "href": "slides/8_pmfs_and_cdfs.html#slide-4",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 4",
    "text": "Slide 4\n\n\n\n\nExample 3: Bernoulli family of RVs\n\n\nSuppose you toss 1 coin, with probability of heads \\(p\\). If \\(X\\) is the number of heads, what is the pmf of \\(X\\)?"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-5",
    "href": "slides/8_pmfs_and_cdfs.html#slide-5",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 5",
    "text": "Slide 5\n\nExample 5 (Household size). The table below shows household sizes in 2019. Data are from https://www.census.gov/data/tables/time-series/demo/families/households.html.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\n\nWhat is the sample space for household sizes?\nDefine the random variable for household sizes.\nDo the values in the table create a pmf? Why or why not?\nMake a plot of the pmf.\n\n\nSolution:"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-6",
    "href": "slides/8_pmfs_and_cdfs.html#slide-6",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 6",
    "text": "Slide 6\n\nDefinition 6. The cumulative distribution function (cdf) of a discrete r.v. \\(X\\) with pmf \\(p_X(x)\\), is defined for every value \\(x\\) by \\[F_X(x) = \\mathbb{P}(X \\leq x) = \\sum \\limits_{\\{all\\ y:\\ y\\leq x\\}}p_X(y)\\]"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-7",
    "href": "slides/8_pmfs_and_cdfs.html#slide-7",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 7",
    "text": "Slide 7\n\nExample 7 (Household size cont’d).\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\n\nGraph the cdf of household sizes in 2019.\nWrite the cdf as a function.\n\n\nSolution:"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#slide-8",
    "href": "slides/8_pmfs_and_cdfs.html#slide-8",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Slide 8",
    "text": "Slide 8\nProperties of discrete cdf’s:\n\n\nChapter 8 Slides"
  },
  {
    "objectID": "slides/24_Continuous_rv.html",
    "href": "slides/24_Continuous_rv.html",
    "title": "Chapter 24: Continuous r.v.’s and pdf’s",
    "section": "",
    "text": "Chapter 24: Continuous r.v.’s and pdf’s\nRecall from Chapter 7:\nDiscrete vs. Continuous r.v.’s\n\nFor a discrete r.v., the set of possible values is either finite or can be put into a countably infinite list.\nContinuous r.v.’s take on values from continuous intervals, or unions of continuous intervals.\n\n\nHow to define probabilities for continuous r.v.’s?\n\nDefinition 1 (Probability density function).   The probability distribution, or probability density function (pdf), of a continuous random variable \\(X\\) is a function \\(f_X(x)\\), such that for all real values \\(a,b\\) with \\(a \\leq b\\),\n\\[\\mathbb{P}(a \\leq X \\leq b) = \\int_a^b f_X(x)dx\\]\n\n\nRemarks:\n\nNote that \\(f_X(x) \\neq \\mathbb{P}(X=x)\\)!!!\nIn order for \\(f_X(x)\\) to be a pdf, it needs to satisfy the properties\n\n\\(f_X(x) \\geq 0\\) for all \\(x\\)\n\\(\\int_{-\\infty}^{\\infty} f_X(x)dx=1\\)\n\n\n\nExample 2.   Let \\(f_X(x)= 2\\), for \\(a \\leq x \\leq 3\\).\n\nFind the value of \\(a\\) so that \\(f_X(x)\\) is a pdf.\nFind \\(\\mathbb{P}(2.7 \\leq X \\leq 2.9)\\).\nFind \\(\\mathbb{P}(2.7 &lt; X \\leq 2.9)\\).\nFind \\(\\mathbb{P}(X = 2.9)\\).\nFind \\(\\mathbb{P}(X \\leq 2.8)\\).\n\n\n\nDefinition 3 (Cumulative distribution function).   The cumulative distribution function (cdf) of a continuous random variable \\(X\\), is the function \\(F_X(x)\\), such that for all real values of \\(x\\), \\[F_X(x)= \\mathbb{P}(X \\leq x) = \\int_{-\\infty}^x f_X(s)ds\\]\n\n\n\nExample 4.   Let \\(f_X(x)= 2\\), for \\(2.5 \\leq x \\leq 3\\). Find \\(F_X(x)\\).\n\n\nRemarks: In general, \\(F_X(x)\\) is increasing and\n\n\\(\\lim_{x\\rightarrow -\\infty} F_X(x)= 0\\)\n\\(\\lim_{x\\rightarrow \\infty} F_X(x)= 1\\)\n\n\nTheorem 5.   If \\(X\\) is a continuous random variable with pdf \\(f_X(x)\\) and cdf \\(F_X(x)\\), then for all real values of \\(x\\) at which \\(F'_X(x)\\) exists, \\[\\frac{d}{dx} F_X(x)= F'_X(x) = f_X(x)\\]\n\n\nExample 6.   Let \\(X\\) be a r.v. with cdf \\[F_X(x)= \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x &lt; 2.5 \\\\\n            2x-5 & \\quad 2.5 \\leq x \\leq 3 \\\\\n            1 & \\quad x &gt; 3\n        \\end{array}\n    \\right.\\] Find the pdf \\(f_X(x)\\).\n\nSolution:\n\nExample 7.   Let \\(X\\) be a r.v. with pdf \\(f_X(x)= 2e^{-2x}\\), for \\(x&gt;0\\).\n\nShow \\(f_X(x)\\) is a pdf.\nFind \\(\\mathbb{P}(1 \\leq X \\leq 3)\\).\nFind \\(F_X(x)\\).\nGiven \\(F_X(x)\\), find \\(f_X(x)\\).\nFind \\(\\mathbb{P}(X \\geq 1 | X \\leq 3)\\).\nFind the median of the distribution of \\(X\\)."
  },
  {
    "objectID": "homework/HW3.html",
    "href": "homework/HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Important\n\n\n\nThis page is no longer under construction. You may start this homework! (2/9/24)"
  },
  {
    "objectID": "homework/HW6.html",
    "href": "homework/HW6.html",
    "title": "Homework 6",
    "section": "",
    "text": "Please turn in this homework on Sakai. Please submit your homework in pdf format. You can type your work on your computer or submit a photo of your written work or any other method that can be turned into a pdf. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in homework.\nTry to complete all of the problems listed below at some point this quarter! You may want to save some of them for studying later! Only turn in the ones listed in the “Turn In” column. Please submit problems in the order they are listed.\nThe more work you include that shows your thought process, the more I can give you feedback.\n\n\n\n\nChapter\nTurn In\nExtra Problems\n\n\n\n\n19\nTB # 6\n# 1, 18, 19\n\n\n18\nTB # 24\n# 1, 26, 27\n\n\nCalculus Review\n\nNTB # 1\n\n\n24\nTB # 19, 20*\n# 2, 3, 7, 17, 18, 22, 23\n\n\n25\nTB # 18, NTB # 2\n# 1, 4, 8, 17, 23, 24\n\n\n26**\nTB # 12, NTB # 3, 4\n# 7, 9, 19, 20\n\n\n27\nTB # 12***\n# 6, 8, 13, 17\n\n\n\n\n* (Ch 24) Also find the cdf \\(F_X(x)\\)\n** Although within Chapter 26, these exercises are primarily practicing the material from Chapter 25.\n** For Ch 27 # 12, in order to find the conditional densities in parts (a) and (b), you will need to calculate \\(f_Y(y)\\) for the specific regions of \\(y\\) specified. After finding the conditional densities in parts (a) and (b), also calculate the conditional probabilities below. Please submit these together with your other work in parts (a) and (b):\n\nFind \\(\\mathbb{P}[0.5 &lt; X &lt; 3 | Y = 4]\\).\nFind \\(\\mathbb{P}[0.5 &lt; X &lt; 3 | Y = 7]\\).\n\n\nNon-textbook problems (NTB):\n\nCalculus Review\n\n\\[\\int_0^yc(x+y)dx\\]\n\\[\\frac{d}{dx}\\bigg(\\frac{4}{9}x^2y^2+\\frac{5}{9}xy^4\\bigg)\\]\n\\[\\frac{d}{dy}\\bigg(\\frac{4}{9}x^2y^2+\\frac{5}{9}xy^4\\bigg)\\]\n\\[\\int_0^y2e^{-x}e^{-y}dx\\]\n\\[\\int_0^\\infty xye^{-(x+y)}dy\\]\n\\[\\int_x^{2x} 2e^{-(x+3y)}dy\\]\nFind the area of the region bounded by the graphs of \\(f(x)=2-x^2\\) and \\(g(x)=x\\) by integrating with respect to \\(x\\).\nFind the area of the region bounded by the graphs of \\(f(x)=2-x^2\\) and \\(g(x)=x\\) by integrating with respect to \\(y\\).\nFind the area of the region bounded by the graphs of \\(x=3-y^2\\) and \\(y=x-1\\) by integrating with respect to \\(x\\).\nFind the area of the region bounded by the graphs of \\(x=3-y^2\\) and \\(y=x-1\\) by integrating with respect to \\(y\\).\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be i.i.d. random variables with common pdf \\(f_X(x)\\) and cdf \\(F_X(x)\\). Find the pdf for the random variable \\(Z\\), where \\(Z = max(X_1, X_2, \\ldots, X_n)\\).\nLet \\(X\\) and \\(Y\\) be independent random variables with respective pdf’s \\(f_X(x)=\\frac{1}{5}\\), for \\(0\\leq x\\leq 5\\), and \\(f_Y(y)=2e^{-2y}\\), for \\(y&gt;0\\).\n\nFind the joint distribution \\(f_{X,Y}(x,y)\\).\nFind the probability that \\(X\\) is less than \\(Y\\).\nLet \\(Z\\) be the random variable that is the smaller of \\(X\\) and \\(Y\\). Find the cumulative distribution function for \\(Z\\).\nFind the pdf for Z.\n\nSuppose that the random variables \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)\\), for \\(0&lt;x&lt;1\\), and \\(\\frac{1}{2}&lt;y&lt;1\\). Set up the equation for the cdf of \\(Z\\), where \\(Z=X/Y\\).\nHint: First determine what the possible values for \\(Z\\) are. Then make a sketch of the domain of the joint pdf and shade in the region representing the cdf of Z for different values of \\(z\\). Make sure to pay close attention to how the region we need to integrate over changes as \\(z\\) changes. The cdf has two different cases depending on the value of \\(z\\). Plug in specific values of \\(z\\) and shade in the region representing the cdf to see why two different cases are needed."
  },
  {
    "objectID": "homework/HW9.html",
    "href": "homework/HW9.html",
    "title": "Homework 9",
    "section": "",
    "text": "Complete all of the problems listed below. Only turn in the ones listed in the “Turn In” column. Please submit problems in the order they are listed.\nYou must show all of your work to receive credit. Don’t forget to define every r.v. you use! In particular, if a similar problem was done in class or an example in the book, make sure to still show every step in the solution and not just cite the examples’ results.\n\n\n\n* Include in your answer an explanation as to why we need the condition that \\(t&lt;\\lambda\\).\n** Do parts (a)-(c) below for #10 and #12:\n\nAnswer the question using the mgf \\(M_X(t)\\) as instructed in the book.\nAnswer the question using \\(R_X(t)\\) (as defined in class, and NTB [Ch43_R_Var] below).\nWhich method did you prefer? Why?\n\n*** Assume the distances between the cars are independent.\n\nNon-textbook problems (NTB):\n\nLet \\(R_X(t)=\\ln(M_X(t))\\). Show that Var\\((X)=R''_X(0)\\).\nThe mgf for a Gamma distribution is \\(M_X(t)=\\frac{1}{(1-t/\\lambda)^r}\\). Use the mgf of an Exponential distribution (from #43.9), to show that the sum of \\(n\\) i.i.d. Exponential(\\(\\lambda)\\) random variables has a Gamma(\\(r,\\lambda\\)) distribution.\nUse the mgf of a Poisson distribution to find the mgf of the following distributions. If the mgf is that of a common named distribution, then name the distribution and state its parameter(s).\n\n1.  The distribution of $\\sum_{i=1}^nX_i$, if $X_i\\sim$Poisson$(\\lambda_i)$ and are independent.\n\n2.  The distribution of $\\sum_{i=1}^3X_i$, if $X_i\\sim$Poisson$(\\lambda)$ and are independent (i.i.d. in this case).\n\n3.  The distribution of $3X$, if $X\\sim$Poisson$(\\lambda)$.\n\n4.  Why are the answers to (b) and (c) different?\n\nUsing mgf’s, show that the sum of \\(n\\) i.i.d. Chi Square random variables with one degree of freedom (\\(\\chi^2_{(1)}\\)) r.v.’s has a Chi Square with \\(n\\) degrees of freedom (\\(\\chi^2_{(n)}\\)) distribution.\n\n*Hint:* First, look up the pdf of a $\\chi^2_{(n)}$. This is a special case of the Gamma distribution with what parameters? Based on that and the information from \\# [\\[Ch43_SumExpGamma\\]](#Ch43_SumExpGamma){reference-type=\"ref\" reference=\"Ch43_SumExpGamma\"} above, you can determine what the mgf of a $\\chi^2_{(n)}$ is, which will help you determine whether the mgf of the sum of $n$ i.i.d. $\\chi^2_{(1)}$ r.v.'s has a $\\chi^2_{(n)}$ distribution.\n\nSelected answers (or hints) not provided at the end the book:\n\n\n\n(a) Poisson\\((\\sum_{i=1}^n \\lambda_i)\\)     (b) Poisson\\((3\\lambda)\\)      (c) \\(M_{3X}(t)=e{\\lambda(e^{3t}-1)}\\) This is not an mgf of a common probability distribution.      (d) In (b) we are adding independent r.v.’s \\(X_i\\), while in (c) we are adding dependent r.v.’s (\\(3X=X+X+X\\); \\(X\\) is dependent with itself).\n\n\n0.0044\n(a) 0.9525     (b) 0.7939     (c) 0.7939\n0.5911\n(a) \\(R=8.225\\sigma+25\\mu\\)     (b) \\(R=16.45\\sigma+100\\mu\\)     (c) \\(R=164.5\\sigma+10,000\\mu\\)     (d) \\(R=1.645\\sqrt{n}\\sigma+n\\mu\\)\n\n\n0.8869\n0.0023\n0.3936\n0.4562\n(b) 0.0022     (c) \\(478.696\\approx 479\\)"
  },
  {
    "objectID": "homework/HW1.html",
    "href": "homework/HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Important\n\n\n\nThis page is no longer under construction. You may start this homework! (1/16/24)"
  },
  {
    "objectID": "homework/HW1.html#homework-structure",
    "href": "homework/HW1.html#homework-structure",
    "title": "Homework 1",
    "section": "",
    "text": "Complete all of the problems listed below. Only turn in the ones listed in the “Turn In” column. Please submit problems in the order they are listed.\nYou must show all of your work to receive credit.\n\n\n\n\n\n\n\n\n\nChapter\nTurn In\nExtra Problems\n\n\n\n\n1\n\n# 3, 7, 9, 11\n\n\n2\nNTB # 1, TB # 30\n# 1, 4, 8, 16, 19, 23\n\n\n22*\nTB # 42, NTB # 2\n# 3, 5, 7, 25, 27, 30, 31, 39-41, 43-48\n\n\n\n\n* Please note the following for Chapter 22:\n\nSee the table on pg. 277, which summarizes some key combinatorics concepts.\nProblems 39-48 are a set that build on one another and more advanced than the other problems. It’ll be much easier to do #42 after doing 39-41.\nI highly recommend reading Chapter 23, which is a series of case studies in counting: poker hands and Yahtzee."
  },
  {
    "objectID": "homework/HW1.html#non-textbook-problems-ntb",
    "href": "homework/HW1.html#non-textbook-problems-ntb",
    "title": "Homework 1",
    "section": "Non-textbook problems (NTB)",
    "text": "Non-textbook problems (NTB)\n\nSuppose the following are the percentage of US adults with the following conditions:\n\n\\(A\\): Hypertension 33%\n\\(B\\): Diabetes 9%\n\\(C\\): Metabolic syndrome 24%\n\\(A\\) or \\(B\\): 39%\n\\(A\\) or \\(C\\): 45%\n\\(B\\) or \\(C\\): 28%\n\\(A\\) or \\(B\\) or \\(C\\): 48%\n\n\nMake a Venn diagram of the 3 conditions labeling the percentage (or probability) for ALL of the 8 “sections”. Hint: Start from the last condition and work your way up!\nFor each of the following (1. - 7. below), (\\(i\\)) write out the event using unions, intersections, and/or complements of the events \\(A\\), \\(B\\), and \\(C\\) (this is NOT finding the probability, that’s in \\(ii\\)); (\\(ii\\)) find the probability of the event; and (\\(iii\\)) write a sentence explaining what the probability is of in terms of the context of the problem.\n\n\\(\\mathbb{P}\\)(event at least one of the 3)\n\\(\\mathbb{P}\\)(event none)\n\\(\\mathbb{P}\\)(event \\(A\\) only)\n\\(\\mathbb{P}\\)(event exactly one)\n\\(\\mathbb{P}\\)(event \\(A\\) and \\(B\\))\n\\(\\mathbb{P}\\)(event \\(A\\) and \\(B\\) but not \\(C\\))\n\\(\\mathbb{P}\\)(event all 3)"
  },
  {
    "objectID": "homework/HW1.html#some-select-answers",
    "href": "homework/HW1.html#some-select-answers",
    "title": "Homework 1",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 2\n\n# 4: 0.35\n# 8: 0.03125\n# 16: 0.48\n# 30: (a) 0.189     (b) 0.811     (c) 0.189"
  },
  {
    "objectID": "homework/HW4.html",
    "href": "homework/HW4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Important\n\n\n\nThis assignment is no longer under construction!! (2/15/2024)"
  },
  {
    "objectID": "readings/Calc_review.html",
    "href": "readings/Calc_review.html",
    "title": "Calculus Review",
    "section": "",
    "text": "Calculus will be used in probability throughout the quarter, as well as in the following math stat classes. Below are topics and links from Paul’s Online Math Notes to help you review algebra and calculus skills that are essential to BSTA 550.\n\nYou can ignore all examples that use trigonometry.\nI listed the sections that you should be familiar with. The webpages have both notes and exercises for you to practice.\nI recommend first reviewing the italicized sections since we will be using series early in the course.\nWe will not use differentiation and integration until the second half of the course (Week 6), so you have some time to review."
  },
  {
    "objectID": "readings/Calc_review.html#introduction",
    "href": "readings/Calc_review.html#introduction",
    "title": "Calculus Review",
    "section": "",
    "text": "Calculus will be used in probability throughout the quarter, as well as in the following math stat classes. Below are topics and links from Paul’s Online Math Notes to help you review algebra and calculus skills that are essential to BSTA 550.\n\nYou can ignore all examples that use trigonometry.\nI listed the sections that you should be familiar with. The webpages have both notes and exercises for you to practice.\nI recommend first reviewing the italicized sections since we will be using series early in the course.\nWe will not use differentiation and integration until the second half of the course (Week 6), so you have some time to review."
  },
  {
    "objectID": "readings/Calc_review.html#topics",
    "href": "readings/Calc_review.html#topics",
    "title": "Calculus Review",
    "section": "Topics",
    "text": "Topics\n\nAlgebra\n\nPreliminaries\nSolving Equations And Inequalities\nGraphing And Functions\nExponential And Logarithm Functions\n\nCalculus 1   [Notes]   [Practice Problems]\n\nDerivatives\n\nDifferentiation Formulas\nProduct and Quotient Rule\nDerivatives of Exponential and Logarithm Functions (Only need exponential functions, and in particular only ex)\nChain Rule\n\nIntegrals\n\nIndefinite Integrals\nComputing Indefinite Integrals\nSubstitution Rule for Indefinite Integrals\nMore Substitution Rule\nArea Problem\n\nDon’t worry about the computations. Read through as a review for how integrals calculate area under the curve as the limit of areas of rectangles.\n\nThe Definition of the Definite Integral\nComputing Definite Integrals\nSubstitution Rule for Definite Integrals\n\nApplications of Integrals\n\nArea Between Curves\n\nExtras\n\nSummation Notation\n\n\nCalculus 2   [Notes]   [Practice Problems]\n\nIntegration Techniques\n\nIntegration by Parts\n\nSequences and Series\n\nSeries - The Basics\nSeries - Special Series (just Geometric Series)"
  },
  {
    "objectID": "schedule1.html",
    "href": "schedule1.html",
    "title": "Schedule",
    "section": "",
    "text": "ScheduleImportant timesCredit to\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJanuary\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n9\n\n\n\n\n\n\n10\n\n\n\n\n\n\n11\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 0 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n16\n\n\n\n\n\n\n17\n\n\n\n\n\n\n18\n\n\n\n\n\n\n19\n\n\n\n\n\n\n\n\nNo Class: MLKJ Day\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nLab 1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n22\n\n\n\n\n\n\n23\n\n\n\n\n\n\n24\n\n\n\n\n\n\n25\n\n\n\n\n\n\n26\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n30\n\n\n\n\n\n\n31\n\n\n\n\n\n\n1\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\nWeek 4Quiz 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFebruary\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n30\n\n\n\n\n\n\n31\n\n\n\n\n\n\n1\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\nWeek 4Quiz 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n6\n\n\n\n\n\n\n7\n\n\n\n\n\n\n8\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n13\n\n\n\n\n\n\n14\n\n\n\n\n\n\n15\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 3 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n19\n\n\n\n\n\n\n20\n\n\n\n\n\n\n21\n\n\n\n\n\n\n22\n\n\n\n\n\n\n23\n\n\n\n\n\n\n\n\nNo Class: Pres Day\n\n\n\n\n\n\n\n\n\n\n\nWeek 7Quiz 2\n\n\n\n\n\n\n\n\n\n\n\nHW 4 due 2/25Midterm feedback due\n\n\n\n\n\n\n\n\n26\n\n\n\n\n\n\n27\n\n\n\n\n\n\n28\n\n\n\n\n\n\n29\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\nWeek 8Virtual Class\n\n\n\n\n\n\n\n\n\n\n\nVirtual Class\n\n\n\n\n\n\n\n\n\n\n\nLab 3 due 3/3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarch\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n26\n\n\n\n\n\n\n27\n\n\n\n\n\n\n28\n\n\n\n\n\n\n29\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\nWeek 8Virtual Class\n\n\n\n\n\n\n\n\n\n\n\nVirtual Class\n\n\n\n\n\n\n\n\n\n\n\nLab 3 due 3/3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n5\n\n\n\n\n\n\n6\n\n\n\n\n\n\n7\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 5 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n12\n\n\n\n\n\n\n13\n\n\n\n\n\n\n14\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\nWeek 10Quiz 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n18\n\n\n\n\n\n\n19\n\n\n\n\n\n\n20\n\n\n\n\n\n\n21\n\n\n\n\n\n\n22\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a tentative list of important, recurring times:\n\nHomeworks are always due at 11pm on the specified day (usually Thursday)\nLabs are always due at 11pm on the specified day (usually Thursday)\nExit tickets are due at 11pm 7 days after class\n\nSo for Monday classes, they are due the following Monday at 11pm\nFor Wednesday classes, they are due the following Wednesday at 11pm\n\nOffice hours with Nicky: Link to Webex\n\nThursdays, 11:30am - 1pm\nExceptions:\n\nJanuary 25th: 10:30am - 12pm\nFebruary 22nd: 11am - 12pm\n\n\nClass meets on Mondays and Wednesdays at 1 - 2:50 pm\n\n\n\nSpecial thank you to Andrew Bray, who taught the Quarto workshop I attended. This schedule page was mostly taken from the Schedule on his STAT 20 course page. You can find the .qmd file for Andrew’s schedule page on his Github."
  },
  {
    "objectID": "slides/2_Probability-solutions.html#overview",
    "href": "slides/2_Probability-solutions.html#overview",
    "title": "Chapter 2: Probability",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "schedule/week_01_sched.html#post-class-surveys",
    "href": "schedule/week_01_sched.html#post-class-surveys",
    "title": "Week 1",
    "section": "Post-Class Surveys",
    "text": "Post-Class Surveys\nSurvey link!!"
  },
  {
    "objectID": "homework/HW2.html#non-textbook-problems-ntb",
    "href": "homework/HW2.html#non-textbook-problems-ntb",
    "title": "Homework 2",
    "section": "Non-textbook problems (NTB)",
    "text": "Non-textbook problems (NTB)\n\nRecall from class, that we defined events \\(A,B,\\) and \\(C\\) to mutually independent if both (1) and (2) below hold. This point of this exercise is to show that \\((1)\\nRightarrow (2),\\) and \\((2)\\nRightarrow (1).\\) \\[\\begin{array}{cc}\n    (1) & \\mathbb{P}(A\\cap B\\cap C)=\\mathbb{P}(A)\\mathbb{P(}B)\\mathbb{P(}C) \\\\\n    (2) & \\mathbb{P}(A\\cap B)=\\mathbb{P}(A)\\mathbb{P(}B) \\\\\n    & \\mathbb{P}(A\\cap C)=\\mathbb{P}(A)\\mathbb{P(}C) \\\\\n    & \\mathbb{P}(B\\cap C)=\\mathbb{P}(B)\\mathbb{P(}C)%\n    \\end{array}%\\]\n\nSuppose two different fair dice are rolled. Let events \\(A,B,\\) and \\(C\\) be defined in the following way: \\[\\begin{array}{cl}\nA: & \\text{Roll a total of 7} \\\\\nB: & \\text{First die is a 6} \\\\\nC: & \\text{Second die is a 2}%\n\\end{array}%\\]\nShow that condition \\((2)\\) holds, but that condition \\((1)\\) does not.\nSuppose two different fair dice are rolled. Let events \\(A,B,\\) and \\(C\\) be defined in the following way: \\[\\begin{array}{cl}\nA: & \\text{Roll a 1 or 2 on the first die} \\\\\nB: & \\text{Roll a 3, 4, or 5 on the second die} \\\\\nC: & \\text{Roll a total of 4, 11, or 12}%\n\\end{array}%\\]\nShow that condition \\((1)\\) holds, but that condition \\((2)\\) does not."
  },
  {
    "objectID": "homework/HW2.html#some-select-answershints",
    "href": "homework/HW2.html#some-select-answershints",
    "title": "Homework 2",
    "section": "Some select answers/hints",
    "text": "Some select answers/hints\n\nChapter 3\n\n# 4: (a) 0.111328    (b) 0.004872    0.995128\n# 10: (c) 0.384 If you have the right answer to (c), then you should be able to figure out the rest (see (e)).\n# 12: No.\nNTB #1: (a) 0.0799     (b) 0.07553     (c) 0.0655\n\nChapter 4\n\n#4: 0.25\n# 12: (a) 0.4285714     (b) 0.4285714     (c) 0.1428571\n\nChapter 5\n\nNTB #3: 0.392\nNTB #4: 11.89 (rounds to about 12)"
  },
  {
    "objectID": "homework/HW2.html#some-select-answers",
    "href": "homework/HW2.html#some-select-answers",
    "title": "Homework 2",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 22\n\n# 30: (a) 2,835     (b) 405     (c) 10,780     (d) 7,980\n# 40: 0.6666667\n# 42: 0.002116402 (This is the answer when \\(n=5\\). Your answer needs to be in terms of \\(n\\).)\n# 44: 0.3\n# 46: 0.3333333\n# 48: 0.007936508 (This is the answer when \\(n=5\\). Your answer needs to be in terms of \\(n\\).)\n\nChapter 3\n\n# 4: (a) 0.111328    (b) 0.004872    0.995128\n# 10: (c) 0.384 If you have the right answer to (c), then you should be able to figure out the rest (see (e)).\n# 12: No.\nNTB #1: (a) 0.0799     (b) 0.07553     (c) 0.0655\n\nChapter 4\n\n#4: 0.25\n# 12: (a) 0.4285714     (b) 0.4285714     (c) 0.1428571"
  },
  {
    "objectID": "homework/HW3.html#non-textbook-problems-ntb",
    "href": "homework/HW3.html#non-textbook-problems-ntb",
    "title": "Homework 3",
    "section": "Non-textbook problems (NTB)",
    "text": "Non-textbook problems (NTB)\n\nA new drug is packaged to contain 30 pills in a bottle. Suppose that 98% of all bottles contain no defective pills, 1.5% contain one defective pill, and 0.5% contain two defective pills. Two pills from a bottle are randomly selected and tested. What is the probability that there are 2 defective pills in the bottle given that one of the two tested pills is defective?"
  },
  {
    "objectID": "homework/HW3.html#some-select-answers",
    "href": "homework/HW3.html#some-select-answers",
    "title": "Homework 3",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 5\n\nNTB #1: 0.392\n\nChapter 7\n\n# 2: \\(X\\in(0,\\infty)\\), continuous; \\(Y\\in\\{0,1,2,\\ldots\\}\\), discrete\n# 10: \\(X_j\\in[0,\\infty),j=1,\\ldots,100\\); \\(Y\\in[0,\\infty)\\); both continuous\n# 16: \\(Y\\) could be 0\n# 18: Yes, a r.v. can be both. Give an example!\n\nChapter 8\n\n# 2: (a) \\(p(x)=\\binom{7}{x}(.5)^7\\) for \\(x=0,1,2,\\ldots,7\\)\n# 9: (a) \\(c = \\frac{1}{8}\\)\n# 10:\n\n\n\n\n\\(x\\)\n2\n4\n6\n8\n\n\n\n\n\\(p(x)\\)\n3/10\n1/2\n3/20\n1/20"
  },
  {
    "objectID": "homework/HW4.html#non-textbook-problems-ntb",
    "href": "homework/HW4.html#non-textbook-problems-ntb",
    "title": "Homework 4",
    "section": "Non-textbook problems (NTB)",
    "text": "Non-textbook problems (NTB)\n\nThe following table shows the results of a survey in which the subjects were a sample of 300 adults residing in a certain metropolitan area. Each subject was asked to indicate which of three policies they favored with respect to smoking in public places. (Table is from Biostatistics: A Foundation for Analysis in the Health Sciences, 10th Edition, Daniel, Wayne W.; Cross, Chad L., pg. 630)\n\n\n\n\n\nLet \\(X=\\) highest education level and \\(Y=\\) policy favored. We can let \\(X=1\\) for college graduate, \\(X=2\\) for high-school graduate, etc., and similarly for \\(Y\\), or just keep the category names for the different levels of \\(X\\) and \\(Y\\)\n\nMake a table for the joint pmf \\(p_{X,Y}(x,y)\\) and briefly describe in words what the values are the probability of.\nFind the marginal pmf \\(p_{X}(x)\\) and briefly describe in words what the values are the probability of.\nFind the marginal pmf \\(p_{Y}(y)\\) and briefly describe in words what the values are the probability of.\nMake a table for the joint cdf \\(F_{X,Y}(x,y)\\) and briefly describe in words what the values are the probability of.\nFind the marginal cdf \\(F_{X}(x)\\) and briefly describe in words what the values are the probability of.\nFind the marginal cdf \\(F_{Y}(y)\\) and briefly describe in words what the values are the probability of.\nMake a table for the conditional pmf \\(p_{X|Y}(x|y)\\) and briefly describe in words what the values are the probability of.\nMake a table for the conditional pmf \\(p_{Y|X}(y|x)\\) and briefly describe in words what the values are the probability of.\n\nForgetful mornings revisited. Using the joint pmf you found in Chapter 9 #2, complete the following questions:\n\nFind the joint cdf of \\(X\\) and \\(Y\\) and briefly explain what \\(F_{X,Y}(x,y)\\) represents in the context of the problem.\nFind the conditional pmf \\(p_{Y|X}(y|x)\\).\n\nForgetful mornings revisited again. Recall from Chapter 9 #2, that \\(X\\) is the number of days until Maude loses her cell phone and each day she has a 1% chance of losing her phone (her behavior on different days being independent). For this problem, ignore the r.v. \\(Y\\), and consider the r.v. \\(X\\) on its own.\n\nWhat is the pmf of \\(X\\)?\nUse the pmf of \\(X\\) to find \\(\\mathbb{E}[X]\\).\n\nApproximately 10% of U.S. Veterans are women. Suppose an investigator plans a study with 4500 participants that are Veterans. How many women can they expect to be included? Your answer must be calculated by defining a random variable and showing how to calculate the expected value.\nCashews revisited. Recall from Chapter 10 #8, that a bowl contains 30 cashews, 20 pecans, 25 almonds, and 25 walnuts, and 3 nuts are randomly selected to eat (without replacement). Again, find the expected value of the number of cashews, but this time by defining the number of cashews as a sum of random variables."
  },
  {
    "objectID": "homework/HW4.html#some-select-answers",
    "href": "homework/HW4.html#some-select-answers",
    "title": "Homework 4",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 9\n\nNTB # 1 Partial answers:\n\n\n\\(p_{X|Y}(X=\\text{high school}| Y=\\text{no smoking at all}) = 0.476\\)\n\n\n\\(p_{Y|X}( Y=\\text{no smoking at all}|X=\\text{high school}) = 0.200\\)\n\n\n\nChapter 10\n\n# 6:  750.5\n# 8:  0.9\n# 10:   201\n# 14:   (a) 1.875     (b) 3.125     \n\nChapter 11\n\n# 2:  1.6\n# 18:  a) 48.5     (b) 96     \n# 20:  \\(\\approx\\) 23.077"
  },
  {
    "objectID": "homework/HW5.html#non-textbook-problems-ntb",
    "href": "homework/HW5.html#non-textbook-problems-ntb",
    "title": "Homework 5",
    "section": "Non-textbook problems (NTB)",
    "text": "Non-textbook problems (NTB)\n\nProve that for a r.v. \\(X\\) and constants \\(a\\) and \\(b\\), that \\[\\mathrm{Var}[aX+b]=a^2\\mathrm{Var}[X].\\] Note: you will not earn credit for citing this as a special case of a more general result.\nLet \\(\\bar{X}\\) be the random variable for the sample mean, \\(\\bar{X}=\\frac{\\sum_{i=1}^nX_i}{n}\\), where the \\(X_i\\) are i.i.d. random variables with common mean \\(\\mu\\) and variance \\(\\sigma^2\\).\n\nFind \\(\\mathbb{E}[\\bar{X}]\\).\nFind \\(Var[\\bar{X}]\\).\n\nLet \\(\\hat{p}\\) be the random variable for the sample proportion, \\(\\hat{p}=\\frac{X}{n}\\), where \\(X\\) is the number of successes in a random sample of size \\(n\\). Assume the probability of success is \\(p\\).\n\nFind \\(\\mathbb{E}[\\hat{p}]\\).\nFind \\(Var[\\hat{p}]\\).\n\nLet \\(X_i\\sim\\) Binomial(\\(n_i,p\\)) be independent r.v.’s for \\(i=1,\\ldots,m\\). What does the r.v. \\(X=\\sum_{i=1}^mX_i\\) count, and what is the distribution of \\(X\\)? Make sure to specify the parameters of \\(X\\)’s distribution.Find \\(\\mathbb{E}[X]\\). Make sure to show your work for (b) and (c). However, you may use without proof what you know about the mean and variance of each \\(X_i\\).Find \\(Var[X]\\).\nRead the Washington Post article The amazing woman who can smell Parkinson’s disease - before symptoms appear (http://www.washingtonpost.com/news/morning-mix/wp/2015/10/23/scottish-woman-detects-a-musky-smell-that-could-radically-improve-how-parkinsons-disease-is-diagnosed/)\nAssuming Joy Milne does not have the ability to detect Parkinson’s disease via smell, answer the following questions:\n\nWhat is the probability of her correctly detecting Parkinson’s by smelling one t-shirt?\nWhat is the probability of her correctly detecting Parkinson’s in 12 out of 12 t-shirts?\n\nLet \\(X_i\\sim\\) Negative Binomial(\\(r_i,p\\)) be independent r.v.’s for \\(i=1,\\ldots,m\\).\n\nWhat does the r.v. \\(X=\\sum_{i=1}^mX_i\\) count, and what is the distribution of \\(X\\)? Make sure to specify the parameters of \\(X\\)’s distribution.\nFind \\(\\mathbb{E}[X]\\). Make sure to show your work for (b) and (c). However, you may use without proof what you know about the mean and variance of each \\(X_i\\).\nFind \\(Var[X]\\)."
  },
  {
    "objectID": "homework/HW5.html#some-select-answers",
    "href": "homework/HW5.html#some-select-answers",
    "title": "Homework 5",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 12\n\n# 2:  64.8\n# 12:  1,096,357\n\nChapter 13\n\n# 4:  (a) 260/9     (b) 2.833     (c) \\(2.679\\times 10^{-5}\\)     (d) Same idea as (c) Replace 10’s with 100.     \n# 6:  (a) \\(p_X(x)=\\binom{4}{x}.3^x .7^{4-x}\\), for \\(x=0,1,\\ldots,4\\)     (d) 0.3483     (e) 0.9163     (f) 0.0233     (g) 1\n# 8:  (a) T     (b) F     (c) F     (d) F     (e) T     (f) T    (g) T\n# 10:  (a) T     (b) T    (c) F     (d) T    (e) T     (f) F    (g) T     (h) T (nonnegative instead of positive)     (i) F\n\nChapter 20\n\n# 2:  (a) 0.0001     (b) Discrete since \\(X\\) has a finite number of possible values. Uniform since each outcome is equally likely.     (c) \\(X\\) = randomly selected 4-digit ID#; \\(X=0000,0001,\\ldots,9999\\)     (d) 5000.5     (e) 8,333,333.25\n\nChapter 15\n\n# 18  (a) Bin(21,0.65)     (b) 4.78     \n\nChapter 16\n\n# 8  (c) \\(1.03\\times 10^{-6}\\)    (d) 10 questions: 91.43 minutes    \n\nChapter 17\n\n# 6   (a) 400, 87.18     (b) No     \n# 12   (c) 0.8000"
  },
  {
    "objectID": "homework/HW6.html#some-select-answers",
    "href": "homework/HW6.html#some-select-answers",
    "title": "Homework 6",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 19\n\n# 6:  (c) 15.625     (d) 0.0486     (f) 0.0488\n# 18:   100\n\nChapter 18\n\n# 24  (c) 0.8571\n# 26  162,754.8\n\nCalculus Review\n\n(a)  \\(c(\\frac{y^{2}}{2}+y^{2})\\)\n(b)  \\(\\frac{8}{9}xy^{2}+\\frac{5}{9}y^{4}\\)\n(c)  \\(\\frac{8}{9}x^{2}y+\\frac{20}{9}xy^{3}\\)\n(d)  \\(-2e^{-2y}+2e^{-y}\\)\n(e)  \\(xe^{-x}\\)\n(f)  \\(-\\frac{2}{3}(e^{-7x}-e^{-4x})\\)\n(g)  \\(\\frac{9}{2}\\)\n(h)  \\(\\frac{9}{2}\\)\n(i)  \\(\\frac{9}{2}\\)\n(j)  \\(\\frac{9}{2}\\)\n\nChapter 24\n\n# 2: (a) Discrete     (b) Discrete     (c) Continuous\n# 22: \\[f_X(x) = \\left\\{\n        \\begin{array}{ll}\n            0 & \\quad x &lt;0 \\\\\n            \\frac{7x}{4} & \\quad 0\\leq x\\leq 1 \\\\\n            0 & \\quad 1&lt; x&lt; 7 \\\\\n            \\frac{1}{8} & \\quad 7\\leq x\\leq 8 \\\\\n            0 & \\quad  x&gt;8 \\\\\n        \\end{array}\n    \\right.\\]\n\nChapter 25\n\n# 4:   7/16\n# 8:  (a) \\(\\frac{25}{228}\\)     (b) \\(f_X(x)=\\frac{1}{12}(x+1)\\), for \\(0\\leq x\\leq 4\\)     (c) \\(f_Y(y)=\\frac{3}{76}(y^2+1)\\), for \\(0\\leq y\\leq 4\\)\n# 18:  5/6\n# 24:  (a) \\(f_X(x)=-2e^{-2x}+2e^{-x}\\), for \\(x\\geq 0\\)     (b) \\(f_Y(y)=2e^{-2y}\\), for \\(y\\geq 0\\)\n\nChapter 26\n\n# 12:  (b) \\(\\frac{233}{256}\\)     (c) \\(\\frac{65}{256}\\)     (d) \\(\\frac{1}{512}\\)\n# 20:  (a) Yes.     (b) \\(\\frac{15}{16}\\)\nNTB # 3: (b) 0.09999546   (d) \\(f_Z(z) =\\Big(\\frac{11}{5} - \\frac{2z}{5}\\Big)e^{-2z}\\), for what values of \\(z\\)?\n\nChapter 27\n\n# 6: \\(f_{X|Y}(x|y)=\\frac{e^{-x/4-y/5}}{4(e^{-y/5}-e^{-9y/20})}\\), for \\(0&lt; x&lt; y\\)\n# 8: \\(f_{X|Y}(x|y)=\\frac{1-x^2}{1-y-\\frac{(1-y)^3}{3}}\\), for \\(0\\leq x, 0\\leq y, x+y\\leq 1\\)\n# 12: (a) \\(f_{X|Y}(x|y)=\\frac{1}{2}\\)    (c) \\(\\frac{4}{7}\\)"
  },
  {
    "objectID": "homework/HW8.html#non-textbook-problems-ntb",
    "href": "homework/HW8.html#non-textbook-problems-ntb",
    "title": "Homework 7",
    "section": "Non-textbook problems (NTB)",
    "text": "Non-textbook problems (NTB)\n\nLet \\(f_X(x)=\\lambda e^{-\\lambda x}\\) for \\(x&gt;0\\), where \\(\\lambda&gt;0\\).\n\nShow \\(Var[X]=\\frac{1}{\\lambda^2}\\). You may use the result from class for \\(\\mathbb{E}[X]\\) without first proving it.\n\nA shipping company handles containers in three different sizes: (1) 27 \\(ft^3\\) (3 x 3 x 3), (2) 125 \\(ft^3\\), and (3) 512 \\(ft^3\\). Let \\(X_i\\) (\\(i = 1, 2, 3\\)) denote the number of type \\(i\\) containers shipped during a given week. Suppose that \\(\\mu_1 =200,\\sigma_1=10,\\mu_2 =250,\\sigma_2=12,\\mu_3 =100,\\sigma_3=8\\).\n\nAssuming that \\(X_1,X_2,X_3\\) are independent, calculate the expected value and variance of the total volume shipped.\nWould your calculations necessarily be correct if the \\(X_i\\)’s were not independent? Explain.\n\nSuppose your waiting time for a bus in the morning is uniformly distributed on [0, 8] (minutes), whereas waiting time in the evening is uniformly distributed on [0, 10] (minutes) independent of morning waiting time. Make sure to FIRST set up an equation for calculating the total waiting time in each question before calculating the mean and variance of the total waiting time. You may use results from class for the expected value and variance of uniform r.v.’s without proving them.\n\nIf you take the bus each morning and evening for a week (7 days), what is your total expected waiting time?\nWhat is the variance of your total waiting time?\nWhat are the expected value and variance of the difference between morning and evening waiting times on a given day?\nWhat are the expected value and variance of the difference between total morning waiting time and total evening waiting time for a particular week?\n\nSuppose that voters arrive at a polling station at the rate of 120 per hour.For each of the following parts, give the name and parameter(s) of the distribution to be used to model the event and set up the expression to find the specified probability.You do not need to compute the probability.\n\nThe probability that the next voter will arrive in less than 30 seconds.\nThe probability that 200 voters will arrive within two hours of each other.\nThe probability that the \\(50^{th}\\) voter will arrive in between 15 and 30 minutes.\n\nThe automatic opening device of a military cargo parachute has been designed to open when the parachute is 200 m above the ground. Suppose opening altitude actually has a normal distribution with mean value 200 m and standard deviation 30 m. Equipment damage will occur if the parachute opens at an altitude of less than 100 m. What is the probability that there is equipment damage to the payload of at least one of the five independentIy dropped parachutes?\nLet \\(R_X(t)=\\ln(M_X(t))\\). Show that Var\\((X)=R''_X(0)\\).\nThe mgf for a Gamma distribution is \\(M_X(t)=\\frac{1}{(1-t/\\lambda)^r}\\). Use the mgf of an Exponential distribution (from #43.9), to show that the sum of \\(n\\) i.i.d. Exponential(\\(\\lambda)\\) random variables has a Gamma(\\(r,\\lambda\\)) distribution.\nUse the mgf of a Poisson distribution to find the mgf of the following distributions. If the mgf is that of a common named distribution, then name the distribution and state its parameter(s).\n\nThe distribution of \\(\\sum_{i=1}^nX_i\\), if \\(X_i\\sim\\)Poisson\\((\\lambda_i)\\) and are independent.\nThe distribution of \\(\\sum_{i=1}^3X_i\\), if \\(X_i\\sim\\)Poisson\\((\\lambda)\\) and are independent (i.i.d. in this case).\nThe distribution of \\(3X\\), if \\(X\\sim\\)Poisson\\((\\lambda)\\).\nWhy are the answers to (b) and (c) different?\n\nUsing mgf’s, show that the sum of \\(n\\) i.i.d. Chi Square random variables with one degree of freedom (\\(\\chi^2_{(1)}\\)) r.v.’s has a Chi Square with \\(n\\) degrees of freedom (\\(\\chi^2_{(n)}\\)) distribution.\nHint: First, look up the pdf of a \\(\\chi^2_{(n)}\\). This is a special case of the Gamma distribution with what parameters? Based on that and the information from # 7 above, you can determine what the mgf of a \\(\\chi^2_{(n)}\\) is, which will help you determine whether the mgf of the sum of \\(n\\) i.i.d. \\(\\chi^2_{(1)}\\) r.v.’s has a \\(\\chi^2_{(n)}\\) distribution."
  },
  {
    "objectID": "homework/HW8.html#some-select-answers",
    "href": "homework/HW8.html#some-select-answers",
    "title": "Homework 7",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 28\n\n# 10: (a) 8/9     (b) 14/3     \n# 18: 4/5\n\nChapter 29\n\n# 10: (a) 26/81     (b) 74/9\n# 14: (a) 67/3     (b) 1/14     (c) 25/12     (d) \\(\\sqrt{25/12}\\)\n# 26: 250\n# 32: See notes (or book) for the proof from the discrete random variables case. The proof doesn’t depend on what type of random variable (discrete vs. continuous) is being used.\nNTB # 3: (a) 63     (b) 287/3     (c) -1, 41/3     (d) -7, 287/3\n\nChapter 30\n\n# 4: \\(f_x(x)=1/2\\) for \\(2\\leq x\\leq 4\\)\n# 8: (a) T     (b) T     (c) F\n# 10: (a) F     (b) T\n# 12: (a) T     (b) T     (c) F     (d) T\n\nChapter 31\n\n# 14: (a) 0.25     (b) 0.02887     (c) 0.063     (d) 0.0145     (e) 0.01625     (f) 0.0055     (f) 6.195    (g) 0.00433     (h) 61.95     (i) 0.0433\n# 17: 2.25\n# 18: 7/15\n\nChapter 32\n\n# 8: 0.2526\n# 5: 0.8047\n# 10: 0.4323\n\nChapter 33\n\n#10: (a) \\(f_x(x)=\\frac{x}{9}e^{-x/3}\\) for \\(x&gt; 0\\)     (b) 0.4963\n\nChapter 35\n\n# 6: (a) 0     (b) -1.13     (c) \\(\\pm 0.32\\)\n# 10: (a) 0.0475     (b) 0.0475     (c) 0.2283     (d) 68.97 to 81.03     (e) 48 to 102     (f) 68.97\n# 24: (a) 0.2119     (b) 0.0011\nNTB # 5:   0.002\n\nChapter 43\n\nNTB # __: (a) Poisson\\((\\sum_{i=1}^n \\lambda_i)\\)     (b) Poisson\\((3\\lambda)\\)      (c) \\(M_{3X}(t)=e{\\lambda(e^{3t}-1)}\\) This is not an mgf of a common probability distribution.      (d) In (b) we are adding independent r.v.’s \\(X_i\\), while in (c) we are adding dependent r.v.’s (\\(3X=X+X+X\\); \\(X\\) is dependent with itself).\n\nChapter 36\n\n# 4: 0.0044\n# 12: (a) 0.9525     (b) 0.7939     (c) 0.7939\n# 14: 0.5911\n# 16: (a) \\(R=8.225\\sigma+25\\mu\\)     (b) \\(R=16.45\\sigma+100\\mu\\)     (c) \\(R=164.5\\sigma+10,000\\mu\\)     (d) \\(R=1.645\\sqrt{n}\\sigma+n\\mu\\)\n\nChapter 37\n\n# 2: 0.8869\n# 4: 0.0023\n# 20: 0.3936\n# 24: 0.4562\n# 30: (b) 0.0022     (c) \\(478.696\\approx 479\\)"
  },
  {
    "objectID": "homework/HW8.html#footnotes",
    "href": "homework/HW8.html#footnotes",
    "title": "Homework 7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI recommend doing the Chapter 29 Extra Problems in the order listed.↩︎\nAssume \\(X\\) and \\(Y\\) are independent.↩︎\nInclude in your answer an explanation as to why we need the condition that \\(t&lt;\\lambda\\).↩︎\nDo parts (a)-(c) below for #10 and #12:\n\nAnswer the question using the mgf \\(M_X(t)\\) as instructed in the book.\nAnswer the question using \\(R_X(t)\\) (as defined in class, and NTB # 6below).\nWhich method did you prefer? Why?\n\n↩︎\nDo parts (a)-(c) below for #10 and #12:\n\nAnswer the question using the mgf \\(M_X(t)\\) as instructed in the book.\nAnswer the question using \\(R_X(t)\\) (as defined in class, and NTB # 6 below).\nWhich method did you prefer? Why?\n\n↩︎\nAssume the distances between the cars are independent.↩︎"
  },
  {
    "objectID": "homework/HW7.html#non-textbook-problems-ntb",
    "href": "homework/HW7.html#non-textbook-problems-ntb",
    "title": "Homework 7",
    "section": "Non-textbook problems (NTB)",
    "text": "Non-textbook problems (NTB)\n\nLet \\(f_X(x)=\\lambda e^{-\\lambda x}\\) for \\(x&gt;0\\), where \\(\\lambda&gt;0\\).\n\nShow \\(Var[X]=\\frac{1}{\\lambda^2}\\). You may use the result from class for \\(\\mathbb{E}[X]\\) without first proving it.\n\nA shipping company handles containers in three different sizes: (1) 27 \\(ft^3\\) (3 x 3 x 3), (2) 125 \\(ft^3\\), and (3) 512 \\(ft^3\\). Let \\(X_i\\) (\\(i = 1, 2, 3\\)) denote the number of type \\(i\\) containers shipped during a given week. Suppose that \\(\\mu_1 =200,\\sigma_1=10,\\mu_2 =250,\\sigma_2=12,\\mu_3 =100,\\sigma_3=8\\).\n\nAssuming that \\(X_1,X_2,X_3\\) are independent, calculate the expected value and variance of the total volume shipped.\nWould your calculations necessarily be correct if the \\(X_i\\)’s were not independent? Explain.\n\nSuppose your waiting time for a bus in the morning is uniformly distributed on [0, 8] (minutes), whereas waiting time in the evening is uniformly distributed on [0, 10] (minutes) independent of morning waiting time. Make sure to FIRST set up an equation for calculating the total waiting time in each question before calculating the mean and variance of the total waiting time. You may use results from class for the expected value and variance of uniform r.v.’s without proving them.\n\nIf you take the bus each morning and evening for a week (7 days), what is your total expected waiting time?\nWhat is the variance of your total waiting time?\nWhat are the expected value and variance of the difference between morning and evening waiting times on a given day?\nWhat are the expected value and variance of the difference between total morning waiting time and total evening waiting time for a particular week?\n\nSuppose that voters arrive at a polling station at the rate of 120 per hour.For each of the following parts, give the name and parameter(s) of the distribution to be used to model the event and set up the expression to find the specified probability.You do not need to compute the probability.\n\nThe probability that the next voter will arrive in less than 30 seconds.\nThe probability that 200 voters will arrive within two hours of each other.\nThe probability that the \\(50^{th}\\) voter will arrive in between 15 and 30 minutes.\n\nThe automatic opening device of a military cargo parachute has been designed to open when the parachute is 200 m above the ground. Suppose opening altitude actually has a normal distribution with mean value 200 m and standard deviation 30 m. Equipment damage will occur if the parachute opens at an altitude of less than 100 m. What is the probability that there is equipment damage to the payload of at least one of the five independentIy dropped parachutes?\nLet \\(R_X(t)=\\ln(M_X(t))\\). Show that Var\\((X)=R''_X(0)\\).\nThe mgf for a Gamma distribution is \\(M_X(t)=\\frac{1}{(1-t/\\lambda)^r}\\). Use the mgf of an Exponential distribution (from #43.9), to show that the sum of \\(n\\) i.i.d. Exponential(\\(\\lambda)\\) random variables has a Gamma(\\(r,\\lambda\\)) distribution.\nUse the mgf of a Poisson distribution to find the mgf of the following distributions. If the mgf is that of a common named distribution, then name the distribution and state its parameter(s).\n\nThe distribution of \\(\\sum_{i=1}^nX_i\\), if \\(X_i\\sim\\)Poisson\\((\\lambda_i)\\) and are independent.\nThe distribution of \\(\\sum_{i=1}^3X_i\\), if \\(X_i\\sim\\)Poisson\\((\\lambda)\\) and are independent (i.i.d. in this case).\nThe distribution of \\(3X\\), if \\(X\\sim\\)Poisson\\((\\lambda)\\).\nWhy are the answers to (b) and (c) different?\n\nUsing mgf’s, show that the sum of \\(n\\) i.i.d. Chi Square random variables with one degree of freedom (\\(\\chi^2_{(1)}\\)) r.v.’s has a Chi Square with \\(n\\) degrees of freedom (\\(\\chi^2_{(n)}\\)) distribution.\nHint: First, look up the pdf of a \\(\\chi^2_{(n)}\\). This is a special case of the Gamma distribution with what parameters? Based on that and the information from # 7 above, you can determine what the mgf of a \\(\\chi^2_{(n)}\\) is, which will help you determine whether the mgf of the sum of \\(n\\) i.i.d. \\(\\chi^2_{(1)}\\) r.v.’s has a \\(\\chi^2_{(n)}\\) distribution."
  },
  {
    "objectID": "homework/HW7.html#some-select-answers",
    "href": "homework/HW7.html#some-select-answers",
    "title": "Homework 7",
    "section": "Some select answers",
    "text": "Some select answers\nSelected answers (or hints) not provided at the end the book:\n\nChapter 28\n\n# 10: (a) 8/9     (b) 14/3     \n# 18: 4/5\n\nChapter 29\n\n# 10: (a) 26/81     (b) 74/9\n# 14: (a) 67/3     (b) 1/14     (c) 25/12     (d) \\(\\sqrt{25/12}\\)\n# 26: 250\n# 32: See notes (or book) for the proof from the discrete random variables case. The proof doesn’t depend on what type of random variable (discrete vs. continuous) is being used.\nNTB # 3: (a) 63     (b) 287/3     (c) -1, 41/3     (d) -7, 287/3\n\nChapter 30\n\n# 4: \\(f_x(x)=1/2\\) for \\(2\\leq x\\leq 4\\)\n# 8: (a) T     (b) T     (c) F\n# 10: (a) F     (b) T\n# 12: (a) T     (b) T     (c) F     (d) T\n\nChapter 31\n\n# 14: (a) 0.25     (b) 0.02887     (c) 0.063     (d) 0.0145     (e) 0.01625     (f) 0.0055     (f) 6.195    (g) 0.00433     (h) 61.95     (i) 0.0433\n# 17: 2.25\n# 18: 7/15\n\nChapter 32\n\n# 8: 0.2526\n# 5: 0.8047\n# 10: 0.4323\n\nChapter 33\n\n#10: (a) \\(f_x(x)=\\frac{x}{9}e^{-x/3}\\) for \\(x&gt; 0\\)     (b) 0.4963\n\nChapter 35\n\n# 6: (a) 0     (b) -1.13     (c) \\(\\pm 0.32\\)\n# 10: (a) 0.0475     (b) 0.0475     (c) 0.2283     (d) 68.97 to 81.03     (e) 48 to 102     (f) 68.97\n# 24: (a) 0.2119     (b) 0.0011\nNTB # 5:   0.002\n\nChapter 43\n\nNTB # __: (a) Poisson\\((\\sum_{i=1}^n \\lambda_i)\\)     (b) Poisson\\((3\\lambda)\\)      (c) \\(M_{3X}(t)=e{\\lambda(e^{3t}-1)}\\) This is not an mgf of a common probability distribution.      (d) In (b) we are adding independent r.v.’s \\(X_i\\), while in (c) we are adding dependent r.v.’s (\\(3X=X+X+X\\); \\(X\\) is dependent with itself).\n\nChapter 36\n\n# 4: 0.0044\n# 12: (a) 0.9525     (b) 0.7939     (c) 0.7939\n# 14: 0.5911\n# 16: (a) \\(R=8.225\\sigma+25\\mu\\)     (b) \\(R=16.45\\sigma+100\\mu\\)     (c) \\(R=164.5\\sigma+10,000\\mu\\)     (d) \\(R=1.645\\sqrt{n}\\sigma+n\\mu\\)\n\nChapter 37\n\n# 2: 0.8869\n# 4: 0.0023\n# 20: 0.3936\n# 24: 0.4562\n# 30: (b) 0.0022     (c) \\(478.696\\approx 479\\)"
  },
  {
    "objectID": "homework/HW7.html#footnotes",
    "href": "homework/HW7.html#footnotes",
    "title": "Homework 7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI recommend doing the Chapter 29 Extra Problems in the order listed.↩︎\nAssume \\(X\\) and \\(Y\\) are independent.↩︎\nInclude in your answer an explanation as to why we need the condition that \\(t&lt;\\lambda\\).↩︎\nDo parts (a)-(c) below for #10 and #12:\n\nAnswer the question using the mgf \\(M_X(t)\\) as instructed in the book.\nAnswer the question using \\(R_X(t)\\) (as defined in class, and NTB # 6below).\nWhich method did you prefer? Why?\n\n↩︎\nDo parts (a)-(c) below for #10 and #12:\n\nAnswer the question using the mgf \\(M_X(t)\\) as instructed in the book.\nAnswer the question using \\(R_X(t)\\) (as defined in class, and NTB # 6 below).\nWhich method did you prefer? Why?\n\n↩︎\nAssume the distances between the cars are independent.↩︎"
  },
  {
    "objectID": "homeworks.html#homework",
    "href": "homeworks.html#homework",
    "title": "Homework and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n10/5/23\n\n\nHomework 1\n\n\n4 min\n\n\n\n\n10/12/23\n\n\nHomework 2\n\n\n5 min\n\n\n\n\n10/19/23\n\n\nHomework 3\n\n\n5 min\n\n\n\n\n10/26/23\n\n\nHomework 4\n\n\n3 min\n\n\n\n\n11/2/23\n\n\nHomework 5\n\n\n5 min\n\n\n\n\n11/16/23\n\n\nHomework 6\n\n\n7 min\n\n\n\n\n11/30/23\n\n\nHomework 7\n\n\n11 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "homeworks.html#homework-solutions",
    "href": "homeworks.html#homework-solutions",
    "title": "Homework and Solutions",
    "section": "Homework Solutions",
    "text": "Homework Solutions\n\n\n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "homeworks.html#assignments",
    "href": "homeworks.html#assignments",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n1/11/24\n\n\nHomework 0\n\n\n4 min\n\n\n\n\n1/25/24\n\n\nHomework 1\n\n\n7 min\n\n\n\n\n2/1/24\n\n\nHomework 2\n\n\n7 min\n\n\n\n\n2/15/24\n\n\nHomework 3\n\n\n8 min\n\n\n\n\n2/25/24\n\n\nHomework 4\n\n\n6 min\n\n\n\n\n3/7/24\n\n\nHomework 5\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "homeworks.html#solutions",
    "href": "homeworks.html#solutions",
    "title": "Homework Assignments and Solutions",
    "section": "Solutions",
    "text": "Solutions\nPlease note that you need to download the .html file to see the LaTeX math properly.\n\n\n\nHomework\n.qmd file\n.html file\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5"
  },
  {
    "objectID": "schedule/week_02_sched.html#homework",
    "href": "schedule/week_02_sched.html#homework",
    "title": "Week 2",
    "section": "Homework",
    "text": "Homework\nHomework 1 due 10/5"
  },
  {
    "objectID": "schedule/week_02_sched.html#post-class-surveys",
    "href": "schedule/week_02_sched.html#post-class-surveys",
    "title": "Week 2",
    "section": "Post-Class Surveys",
    "text": "Post-Class Surveys\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_02_sched.html#statistician-of-the-week-regina-nuzzo",
    "href": "schedule/week_02_sched.html#statistician-of-the-week-regina-nuzzo",
    "title": "Week 2",
    "section": "Statistician of the Week: Regina Nuzzo",
    "text": "Statistician of the Week: Regina Nuzzo"
  },
  {
    "objectID": "schedule/syllabus.html",
    "href": "schedule/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course is designed to introduce history, concepts and distributions in probability, Monte Carlo simulation techniques, and Markov chains. Students will also learn how to write R codes for various statistical computations and plots. Previous experience in R is not required. R is free software available from http://www.r-project.org.\n\n\nAt the end of this course, students should be able to…\n\nUnderstand basic concepts in probability\nCompute probabilities for random variables\nCompute probabilities using basic distributions\nPerform statistical computations and simulations using R"
  },
  {
    "objectID": "schedule/syllabus.html#textbook",
    "href": "schedule/syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\n\nIntroduction to Probability\n\nAuthors: Mark Daniel Ward and Ellen Gundlach\nPublisher: W. H. Freeman\nEdition: 1st\nISBN-13: 978-0716771098\nTextbook in Sakai\n\n\n\nSupplemental Readings (Optional)\n\nStatistical Inference, Casella and Berger, 2nd ed. (This will be the textbook for BSTA 551-552 Math Stat.)\nIntroduction to Probability, Charles M. Grinstead and J. Laurie Snell, http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/pdf.html\nProbability With Applications and R, Robert P. Dobrow, Wiley 2013 (eBook available from OHSU library)\nAn Introduction to R (free pdf available from http://cran.r-project.org/manuals.html)"
  },
  {
    "objectID": "schedule/syllabus.html#online-resources",
    "href": "schedule/syllabus.html#online-resources",
    "title": "Syllabus",
    "section": "Online Resources",
    "text": "Online Resources\n\nSlack\nWe will use Slack as our main form of communication for the class. If you are unsure how to do a homework problem or have other questions, please ask myself or the TAs either by email or posting your question(s) on Slack. Please know that Slack is not guarded by the OSHU firewall, so if you have a question about accommodations or any sensitive topics, you may wish to message me via email. You can still message me regarding sensitive information on Slack, but I will not initiate those conversations on Slack.\n**Please use this invitation link for our Slack workspace!**\n\nTips on asking questions\n\nIf you are unsure how to do a homework problem or have other questions, please ask myself or the TA’s either by email or posting your question(s) on Slack. \nWhen reaching out for help for a homework problem, please include some context on what you have already tried.\n\nFor example, including a photo of your work thus far with an explanation of where you think you might be wrong is a quick way for me to look it over your work and help you troubleshoot. This helps me see what parts you already understand and which you need help with.\nIf you are unsure how to start a problem, look through your notes and the book for examples that you think might be similar. When reaching out, mention these examples and why you think they might be helpful, but also why you are still unsure on how to proceed.\nIf you only write something similar to “I don’t know how to do problem xxx,” then my response will be to ask you what you tried so far. Thus, it will be quicker for you to let me know this information right away.\n\nIf asking for help about a specific example that you don’t understand, please also provide some detail beyond “I don’t understand example xxx.”\n\nWhich steps of the problem do you not understand? You can refer to a line number, for example.\nIf you don’t understand the first step, do you understand the ones following it?\n\nIn general, when asking a question, please provide the homework number it is from along with the chapter and problem number. If it’s an example from the notes or book, an example number or slide number will help finding it.\nWant more tips on asking questions? This list below is a good start. It’s from https://www.weareteachers.com/8-ways-to-pose-better-questions-in-math-class/.\nFinally, if I don’t respond within a day and you need a response soon, please remind me by emailing or messaging me again.\n\n\n\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nExplain Everything\nI plan to use Explain Everything on my iPad to deliver lectures. This application allows me to take real-time notes, access Poll Everywhere, and record the lecture. I hope to use these features to allow you to follow me during lecture and have access to a recording for asynchronous viewing. While I will try to always make a recorded lecture available to you after class, I want you to try to attend class in person. I understand that life events get in the way of in-person attendance, but your attendance in-person brings me joy while I teach, and then further motivates me to be a great teacher. \nNote: I am new to this application so I ask you for some grace as I learn to navigate it. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, we recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 414 Website\nPennState has a class offered to advanced undergraduates that has some overlap with our class. They have all their course notes posted on thispage. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "schedule/syllabus.html#types-of-assessments",
    "href": "schedule/syllabus.html#types-of-assessments",
    "title": "Syllabus",
    "section": "Types of assessments",
    "text": "Types of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "schedule/syllabus.html#breakdown",
    "href": "schedule/syllabus.html#breakdown",
    "title": "Syllabus",
    "section": "Breakdown",
    "text": "Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade\n\n\nHomework\nFormative\nApprox. weekly\n55%\n\n\nPost-class survey\nN/A\nTwice Weekly\n5%\n\n\nMidterm Exam\nSummative\n\n20%\n\n\nFinal Exam\nSummative\n\n20%\n\n\n\n\n\nCriteria\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn something in (whether it is incomplete, complete, correct, or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nAfter the due date, either the TA or myself will give you feedback (on one or more complete problems) and post the solutions.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points.\n\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on December 8, 2023. All coursework is expected to be completed by then. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me asking for feedback for your late homework.\nFor non-homework assignments, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class and participate in-class polls and the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. This is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket to demonstrate attendance."
  },
  {
    "objectID": "schedule/syllabus.html#ongoing-course-feedback",
    "href": "schedule/syllabus.html#ongoing-course-feedback",
    "title": "Syllabus",
    "section": "Ongoing Course Feedback",
    "text": "Ongoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 513/613 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith for my race. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material."
  },
  {
    "objectID": "schedule/syllabus.html#midterm-feedback",
    "href": "schedule/syllabus.html#midterm-feedback",
    "title": "Syllabus",
    "section": "Midterm Feedback",
    "text": "Midterm Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your midterm exam grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback."
  },
  {
    "objectID": "schedule/syllabus.html#final-course-feedback",
    "href": "schedule/syllabus.html#final-course-feedback",
    "title": "Syllabus",
    "section": "Final Course Feedback",
    "text": "Final Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement."
  },
  {
    "objectID": "schedule/syllabus.html#instructor-expectations",
    "href": "schedule/syllabus.html#instructor-expectations",
    "title": "Syllabus",
    "section": "Instructor Expectations",
    "text": "Instructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai."
  },
  {
    "objectID": "schedule/syllabus.html#student-expectations-and-resources",
    "href": "schedule/syllabus.html#student-expectations-and-resources",
    "title": "Syllabus",
    "section": "Student Expectations and Resources",
    "text": "Student Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the semester so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact the instructor or any of the TAs.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of papers or homework, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "schedule/syllabus.html#course-learning-objectives",
    "href": "schedule/syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nAt the end of this course, students should be able to…\n\nUnderstand basic concepts in probability\nCompute probabilities for random variables\nCompute probabilities using basic distributions\nPerform statistical computations and simulations using R"
  },
  {
    "objectID": "schedule/syllabus.html#description",
    "href": "schedule/syllabus.html#description",
    "title": "Syllabus",
    "section": "",
    "text": "This course is designed to introduce history, concepts and distributions in probability, Monte Carlo simulation techniques, and Markov chains. Students will also learn how to write R codes for various statistical computations and plots. Previous experience in R is not required. R is free software available from http://www.r-project.org.\n\n\nAt the end of this course, students should be able to…\n\nUnderstand basic concepts in probability\nCompute probabilities for random variables\nCompute probabilities using basic distributions\nPerform statistical computations and simulations using R"
  },
  {
    "objectID": "schedule/syllabus.html#instructors",
    "href": "schedule/syllabus.html#instructors",
    "title": "Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page."
  },
  {
    "objectID": "schedule/syllabus.html#meeting-times",
    "href": "schedule/syllabus.html#meeting-times",
    "title": "Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays         10:30 AM – 12:00 PM PST in room RLSB 1S008\nWednesdays   10:30 AM – 12:00 PM PST in room RLSB 1S008\n\nKnown Exceptions\nWednesday, October 25       10:30 AM – 12:00 PM PST on Webex OR pre-recorded\nWednesday, November 22    No class"
  },
  {
    "objectID": "schedule/syllabus.html#materials",
    "href": "schedule/syllabus.html#materials",
    "title": "Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbook\n\nIntroduction to Probability\n\nAuthors: Mark Daniel Ward and Ellen Gundlach\nPublisher: W. H. Freeman\nEdition: 1st\nISBN-13: 978-0716771098\nTextbook in Sakai\n\n\n\nSupplemental Readings (Optional)\n\nStatistical Inference, Casella and Berger, 2nd ed. (This will be the textbook for BSTA 551-552 Math Stat.)\nIntroduction to Probability, Charles M. Grinstead and J. Laurie Snell, http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/pdf.html\nProbability With Applications and R, Robert P. Dobrow, Wiley 2013 (eBook available from OHSU library)\nAn Introduction to R (free pdf available from http://cran.r-project.org/manuals.html)\n\n\n\n\nOnline Resources\n\nSlack\nWe will use Slack as our main form of communication for the class. If you are unsure how to do a homework problem or have other questions, please ask myself or the TAs either by email or posting your question(s) on Slack. Please know that Slack is not guarded by the OSHU firewall, so if you have a question about accommodations or any sensitive topics, you may wish to message me via email. You can still message me regarding sensitive information on Slack, but I will not initiate those conversations on Slack.\n**Please use this invitation link for our Slack workspace!**\n\nTips on asking questions\n\nIf you are unsure how to do a homework problem or have other questions, please ask myself or the TA’s either by email or posting your question(s) on Slack. \nWhen reaching out for help for a homework problem, please include some context on what you have already tried.\n\nFor example, including a photo of your work thus far with an explanation of where you think you might be wrong is a quick way for me to look it over your work and help you troubleshoot. This helps me see what parts you already understand and which you need help with.\nIf you are unsure how to start a problem, look through your notes and the book for examples that you think might be similar. When reaching out, mention these examples and why you think they might be helpful, but also why you are still unsure on how to proceed.\nIf you only write something similar to “I don’t know how to do problem xxx,” then my response will be to ask you what you tried so far. Thus, it will be quicker for you to let me know this information right away.\n\nIf asking for help about a specific example that you don’t understand, please also provide some detail beyond “I don’t understand example xxx.”\n\nWhich steps of the problem do you not understand? You can refer to a line number, for example.\nIf you don’t understand the first step, do you understand the ones following it?\n\nIn general, when asking a question, please provide the homework number it is from along with the chapter and problem number. If it’s an example from the notes or book, an example number or slide number will help finding it.\nWant more tips on asking questions? This list below is a good start. It’s from https://www.weareteachers.com/8-ways-to-pose-better-questions-in-math-class/.\nFinally, if I don’t respond within a day and you need a response soon, please remind me by emailing or messaging me again.\n\n\n\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nExplain Everything\nI plan to use Explain Everything on my iPad to deliver lectures. This application allows me to take real-time notes, access Poll Everywhere, and record the lecture. I hope to use these features to allow you to follow me during lecture and have access to a recording for asynchronous viewing. While I will try to always make a recorded lecture available to you after class, I want you to try to attend class in person. I understand that life events get in the way of in-person attendance, but your attendance in-person brings me joy while I teach, and then further motivates me to be a great teacher. \nNote: I am new to this application so I ask you for some grace as I learn to navigate it. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, we recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 414 Website\nPennState has a class offered to advanced undergraduates that has some overlap with our class. They have all their course notes posted on thispage. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "schedule/syllabus.html#assessment",
    "href": "schedule/syllabus.html#assessment",
    "title": "Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 6 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by TAs. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Thursday at 11 PM.\n\n\nMidterm Exam\nIn person\nOnce\nThe purpose of the midterm is to assess how well you have achieved the learning objectives through questions covering important concepts, conducting statistical processes, and interpreting output. We will have our midterm in-class, and it will be open book. Students must work on the exam independently.\n\n\nFinal Exam\nIn person\nOnce\nThe purpose of the final is to assess how well you have achieved the learning objectives through questions covering important concepts, conducting statistical processes, and interpreting output. We will have our final in-class, and it will be open book. Students must work on the exam independently. While the final exam is not explicitly cumulative, certain topics learned before the midterm will carry over to material in the later part of the course.\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom.\n\n\n\nBreakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade\n\n\nHomework\nFormative\nApprox. weekly\n55%\n\n\nPost-class survey\nN/A\nTwice Weekly\n5%\n\n\nMidterm Exam\nSummative\n\n20%\n\n\nFinal Exam\nSummative\n\n20%\n\n\n\n\n\nCriteria\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn something in (whether it is incomplete, complete, correct, or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nAfter the due date, either the TA or myself will give you feedback (on one or more complete problems) and post the solutions.\n\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "schedule/syllabus.html#course-instructor-evaluations",
    "href": "schedule/syllabus.html#course-instructor-evaluations",
    "title": "Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 513/613 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith for my race. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nMidterm Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your midterm exam grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback.\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement."
  },
  {
    "objectID": "schedule/syllabus.html#schedule",
    "href": "schedule/syllabus.html#schedule",
    "title": "Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "schedule/syllabus.html#how-to-succeed-in-this-course",
    "href": "schedule/syllabus.html#how-to-succeed-in-this-course",
    "title": "Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time where a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nMidterm / Final Exam\nSummative\n\nIdentify and achieve learning objectives in each lecture\nUnderstand why certain statistics tools are used for certain cases\nPractice problems that you struggled with\nCome to Office Hours for help with specific problems or concepts\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\nDo not ask for a regrade unless you have viewed the solutions\n\n\n\nClass Surveys\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "schedule/syllabus.html#course-policies-and-resources",
    "href": "schedule/syllabus.html#course-policies-and-resources",
    "title": "Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on December 8, 2023. All coursework is expected to be completed by then. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me asking for feedback for your late homework.\nFor non-homework assignments, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class and participate in-class polls and the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. This is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket to demonstrate attendance.\n\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the semester so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact the instructor or any of the TAs.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of papers or homework, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "schedule/syllabus.html#course-communications",
    "href": "schedule/syllabus.html#course-communications",
    "title": "Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai/Slack announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section. I will copy these announcements in Slack if they do not involve changes to the schedule. Unfortunately, there are certain announcements that OHSU requires I initiate behind the firewall.\nGeneral course questions\nIt is normal to have many questions about things that relate to the course, such as clarification about assignments, course materials, or assessments. Please post these on our Slack Workspace. Please use the channels that I created for questions. You are encouraged to give answers and help each other. The TAs and I will monitor these threads, so I will endorse or correct responses as needed. Please give us 24 hours to respond to questions within Monday-Friday. Work-life balance is important for us as well, so we will try to respond as quickly as we can within our healthy limits. \nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered. Do not send messages asking general information about the class; please post those on Slack instead."
  },
  {
    "objectID": "schedule/syllabus.html#further-student-resources",
    "href": "schedule/syllabus.html#further-student-resources",
    "title": "Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nGrammarly Subscription\nThe School of Public Health students have access to a subscription version of Grammarly. While Grammarly cannot improve the argument and flow of your work, it can help with spelling, grammar, and sentence structure. If you are interested in this tool, please add your name to this email form and we will get you added to the subscription.  Be sure to use your PSU login credentials to access the form.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,265 monthly, you may wish to enroll."
  },
  {
    "objectID": "schedule/syllabus.html#school-policies-and-resources",
    "href": "schedule/syllabus.html#school-policies-and-resources",
    "title": "Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook: https://ohsu-psu-sph.org/graduate/handbooks-policies-forms/.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’sResponse to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "schedule/syllabus.html#ohsu-competencies",
    "href": "schedule/syllabus.html#ohsu-competencies",
    "title": "Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "schedule/syllabus.html#institutional-policies-and-resources",
    "href": "schedule/syllabus.html#institutional-policies-and-resources",
    "title": "Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities:\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses:\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information:\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention:\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion:\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010:\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "slides/9_joint_distributions.qqmd.html",
    "href": "slides/9_joint_distributions.qqmd.html",
    "title": "Chapter 9: Independence and Conditioning - or, Joint Distributions",
    "section": "",
    "text": "Chapter 9: Independence and Conditioning - or, Joint Distributions\n\nDefinition 1. The joint pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[p_{X,Y}(x,y) = \\mathbb{P}(X=x\\ and\\ Y=y) = \\mathbb{P}(X=x, Y=y)\\]\n\n\nExample 2. Let \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)\n\n\nSolution:\nRemarks: Some properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)\n\n\n\nDefinition 3. The joint cdf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]\n\n\n\nExample 4. Find the joint cdf \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\) in Example 2.\n\nSolution:\n\nExample 5. Find the marginal cdfs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\) for Example 4.\n\nSolution:\n\nRemark: Some properties of joint cdf’s:\nIndependence and Conditioning\nRecall that for events \\(A\\) and \\(B\\),\n\n\\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\)\n\\(A\\) and \\(B\\) are independent if and only if\n\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\cdot\\mathbb{P}(B)\\)\n\n\nIndependence and conditioning are defined similarly for r.v.’s, since \\[p_X(x) = \\mathbb{P}(X=x)\\ \\mathrm{and}\\ \\ p_{X,Y}(x,y) = \\mathbb{P}(X = x ,Y = y).\\]\n\nDefinition 6. The conditional pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is defined as \\[p_{X|Y}(x|y) = \\mathbb{P}(X = x |Y = y) = \\frac{\\mathbb{P}(X = x\\ and\\ Y = y)}{\\mathbb{P}(Y = y)}\n=\\frac{p_{X,Y}(x,y) }{p_{Y}(y) }\\] if \\(p_{Y}(y) &gt; 0\\).\n\nRemark: The following properties follow from the conditional pmf definition:\n\nExample 7. Using \\(X\\) and \\(Y\\) from Example 2:\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\nSolution:\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counterexample.\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\).\n\n\nExample 8. Hypothetical 4-sided die\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\).\n\n\nSolution:\nExample 8 cont’d."
  },
  {
    "objectID": "schedule/week_03_sched.html#homework",
    "href": "schedule/week_03_sched.html#homework",
    "title": "Week 3",
    "section": "Homework",
    "text": "Homework\nHomework 1 due 10/1"
  },
  {
    "objectID": "schedule/week_03_sched.html#post-class-surveys",
    "href": "schedule/week_03_sched.html#post-class-surveys",
    "title": "Week 3",
    "section": "Post-Class Surveys",
    "text": "Post-Class Surveys\nSurvey link!!"
  },
  {
    "objectID": "schedule/week_03_sched.html#statistician-of-the-week-regina-nuzzo",
    "href": "schedule/week_03_sched.html#statistician-of-the-week-regina-nuzzo",
    "title": "Week 3",
    "section": "Statistician of the Week: Regina Nuzzo",
    "text": "Statistician of the Week: Regina Nuzzo\n\n\n\n\n\n\n\nRegina Nuzzo\n\n\n\n\n\n\nDr. Nuzzo received her PhD in Statistics from Stanford University and is now Professor of Science, Technology, & Mathematics at Gallaudet University. Gallaudet University, federally funded and located in Washington, DC, is the only higher education institution where all programs are designed for the education of the deaf and hard of hearing. Dr. Nuzzo teaches statistics using American Sign Language.\nShe is the Senior Advisor for Statistics Communication and Media Innovation at the American Statistical Association and a freelance writer.\n\n\n\nTopics covered\nDr. Nuzzo is a statistician and a science journalist. Her work has appeared in Nature, Los Angeles Times, New York Times, Reader’s Digest, New Scientist, and Scientific American. Most of her work is in the “Health” or “Science” sections of the aforementioned outlets. Primarily, she works to help lay-audiences understand science and statistics in particular. She earned the American Statistical Association’s 2014 Excellence in Statistical Reporting Award for her article on p-values in Nature. Her work led to the ASA’s statement on p-values.\n\n\nRelevant work\n\nNuzzo, R. “Scientific method: Statistical errors.” Nature 506, 150–152 (2014).\nNuzzo, R. “Tips for Communicating Statistical Significance.” Science, Health, and Public Trust, National Institutes of Health, 2018.\nNuzzo, R. “Vying for a soul mate? Psych out the competition with science.” Health: Features. Los Angeles Times, 2008.\n\n\n\nOutside links\n\nWikipedia\nacademic\nLinkedin\npersonal\n\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin. I also invite you to check out this youtube video of her Women Rise Keynote address where she discusses her hearing impairment, career growth, and her work with p-values."
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 512/612 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 512/612! In this course, we will focus on linear models, and build our understanding of regression analysis. We will build some theoretical understanding in order to interpret and apply regression models appropriately. We will learn how to build a regression model, interpret the model, and diagnose potential issues with our model.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nAnalyze real-world data to answer questions about multivariable relationships for a continuous outcome\nBuild, fit, and evaluate linear regression models\nAssess whether a proposed model is appropriate and describe its limitations\nUse R and Quarto to write reproducible reports\nCommunicate results from statistical analyses to a general audience\n\nThese learning objectives were adapted from Maria Tackett’s Regression Analysis course."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 512/612 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page. This also has office hours!"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 512/612 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          1:00 PM – 2:50 PM PST in RLSB 3A003 A/B\nWednesdays    1:00 PM – 2:50 PM PST in RLSB 3A003 A/B\nHere is the full schedule:\n\n\nKnown Exceptions\nMonday, February 26              1:00 PM – 2:50 PM PST on Webex OR pre-recorded\nWednesday, February 28        1:00 PM – 2:50 PM PST on Webex OR pre-recorded"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 512/612 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbooks\nIn lieu of a formally published textbook, we will be referencing the following two online textbooks. Similar to our class, the books integrate R into their lessons.\n\nA Progressive Introduction to Linear Models by Joshua French\nIntroduction to Regression Methods for Public Health Using R by Ramzi W. Nahhas\n\n\nSupplemental Readings (Optional)\n\nAn Introduction to R (free pdf available)\n\n\n\n\nOnline Resources\n\nSlack\nWe will use Slack as our main form of communication for the class. If you are unsure how to do a homework problem or have other questions, please ask me by posting your question(s) on Slack. Please know that Slack is not guarded by the OSHU firewall, so if you have a question about accommodations or any sensitive topics, you may wish to message me via email. You can still message me regarding sensitive information on Slack, but I will not initiate those conversations on Slack.\n**Please use this invitation link for our Slack workspace!**\nTips on asking questions:\n\nWhen reaching out for help for a homework problem, please include some context on what you have already tried.\n\nFor example, including a photo of your work thus far with an explanation of where you think you might be wrong is a quick way for me to look over your work and help you troubleshoot. This helps me see what parts you already understand and which you need help with. If your question involves code, please include the copied code (not a screenshot) and an attachment to your full file. This way I can see the exact line you need help with and the full code in case the problem starts earlier.\nIf you are unsure how to start a problem, look through your notes and the book for examples that you think might be similar. When reaching out, mention these examples and why you think they might be helpful, but also why you are still unsure on how to proceed.\nIf you only write something similar to “I don’t know how to do problem xxx,” then my response will be to ask you what you tried so far. Thus, it will be quicker for you to let me know this information right away.\n\nIf asking for help about a specific example that you don’t understand, please also provide some detail beyond “I don’t understand example xxx.”\n\nWhich steps of the problem do you not understand? You can refer to a line number, for example.\nIf you don’t understand the first step, do you understand the ones following it?\n\nIn general, when asking a question, please provide the homework number it is from along with the chapter and problem number. If it’s an example from the notes or book, an example number or slide number will help finding it.\nWant more tips on asking questions? This list on this site will help!\nFinally, if I don’t respond within a day and you need a response soon, please remind me by emailing or messaging me again.\n\n\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nExplain Everything\nI plan to use Explain Everything on my iPad to deliver lectures. This application allows me to take real-time notes, access Poll Everywhere, and record the lecture. I hope to use these features to allow you to follow me during lecture and have access to a recording for asynchronous viewing. While I will try to always make a recorded lecture available to you after class, I want you to try to attend class in person. I understand that life events get in the way of in-person attendance, but your attendance in-person brings me joy while I teach, and then further motivates me to be a great teacher. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, I recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 501 Website\nPennState has a class offered to online MS students that has some overlap with our class. They have all their course notes posted on this page. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 512/612 Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 7 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by me. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Thursday at 11 PM.\n\n\nQuizzes\nIn person\nEvery 3 weeks\nThe purpose of the quizzes is to assess how well you have achieved the learning objectives through questions covering important concepts, conducting statistical processes, and interpreting output. We will have our quizzes in-class, and it will be open book. Students must work on the quizzes independently.\n\n\nProject (Labs and Report)\nOnline\n4 labs, 1 final report\nThe project will be a combination of submitted labs that will span the quarter and one final report submitted at the end of the quarter. This is meant to translate the tools learned in the course to the work one may do in the workforce. This will help instill the procedure for shaping research goals, model selection, analyzing data, and interpreting meaningful results. Labs will guide you through the needed analysis and background for the project. The final report will summarize your work over the labs and more closely align with a journal article. Students will work independently on each lab.\n\n\n\n\n\n\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 512/612 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nMid-quarter Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback.\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 512/612 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "syllabus.html#how-to-succeed-in-this-course",
    "href": "syllabus.html#how-to-succeed-in-this-course",
    "title": "BSTA 512/612 Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nQuizzes\nSummative\n\nIdentify and achieve learning objectives in each lecture\nUnderstand why certain statistics tools are used for certain cases\nPractice testing yourself and others on concepts\nCome to Office Hours for help with specific problems or concepts\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\nDo not ask for a regrade unless you have viewed the solutions\n\n\n\nProject Labs and Report\nFormative and Summtive\n\nStart the lab as early as possible\nWork on R coding and check with classmates on work\nCome to Office Hours for help with specific R work\nFor the report, compile your work from the labs, and decide what is important in the analysis.\n\n\nThis will be graded at the end of the semester, so you will not have a chance to interact with my feedback as much\nIf you have questions about your grade, you may email me\nKeep the project paper for future reference\nYou can add this project to your resume!\n\n\n\nClass Exit Tickets\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on March 22, 2024. All coursework is expected to be completed by then. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me asking for feedback for your late homework.\nFor non-homework assignments, including labs, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. This is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our quizzes! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools, or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai/Slack announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section. I will copy these announcements in Slack if they do not involve changes to the schedule. Unfortunately, there are certain announcements that OHSU requires I initiate behind the firewall.\nGeneral course questions\nIt is normal to have many questions about things that relate to the course, such as clarification about assignments, course materials, or assessments. Please post these on our Slack Workspace. Please use the channels that I created for questions. You are encouraged to give answers and help each other. I will monitor these threads, so I will endorse or correct responses as needed. Please give me 24 hours to respond to questions within Monday-Friday. Work-life balance is important for me as well, so I will try to respond as quickly as I can within my healthy limits. \nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered. Do not send messages asking general information about the class; please post those on Slack instead."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nGrammarly Subscription\nThe School of Public Health students have access to a subscription version of Grammarly. While Grammarly cannot improve the argument and flow of your work, it can help with spelling, grammar, and sentence structure. If you are interested in this tool, please add your name to this email form and they will get you added to the subscription. Be sure to use your PSU login credentials to access the form.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "syllabus.html#school-policies-and-resources",
    "href": "syllabus.html#school-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’s Response to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "syllabus.html#ohsu-competencies",
    "href": "syllabus.html#ohsu-competencies",
    "title": "BSTA 512/612 Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "syllabus.html#institutional-policies-and-resources",
    "href": "syllabus.html#institutional-policies-and-resources",
    "title": "BSTA 512/612 Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "slides/2_Probability-solutions.html#learning-objectives",
    "href": "slides/2_Probability-solutions.html#learning-objectives",
    "title": "Chapter 2: Probability",
    "section": "Learning Objectives",
    "text": "Learning Objectives"
  },
  {
    "objectID": "slides/2_Probability-solutions.html",
    "href": "slides/2_Probability-solutions.html",
    "title": "Chapter 2: Probability",
    "section": "",
    "text": "Example 1\n\n\nSuppose you have a regular well-shuffled deck of cards. What’s the probability of drawing:\n\nany heart\nthe queen of hearts\nany queen\n\nSolution:\n\nany heart = 13/52 = 1/4\nthe queen of hearts = 1/52\nany queen = 4/52 = 1/13\n\n\n\n\n\n\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}.\\]\n\n\n\n\\(\\mathbb{P}(A)\\) is a function with\n\ninput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\noutput: a number between 0 and 1 (inclusive)\n\n\\[\\mathbb{P}(A): S \\rightarrow [0,1]\\]\nA function that follows some specific rules though!\nSee Probability Axioms on next slide."
  },
  {
    "objectID": "slides/2_Probability.html#probabilities-of-equally-likely-events-12",
    "href": "slides/2_Probability.html#probabilities-of-equally-likely-events-12",
    "title": "Chapter 2: Probability",
    "section": "Probabilities of equally likely events (1/2)",
    "text": "Probabilities of equally likely events (1/2)\n\n\nExample 1\n\n\nSuppose you have a regular well-shuffled deck of cards. What’s the probability of drawing:\n\nany heart\nthe queen of hearts\nany queen"
  },
  {
    "objectID": "slides/2_Probability.html#probabilities-of-equally-likely-events-22",
    "href": "slides/2_Probability.html#probabilities-of-equally-likely-events-22",
    "title": "Chapter 2: Probability",
    "section": "Probabilities of equally likely events (2/2)",
    "text": "Probabilities of equally likely events (2/2)\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}.\\]"
  },
  {
    "objectID": "slides/2_Probability.html#a-probability-is-a-function",
    "href": "slides/2_Probability.html#a-probability-is-a-function",
    "title": "Chapter 2: Probability",
    "section": "A probability is a function…",
    "text": "A probability is a function…\n\\(\\mathbb{P}(A)\\) is a function with\n\nInput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\nOutput: a number between 0 and 1 (inclusive)\n\n\\[\\mathbb{P}(A): S \\rightarrow [0,1]\\]\nA function that follows some specific rules though!\n \nSee Probability Axioms on next slide."
  },
  {
    "objectID": "slides/2_Probability.html#probability-axioms-1",
    "href": "slides/2_Probability.html#probability-axioms-1",
    "title": "Chapter 2: Probability",
    "section": "Probability Axioms",
    "text": "Probability Axioms\n\n\nAxiom 1\n\n\nFor every event \\(A\\), \\(0\\leq\\mathbb{P}(A)\\leq 1\\).\n\n\n\n\nAxiom 2\n\n\nFor the sample space \\(S\\), \\(\\mathbb{P}(S)=1\\).\n\n\n\n\nAxiom 3\n\n\nIf \\(A_1, A_2, A_3, \\ldots\\), is a collection of disjoint events, then \\[\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i).\\]"
  },
  {
    "objectID": "slides/2_Probability.html#some-probability-properties-1",
    "href": "slides/2_Probability.html#some-probability-properties-1",
    "title": "Chapter 2: Probability",
    "section": "Some probability properties",
    "text": "Some probability properties\nUsing the Axioms, we can prove all other probability properties!\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\n\nProposition 4\n\n\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\n\n\nProposition 5\n\n\n\\(\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\\\ \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\\\ \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C)\\)"
  },
  {
    "objectID": "slides/2_Probability.html#proposition-1-proof",
    "href": "slides/2_Probability.html#proposition-1-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 1 Proof",
    "text": "Proposition 1 Proof\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)"
  },
  {
    "objectID": "slides/2_Probability.html#proposition-2-proof",
    "href": "slides/2_Probability.html#proposition-2-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 2 Proof",
    "text": "Proposition 2 Proof\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)"
  },
  {
    "objectID": "slides/2_Probability.html#proposition-3-proof",
    "href": "slides/2_Probability.html#proposition-3-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 3 Proof",
    "text": "Proposition 3 Proof\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)"
  },
  {
    "objectID": "slides/2_Probability.html#partitions-1",
    "href": "slides/2_Probability.html#partitions-1",
    "title": "Chapter 2: Probability",
    "section": "Partitions",
    "text": "Partitions\n\n\n\n\nDefinition: Partition\n\n\nA set of events \\(\\{A_i\\}_{i=1}^{n}\\) create a partition of \\(A\\), if\n\nthe \\(A_i\\)’s are disjoint (mutually exclusive) and\n\\(\\bigcup \\limits_{i=1}^n A_i = A\\)\n\n\n\n\n\nExample 2\n\n\n\nIf \\(A \\subset B\\), then \\(\\{A, B \\cap A^C\\}\\) is a partition of \\(B\\).\nIf \\(S = \\bigcup \\limits_{i=1}^n A_i\\), and the \\(A_i\\)’s are disjoint, then the \\(A_i\\)’s are a partition of the sample space.\n\n\n\n\n\n\n\nCreating partitions is sometimes used to help calculate probabilities, since by Axiom 3 we can add the probabilities of disjoint events."
  },
  {
    "objectID": "slides/2_Probability.html#venn-diagram-probabilities-1",
    "href": "slides/2_Probability.html#venn-diagram-probabilities-1",
    "title": "Chapter 2: Probability",
    "section": "Venn Diagram Probabilities",
    "text": "Venn Diagram Probabilities\n\n\n\n\nExample 3\n\n\nIf a subject has an\n\n80% chance of taking their medication this week,\n70% chance of taking their medication next week, and\n10% chance of not taking their medication either week,\n\nthen find the probability of them taking their medication exactly one of the two weeks.\n\n\n\nHint: Draw a Venn diagram labelling each of the parts to find the probability.\n\n\n\n\nChapter 2 Slides"
  },
  {
    "objectID": "slides/2_Probability.html#pick-an-equally-likely-card-any-equally-likely-card",
    "href": "slides/2_Probability.html#pick-an-equally-likely-card-any-equally-likely-card",
    "title": "Chapter 2: Probability",
    "section": "Pick an equally likely card, any equally likely card",
    "text": "Pick an equally likely card, any equally likely card\n\n\nExample 1\n\n\nSuppose you have a regular well-shuffled deck of cards. What’s the probability of drawing:\n\nany heart\nthe queen of hearts\nany queen"
  },
  {
    "objectID": "slides/2_Probability.html#lets-break-down-this-probability",
    "href": "slides/2_Probability.html#lets-break-down-this-probability",
    "title": "Chapter 2: Probability",
    "section": "Let’s break down this probability",
    "text": "Let’s break down this probability\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}.\\]"
  },
  {
    "objectID": "slides/2_Probability.html#proposition-4-visual-proof",
    "href": "slides/2_Probability.html#proposition-4-visual-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 4 Visual Proof",
    "text": "Proposition 4 Visual Proof\n\n\nProposition 4\n\n\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)"
  },
  {
    "objectID": "slides/2_Probability.html#proposition-5-visual-proof",
    "href": "slides/2_Probability.html#proposition-5-visual-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 5 Visual Proof",
    "text": "Proposition 5 Visual Proof\n\n\nProposition 5\n\n\n\\(\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C)\\)"
  },
  {
    "objectID": "slides/2_Probability.html#weekly-medications",
    "href": "slides/2_Probability.html#weekly-medications",
    "title": "Chapter 2: Probability",
    "section": "Weekly medications",
    "text": "Weekly medications\n\n\n\n\nExample 3\n\n\nIf a subject has an\n\n80% chance of taking their medication this week,\n70% chance of taking their medication next week, and\n10% chance of not taking their medication either week,\n\nthen find the probability of them taking their medication exactly one of the two weeks.\n\n\n\nHint: Draw a Venn diagram labelling each of the parts to find the probability.\n\n\n\n\nChapter 2 Slides"
  },
  {
    "objectID": "schedule/week_01_sched.html#class-exit-tickets",
    "href": "schedule/week_01_sched.html#class-exit-tickets",
    "title": "Week 1",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (9/25)\n Wednesday (9/27)"
  },
  {
    "objectID": "schedule/week_02_sched.html#class-exit-tickets",
    "href": "schedule/week_02_sched.html#class-exit-tickets",
    "title": "Week 2",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (10/2)\n Wednesday (10/4)"
  },
  {
    "objectID": "schedule/week_02_sched.html#statistician-of-the-week",
    "href": "schedule/week_02_sched.html#statistician-of-the-week",
    "title": "Week 2",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_01_sched.html#on-the-horizon",
    "href": "schedule/week_01_sched.html#on-the-horizon",
    "title": "Week 1",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 0 due 9/28\nHomework 1 due 10/5"
  },
  {
    "objectID": "schedule/week_02_sched.html#on-the-horizon",
    "href": "schedule/week_02_sched.html#on-the-horizon",
    "title": "Week 2",
    "section": "On the Horizon",
    "text": "On the Horizon\nHomework 1 due 10/5"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-1-coin-13",
    "href": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-1-coin-13",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Coin Toss Example: 1 coin (1/3)",
    "text": "Coin Toss Example: 1 coin (1/3)\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-1-coin-23",
    "href": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-1-coin-23",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Coin Toss Example: 1 coin (2/3)",
    "text": "Coin Toss Example: 1 coin (2/3)\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\n\n \n\n\nNote\n\n\nWhen something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur."
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-1-coin-33",
    "href": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-1-coin-33",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Coin Toss Example: 1 coin (3/3)",
    "text": "Coin Toss Example: 1 coin (3/3)\n\n\n\n\nDefinition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all outcomes\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some outcomes. An event can include multiple outcomes or no outcomes.\n\n\n\n\nWhat is the sample space?\n\n\\(S =\\)\n\n\n \n\nWhat are the possible events?\n\n\n\n\n\n\n \nWhen thinking about events, think about outcomes that you might be asking the probability of."
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-2-coins",
    "href": "slides/1_Outcomes_Events_Sample.html#coin-toss-example-2-coins",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Coin Toss Example: 2 coins",
    "text": "Coin Toss Example: 2 coins\nSuppose you toss two coins.\n\nWhat is the sample space? Assume the coins are distinguishable\n\n\\(S =\\)\n\n\n \n\nWhat are some possible events?\n\n\\(A =\\) exactly one \\(H =\\)\n\\(B =\\) at least one \\(H =\\)"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#more-info-on-events-and-sample-spaces",
    "href": "slides/1_Outcomes_Events_Sample.html#more-info-on-events-and-sample-spaces",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "More info on events and sample spaces",
    "text": "More info on events and sample spaces\n\nWe usually use capital letters from the beginning of the alphabet to denote events. However, other letters might be chosen to be more descriptive.\n\n \n\nWe use the notation \\(|S|\\) to denote the size of the sample space.\n\n \n\nThe total number of possible events is \\(2^{|S|}\\), which is the total number of possible subsets of \\(S\\). We will prove this later in the course.\n\n \n\nThe empty set, denoted by \\(\\emptyset\\), is the set containing no outcomes."
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#example-keep-sampling-until",
    "href": "slides/1_Outcomes_Events_Sample.html#example-keep-sampling-until",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Example: Keep sampling until…",
    "text": "Example: Keep sampling until…\nSuppose you keep sampling people until you have someone with high blood pressure (BP)\n \nWhat is the sample space?\n\nLet \\(H =\\) denote someone with high BP.\nLet \\(H^C =\\) denote someone with not high blood pressure, such as low or regular BP.\n\n \n\nThen, \\(S =\\)"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#set-theory-12",
    "href": "slides/1_Outcomes_Events_Sample.html#set-theory-12",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Set Theory (1/2)",
    "text": "Set Theory (1/2)\n\n\n \n\n\nDefinition: Union\n\n\nThe union of events \\(A\\) and \\(B\\), denoted by \\(A \\cup B\\), contains all outcomes that are in \\(A\\) or \\(B\\) or both\n\n\n\n\nDefinition: Intersection\n\n\nThe intersection of events \\(A\\) and \\(B\\), denoted by \\(A \\cap B\\), contains all outcomes that are both in \\(A\\) and \\(B\\).\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#set-theory-22",
    "href": "slides/1_Outcomes_Events_Sample.html#set-theory-22",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Set Theory (2/2)",
    "text": "Set Theory (2/2)\n\n\n \n\n\nDefinition: Complement\n\n\nThe complement of event \\(A\\), denoted by \\(A^C\\) or \\(A'\\), contains all outcomes in the sample space \\(S\\) that are not in \\(A\\) .\n\n\n\n\nDefinition: Mutually Exclusive\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive, or disjoint, if they have no outcomes in common. In this case \\(A \\cap B = \\emptyset\\), where \\(\\emptyset\\) is the empty set.\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#bp-example-variation-13",
    "href": "slides/1_Outcomes_Events_Sample.html#bp-example-variation-13",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (1/3)",
    "text": "BP example variation (1/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\n \nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\nEvent all \\(n\\) subjects have high BP\nEvent at least one subject has high BP\nEvent all of them do not have high BP\nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#bp-example-variation-23",
    "href": "slides/1_Outcomes_Events_Sample.html#bp-example-variation-23",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (2/3)",
    "text": "BP example variation (2/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\n \nEvent all \\(n\\) subjects have high BP\n \n \nEvent at least one subject has high BP"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#bp-example-variation-33",
    "href": "slides/1_Outcomes_Events_Sample.html#bp-example-variation-33",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (3/3)",
    "text": "BP example variation (3/3)\n\nEvent all of them do not have high BP\n \n \n \nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#de-morgans-laws",
    "href": "slides/1_Outcomes_Events_Sample.html#de-morgans-laws",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "De Morgan’s Laws",
    "text": "De Morgan’s Laws\n\n\nTheorem: De Morgan’s 1st Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcap\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcup\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“all not A = \\((\\)at least one event A\\()^C\\)”\n\n\nTheorem: De Morgan’s 2nd Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcup\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“at least one event not A = \\((\\)all A\\()^C\\)”"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#remarks-on-de-morgans-laws",
    "href": "slides/1_Outcomes_Events_Sample.html#remarks-on-de-morgans-laws",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Remarks on De Morgan’s Laws",
    "text": "Remarks on De Morgan’s Laws\n\nThese laws also hold for infinite collections of events.\n \nDraw Venn diagrams to convince yourself that these are true!\n \nThese laws are very useful when calculating probabilities.\n\nThis is because calculating the probability of the intersection of events is often much easier than the union of events.\nThis is not obvious right now, but we will see in the coming chapters why.\n\n\n\n\nChapter 1 Slides"
  },
  {
    "objectID": "slides/0_Intro.html#nicky-wakim-sheher",
    "href": "slides/0_Intro.html#nicky-wakim-sheher",
    "title": "Welcome to BSTA 550!",
    "section": "Nicky Wakim (she/her)",
    "text": "Nicky Wakim (she/her)\n\n\n\nCall me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, spikeball, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "slides/0_Intro.html#lets-go-through-the-syllabus",
    "href": "slides/0_Intro.html#lets-go-through-the-syllabus",
    "title": "Welcome to BSTA 550!",
    "section": "Let’s go through the syllabus!",
    "text": "Let’s go through the syllabus!\nSyllabus page\n\n\nIntro"
  },
  {
    "objectID": "homework/HW0.html",
    "href": "homework/HW0.html",
    "title": "Homework 0",
    "section": "",
    "text": "This homework must be turned into Sakai. I want to make sure we are all familiar with the process of downloading a .qmd file, editing it, and resubmitting an .html and .qmd file.\nHere are the instructions for downloading and submitting your homework:\n\nGo to this github site to download the homework’s .qmd file.\nWhen you reach the site, it should look like this:\nClick the “Download raw file” icon () to download the .qmd file. This will likely download the file into your “Downloads” folder. It is up to you to move the file into the appropriate folder.\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd . This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\nEdit the document with your explanations and code. If you are writing out an answer or calculating by hand, you can take a picture of your work and embed it within the .qmd file. The pictures will be viewable on the .html file.\nPlease upload your homework to this Sakai assignment. Upload both your .qmd code file and the rendered .html file.\n\nThese instructions will not appear on every homework. You may come back to this page for reference.\n\n\nThis homework is meant to introduce yourself to me and your peers. In the first class, we will all briefly introduce ourselves, but we don’t have enough time in-depth introductions. Thus, I’d like you to share some information with the class over Slack.\n\n\n\nGrading will be done as a check/no check for turning in your work. If you are stressed about time, please turn in whatever you have completed."
  },
  {
    "objectID": "homework/HW0.html#purpose",
    "href": "homework/HW0.html#purpose",
    "title": "Homework 0",
    "section": "",
    "text": "This homework is meant to introduce yourself to me. Most of you have probably been in several classes together, but I’m a brand new face! In the first class, I will briefly introduce myself, but we don’t have enough time for in-class introductions for everyone. Thus, I’d like you to share some information with me."
  },
  {
    "objectID": "homework/HW0.html#gradingrubric",
    "href": "homework/HW0.html#gradingrubric",
    "title": "Homework 0",
    "section": "",
    "text": "Grading will be done using a scale from 0-5. We will use the following scale on only this homework assignment: If you are stressed about time, please give yourself an extension."
  },
  {
    "objectID": "homework/HW0.html#question-1",
    "href": "homework/HW0.html#question-1",
    "title": "Homework 0",
    "section": "Question 1",
    "text": "Question 1\nPlease upload a picture of yourself to Slack. Please follow the below steps to add a picture of yourself in Slack:\n\nGo to the top right corner in Slack and click your profile picture.\nClick \"Profile.\"\nIn the picture of the clip art person, click \"Upload Photo\" in top right corner.\nUpload a picture file of yourself."
  },
  {
    "objectID": "homework/HW0.html#question-2",
    "href": "homework/HW0.html#question-2",
    "title": "Homework 0",
    "section": "Question 2",
    "text": "Question 2\nProvide a pronunciation of your name: Please follow the below steps to add an audio and written pronunciation of your name in Slack:\n\nGo to the top right corner in Slack and click your profile picture.\nClick \"Profile.\"\nTo the right of your name, click \"Edit.\"\nUnder \"Name Recording,\" click \"Record Audio Clip.\"\nPlease say your name once at a normal pace then once slowly. Please listen to my audio for an example.\nYou may also edit the written pronunciation of your name. This is optional. I realize that doing this is may require a lot of time researching phonetics."
  },
  {
    "objectID": "homework/HW0.html#question-3",
    "href": "homework/HW0.html#question-3",
    "title": "Homework 0",
    "section": "Question 3",
    "text": "Question 3\nPlease complete the following whenisgood poll so that we can schedule office hours. Please use a unique identifier (does not have to be your name), so that I can make sure each student can attend at least one office hour."
  },
  {
    "objectID": "homework/HW0.html#question-4",
    "href": "homework/HW0.html#question-4",
    "title": "Homework 0",
    "section": "Question 4",
    "text": "Question 4\nCompletion of this question is optional. If you are comfortable sharing your pronouns, please edit your name in Slack to include them. For example, I have changed my name to be \"Nicky Wakim (she/her)\". If you prefer to not share your pronouns, then I will refer to you using they/them pronouns."
  },
  {
    "objectID": "homework/HW0.html#question-5",
    "href": "homework/HW0.html#question-5",
    "title": "Homework 0",
    "section": "Question 5",
    "text": "Question 5\nPlease post an introduction in the #random channel. Your introduction may include information like:\n\nPreferred name\nPronouns\nCareer interests\nHobbies\nFamily, children, and/or fur babies\nAdventures\nWillingness to join a study group\nBig three astrology signs\nMotivation for taking the course\nAnticipated Spring trip/activity\n\nPlease feel free to get creative with what you share! You don’t need to adhere to this list."
  },
  {
    "objectID": "homework/HW0.html#question-6",
    "href": "homework/HW0.html#question-6",
    "title": "Homework 0",
    "section": "Question 6",
    "text": "Question 6\nCompletion of this question is only necessary if you have accommodations. If you have any learning accommodations, please email me about your needs. I should receive a direct email from the Office of Student Access, but it is important that we discuss how accommodations will translate to our class."
  },
  {
    "objectID": "slides/0_Intro.html",
    "href": "slides/0_Intro.html",
    "title": "Welcome to BSTA 550!",
    "section": "",
    "text": "Call me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, spikeball, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "slides/0_Intro.html#some-important-tasks",
    "href": "slides/0_Intro.html#some-important-tasks",
    "title": "Welcome to BSTA 550!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nJoin the Slack page!\nStar the class website: https://nwakim.github.io/F2023_BSTA_550/\nComplete the WhenIsGood for office hours\nComplete Homework 0 by this Thursday at 11pm!\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!"
  },
  {
    "objectID": "homework/HW0.html#directions",
    "href": "homework/HW0.html#directions",
    "title": "Homework 0",
    "section": "",
    "text": "This homework must be turned into Sakai. I want to make sure we are all familiar with the process of downloading a .qmd file, editing it, and resubmitting an .html and .qmd file.\nHere are the instructions for downloading and submitting your homework:\n\nGo to this github site to download the homework’s .qmd file.\nWhen you reach the site, it should look like this:\nClick the “Download raw file” icon () to download the .qmd file. This will likely download the file into your “Downloads” folder. It is up to you to move the file into the appropriate folder.\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd . This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\nEdit the document with your explanations and code. If you are writing out an answer or calculating by hand, you can take a picture of your work and embed it within the .qmd file. The pictures will be viewable on the .html file.\nPlease upload your homework to this Sakai assignment. Upload both your .qmd code file and the rendered .html file.\n\nThese instructions will not appear on every homework. You may come back to this page for reference.\n\n\nThis homework is meant to introduce yourself to me and your peers. In the first class, we will all briefly introduce ourselves, but we don’t have enough time in-depth introductions. Thus, I’d like you to share some information with the class over Slack.\n\n\n\nGrading will be done as a check/no check for turning in your work. If you are stressed about time, please turn in whatever you have completed."
  },
  {
    "objectID": "homework/HW0.html#questions",
    "href": "homework/HW0.html#questions",
    "title": "Homework 0",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nPlease upload a picture of yourself to Slack. Please follow the below steps to add a picture of yourself in Slack:\n\nGo to the top right corner in Slack and click your profile picture.\nClick “Profile.”\nIn the picture of the clip art person, click “Upload Photo” in top right corner.\nUpload a picture file of yourself.\n\nPlease include the picture you used below. This will make sure we are all able to insert a picture within Quarto. It will also make sure I can see the photo within the .html file.\n\n\nQuestion 2\nProvide a pronunciation of your name: Please follow the below steps to add an audio and written pronunciation of your name in Slack:\n\nGo to the top right corner in Slack and click your profile picture.\nClick “Profile.”\nTo the right of your name, click “Edit.”\nUnder “Name Recording,” click “Record Audio Clip.”\nPlease say your name once at a normal pace then once slowly. Please listen to my audio for an example.\nYou may also edit the written pronunciation of your name. This is optional. I realize that doing this is may require a lot of time researching phonetics.\n\n\n\nQuestion 3\nPlease complete the following whenisgood poll so that we can schedule office hours. Please use a unique identifier (does not have to be your name), so that I can make sure each student can attend at least one office hour.\n\n\nQuestion 4\nCompletion of this question is optional. If you are comfortable sharing your pronouns, please edit your name in Slack to include them. For example, I have changed my name to be “Nicky Wakim (she/her)”. If you prefer to not share your pronouns, then I will refer to you using they/them pronouns.\n\n\nQuestion 5\nPlease post an introduction in the #random channel on Slack. You do not need to include all of the below items, but your introduction may include information like:\n\nPreferred name\nPronouns\nAre you new to Portland?\nCareer interests\nHobbies\nFamily, children, and/or fur babies\nFavorite/recent adventures, restaurants, TV shows, books, podcasts, games, etc.\nWillingness to join a study group\nMotivation for taking the course\nAnticipated Winter trip/activity\nAny resolutions/vibes/outlooks that you are thinking about as we start the new calendar year/quarter\n\nPlease feel free to get creative with what you share! You don’t need to adhere to this list.\n\n\nQuestion 6\nCompletion of this question is only necessary if you have accommodations. If you have any learning accommodations, please email me about your needs. I should receive a direct email from the Office of Student Access, but it is important that we discuss how accommodations will translate to our class."
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 512/612 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "schedule/week_08_sched.html#on-the-horizon",
    "href": "schedule/week_08_sched.html#on-the-horizon",
    "title": "Week 8",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_08_sched.html#class-exit-tickets",
    "href": "schedule/week_08_sched.html#class-exit-tickets",
    "title": "Week 8",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (11/13)\n Wednesday (11/15)"
  },
  {
    "objectID": "schedule/week_08_sched.html#statistician-of-the-week-name",
    "href": "schedule/week_08_sched.html#statistician-of-the-week-name",
    "title": "Week 8",
    "section": "Statistician of the Week: Name",
    "text": "Statistician of the Week: Name"
  },
  {
    "objectID": "schedule/week_03_sched.html#on-the-horizon",
    "href": "schedule/week_03_sched.html#on-the-horizon",
    "title": "Week 3",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_03_sched.html#class-exit-tickets",
    "href": "schedule/week_03_sched.html#class-exit-tickets",
    "title": "Week 3",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (9/25)\n Wednesday (9/27)"
  },
  {
    "objectID": "schedule/week_03_sched.html#statistician-of-the-week",
    "href": "schedule/week_03_sched.html#statistician-of-the-week",
    "title": "Week 3",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:\n\n\n\n\n\n\n\nDavid Blackwell\n\n\n\n\n\n\nBlackwell was the first black person to receive a PhD in statistics (from University of Illinois at Urbana-Champaign, in 1941 at the age of 22) in the US and the first black scholar to be admitted to the National Academy of Sciences. He was a statistician at UC Berkeley for more than 50 years. He was hired in 1954 after the department almost made him an offer in 1942 (but declined to do so when one faculty member’s wife said she didn’t want Blackwell hired because she wouldn’t feel comfortable having faculty events in her home with a black man). Hear Blackwell tell the story in his own words.\n\n\n\nTopics covered\nBlackwell contributed to game theory, probability theory, information science, and Bayesian statistics. The Rao-Blackwell theorem (often seen in a senior level undergraduate class on statistical theory) is named after him.\n\n\nRelevant work\n\nBlackwell, D. (1947). “Conditional expectation and unbiased sequential estimation”. Annals of Mathematical Statistics. 18 (1): 105–110. doi:10.1214/aoms/1177730497.\n\n\n\nOutside links\n\nWikipedia\nMAD\n\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin."
  },
  {
    "objectID": "schedule/week_09_sched.html#on-the-horizon",
    "href": "schedule/week_09_sched.html#on-the-horizon",
    "title": "Week 9",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_09_sched.html#class-exit-tickets",
    "href": "schedule/week_09_sched.html#class-exit-tickets",
    "title": "Week 9",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (11/20)"
  },
  {
    "objectID": "schedule/week_09_sched.html#statistician-of-the-week",
    "href": "schedule/week_09_sched.html#statistician-of-the-week",
    "title": "Week 9",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_10_sched.html#on-the-horizon",
    "href": "schedule/week_10_sched.html#on-the-horizon",
    "title": "Week 10",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_10_sched.html#class-exit-tickets",
    "href": "schedule/week_10_sched.html#class-exit-tickets",
    "title": "Week 10",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (11/27)\n Wednesday (11/29)"
  },
  {
    "objectID": "schedule/week_10_sched.html#statistician-of-the-week",
    "href": "schedule/week_10_sched.html#statistician-of-the-week",
    "title": "Week 10",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_11_sched.html#on-the-horizon",
    "href": "schedule/week_11_sched.html#on-the-horizon",
    "title": "Week 11",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_11_sched.html#end-of-quarter-feedback",
    "href": "schedule/week_11_sched.html#end-of-quarter-feedback",
    "title": "Week 11",
    "section": "End of quarter feedback!",
    "text": "End of quarter feedback!"
  },
  {
    "objectID": "schedule/week_11_sched.html#statistician-of-the-week",
    "href": "schedule/week_11_sched.html#statistician-of-the-week",
    "title": "Week 11",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_07_sched.html#on-the-horizon",
    "href": "schedule/week_07_sched.html#on-the-horizon",
    "title": "Week 7",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_07_sched.html#class-exit-tickets",
    "href": "schedule/week_07_sched.html#class-exit-tickets",
    "title": "Week 7",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (11/6)\n Wednesday (11/8)"
  },
  {
    "objectID": "schedule/week_07_sched.html#statistician-of-the-week",
    "href": "schedule/week_07_sched.html#statistician-of-the-week",
    "title": "Week 7",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_06_sched.html#on-the-horizon",
    "href": "schedule/week_06_sched.html#on-the-horizon",
    "title": "Week 6",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_06_sched.html#class-exit-tickets",
    "href": "schedule/week_06_sched.html#class-exit-tickets",
    "title": "Week 6",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (10/30)"
  },
  {
    "objectID": "schedule/week_06_sched.html#statistician-of-the-week",
    "href": "schedule/week_06_sched.html#statistician-of-the-week",
    "title": "Week 6",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_05_sched.html#on-the-horizon",
    "href": "schedule/week_05_sched.html#on-the-horizon",
    "title": "Week 5",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_05_sched.html#class-exit-tickets",
    "href": "schedule/week_05_sched.html#class-exit-tickets",
    "title": "Week 5",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (10/23)\n Wednesday (10/25)"
  },
  {
    "objectID": "schedule/week_05_sched.html#statistician-of-the-week",
    "href": "schedule/week_05_sched.html#statistician-of-the-week",
    "title": "Week 5",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_04_sched.html#on-the-horizon",
    "href": "schedule/week_04_sched.html#on-the-horizon",
    "title": "Week 4",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "schedule/week_04_sched.html#class-exit-tickets",
    "href": "schedule/week_04_sched.html#class-exit-tickets",
    "title": "Week 4",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (10/16)\n Wednesday (10/18)"
  },
  {
    "objectID": "schedule/week_04_sched.html#statistician-of-the-week",
    "href": "schedule/week_04_sched.html#statistician-of-the-week",
    "title": "Week 4",
    "section": "Statistician of the Week:",
    "text": "Statistician of the Week:"
  },
  {
    "objectID": "schedule/week_02_sched.html#statistician-of-the-week-talithia-williams",
    "href": "schedule/week_02_sched.html#statistician-of-the-week-talithia-williams",
    "title": "Week 2",
    "section": "Statistician of the Week: Talithia Williams",
    "text": "Statistician of the Week: Talithia Williams\n\n\n\n\n\n\n\nTalithia Williams\n\n\n\n\n\n\nDr. Williams earned a BS in Mathematics from Spelman College, an MS in Mathematics from Howard University, and a PhD (2008) in Statistics from Rice University. Dr. Williams is Associate Professor and Director of the Clinic Program at Harvey Mudd College. She has also served as Associate Dean for Faculty Development and Diversity at Harvey Mudd.\n\n\n\nTopics covered\nDr. Williams works on statistical models which describe spatial and temporal aspects of data. Some of her most important work has focused on developing models to predict cataract surgical rates for countries in Africa.\n\n\nRelevant work\n\nDray, A. and Williams, T. An incidence estimation model for multi-stage diseases with differential mortality. Statistics in Medicine, 2012.\nLewallen, S., Courtright, P., Etya’ale, D., Mathenge, W., Schmidt, E., Oye, J., Clark, A., Williams, T. Cataract Incidence in Sub-Saharan Africa: What does Mathematical Modeling tell us about Geographic Variations and Surgical Needs?. Ophthalmic epidemiology, 2013.\n\n\n\nOutside links\n\nWikipedia\nMAD\nLinkedin\npersonal\n\n\n\nOther\nDr. Williams was the co-host of the 2018 PBS Nova Wonders series.\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin."
  },
  {
    "objectID": "schedule/week_04_sched.html#statistician-of-the-week-joy-buolamwini",
    "href": "schedule/week_04_sched.html#statistician-of-the-week-joy-buolamwini",
    "title": "Week 4",
    "section": "Statistician of the Week: Joy Buolamwini",
    "text": "Statistician of the Week: Joy Buolamwini\n\n\n\n\n\n\n\nJoy Buolamwini\n\n\n\n\n\n\nDr. Buolamwini earned a BS in Computer Science from Georgia Institute of Technology, an Master’s from University of Oxford, and MS and PhD (2022) degrees in Media Arts & Sciences from Massachusetts Institute of Technology. While a graduate student, Dr. Buolamwini was part of the MIT Media Lab. Additionally, she is the founder of the Algorithmic Justice League.\n\n\n\nTopics covered\nDr. Buolamwini has done substantial work demonstrating how algorithms can encode bias. Her undergraduate senior project was to create a inspired “mask” mirror as a way to raise spirits for the person who looked into the mirror. The project relied on off the shelf facial recognition software that could not recognize Dr. Buolamwini’s face.\nSince then, she has focused her work on demonstrating bias across racial and gender spectra in off the shelf software. Her work has been cited as directly influencing Microsoft and Google’s changes to their algorithms.\nAmong many other aspects, a big focus of Dr. Buolamwini’s work is pointing out the biased data which directly impacts how algorithms learn how to do tasks.\n\n\nRelevant work\n\nBuolamwini, J., Gebru, T. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. Proceedings of Machine Learning Research 81:1–15, 2018 Conference on Fairness, Accountability, and Transparency.\nRaji, I & Buolamwini, J. Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products. Conference on Artificial Intelligence, Ethics, and Society, 2019\n\n\n\nOutside links\n\nWikipedia\nLinkedin\npersonal\n\n\n\nOther\nDr. Buolamwini has done a lot of work on how data propagates through systems to encode the same types of bias into different algorithms. In her video AI, Ain’t I a Woman? she demonstrates how systems designed to determine gender are particularly poor when using dark skinned faces.\nHer work was featured in a recent documentary Coded Bias.\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin."
  },
  {
    "objectID": "schedule/week_05_sched.html#statistician-of-the-week-liz-hare",
    "href": "schedule/week_05_sched.html#statistician-of-the-week-liz-hare",
    "title": "Week 5",
    "section": "Statistician of the Week: Liz Hare",
    "text": "Statistician of the Week: Liz Hare\n\n\n\n\n\n\n\nLiz Hare\n\n\n\n\n\n\nDr. Hare got her BA from Bryn Mawr College and her PhD in Genetics (1998) from The George Washington University. She works primarily in dog / animal genetics; although, as a quantitative geneticist her statistical and computational methodology is quite sophisticated.\nDr. Hare is active in the MiR (Minorities in R) Community which aims to support historically underrepresented R users around the world.\n\n\n\nTopics covered\nHer computational language of choice is R, and much of her work has focused on open science with an eye toward inclusion and equity. In many software programs, the user has the ability to include alt text: text descriptions that convey the content and meaning to blind and low-vision readers.\n\nyou really need to tell us what the data is saying and why you included it.\n\n\nWhat kind of graph or chart is it?\nWhat variables are on the axes?\nWhat are the ranges of the variables?\nWhat does the appearance tell you about the relationships between the variables?\n\n\n\nRelevant work\n\nL. Hare, Writing Alt Text to Communicate the Meaning in Data Visualizations, Do No Harm Guide: centering accessibility in data visualization, eds Schwabish, Popkin, Feng, Chapter 4, 2022.\n\n\n\nOutside links\n\nGoogle Scholar\nLinkedin\nprofessional\nfosstodon"
  },
  {
    "objectID": "schedule/week_03_sched.html#statistician-of-the-week-david-blackwell",
    "href": "schedule/week_03_sched.html#statistician-of-the-week-david-blackwell",
    "title": "Week 3",
    "section": "Statistician of the Week: David Blackwell",
    "text": "Statistician of the Week: David Blackwell\n\n\n\n\n\n\n\nDavid Blackwell\n\n\n\n\n\n\nBlackwell was the first black person to receive a PhD in statistics (from University of Illinois at Urbana-Champaign, in 1941 at the age of 22) in the US and the first black scholar to be admitted to the National Academy of Sciences. He was a statistician at UC Berkeley for more than 50 years. He was hired in 1954 after the department almost made him an offer in 1942 (but declined to do so when one faculty member’s wife said she didn’t want Blackwell hired because she wouldn’t feel comfortable having faculty events in her home with a black man). Hear Blackwell tell the story in his own words.\n\n\n\nTopics covered\nBlackwell contributed to game theory, probability theory, information science, and Bayesian statistics. The Rao-Blackwell theorem (often seen in a senior level undergraduate class on statistical theory) is named after him.\n\n\nRelevant work\n\nBlackwell, D. (1947). “Conditional expectation and unbiased sequential estimation”. Annals of Mathematical Statistics. 18 (1): 105–110. doi:10.1214/aoms/1177730497.\n\n\n\nOutside links\n\nWikipedia\nMAD\n\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin."
  },
  {
    "objectID": "schedule/week_06_sched.html#statistician-of-the-week-lester-mackey",
    "href": "schedule/week_06_sched.html#statistician-of-the-week-lester-mackey",
    "title": "Week 6",
    "section": "Statistician of the Week: Lester Mackey",
    "text": "Statistician of the Week: Lester Mackey\n\n\n\n\n\n\n\nLester Mackey\n\n\n\n\n\n\nDr. Mackey is a machine learning researcher at Microsoft Research New England and an adjunct professor at Stanford University. His PhD (Computer Science 2012) and MA (Statistics 2011) are both from University of California, Berkeley, while his undergraduate degree (Computer Science 2007) is from Princeton University.\nHe is involved in Stanford’s initiative of Statistics for Social Good and has the following quote on his website:\n\nQuixotic though it may sound, I hope to use computer science and statistics to change the world for the better.\n\n\n\n\nTopics covered\nFrom Dr. Mackey’s personal website his areas of research are:\n\nstatistical machine learning\nscalable algorithms\nhigh-dimensional statistics\napproximate inference\nprobability\n\n\n\nRelevant work\n\nKoulik Khamaru, Yash Deshpande, Lester Mackey, and Martin J. Wainwright, Near-optimal inference in adaptive linear regression\n\n\nWhen data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals…\n\n\nPierre Bayle, Alexandre Bayle, Lucas Janson, and Lester Mackey, Cross-validation Confidence Intervals for Test Error Advances in Neural Information Processing Systems (NeurIPS), December 2020.\n\n\nThis work develops central limit theorems for cross-validation and consistent estimators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our real-data experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature…\n\n\n\nOutside links\n\nMathematically Gifted & Black\nLinkedin\npersonal\n\n\n\nOther\nThe precursor to kaggle was a $1 million prize given by Netflix to the most accurate prediction of ratings that people give to the movies they watch. As undergraduates, Dr. Mackey and two friends led the competition for a few hours in its first year. Later, groups merged and Dr. Mackey’s group merged with a few others, forming The Ensemble. Their final analysis came in second with the exact same error rates as the winning entry. The winning entry, however, had been submitted 20 minutes prior. Sigh."
  },
  {
    "objectID": "schedule/week_07_sched.html#statistician-of-the-week-w.e.b.-du-bois",
    "href": "schedule/week_07_sched.html#statistician-of-the-week-w.e.b.-du-bois",
    "title": "Week 7",
    "section": "Statistician of the Week: W.E.B. Du Bois",
    "text": "Statistician of the Week: W.E.B. Du Bois\n\n\n\n\n\n\n\nW.E.B. Du Bois\n\n\n\n\n\n\nDu Bois was a sociologist and among the earliest data scientists. As Battle-Baptiste and Rusert say, his work can be thought of as\n\nthe rendering of information in a visual format to help communicate data while also generating new patterns and knoweldge throughout the act of visualization itselt.1\n\n\n\n\nTopics covered\nDu Bois was a sociologist who contributed to the field of data visualization through infographics related to the African American in the early twentieth century.\n\n\nRelevant work\n\nRusert, B., and Battle-Baptiste, W. “W. E. B. Du Bois’s Data Portraits: Visualizing Black America”, Princeton Architectural Press, 2018. https://papress.com/products/w-e-b-du-boiss-data-portraits-visualizing-black-america\n\n\n\nOutside links\n\nWikipedia\nTidyTuesday data viz and TidyTuesday challenge provided the data needed to re-create most of Du Bois’s original graphs (his originals were drawn by hand).\nData Journalism in the study of W.E.B. Du Bois\nW.E.B. Du Bois: retracing his attempt to challenge racism with data\nW.E.B. Du Bois’ Visionary Infographics Come Together for the First Time in Full Color\n\n\n\nOther\nIn 1900 Du Bois contributed approximately 60 data visualizations to an exhibit at the Exposition Universelle in Paris, an exhibit designed to illustrate the progress made by African Americans since the end of slavery (only 37 years prior, in 1863).\nAt their core, the data visualizations advocate for African American progress. They not only speak to the progress that had been made, but they centered many of the challenges that continued to exist at the time. The set of visualizations demonstrate how powerfully a picture can tell 1000 words, as the information Du Bois used was primarily available from public records (e.g., census and other government reports).\nWhitney Battle-Baptiste and Britt Rusert have reproduced and narrated the images from the exhibit in W.E.B. Du Bois’s Data Portraits: Visualizing Black America, the color line at the turn of the twentieth century."
  },
  {
    "objectID": "schedule/week_07_sched.html#footnotes",
    "href": "schedule/week_07_sched.html#footnotes",
    "title": "Week 7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBattle-Baptiste and Rusert, W.E.B. Du Bois’s Data Portraits: Visualizing Black America, the color line at the turn of the twentieth century, 2018, page 8.↩︎"
  },
  {
    "objectID": "schedule/week_08_sched.html#statistician-of-the-week-desi-small-rodriguez",
    "href": "schedule/week_08_sched.html#statistician-of-the-week-desi-small-rodriguez",
    "title": "Week 8",
    "section": "Statistician of the Week: Desi Small-Rodriguez",
    "text": "Statistician of the Week: Desi Small-Rodriguez\n\n\n\n\n\n\n\nDesi Small-Rodriguez\n\n\n\n\n\n\nDr. Small-Rodriguez is a social demographer and an Assistant Professor of Sociology and American Indian Studies at UCLA. She received a PhD in Sociology from the University of Arizona and a PhD in Demography from the University of Waikato. Dr. Small-Rodriguez is Northern Cheyenne and Chicana and grounds her work in Indigenous studies, sociology of race and ethnicity, critical demography, and health policy research. She directs the Data Warriors Lab (a mobile data sovereignty lab serving Indigenous communities) and was previously a member of the Collaboratory for Indigenous Data Governance. She is a founding member of the Global Indigenous Data Alliance.\n\n\n\nTopics covered\nDr. Small-Rodriguez is passionate about Indigenous data sovereignty and Indigenous data governance. Using networks of Indigenous scholars and survey methods, she works toward the following two goals: (1) better collection and use of data on Indigenous people that has been gathered by external sources such as the census and other federal entities; (2) development of data methods and practitioners within the Indigenous community. Dr. Small-Rodriguez also works for health and economic justice on Indian Reservations.\n\n\nRelevant work\n\nS.R. Carroll, D. Rodriguez-Lonebear, A. Martinez, “Indigenous Data Governance: Strategies from United States Native Nations.” Data Science Journal, 2019. https://datascience.codata.org/article/10.5334/dsj-2019-031/\n\n\n“Indigenous data sovereignty is the right of each Native nation to govern the collection, ownership, and application of the tribe’s data.”\n\n\nRodriguez-Lonebear D, Barceló NE, Akee R, Carroll SR. “American Indian Reservations and COVID-19: Correlates of Early Infection Rates in the Pandemic.” J Public Health Manag Pract. 2020. doi: 10.1097/PHH.0000000000001206.\n\n\n\nOutside links\n\nacademic\nLinkedin\npersonal"
  },
  {
    "objectID": "schedule/week_09_sched.html#statistician-of-the-week-mike-dairyko",
    "href": "schedule/week_09_sched.html#statistician-of-the-week-mike-dairyko",
    "title": "Week 9",
    "section": "Statistician of the Week: Mike Dairyko",
    "text": "Statistician of the Week: Mike Dairyko\n\n\n\n\n\n\n\nMike Dairyko\n\n\n\n\n\n\nDr. Dairyko was a Posse Scholar at Pomona College where a linear algebra class set him on a career path centered around mathematics. Through that class he found his way to two different summer REU programs and eventually to a PhD in Applied Mathematics from Iowa State University (2018). While initially believing that he would stay in academia after his graduate work, being introduced to machine learning methods caused him to pursue data science jobs after graduation.\nDr. Dairyko served as a Senior Manager of Data Science at the Milwaukee Brewers and is now the Director of Ticketing Analytics at the Milwaukee Bucks. Helping the organization get the most out of budgeting, revenue, and ticket sales allows him to fully use his training in mathematics and data science.\n\n\n\nTopics covered\nDr. Dairyko’s graduate work is in graph theory, in particular, exponential domination. In a graph, exponential domination is the extent to which a particular vertex influences the remaining vertices in a graph. His published work falls very much within the realm of mathematics, proving that particular properties of graphs exist. However, graph theory is intimately related to machine learning; for example, it is the foundational structure of a neural network. Understanding properties of graphs help data scientists develop even more powerful models to harness information from data.\n\n\nRelevant work\n\nM. Dairyko, A linear programming method for exponential domination. The Golden Anniversary Celebration of the National Association of Mathematicians, Volume 759 of Contemporary mathematics. Eds O. Ortega, E. Lawrence, E. Goins (2020).\nM. Dairyko, L.Hogben, J. Lin, J. Lockhart, D. Roberson, S. Severini, M. Young, Note on von Neumann and Rényi entropies of a graph. Linear Algebra and its Applications, 2017.\n\n\n\nOutside links\n\nMAD\nLinkedin\nGoogle Scholar\n\n\n\nOther\nDr. Dairyko’s path from mathematics to data science has been written about in SIAM and in the Iowa State University newsletter Math Matters."
  },
  {
    "objectID": "schedule/week_10_sched.html#statistician-of-the-week-florence-nightingale",
    "href": "schedule/week_10_sched.html#statistician-of-the-week-florence-nightingale",
    "title": "Week 10",
    "section": "Statistician of the Week: Florence Nightingale",
    "text": "Statistician of the Week: Florence Nightingale\n\n\n\n\n\n\n\nFlorence Nightingale\n\n\n\n\n\n\nNightingale was a nurse who is considered to be the founder of modern nursing. In particular, she was admant about the importance of hygiene and sanitary conditions. She was born into a wealthy and well-connected family and had a large amount of privilege. Educated by her father, she showed an ability toward making analytic arguments at an early age. For her work, she was awarded the Royal Red Cross, the Lady of Grace of the Order of St John, and the Order of Merit.\n\n\n\nTopics covered\nNightingale was a nurse and statistician who used her analytic abilities to better understand and improve public health. She served in the Crimean War where Britain and France fought against the Russian invasion of the Ottoman Empire. Nightingale worked to convince Queen Victoria that poor sanitation and overcrowding were causing unnecessary death. She was able to show, for example, that peacetime soldiers (who lived in poorly kept barracks) were dying in much higher number than comparable civilian men. Her genius was to collect data meticulously and to display it in ways that were accessible to the general public. Her visualizations are lauded as pioneering and the first of their kind to tell effective stories of important issues.\n\n\nRelevant work\n\nRJ Andrews, “Florence Nightingale’s Data Revolution” in Scientific American 327, 2, 78-85, 2022. doi:10.1038/scientificamerican0822-78,\n\n\n\n\n\n\n“Diagram of the causes of mortality in the army in the East” was published in Notes on Matters Affecting the Health, Efficiency, and Hospital Administration of the British Army and sent to Queen Victoria in 1858.\n\n\n\n\n\nS Julious, “On the battlefields of the Crimean War and in the hills of Sheffield, Florence Nightingale’s legacy lives on”, The Tribune, Feb 17, 2023.\n\n\n\nOutside links\n\nWikipedia"
  },
  {
    "objectID": "schedule/week_11_sched.html#statistician-of-the-week-maricela-cruz",
    "href": "schedule/week_11_sched.html#statistician-of-the-week-maricela-cruz",
    "title": "Week 11",
    "section": "Statistician of the Week: Maricela Cruz",
    "text": "Statistician of the Week: Maricela Cruz\n\n\n\n\n\n\n\nMaricela Cruz\n\n\n\n\n\n\nDr. Cruz did her undergraduate work at Pomona College, majoring in mathematics. Her PhD in Statistics is from University of California, Irvine. She is now Assistant Biostatistics Investigator, Kaiser Permanente Washington Health Research Institute (KPWHRI) and Affiliate Assistant Investigator, Department of Biostatistics, University of Washington.\nIn an interview done by Lathisms, Dr. Cruz reminds us:\n\nNavigating institutions that have systematically excluded groups of people can be taxing, especially for people from one or more of these groups. Surround yourself with those who believe in you, give you the space to ask ‘silly’ questions, value your input, and/or understand your struggles. Do activities that will help you blow off steam. No one person or activity will meet all your support needs, so find the right group and balance for you.\n\n\n\n\nTopics covered\nDr. Cruz’s dissertation work was on interrupted time series models used to determine intervention timing. Indeed, her primary research questions are to understand complex health interventions. At KPWHRI she has continued to work on epidemiological methods, particularly those appropriate for longitudinal and multilevel data.\n\n\nRelevant work\n\nCruz M, Ombao H, Gillen DL, A Generalized Interrupted Time Series Model for Assessing Complex Health Care Interventions. Statistics in Biosciences, 2022.\nCruz M, Gillen DL, Bender M, & Ombao H, Assessing health care interventions via an interrupted time series model: study power and design considerations. Statistics in Medicine, 2019.\n\n\n\nOutside links\n\nLathisms\nLinkedin\nKaiser Permanente"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#basic-counting-examples-13",
    "href": "slides/22_Counting_Intro.html#basic-counting-examples-13",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Basic Counting Examples (1/3)",
    "text": "Basic Counting Examples (1/3)\n\n\nExample 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them without replacement and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#basic-counting-examples-23",
    "href": "slides/22_Counting_Intro.html#basic-counting-examples-23",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Basic Counting Examples (2/3)",
    "text": "Basic Counting Examples (2/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n \n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#basic-counting-examples-33",
    "href": "slides/22_Counting_Intro.html#basic-counting-examples-33",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Basic Counting Examples (3/3)",
    "text": "Basic Counting Examples (3/3)\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.3\n\n\nHow many ways to order them without replacement and only need 6?\n\n\n \n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#permutations-and-combinations-1",
    "href": "slides/22_Counting_Intro.html#permutations-and-combinations-1",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Permutations and Combinations",
    "text": "Permutations and Combinations\n\n\nDefinition: Permutations\n\n\nPermutations are the number of ways to arrange in order \\(r\\) distinct objects when there are \\(n\\) total.\n\\[nPr = \\frac{n!}{(n-r)!}\\]\n\n\n\n\nDefinition: Combinations\n\n\nCombinations are the number of ways to choose (order doesn’t matter) \\(r\\) objects from \\(n\\) without replacement.\n\\[nCr = \\textrm{\"n choose r\"} = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#some-combinations-properties",
    "href": "slides/22_Counting_Intro.html#some-combinations-properties",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Some combinations properties",
    "text": "Some combinations properties\n\n\\[\\binom{n}{r} = \\binom{n}{n-r}\\]\n \n \n\\[\\binom{n}{1} = n\\]\n \n \n\\[\\binom{n}{0} = 1\\]"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#more-examples-order-matters-vs.-not-12",
    "href": "slides/22_Counting_Intro.html#more-examples-order-matters-vs.-not-12",
    "title": "Chapter 22: Introduction to Counting",
    "section": "More examples: order matters vs. not (1/2)",
    "text": "More examples: order matters vs. not (1/2)\n\n\n\n\nExample 2\n\n\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\norder doesn’t matter?"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#more-examples-order-matters-vs.-not-22",
    "href": "slides/22_Counting_Intro.html#more-examples-order-matters-vs.-not-22",
    "title": "Chapter 22: Counting",
    "section": "More examples: order matters vs. not (2/2)",
    "text": "More examples: order matters vs. not (2/2)\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\n\n\\[\\frac{13P2}{52P2} = \\frac{\\frac{13!}{11!}}{\\frac{52!}{50!}} = \\frac{13\\cdot12}{52\\cdot51}\\]\n\norder doesn’t matter?\n\n\\[\\frac{\\binom{13}{2}}{\\binom{52}{2}} = \\frac{\\frac{13!}{2!11!}}{\\frac{52!}{2!50!}} = \\frac{13\\cdot12}{52\\cdot51}\\]"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#table-of-different-cases",
    "href": "slides/22_Counting_Intro.html#table-of-different-cases",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Table of different cases",
    "text": "Table of different cases\nSee table on pg. 277 of textbook\n\n\\(n\\) = total number of objects\n\\(r\\) = number objects needed\n\n\n\n\n\n\n\n\n\nwith replacement\nwithout replacement\n\n\n\n\norder matters\n\\[n^r\\]\n\\[nPr = \\frac{n!}{(n-r)!}\\]\n\n\norder doesn’t matter\n\\[ \\binom{n+r-1}{r}\\]\n\\[nCr = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html",
    "href": "slides_solns/2_Probability-solutions.html",
    "title": "Chapter 2: Probability",
    "section": "",
    "text": "Example 1\n\n\nSuppose you have a regular well-shuffled deck of cards. What’s the probability of drawing:\n\nany heart\nthe queen of hearts\nany queen\n\nSolution:\n\nany heart = 13/52 = 1/4\nthe queen of hearts = 1/52\nany queen = 4/52 = 1/13\n\n\n\n\n\n\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}.\\]\n\n\n\n\\(\\mathbb{P}(A)\\) is a function with\n\ninput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\noutput: a number between 0 and 1 (inclusive)\n\n\\[\\mathbb{P}(A): S \\rightarrow [0,1]\\]\nA function that follows some specific rules though!\nSee Probability Axioms on next slide."
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#probabilities-of-equally-likely-events-12",
    "href": "slides_solns/2_Probability-solutions.html#probabilities-of-equally-likely-events-12",
    "title": "Chapter 2: Probability",
    "section": "",
    "text": "Example 1\n\n\nSuppose you have a regular well-shuffled deck of cards. What’s the probability of drawing:\n\nany heart\nthe queen of hearts\nany queen\n\nSolution:\n\nany heart = 13/52 = 1/4\nthe queen of hearts = 1/52\nany queen = 4/52 = 1/13"
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#probabilities-of-equally-likely-events-22",
    "href": "slides_solns/2_Probability-solutions.html#probabilities-of-equally-likely-events-22",
    "title": "Chapter 2: Probability",
    "section": "",
    "text": "If \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}.\\]"
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#a-probability-is-a-function",
    "href": "slides_solns/2_Probability-solutions.html#a-probability-is-a-function",
    "title": "Chapter 2: Probability",
    "section": "",
    "text": "\\(\\mathbb{P}(A)\\) is a function with\n\ninput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\noutput: a number between 0 and 1 (inclusive)\n\n\\[\\mathbb{P}(A): S \\rightarrow [0,1]\\]\nA function that follows some specific rules though!\nSee Probability Axioms on next slide."
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#probability-axioms-1",
    "href": "slides_solns/2_Probability-solutions.html#probability-axioms-1",
    "title": "Chapter 2: Probability",
    "section": "Probability Axioms",
    "text": "Probability Axioms\n\n\nAxiom 1\n\n\nFor every event \\(A\\), \\(0\\leq\\mathbb{P}(A)\\leq 1\\).\n\n\n\n\nAxiom 2\n\n\nFor the sample space \\(S\\), \\(\\mathbb{P}(S)=1\\).\n\n\n\n\nAxiom 3\n\n\nIf \\(A_1, A_2, A_3, \\ldots\\), is a collection of disjoint events, then \\[\\mathbb{P}\\Big( \\bigcup \\limits_{i=1}^{\\infty}A_i\\Big) =  \\sum_{i=1}^{\\infty}\\mathbb{P}(A_i).\\]"
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#some-probability-properties-1",
    "href": "slides_solns/2_Probability-solutions.html#some-probability-properties-1",
    "title": "Chapter 2: Probability",
    "section": "Some probability properties",
    "text": "Some probability properties\nUsing the Axioms, we can prove all other probability properties!\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\n\nProposition 4\n\n\n\\(\\mathbb{P}(A \\cup B) = \\mathbb{P}(A) + \\mathbb{P}(B) - \\mathbb{P}(A \\cap B)\\)\n\n\n\n\nProposition 5\n\n\n\\(\\mathbb{P}(A \\cup B \\cup C) = \\mathbb{P}(A) + \\mathbb{P}(B) + \\\\ \\mathbb{P}(C) - \\mathbb{P}(A \\cap B) - \\mathbb{P}(A \\cap C) - \\\\ \\mathbb{P}(B \\cap C) + \\mathbb{P}(A \\cap B \\cap C)\\)"
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#proposition-1-proof",
    "href": "slides_solns/2_Probability-solutions.html#proposition-1-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 1 Proof",
    "text": "Proposition 1 Proof\n\ncountdown::countdown(1, top = 0)\n\n\n−+\n01:00\n\n\n\n\n\nProposition 1\n\n\nFor any event \\(A\\), \\(\\mathbb{P}(A)= 1 - \\mathbb{P}(A^C)\\)\n\n\n\n\nProposition 1 Proof\n\n\nSince \\(A\\) and \\(A^C\\) are disjoint, we know from Axiom 3 that \\(\\mathbb{P}(A \\cup A^C) = \\mathbb{P}(A) + \\mathbb{P}(A^C)\\).\nHowever, \\(S = A \\cup A^C\\), and by Axiom 2, \\(\\mathbb{P}(S) = 1\\), implying \\(\\mathbb{P}(A \\cup A^C) = \\mathbb{P}(S) = 1\\).\nThus, \\(\\mathbb{P}(A) + \\mathbb{P}(A^C) = 1\\), or \\(\\mathbb{P}(A) = 1 - \\mathbb{P}(A^C)\\)"
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#proposition-2-proof",
    "href": "slides_solns/2_Probability-solutions.html#proposition-2-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 2 Proof",
    "text": "Proposition 2 Proof\n\n\nProposition 2\n\n\n\\(\\mathbb{P}(\\emptyset)=0\\)\n\n\n\n\nProposition 2 Proof (different from book)\n\n\nWe know \\(\\emptyset = S^C\\).\nThus by Prop 1,\n\\[\\mathbb{P}(\\emptyset) = \\mathbb{P}(S^C) = 1- \\mathbb{P}(S) = 1-1 =0\n.\\]"
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#proposition-3-proof",
    "href": "slides_solns/2_Probability-solutions.html#proposition-3-proof",
    "title": "Chapter 2: Probability",
    "section": "Proposition 3 Proof",
    "text": "Proposition 3 Proof\n\n\nProposition 3\n\n\nIf \\(A \\subseteq B\\), then \\(\\mathbb{P}(A) \\leq \\mathbb{P}(B)\\)\n\n\n\n\nProposition 3 Proof\n\n\nCreate a partition! Make a Venn diagram.\n\\[%\\left(\n\\begin{array}{ccl}\nB &=& A \\cup (B \\cap A^C) \\\\\n\\mathbb{P}(B) &=& \\mathbb{P}(A) + \\mathbb{P}(B \\cap A^C) \\\\\n\\mathbb{P}(B)  & \\geq &   \\mathbb{P}(A),\n\\end{array}\n%\\right)\\] since \\(\\mathbb{P}(B \\cap A^C) \\geq 0\\)."
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#partitions-1",
    "href": "slides_solns/2_Probability-solutions.html#partitions-1",
    "title": "Chapter 2: Probability",
    "section": "Partitions",
    "text": "Partitions\n\n\nDefinition: Partition\n\n\nA set of events \\(\\{A_i\\}_{i=1}^{n}\\) create a partition of \\(A\\), if\n\nthe \\(A_i\\)’s are disjoint (mutually exclusive) and\n\\(\\bigcup \\limits_{i=1}^n A_i = A\\)\n\n\n\n\n\nExample 2\n\n\n\nIf \\(A \\subset B\\), then \\(\\{A, B \\cap A^C\\}\\) is a partition of \\(B\\).\nIf \\(S = \\bigcup \\limits_{i=1}^n A_i\\), then the \\(A_i\\)’s are a partition of the sample space.\n\n\n\nCreating partitions is sometimes used to help calculate probabilities, since by Axiom 3 we can add the probabilities of disjoint events."
  },
  {
    "objectID": "slides_solns/2_Probability-solutions.html#venn-diagram-probabilities-1",
    "href": "slides_solns/2_Probability-solutions.html#venn-diagram-probabilities-1",
    "title": "Chapter 2: Probability",
    "section": "Venn Diagram Probabilities",
    "text": "Venn Diagram Probabilities\n\n\n\nExample 3\n\n\nIf a subject has an\n\n80% chance of taking their medication this week,\n70% chance of taking their medication next week, and\n10% chance of not taking their medication either week,\n\nthen find the probability of them taking their medication exactly one of the two weeks.\n\nHint: Draw a Venn diagram labelling each of the parts to find the probability.Answer: \\(\\mathbb{P}(A \\cap B) = 0.6\\)."
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html",
    "href": "slides_solns/22_Counting_Intro-solutions.html",
    "title": "Chapter 22: Counting",
    "section": "",
    "text": "Example 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them if without replacements and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?\n\n\n\n\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n\\(10!\\)\n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\n\n\n\nneed 10 total?\n\n\\(10^{10}\\)\n\nneed 6 total?\n\n\\(10^6\\)\n\n\n\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.3\n\n\nHow many ways to order them if without replacements and only need 6?\n\n\n\\[\\frac{10!}{4!}=151,200\\]\n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?\n\n\n\\[\\frac{\\frac{10!}{4!}}{6!} = \\frac{10!}{4!6!}=210\\]\nThere are \\(6!\\) ways to order the 6 subjects."
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#basic-counting-examples-13",
    "href": "slides_solns/22_Counting_Intro-solutions.html#basic-counting-examples-13",
    "title": "Chapter 22: Counting",
    "section": "",
    "text": "Example 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them if without replacements and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#basic-counting-examples-23",
    "href": "slides_solns/22_Counting_Intro-solutions.html#basic-counting-examples-23",
    "title": "Chapter 22: Counting",
    "section": "",
    "text": "Suppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n\\(10!\\)\n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\n\n\n\nneed 10 total?\n\n\\(10^{10}\\)\n\nneed 6 total?\n\n\\(10^6\\)"
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#basic-counting-examples-33",
    "href": "slides_solns/22_Counting_Intro-solutions.html#basic-counting-examples-33",
    "title": "Chapter 22: Counting",
    "section": "",
    "text": "Suppose we have 10 (distinguishable) subjects for study.\n\n\nExample 1.3\n\n\nHow many ways to order them if without replacements and only need 6?\n\n\n\\[\\frac{10!}{4!}=151,200\\]\n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?\n\n\n\\[\\frac{\\frac{10!}{4!}}{6!} = \\frac{10!}{4!6!}=210\\]\nThere are \\(6!\\) ways to order the 6 subjects."
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#permutations-and-combinations-1",
    "href": "slides_solns/22_Counting_Intro-solutions.html#permutations-and-combinations-1",
    "title": "Chapter 22: Counting",
    "section": "Permutations and Combinations",
    "text": "Permutations and Combinations\n\n\nDefinition: Permutations\n\n\nPermutations are the number of ways to arrange in order \\(r\\) distinct objects when there are \\(n\\) total.\n\\[nPr = \\frac{n!}{(n-r)!}\\]\n\n\n\n\nDefinition: Combinations\n\n\nCombinations are the number of ways to choose (order doesn’t matter) \\(r\\) objects from \\(n\\) without replacement.\n\\[nCr = \\textrm{\"n choose r\"} = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\]"
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#some-combinations-properties",
    "href": "slides_solns/22_Counting_Intro-solutions.html#some-combinations-properties",
    "title": "Chapter 22: Counting",
    "section": "Some combinations properties",
    "text": "Some combinations properties\n\n\\[\\binom{n}{r} = \\binom{n}{n-r}\\]\n\\[\\binom{n}{1} = n\\]\n\\[\\binom{n}{0} = 1\\]"
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-12",
    "href": "slides_solns/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-12",
    "title": "Chapter 22: Counting",
    "section": "More examples: order matters vs. not (1/2)",
    "text": "More examples: order matters vs. not (1/2)\n\n\nExample 2\n\n\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\norder doesn’t matter?"
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-22",
    "href": "slides_solns/22_Counting_Intro-solutions.html#more-examples-order-matters-vs.-not-22",
    "title": "Chapter 22: Counting",
    "section": "More examples: order matters vs. not (2/2)",
    "text": "More examples: order matters vs. not (2/2)\nSuppose we draw 2 cards from a standard deck without replacement. What is the probability that both are spades when\n\norder matters?\n\n\\[\\frac{13P2}{52P2} = \\frac{\\frac{13!}{11!}}{\\frac{52!}{50!}} = \\frac{13\\cdot12}{52\\cdot51}\\]\n\norder doesn’t matter?\n\n\\[\\frac{\\binom{13}{2}}{\\binom{52}{2}} = \\frac{\\frac{13!}{2!11!}}{\\frac{52!}{2!50!}} = \\frac{13\\cdot12}{52\\cdot51}\\]"
  },
  {
    "objectID": "slides_solns/22_Counting_Intro-solutions.html#table-of-different-cases",
    "href": "slides_solns/22_Counting_Intro-solutions.html#table-of-different-cases",
    "title": "Chapter 22: Counting",
    "section": "Table of different cases",
    "text": "Table of different cases\nSee table on pg. 277 of textbook\n\n\\(n\\) = total number of objects\n\\(r\\) = number objects needed\n\n\n\n\n\n\n\n\n\nwith replacement\nwithout replacement\n\n\n\n\norder matters\n\\(n^r\\)\n\\(nPr = \\frac{n!}{(n-r)!}\\)\n\n\norder doesn’t matter\n\\(\\binom{n+r-1}{r}\\)\n\\(nCr = \\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\)"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Suppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?\n\n\n\n\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\nNote: When something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur.\n\n\n\n\n\n\n\n\nDefinition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all possible outcomes.\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some possible outcomes.\n\n\n\n\nWhat is the sample space?\n\n\\(S = \\{H, T\\}\\)\n\nWhat are the possible events?\n\n\\(\\{H\\}\\)\n\\(\\{T\\}\\)\n\\(\\{H, T\\}\\)\n\\(\\emptyset\\)\n\n \nWhen thinking about events, think about outcomes that you might be asking the probability of."
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-13",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-13",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Suppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-23",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-23",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Suppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\nNote: When something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur."
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-33",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-1-coin-33",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Definition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all possible outcomes.\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some possible outcomes.\n\n\n\n\nWhat is the sample space?\n\n\\(S = \\{H, T\\}\\)\n\nWhat are the possible events?\n\n\\(\\{H\\}\\)\n\\(\\{T\\}\\)\n\\(\\{H, T\\}\\)\n\\(\\emptyset\\)\n\n \nWhen thinking about events, think about outcomes that you might be asking the probability of."
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-2-coins",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#coin-toss-example-2-coins",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Coin Toss Example: 2 coins",
    "text": "Coin Toss Example: 2 coins\nSuppose you toss two coins.\n\nWhat is the sample space? Assume the coins are distinguishable\n\n\\(S =\\) \\[S = \\{HH, TT, HT, TH\\}\\]\n\nWhat are some possible events?\n\n\\(A\\) = exactly one \\(H\\) = \\(\\{HT, TH\\}\\)\n\\(B\\) = at least one \\(H\\) = \\(\\{HH, HT, TH\\}\\)"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#more-info-on-events-and-sample-spaces",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#more-info-on-events-and-sample-spaces",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "More info on events and sample spaces",
    "text": "More info on events and sample spaces\n\nWe usually use capital letters from the beginning of the alphabet to denote events. However, other letters might be chosen to be more descriptive.\nWe use the notation \\(|S|\\) to denote the size of the sample space.\nThe total number of possible events is \\(2^{|S|}\\), which is the total number of possible subsets of \\(S\\). We will prove this later in the course.\nThe empty set, denoted by \\(\\emptyset\\), is the set containing no outcomes."
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#example-keep-sampling-until",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#example-keep-sampling-until",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Example: Keep sampling until…",
    "text": "Example: Keep sampling until…\nSuppose you keep sampling people until you have someone with high blood pressure (BP). What is the sample space?\n\nLet \\(H =\\) denote someone with high BP.\nLet \\(H^C =\\) denote someone with not high blood pressure, such as low or regular BP.\nThen, \\(S =\\)\n\n\\[S = \\{H, (H, H^C), (H, H, H^C), (H, H, H, H^C), \\ldots\\]"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#set-theory-12",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#set-theory-12",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Set Theory (1/2)",
    "text": "Set Theory (1/2)\n\n\n\n\nDefinition: Union\n\n\nThe union of events \\(A\\) and \\(B\\), denoted by \\(A \\cup B\\), contains all outcomes that are in \\(A\\) or \\(B\\).\n\n\n\n\nDefinition: Intersection\n\n\nThe intersection of events \\(A\\) and \\(B\\), denoted by \\(A \\cap B\\), contains all outcomes that are both in \\(A\\) and \\(B\\).\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#set-theory-22",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#set-theory-22",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Set Theory (2/2)",
    "text": "Set Theory (2/2)\n\n\n\n\nDefinition: Complement\n\n\nThe complement of event \\(A\\), denoted by \\(A^C\\) or \\(A'\\), contains all outcomes in the sample space \\(S\\) that are not in \\(A\\) .\n\n\n\n\nDefinition: Mutually Exclusive\n\n\nEvents \\(A\\) and \\(B\\) are mutually exclusive, or disjoint, if they have no outcomes in common. In this case \\(A \\cap B = \\emptyset\\), where \\(\\emptyset\\) is the empty set.\n\n\n\nVenn diagrams"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-13",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-13",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (1/3)",
    "text": "BP example variation (1/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\nEvent all \\(n\\) subjects have high BP\nEvent at least one subject has high BP\nEvent all of them do not have high BP\nEvent at least one subject does not have high BP"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-23",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-23",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (2/3)",
    "text": "BP example variation (2/3)\n\nSuppose you have \\(n\\) subjects in a study.\nLet \\(H_i\\) be the event that person \\(i\\) has high BP, for \\(i=1\\ldots n\\).\n\nUse set theory notation to denote the following events:\n\nEvent subject \\(i\\) does not have high BP\n\\[H_i^C\\]\nEvent all \\(n\\) subjects have high BP\n\\[H_1 \\textrm{ and } H_2 \\textrm{ and } \\ldots = \\bigcap\\limits_{i=1}^{n}H_i\\]\nEvent at least one subject has high BP\n\\[H_1 \\textrm{ or } H_2 \\textrm{ or } \\ldots = \\bigcup\\limits_{i=1}^{n}H_i\\]"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-33",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#bp-example-variation-33",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "BP example variation (3/3)",
    "text": "BP example variation (3/3)\n\nEvent all of them do not have high BP\n\\(H_1^C\\) and \\(H_2^C\\) and... \\[\\bigcap\\limits_{i=1}^{n}H_i^C = \\Big(\\bigcup\\limits_{i=1}^{n}H_i\\Big)^C\\] = complement of at least one person having high BP\nEvent at least one subject does not have high BP\n\\(H_1^C\\) or \\(H_2^C\\) or... \\[\\bigcup\\limits_{i=1}^{n}H_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}H_i\\Big)^C\\] = complement of all having high BP"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#de-morgans-laws",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#de-morgans-laws",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "De Morgan’s Laws",
    "text": "De Morgan’s Laws\n\n\nTheorem: De Morgan’s 1st Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcap\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcup\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“all not A = \\((\\)at least one event A\\()^C\\)”\n\n\nTheorem: De Morgan’s 2nd Law\n\n\nFor a collection of events (sets) \\(A_1, A_2, A_3, \\ldots\\)\n\\[\\bigcup\\limits_{i=1}^{n}A_i^C = \\Big(\\bigcap\\limits_{i=1}^{n}A_i\\Big)^C\\]\n\n\n“at least one event not A = \\((\\)all A\\()^C\\)”"
  },
  {
    "objectID": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#remarks-on-de-morgans-laws",
    "href": "slides_solns/1_Outcomes_Events_Sample_space-solutions.html#remarks-on-de-morgans-laws",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Remarks on De Morgan’s Laws",
    "text": "Remarks on De Morgan’s Laws\n\nThese laws also hold for infinite collections of events.\nDraw Venn diagrams to convince yourself that these are true!\nThese laws are very useful when calculating probabilities.\n\nThis is because calculating the probability of the intersection of events is often much easier than the union of events.\nThis is not obvious right now, but we will see in the coming chapters why."
  },
  {
    "objectID": "slides/2_Probability.html",
    "href": "slides/2_Probability.html",
    "title": "Chapter 2: Probability",
    "section": "",
    "text": "Example 1\n\n\nSuppose you have a regular well-shuffled deck of cards. What’s the probability of drawing:\n\nany heart\nthe queen of hearts\nany queen\n\n\n\n\n\n\nIf \\(S\\) is a finite sample space, with equally likely outcomes, then\n\\[\\mathbb{P}(A) = \\frac{|A|}{|S|}.\\]\n\n\n\n\\(\\mathbb{P}(A)\\) is a function with\n\nInput: event \\(A\\) from the sample space \\(S\\), (\\(A \\subseteq S\\))\nOutput: a number between 0 and 1 (inclusive)\n\n\\[\\mathbb{P}(A): S \\rightarrow [0,1]\\]\nA function that follows some specific rules though!\n \nSee Probability Axioms on next slide."
  },
  {
    "objectID": "slides/0_Intro.html#lets-visit-the-website",
    "href": "slides/0_Intro.html#lets-visit-the-website",
    "title": "Welcome to BSTA 550!",
    "section": "Let’s visit the website",
    "text": "Let’s visit the website\n\nHomepage\n\nGitHub\n\nSyllabus\nSchedule\n\nWeeks, class info, exams, homeworks\n\nSearch\n\n\n\nImportant Note\n\n\nThis is my first time teaching the course. I will work hard to answer your questions in class, but I will often need some time outside of class to make sure I give you the best answer possible! Also, many of the examples are not my own. I will work to improve examples, but if you have feedback or suggestions, I am happy to hear them!"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html",
    "href": "slides/1_Outcomes_Events_Sample.html",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "",
    "text": "Suppose you toss one coin.\n\nWhat are the possible outcomes?\n \nWhat is the sample space?\n \nWhat are the possible events?\n\n\n\n\nSuppose you toss one coin.\n\nWhat are the possible outcomes?\n\nHeads (\\(H\\))\nTails (\\(T\\))\n\n\n \n\n\nNote\n\n\nWhen something happens at random, such as a coin toss, there are several possible outcomes, and exactly one of the outcomes will occur.\n\n\n\n\n\n\n\n\n\nDefinition: Sample Space\n\n\nThe sample space \\(S\\) is the set of all outcomes\n\n\n\n\nDefinition: Event\n\n\nAn event is a collection of some outcomes. An event can include multiple outcomes or no outcomes.\n\n\n\n\nWhat is the sample space?\n\n\\(S =\\)\n\n\n \n\nWhat are the possible events?\n\n\n\n\n\n\n \nWhen thinking about events, think about outcomes that you might be asking the probability of."
  },
  {
    "objectID": "homework/HW1.html#directions",
    "href": "homework/HW1.html#directions",
    "title": "Homework 1",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the HW1 datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as your HW1 .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "schedule/week_01_sched.html#why-is-the-number-of-possible-events-2s",
    "href": "schedule/week_01_sched.html#why-is-the-number-of-possible-events-2s",
    "title": "Week 1",
    "section": "Why is the number of possible events \\(2^{|S|}\\)?",
    "text": "Why is the number of possible events \\(2^{|S|}\\)?"
  },
  {
    "objectID": "weekly_pages/week_01_sched.html#resources",
    "href": "weekly_pages/week_01_sched.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording(s)\n\n\n\n\n\nIntro\n\n\n\n\n\n1\nOutcomes, Events, and Sample Space\n\n\n\n\n\n2\nProbability\n\n\n\n\n\n22\nIntroduction to Counting\n\n\n\n\n\n23\nCase Study on Counting\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "weekly_pages/week_01_sched.html#on-the-horizon",
    "href": "weekly_pages/week_01_sched.html#on-the-horizon",
    "title": "Week 1",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 0 due 9/28\nHomework 1 due 10/5"
  },
  {
    "objectID": "weekly_pages/week_01_sched.html#class-exit-tickets",
    "href": "weekly_pages/week_01_sched.html#class-exit-tickets",
    "title": "Week 1",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (9/25)\n Wednesday (9/27)"
  },
  {
    "objectID": "weekly_pages/week_01_sched.html#additional-information",
    "href": "weekly_pages/week_01_sched.html#additional-information",
    "title": "Week 1",
    "section": "Additional Information",
    "text": "Additional Information\nAs we start the course, here are some administrative items that we need to do:\n\nPlease join the Slack page\nPlease read the syllabus on your own time\n\n\nExtra Practice/Learning\n\nIf you would like a Calculus review, please see this page!\nCombinatorics practice problems\n\nhandout (& answers)\nTry to complete as many of these as you can before class on Wednesday.\nWe will discuss some of them Wednesday in class.\n\nPixar has a series of videos explaining how they use combinatorics in making animations\n\nThis is a great & fun introduction to the basic principles of counting\nI highly recommend looking at them, especially if you have not studied permutations and combinations before.\n\nThere is a table on p. 277 of the book with formulas for 4 different common counting cases (does order matter (y/n) vs. sampling with replacement (y/n).\n\nIn class we covered all cases except “order does not matter and sampling with replacement.”\n\nThis case is often referred to as the “stars and bars” problem.\nSee this page for a proof to the Stars and Bars Theorem.\n\nNote: Their notation is opposite of what our textbook uses. The website uses k instead of n and n instead of r."
  },
  {
    "objectID": "weekly_pages/week_01_sched.html#statistician-of-the-week-regina-nuzzo",
    "href": "weekly_pages/week_01_sched.html#statistician-of-the-week-regina-nuzzo",
    "title": "Week 1",
    "section": "Statistician of the Week: Regina Nuzzo",
    "text": "Statistician of the Week: Regina Nuzzo\n\n\n\n\n\n\n\nRegina Nuzzo\n\n\n\n\n\n\nDr. Nuzzo received her PhD in Statistics from Stanford University and is now Professor of Science, Technology, & Mathematics at Gallaudet University. Gallaudet University, federally funded and located in Washington, DC, is the only higher education institution where all programs are designed for the education of the deaf and hard of hearing. Dr. Nuzzo teaches statistics using American Sign Language.\nShe is the Senior Advisor for Statistics Communication and Media Innovation at the American Statistical Association and a freelance writer.\n\n\n\nTopics covered\nDr. Nuzzo is a statistician and a science journalist. Her work has appeared in Nature, Los Angeles Times, New York Times, Reader’s Digest, New Scientist, and Scientific American. Most of her work is in the “Health” or “Science” sections of the aforementioned outlets. Primarily, she works to help lay-audiences understand science and statistics in particular. She earned the American Statistical Association’s 2014 Excellence in Statistical Reporting Award for her article on p-values in Nature. Her work led to the ASA’s statement on p-values.\n\n\nRelevant work\n\nNuzzo, R. “Scientific method: Statistical errors.” Nature 506, 150–152 (2014).\nNuzzo, R. “Tips for Communicating Statistical Significance.” Science, Health, and Public Trust, National Institutes of Health, 2018.\nNuzzo, R. “Vying for a soul mate? Psych out the competition with science.” Health: Features. Los Angeles Times, 2008.\n\n\n\nOutside links\n\nWikipedia\nacademic\nLinkedin\npersonal\n\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin. I also invite you to check out this youtube video of her Women Rise Keynote address where she discusses her hearing impairment, career growth, and her work with p-values."
  },
  {
    "objectID": "weekly_pages/week_01_sched.html#muddiest-points",
    "href": "weekly_pages/week_01_sched.html#muddiest-points",
    "title": "Week 1",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why is the number of possible events \\(2^{|S|}\\)?\nIn class, we were wondering why/if \\(2^{|S|}\\) is the general formula for calculating the total number of possible events. We were specifically wondering if the \\(2\\) came from the fact that we had two options (heads and tails) for our outcome. Let’s work through the example of a 6-sided die to explain this further. The sample space is \\(S=\\{1, 2, 3, 4, 5, 6\\}\\). So is the total number of possible events \\(2^6\\) or \\(6^6\\) or something else? We can actually think about an event by using an indicator variable for each outcome of the sample space. An indicator variable is just a way to give us a yes/no answer to a question. So in this case, we are wondering: is this outcome a part of our event? If our event is \\(\\{1\\}\\) then for the outcome \\(1\\), the answer is “yes, the outcome is part of the event. For outcomes \\(2-6\\), the answer is”no, the outcome is not apart of the event.”\n\nFor each outcome, we have a “yes” or “no” answer. We can look at another example of an event. Let’s say our event is rolling an even number:\n\nFor \\(2\\), \\(4\\), and \\(6\\), the answer is “yes.” We can define the indicator variable for whether an outcome is in an event or not. The indicator gives a 1 or 0 for yes and no respectively.\n\nAs stated above, the \\(2\\) in \\(2^6\\) comes from the \\(2\\) options from our indicator. Each side has two options, and there are \\(6\\) sides. Thus, \\(2^6\\) possible events.\n\n\n2. What is an event??\nI think this will become clearer when we start thinking about events in the context of probability. When we think of events outside of probability, we may think of something we actually do or something that happens, like going to a concert or coming to class or missing the streetcar. In this case, we think of the event as the single thing (out of all the options) that actually occured. For example, if I’m taking the streetcar to class, I can think of two definitive options of what might occur: I miss the streetcar or I get on the streetcar. Only one of these things can occur, which I may call an event colloquially.\nIt is important to make the distinction with events defined within probability. Events are not necessarily a single thing that occurred. Instead it can be a collection of things that may occur. In the example of the streetcar, I can define my event to include both options. Thus, my event is that I make the streetcar or I miss it. Both of these things cannot happen simultaneously, but if I want to calculate the probability that I miss or make the streetcar, then it is helpful to have the event defined."
  },
  {
    "objectID": "weekly_pages/week_01_sched.html#clearest-points",
    "href": "weekly_pages/week_01_sched.html#clearest-points",
    "title": "Week 1",
    "section": "Clearest Points",
    "text": "Clearest Points\nMostly: heads/tails example, sample space, how to draw a quarter, possible events for two coins."
  },
  {
    "objectID": "weeks/week_01_sched.html#resources",
    "href": "weeks/week_01_sched.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording(s)\n\n\n\n\n\nIntro\n\n\n\n\n\n1\nReview\n\n\n\n\n\n\n2\nData Management\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser.\n\nFor example, in Chrome: I would click on the 3 vertical dots in the top right corner, then click Print, then change the Destination to “Save as PDF.”\nIt doesn’t seem to work well in Safari… Let me know if you’re having trouble.\n\n\nHere is the link to my Poll Everywhere!!"
  },
  {
    "objectID": "weeks/week_01_sched.html#on-the-horizon",
    "href": "weeks/week_01_sched.html#on-the-horizon",
    "title": "Week 1",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 0 due 1/11"
  },
  {
    "objectID": "weeks/week_01_sched.html#class-exit-tickets",
    "href": "weeks/week_01_sched.html#class-exit-tickets",
    "title": "Week 1",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (1/8)\n Wednesday (1/10)"
  },
  {
    "objectID": "weeks/week_01_sched.html#additional-information",
    "href": "weeks/week_01_sched.html#additional-information",
    "title": "Week 1",
    "section": "Additional Information",
    "text": "Additional Information\nAs we start the course, here are some administrative items that we need to do:\n\nPlease join the Slack page\nPlease read the syllabus on your own time"
  },
  {
    "objectID": "weeks/week_01_sched.html#statistician-of-the-week-regina-nuzzo",
    "href": "weeks/week_01_sched.html#statistician-of-the-week-regina-nuzzo",
    "title": "Week 1",
    "section": "Statistician of the Week: Regina Nuzzo",
    "text": "Statistician of the Week: Regina Nuzzo\n\n\n\n\n\n\n\nRegina Nuzzo\n\n\n\n\n\n\nDr. Nuzzo received her PhD in Statistics from Stanford University and is now Professor of Science, Technology, & Mathematics at Gallaudet University. Gallaudet University, federally funded and located in Washington, DC, is the only higher education institution where all programs are designed for the education of the deaf and hard of hearing. Dr. Nuzzo teaches statistics using American Sign Language.\nShe is the Senior Advisor for Statistics Communication and Media Innovation at the American Statistical Association and a freelance writer.\n\n\n\nTopics covered\nDr. Nuzzo is a statistician and a science journalist. Her work has appeared in Nature, Los Angeles Times, New York Times, Reader’s Digest, New Scientist, and Scientific American. Most of her work is in the “Health” or “Science” sections of the aforementioned outlets. Primarily, she works to help lay-audiences understand science and statistics in particular. She earned the American Statistical Association’s 2014 Excellence in Statistical Reporting Award for her article on p-values in Nature. Her work led to the ASA’s statement on p-values.\n\n\nRelevant work\n\nNuzzo, R. “Scientific method: Statistical errors.” Nature 506, 150–152 (2014).\nNuzzo, R. “Tips for Communicating Statistical Significance.” Science, Health, and Public Trust, National Institutes of Health, 2018.\nNuzzo, R. “Vying for a soul mate? Psych out the competition with science.” Health: Features. Los Angeles Times, 2008.\n\n\n\nOutside links\n\nWikipedia\nacademic\nLinkedin\npersonal\n\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin. I also invite you to check out this youtube video of her Women Rise Keynote address where she discusses her hearing impairment, career growth, and her work with p-values."
  },
  {
    "objectID": "weeks/week_01_sched.html#muddiest-points",
    "href": "weeks/week_01_sched.html#muddiest-points",
    "title": "Week 1",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Relationship between distributions\nJust to clarify! We will be using the distributions in the context of hypothesis testing. I just wanted you to see some of the cool connections between the distributions. (We don’t need to know the connections for a lot of what we do in this class.)\nAlso, I will discuss each distribution again as we hit the hypothesis tests that use them!\nThere is a big, scary (but fun!) infographic at the end of a famous stat textbook (Casella and Berger) that shows all the connections between distributions:\n\nWe mostly talk about the red, circled area. Each line with the directional arrow represents a specific transformation that is needed to go from the starting distribution to the distribution at the end of the arrow.\nAgain, this is NOT information we need to perform regression, but it is really interesting to see the connections between these distributions.\n\n\n2. A word about the distributions\nI feel like I might’ve scared us with all the distribution talk.\nI want to be clear: We will further discuss and explore the distributions as we use them within the course. It will be more important to understand their use within regression then knowing the distribution in depth. Basically, when we implement specific hypothesis tests, we just need to know which distribution is most appropriate for the test.\n\n\n3. Is the F distribution used for things outside of ANOVA?\nYes! In regression, we often use the F-distribution through the F-test (same as ANOVA) However, in regression, we are comparing the variance of two models, that may differ by a coefficient. See the STAT 501 page for more info if interested. In ANOVA, we are comparing variance between and within groups. Both use the same test, but with different goals!\n\n\n4. Multivariable vs multivariate?\nSome people misuse “multivariate” instead of “multivariable” modeling. In this class, we will only look at multivariable regression. Here’s the big difference:\n\nMultivariable: model with multiple independent variables (covariates, predictors)\n\nIf we want to see how our outcome (height) is related to parent height, birth country, sex assigned at birth, etc.\n\nMultivariate: model with multiple dependent variables (outcome)\n\nIf we want to extend the outcome from height to height and head circumference. Multivariate modeling would try to model both outcomes together and see how they are related to other variables.\n\n\n\n\n5. More on the functions and problems we had in class!\n\n\n\n\n\n\nImportant\n\n\n\nThis section has some examples from this YouTube video series. There is a video on mutate, pipe, filter, select, rename, arrange, and summarize. Note that some of the function in the series is outdated. The use of if_else() in the video is outdated, and it is more common to use case_when() now. The use of gather and spread have been replaced by pivot_longer and pivot_wider.\n\n\nTo discuss these functions below, I want to use a different dataset than what we used for examples in class. I’m hoping this allows us to see each function from a different angle. I’ll use the dataset that we used for some of the ggplot examples: mtcars. Let’s load the tidyverse and take a look at the dataset:\n\nlibrary(tidyverse)\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmutate()\nFor mutate, there are a few common uses:\n\ncreate another numeric variable by manipulating other variables\ncreate a categorical variable by creating cases from other variables\n\n\nCreate another numeric variable by manipulating other variables\nThe wt variable is the weight of the car in tons. Let’s say we want the full weight in pounds (lbs). I’ll create a new variable that is 1000 times (1 ton = 1000 lbs) the weight in the dataset.\n\nmtcars1 = mtcars %&gt;%\n  mutate(weight_lb = wt * 1000)\n\nglimpse(mtcars1)\n\nRows: 32\nColumns: 12\n$ mpg       &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, …\n$ cyl       &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, …\n$ disp      &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.…\n$ hp        &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180…\n$ drat      &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, …\n$ wt        &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.15…\n$ qsec      &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.9…\n$ vs        &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ am        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ gear      &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, …\n$ carb      &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, …\n$ weight_lb &lt;dbl&gt; 2620, 2875, 2320, 3215, 3440, 3460, 3570, 3190, 3150, 3440, …\n\n\nWe can also perform several mutations at the same time. Let’s say we want the weight in pounds AND the horse power per cylinder (hp per cyl). We can perform both manipulations:\n\nmtcars2 = mtcars %&gt;%\n  mutate(weight_lb = wt * 1000, \n         hp_per_cyl = hp/cyl)\n\nglimpse(mtcars2)\n\nRows: 32\nColumns: 13\n$ mpg        &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,…\n$ cyl        &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4,…\n$ disp       &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140…\n$ hp         &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 18…\n$ drat       &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92,…\n$ wt         &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.1…\n$ qsec       &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.…\n$ vs         &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,…\n$ am         &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,…\n$ gear       &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4,…\n$ carb       &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1,…\n$ weight_lb  &lt;dbl&gt; 2620, 2875, 2320, 3215, 3440, 3460, 3570, 3190, 3150, 3440,…\n$ hp_per_cyl &lt;dbl&gt; 18.33333, 18.33333, 23.25000, 18.33333, 21.87500, 17.50000,…\n\n\nYou can even use the same syntax if you need to change a variable that depends on a previous mutation. Let’s say I want the ratio of weight in pounds to the car’s horse power.\n\nmtcars3 = mtcars %&gt;%\n  mutate(weight_lb = wt * 1000, \n         w_to_hp = weight_lb / hp)\n\nglimpse(mtcars3)\n\nRows: 32\nColumns: 13\n$ mpg       &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, …\n$ cyl       &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, …\n$ disp      &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.…\n$ hp        &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180…\n$ drat      &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, …\n$ wt        &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.15…\n$ qsec      &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.9…\n$ vs        &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ am        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, …\n$ gear      &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, …\n$ carb      &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, …\n$ weight_lb &lt;dbl&gt; 2620, 2875, 2320, 3215, 3440, 3460, 3570, 3190, 3150, 3440, …\n$ w_to_hp   &lt;dbl&gt; 23.81818, 26.13636, 24.94624, 29.22727, 19.65714, 32.95238, …\n\n\n\n\nCreate a categorical variable by creating cases from other variables\nRecall in class we used mutate to label the numeric values of am to a categorical variable transmission. We create a new categorical variable from a binary, numeric variable.\n\nmtcars4 = mtcars %&gt;%\n  mutate(transmission = case_when(am == 0 ~ \"automatic\",\n                                  am == 1 ~ \"manual\"))\nglimpse(mtcars4)\n\nRows: 32\nColumns: 12\n$ mpg          &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.…\n$ cyl          &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, …\n$ disp         &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1…\n$ hp           &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, …\n$ drat         &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9…\n$ wt           &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3…\n$ qsec         &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2…\n$ vs           &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ am           &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ gear         &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, …\n$ carb         &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, …\n$ transmission &lt;chr&gt; \"manual\", \"manual\", \"manual\", \"automatic\", \"automatic\", \"…\n\n\nWe can also create a categorical variable from a continuous numeric variable. Let’s say we want to divide the miles per gallon into three categories: low, medium, and high. We can use mutate() and case_when() to do so:\n\nmtcars5 = mtcars %&gt;%\n  mutate(mpg_cat = case_when(mpg &gt; 22 ~ \"high\",\n                             mpg &gt; 15 ~ \"medium\", \n                             .default = \"low\"))\nglimpse(mtcars5)\n\nRows: 32\nColumns: 12\n$ mpg     &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17…\n$ cyl     &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,…\n$ disp    &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8,…\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ drat    &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.…\n$ wt      &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150,…\n$ qsec    &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90,…\n$ vs      &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n$ am      &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,…\n$ gear    &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,…\n$ carb    &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,…\n$ mpg_cat &lt;chr&gt; \"medium\", \"medium\", \"high\", \"medium\", \"medium\", \"medium\", \"low…\n\n\nNotice that I used .default in the last case. This means “for all other values of mpg, assign it to ‘low’.”\n\n\n\npipe %&gt;%\nCheck out the videos explanation! I can’t explain it much better!!\n\n\nselect() everything but a certain variable\nIt seems like we’re mostly okay with the select() function, but want more information on selecting everything but a certain varaible.\nWhen we select variables, we have the option to identify variables we want to keep or remove. If we want to keep a variable, we would just list the variable’s column name. If we want to remove a variable, we use the minus sign to let R know that we do NOT want that variable. We typically do not tell R to remove and keep variables within the same select() function. Let’s recall the variables within the original mtcars dataset:\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\nNow we want to keep the following variables: cyl, mpg, disp, and qsec.\n\nmtcars6 = mtcars %&gt;%\n  select(mpg, cyl, disp, qsec)\nglimpse(mtcars6)\n\nRows: 32\nColumns: 4\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n\n\nIn this case there was less typing to ID the variables we wanted to keep, so we inputted the column names. If we wanted to ID the variables we wanted to remove, what variables would identify to get the same remaining variables that are in mtcars6?\nSo we want to remove the following variables:\n\nmtcars7 = mtcars %&gt;%\n  select(-hp, -drat, -wt, -vs, -am, -gear, -carb)\nglimpse(mtcars7)\n\nRows: 32\nColumns: 4\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n\n\nSo both get the job done, but one is definitely quicker to type!\n\n\n\n\n\n\nWe don’t have to write the minus in front of every variable\n\n\n\nWhen we are removing several variables, we can combine them into a vector to remove:\n\nmtcars8 = mtcars %&gt;%\n  select(-c(hp, drat, wt, vs, am, gear, carb))\nglimpse(mtcars8)\n\nRows: 32\nColumns: 4\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n\n\nThe vector above is: c(hp, drat, wt, vs, am, gear, carb) and the minus sign in front of the vector will be applied to each variable.\n\n\n\n\npivot_longer()\nI want to address the function with the faculty dataset from class.\n\n# Note, I've put the data in a folder \"data\" that is in the same folder as this page's file\nstaff = read_csv(\"data/instructional-staff.csv\")\nstaff\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;\n\n\nNote that I am not using glimpse() here because we only have 5 rows corresponding to the faculty type. What would a glimpse of the data look like?\n\nglimpse(staff)\n\nRows: 5\nColumns: 12\n$ faculty_type &lt;chr&gt; \"Full-Time Tenured Faculty\", \"Full-Time Tenure-Track Facu…\n$ `1975`       &lt;dbl&gt; 29.0, 16.1, 10.3, 24.0, 20.5\n$ `1989`       &lt;dbl&gt; 27.6, 11.4, 14.1, 30.4, 16.5\n$ `1993`       &lt;dbl&gt; 25.0, 10.2, 13.6, 33.1, 18.1\n$ `1995`       &lt;dbl&gt; 24.8, 9.6, 13.6, 33.2, 18.8\n$ `1999`       &lt;dbl&gt; 21.8, 8.9, 15.2, 35.5, 18.7\n$ `2001`       &lt;dbl&gt; 20.3, 9.2, 15.5, 36.0, 19.0\n$ `2003`       &lt;dbl&gt; 19.3, 8.8, 15.0, 37.0, 20.0\n$ `2005`       &lt;dbl&gt; 17.8, 8.2, 14.8, 39.3, 19.9\n$ `2007`       &lt;dbl&gt; 17.2, 8.0, 14.9, 40.5, 19.5\n$ `2009`       &lt;dbl&gt; 16.8, 7.6, 15.1, 41.1, 19.4\n$ `2011`       &lt;dbl&gt; 16.7, 7.4, 15.4, 41.3, 19.3\n\n\nBoth views are indicators that the dataset is in a “wide” format where each year has its own column. We want our data to be in a tidy format, which means each column is a variable and each cell has a value. However, the years are actually values for a variable “year.” By using pivot_longer(), we can tell R to take those columns for years and make them their own column where year is the value. That means for a year like 1975, there are five numbers corresponding to the five faculty types. Those five numbers are the percentage of the specific faculty type in that year. So we want to end with columns: faculty type, year, and percentage.\nTo start with an easier implementation of pivot_longer(), let’s remove the faculty type using select()\n\nstaff2 = staff %&gt;%\n  select(-faculty_type)\nstaff2\n\n# A tibble: 5 × 11\n  `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007` `2009` `2011`\n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2   16.8   16.7\n2   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8      7.6    7.4\n3   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9   15.1   15.4\n4   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5   41.1   41.3\n5   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5   19.4   19.3\n\n\nNow we can pivot the years! We first identify the columns that we want to pivot. To pivot all the columns, we say cols = everything(). pivot_longer() knows you want the column names to now become values of a variable, but it does not know what to call that variable. So now we identify the new variable name of the column that will contain all the years (our old column names). We identify the new variable name with: names_to. Finally, we need to adress the old cell values that were under each year in our wide dataset. Those cell values will make up a new column/variable. Remember that each year had 5 values underneath it, so we need to include all 55 cell values. Similar to names_to, we need to identify the new column names for all those values. We use values_to to identify the column name for the cell values of our old wide formatted data.\n\nstaff_long = staff2 %&gt;%\n  pivot_longer(\n    cols = everything(),    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 2\n   year  percentage\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 1975        29  \n 2 1989        27.6\n 3 1993        25  \n 4 1995        24.8\n 5 1999        21.8\n 6 2001        20.3\n 7 2003        19.3\n 8 2005        17.8\n 9 2007        17.2\n10 2009        16.8\n11 2011        16.7\n12 1975        16.1\n13 1989        11.4\n14 1993        10.2\n15 1995         9.6\n16 1999         8.9\n17 2001         9.2\n18 2003         8.8\n19 2005         8.2\n20 2007         8  \n\n\nI included the first 20 rows so we could see that the years repeat. This is because there were 5 percentages for each year. While this is in the desired long format, we now see that we’re missing the information on faculty type. Each percentage in each year corresponded to a specific faculty type:\n\nstaff_long %&gt;% filter(year == 1975)\n\n# A tibble: 5 × 2\n  year  percentage\n  &lt;chr&gt;      &lt;dbl&gt;\n1 1975        29  \n2 1975        16.1\n3 1975        10.3\n4 1975        24  \n5 1975        20.5\n\n\nWhich percentage is for which faculty??\nWe could use another function called join() to try to remedy the situation, but it’s much easier to redo the pivot function. We will go back to staff which still has the faculty type:\n\nstaff\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;\n\n\nNow we can implement pivot_longer(). We will identify the columns we want to pivot as column 2 through 12 so we exclude the faculty type from the pivoting. BUT the really nice thing is that pivot_longer() will remember the percentages that correspond to a specific combination of faculty type and year! Let’s try it again:\n\nstaff_long2 = staff %&gt;%\n  pivot_longer(\n    cols = 2:12,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long2, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 3\n   faculty_type                   year  percentage\n   &lt;chr&gt;                          &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty      1975        29  \n 2 Full-Time Tenured Faculty      1989        27.6\n 3 Full-Time Tenured Faculty      1993        25  \n 4 Full-Time Tenured Faculty      1995        24.8\n 5 Full-Time Tenured Faculty      1999        21.8\n 6 Full-Time Tenured Faculty      2001        20.3\n 7 Full-Time Tenured Faculty      2003        19.3\n 8 Full-Time Tenured Faculty      2005        17.8\n 9 Full-Time Tenured Faculty      2007        17.2\n10 Full-Time Tenured Faculty      2009        16.8\n11 Full-Time Tenured Faculty      2011        16.7\n12 Full-Time Tenure-Track Faculty 1975        16.1\n13 Full-Time Tenure-Track Faculty 1989        11.4\n14 Full-Time Tenure-Track Faculty 1993        10.2\n15 Full-Time Tenure-Track Faculty 1995         9.6\n16 Full-Time Tenure-Track Faculty 1999         8.9\n17 Full-Time Tenure-Track Faculty 2001         9.2\n18 Full-Time Tenure-Track Faculty 2003         8.8\n19 Full-Time Tenure-Track Faculty 2005         8.2\n20 Full-Time Tenure-Track Faculty 2007         8  \n\n\nAnd now I’ll just dump a couple other ways to identify the columns we want to pivot:\n\nIn this one, we name the column years. It’s like 2:12, but it’s helpful when it’s hard to see what number the column is. Also, this is a special case because the columns are numbers, so we need to use ’ to wrap around the year. In the mtcars dataset, a similar approach would be cyl:vs to select all the variables between cyl and vs. You can also make a vector of variable names if they are not next to each other.\n\n\nstaff_long3 = staff %&gt;%\n  pivot_longer(\n    cols = '1975':'2011',    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long3, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 3\n   faculty_type                   year  percentage\n   &lt;chr&gt;                          &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty      1975        29  \n 2 Full-Time Tenured Faculty      1989        27.6\n 3 Full-Time Tenured Faculty      1993        25  \n 4 Full-Time Tenured Faculty      1995        24.8\n 5 Full-Time Tenured Faculty      1999        21.8\n 6 Full-Time Tenured Faculty      2001        20.3\n 7 Full-Time Tenured Faculty      2003        19.3\n 8 Full-Time Tenured Faculty      2005        17.8\n 9 Full-Time Tenured Faculty      2007        17.2\n10 Full-Time Tenured Faculty      2009        16.8\n11 Full-Time Tenured Faculty      2011        16.7\n12 Full-Time Tenure-Track Faculty 1975        16.1\n13 Full-Time Tenure-Track Faculty 1989        11.4\n14 Full-Time Tenure-Track Faculty 1993        10.2\n15 Full-Time Tenure-Track Faculty 1995         9.6\n16 Full-Time Tenure-Track Faculty 1999         8.9\n17 Full-Time Tenure-Track Faculty 2001         9.2\n18 Full-Time Tenure-Track Faculty 2003         8.8\n19 Full-Time Tenure-Track Faculty 2005         8.2\n20 Full-Time Tenure-Track Faculty 2007         8  \n\n\n\nThis one is the same as our in-class code. We “remove” faculty type from our identified columns\n\n\nstaff_long4 = staff %&gt;%\n  pivot_longer(\n    cols = -faculty_type,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n\nhead(staff_long4, 20) # I'm asking R to show me the first 20 rows\n\n# A tibble: 20 × 3\n   faculty_type                   year  percentage\n   &lt;chr&gt;                          &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty      1975        29  \n 2 Full-Time Tenured Faculty      1989        27.6\n 3 Full-Time Tenured Faculty      1993        25  \n 4 Full-Time Tenured Faculty      1995        24.8\n 5 Full-Time Tenured Faculty      1999        21.8\n 6 Full-Time Tenured Faculty      2001        20.3\n 7 Full-Time Tenured Faculty      2003        19.3\n 8 Full-Time Tenured Faculty      2005        17.8\n 9 Full-Time Tenured Faculty      2007        17.2\n10 Full-Time Tenured Faculty      2009        16.8\n11 Full-Time Tenured Faculty      2011        16.7\n12 Full-Time Tenure-Track Faculty 1975        16.1\n13 Full-Time Tenure-Track Faculty 1989        11.4\n14 Full-Time Tenure-Track Faculty 1993        10.2\n15 Full-Time Tenure-Track Faculty 1995         9.6\n16 Full-Time Tenure-Track Faculty 1999         8.9\n17 Full-Time Tenure-Track Faculty 2001         9.2\n18 Full-Time Tenure-Track Faculty 2003         8.8\n19 Full-Time Tenure-Track Faculty 2005         8.2\n20 Full-Time Tenure-Track Faculty 2007         8  \n\n\n\n\nacross()\nI really ran out of time before getting to this one. For now, you can look at the examples from this site to see the capabilities of across() . I invite you to try them out on the various datasets in our lecture.\n\n\n\n6. tbl_summary(): Trying to figure out how to change the median values to mean\nOh, wow! Turns out we solved it in class, but I made big mistake with my slides. The code for the table was in two places, but we fixed the one that was NOT running and showing on the slide!\nSo turns out, it worked!!\nHere’s the code:\n\nlibrary(tidyverse)  ## Need to load to use selec() and %&gt;%\nlibrary(gtsummary)  ## Needed package for tbl_summary()\n\ndata(\"dds.discr\")\n\ndds.discr1 = dds.discr %&gt;% \n  rename(SAB = gender, \n         R_E = ethnicity)\n\ndds.discr1 %&gt;%\n  select(-id, -age.cohort) %&gt;%\n  tbl_summary(label = c(age ~ \"Age\", \n                        R_E ~ \"Race/Ethnicity\", \n                        SAB ~ \"Sex Assigned at Birth\", \n                        expenditures ~ \"Expenditures\") ,\n              statistic = list(all_continuous() ~ \"{mean} ({sd})\"))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    Age\n23 (18)\n    Sex Assigned at Birth\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    Expenditures\n18,066 (19,543)\n    Race/Ethnicity\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n  \n  \n  \n    \n      1 Mean (SD); n (%)\n    \n  \n\n\n\n\n\n\n6. Are there benefits to ggplot compared to the base R graphing functions?\nThe main benefit I see for ggplot is that the syntax and grammar of our coding in tidyr and dplyr is very similar to ggplot. Your effort in strengthening one will help with the others.\nI am certainly not going to force you to use ggplot over base R. At the end of the day, it is really whatever makes the most sense to you. I will say: ggplot2 seems to be where most statisticians and epidemiologists are headed. And I really believe that ggplot is more efficient with coding."
  },
  {
    "objectID": "weeks/week_01_sched.html#clearest-points",
    "href": "weeks/week_01_sched.html#clearest-points",
    "title": "Week 1",
    "section": "Clearest Points",
    "text": "Clearest Points\nMostly: heads/tails example, sample space, how to draw a quarter, possible events for two coins."
  },
  {
    "objectID": "weeks/week_02_sched.html#resources",
    "href": "weeks/week_02_sched.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n3\nSimple Linear Regression\n\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser."
  },
  {
    "objectID": "weeks/week_02_sched.html#on-the-horizon",
    "href": "weeks/week_02_sched.html#on-the-horizon",
    "title": "Week 2",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 1 due 1/18\nHomework 1 due 1/25"
  },
  {
    "objectID": "weeks/week_02_sched.html#class-exit-tickets",
    "href": "weeks/week_02_sched.html#class-exit-tickets",
    "title": "Week 2",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Wednesday (1/17)"
  },
  {
    "objectID": "weeks/week_02_sched.html#additional-information",
    "href": "weeks/week_02_sched.html#additional-information",
    "title": "Week 2",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_02_sched.html#statistician-of-the-week-talithia-williams",
    "href": "weeks/week_02_sched.html#statistician-of-the-week-talithia-williams",
    "title": "Week 2",
    "section": "Statistician of the Week: Talithia Williams",
    "text": "Statistician of the Week: Talithia Williams\n\n\n\n\n\n\n\nTalithia Williams\n\n\n\n\n\n\nDr. Williams earned a BS in Mathematics from Spelman College, an MS in Mathematics from Howard University, and a PhD (2008) in Statistics from Rice University. Dr. Williams is Associate Professor and Director of the Clinic Program at Harvey Mudd College. She has also served as Associate Dean for Faculty Development and Diversity at Harvey Mudd.\n\n\n\nTopics covered\nDr. Williams works on statistical models which describe spatial and temporal aspects of data. Some of her most important work has focused on developing models to predict cataract surgical rates for countries in Africa.\n\n\nRelevant work\n\nDray, A. and Williams, T. An incidence estimation model for multi-stage diseases with differential mortality. Statistics in Medicine, 2012.\nLewallen, S., Courtright, P., Etya’ale, D., Mathenge, W., Schmidt, E., Oye, J., Clark, A., Williams, T. Cataract Incidence in Sub-Saharan Africa: What does Mathematical Modeling tell us about Geographic Variations and Surgical Needs?. Ophthalmic epidemiology, 2013.\n\n\n\nOutside links\n\nWikipedia\nMAD\nLinkedin\npersonal\n\n\n\nOther\nDr. Williams was the co-host of the 2018 PBS Nova Wonders series.\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin."
  },
  {
    "objectID": "weeks/week_02_sched.html#muddiest-points",
    "href": "weeks/week_02_sched.html#muddiest-points",
    "title": "Week 2",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. What does the epsilon mean and how does it relate to the line in the linear model?\n\\(\\epsilon\\) is our error term, our residual. It is the difference between our observed value \\(Y\\) and the expected value of \\(Y\\) given \\(X\\). It’s a mathematical way to represent the fact that not every oberved \\(Y\\) value directly falls on our line. \\(\\epsilon\\) is the difference between our line and our observed value for \\(Y\\).\n\n\n2. Different betas and stuff: make the table for the class!! and epsilon\nBelow is a table that I started to construct with a student after class. We often use the model or the line to represent linear regression. When we refer to the model, most people think of the row named model. The line is just another way to represent the model. Remember that \\(\\epsilon = Y - E(Y|X)\\) and \\(\\widehat\\epsilon = Y - \\widehat{E}(Y|X)\\). Try substituting \\(\\epsilon = Y - E(Y|X)\\) into the population model \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Does it simplify to the population line?\nI think it can help a lot with this confusion.\n\n\n\n\n\n\n\n\n\nPopulation\nEstimated\n\n\n\n\nModel\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\\[Y = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X + \\widehat\\epsilon \\]\n\n\nLine\n\\[E(Y|X) = \\beta_0 + \\beta_1 X \\] OR\n\\[\\mu_Y = \\beta_0 + \\beta_1 X \\]\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\]\nOR\n\\[\n\\widehat{E}[Y|X] = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\n\\]\nOR\n\\[\n\\widehat{E[Y|X]} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\n\\]\n\n\n\n\n2.1 Someone else asked: Why does the population model have an error term epsilon in the equation but the estimated line does not?\nI think this is referring to this slide. This was because I wanted to put the population model next to the estimated line. I realize this is very confusing. Both estimated and population models can be represented as the lines and models in the above table.\n\n\n2.2 Someone else asked: Why does the population equation even matter?\nHuh, I’m scratching my head with this one. Why does it matter? We basically mirror all the mathematical manipulations with the estimated model anyway…\nBut then I thought: What would our world or our class lectures look like without the population model? The answer might be more philosophical than mathematical. The representation of the true, underlying model that we are aspiring for with our sample data reminds us that our estimated model is not perfect. That we are just trying out best to uncover some fraction of the truth. And at the end of the day, when we perform hypothesis tests, we’re working to provide evidence fro the value of the coefficient parameters from the population model. We know what the estimated values are, but can they help us get an idea of what the parameter values are?\n\n\n\n3. Math for minimizing SSE (aka OLS process)\nI am very sorry that this math was intimidating! Most of us don’t need to see the math, but there are a handful of students that should see it, and get a sense of the underlying math. Just wanted to make sure they saw it!\nThe important things for us to know is the information on the slide for Step 1 and 2, where we talk about the process itself. If I asked you why we minimize the SSE with respect to our coefficients, would you be able to answer?"
  },
  {
    "objectID": "weeks/week_02_sched.html#clearest-points",
    "href": "weeks/week_02_sched.html#clearest-points",
    "title": "Week 2",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_04_sched.html#resources",
    "href": "weeks/week_04_sched.html#resources",
    "title": "Week 4",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n6\nSLR: Model Diagnostics 1\n\n\n\n\n\n7\nSLR: Model Diagnostics 2"
  },
  {
    "objectID": "weeks/week_04_sched.html#on-the-horizon",
    "href": "weeks/week_04_sched.html#on-the-horizon",
    "title": "Week 4",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 2 due 2/1\nLab 2 due 2/8"
  },
  {
    "objectID": "weeks/week_04_sched.html#class-exit-tickets",
    "href": "weeks/week_04_sched.html#class-exit-tickets",
    "title": "Week 4",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (1/29)\n Wednesday (1/31)"
  },
  {
    "objectID": "weeks/week_04_sched.html#additional-information",
    "href": "weeks/week_04_sched.html#additional-information",
    "title": "Week 4",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_04_sched.html#statistician-of-the-week-joy-buolamwini",
    "href": "weeks/week_04_sched.html#statistician-of-the-week-joy-buolamwini",
    "title": "Week 4",
    "section": "Statistician of the Week: Joy Buolamwini",
    "text": "Statistician of the Week: Joy Buolamwini\n\n\n\n\n\n\n\nJoy Buolamwini\n\n\n\n\n\n\nDr. Buolamwini earned a BS in Computer Science from Georgia Institute of Technology, an Master’s from University of Oxford, and MS and PhD (2022) degrees in Media Arts & Sciences from Massachusetts Institute of Technology. While a graduate student, Dr. Buolamwini was part of the MIT Media Lab. Additionally, she is the founder of the Algorithmic Justice League.\n\n\n\nTopics covered\nDr. Buolamwini has done substantial work demonstrating how algorithms can encode bias. Her undergraduate senior project was to create a inspired “mask” mirror as a way to raise spirits for the person who looked into the mirror. The project relied on off the shelf facial recognition software that could not recognize Dr. Buolamwini’s face.\nSince then, she has focused her work on demonstrating bias across racial and gender spectra in off the shelf software. Her work has been cited as directly influencing Microsoft and Google’s changes to their algorithms.\nAmong many other aspects, a big focus of Dr. Buolamwini’s work is pointing out the biased data which directly impacts how algorithms learn how to do tasks.\n\n\nRelevant work\n\nBuolamwini, J., Gebru, T. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. Proceedings of Machine Learning Research 81:1–15, 2018 Conference on Fairness, Accountability, and Transparency.\nRaji, I & Buolamwini, J. Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products. Conference on Artificial Intelligence, Ethics, and Society, 2019\n\n\n\nOutside links\n\nWikipedia\nLinkedin\npersonal\n\n\n\nOther\nDr. Buolamwini has done a lot of work on how data propagates through systems to encode the same types of bias into different algorithms. In her video AI, Ain’t I a Woman? she demonstrates how systems designed to determine gender are particularly poor when using dark skinned faces.\nHer work was featured in a recent documentary Coded Bias.\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin."
  },
  {
    "objectID": "weeks/week_04_sched.html#muddiest-points",
    "href": "weeks/week_04_sched.html#muddiest-points",
    "title": "Week 4",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Equality of the residuals - what’s the bias refer in a residual plot? Is that suggesting a non linear relationship between two variables?\nHere is the plot that this question is referring to:\n\n\n\n\n\nThe answer is already in the question! The residual plot can also be used to look at linearity! The above plots that say “unbiased” mean they do not follow the linearity assumption.\n\n\n2. QQ Plot: What is it? And can you explain the axes, meaning of “quantiles”, and why assuming normality would result in a straight line?\nI cannot answer this question better than this video! They go through a smaller dataset of gene expression values and how to make a QQ plot from the data. Remember, our QQ plot is of our residual values!!\n\n\n3. I’m still a little confused on how to determine if a dataset has a normal distribution. Feels like a subjective decision.\nFirst thing that I want to address: when we are talking about normality, we are not determining if the dataset follows a normal distribution. We are determining if the fitted model violates the normality assumption that we need to use in our population model. We do this by seeing if the fitted residuals follow a normal distribution. I just want to draw attention to this. There is very particular language being used here.\nSecond thing… Yes! These diagnostic tools are somewhat subjective. You are welcome to use the Shapiro-Wilk test every time you look at a QQ plot! I realize a test with a conclusion might feel more objective and comfortable as we are learning about the model diagnostics. I suggest trying to make a conclusion visually with a QQ plot, then see if it matches the Shapiro-Wilk test. Remember, even in the Shapiro-Wilk test, the null hypothesis is that the fitted residuals come from a normal distribution. So we have to work to disprove that. You can come to the QQ plot with that same prior. If the QQ plot gives blantent evidence that the fitted residuals are not normally distributed, then we violate the assumption.\nWe’ll keep practicing! As we keep going through regression, we’ll realize that model building is very much an art! There is no one answer in statistics!\n\n\n4. What are the small nuances in interpreting the normality through a QQ plot?\nThanks for this question! This helped me realize that I was not articulating very well some of my more subconscious thoughts in a QQ plot.\nBelow are the distribution samples ant their QQ plots from lecture:\n\n\n\n\n\nI drew red, blue and green lines to bracket certain areas of the plots. I basically start by looking within the red brackets. Do all the points seem to stay close to the black line? If this doesn’t hold for the red bracketed area, then I would say our fitted residuals are not normal. Then I look at the area from the red lines to the blue lines. This is less definite, but if the points don’t seem to stay close to the black line, then I’d say our fitted residuals are not normal. Then I’d look at the are between the blue and green line. If the points aren’t close to the black line, then I am likely okay with it and would NOT make the conclusion that the fitted residuals are NOT normal. Notice, that I am not saying I call them normal. They seem to not violate the normal assumption.\nExtra note: The t-distribution is similar to a normal, but it includes larger tails. This is to adjust the normal distribution when our sample size of data is small. However, our assumption aims for fitted residuals to follow a normal distribution. We can be a little more flexible with the QQ plot when we have a smaller sample size, but we should not aim for a t-distribution. Both the normal and t-distribution samples “passed” my normality assessment.\nWe can check out the example:\n\n\n\n\n\nIn this example, I would say that the fitted residuals violate the normal assumption. Notice that we have points off the black line between the red and blue lines. And even within the red lines, we have some curve. This is okay for our example! That’s because we have not yet included other (likely needed) variables in the model. And what does that mean? The other variables in the model will help explain MORE variance in our Y, which would alter the fitted residuals!!\nDraw the red, blue, and green lines on the other QQ plot slides. See what you find, especially when we have different sample sizes!\n\nStill compiling these muddy points:\n\n\nExplain formula for internally standardized residuals. 2. How do we determine WHAT the source of the issue is for an outlier… i.e. “outlier detection”. Is it just highly situational most times? 3. Leverage (h sub i) what is the significance of it? What does it “tell us” as values change?\n“cubic or square transformations must include original X, don’t do this for Y”\nThe transformations of the X and Y started to feel really abstract and kinda confusing\nthe relationship between what we’re learning about models and how we’ll actually apply that knowledge to a figure in a report, for example\nI understand the reasoning for transforming to make the data to fit the LINE assumptions, but if it makes it hard to interpret, why do we do it?\n\nHi and Hat metrics and not knowing the magnitude or reason for it was confusing.\nhttps://online.stat.psu.edu/stat501/lesson/11/11.2\n\n\nwhat are some other real life examples of influential points besides human error. Can they appear more organically\nI’m a little confused in the section about the power ladder – we’re supposed to be looking at the skew of the distribution of the residuals, but the examples shown were of different distributions (x alone, y alone, and x vs y). What should we be looking at to determine what transform, if any, to use?\nI’m not super clear on why transformations are even ok to do if they fundamentally change the shape of the data. You had a good sentence right at the end (second to last slide?) – “for every change in x, there’s a (…) change in longevity squared” or something like that? I think it’d be helpful to repeat or elaborate on that.\nI’m still not very clear on how to identify outliers vs high leveraged data points by looking at a plot alone.\n\n\nWhat is the reasoning behind why we usually want to transform the independent variable instead of the dependent?\nThis is more about interpretations in multiple linear models. We can have a model with multiple variables as covariates (so multiple X’s on the right side of the equation). If we transform Y, then all the interpretations with every covariate changes. If we can fit model assumptions better with a transformation to one X, then we only change one interpretation!\n\n\n\n\n\n\n\nIn the Tukey’s power ladder, is log(x) mean log10(x) or natural log(x)?\n\n\n\n\n\nExtra materials with examples of transformations\nhttps://www.youtube.com/watch?v=HIcqQhn3vSM&ab_channel=jbstatistics\nhttps://online.stat.psu.edu/stat501/lesson/9"
  },
  {
    "objectID": "weeks/week_04_sched.html#clearest-points",
    "href": "weeks/week_04_sched.html#clearest-points",
    "title": "Week 4",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_03_sched.html#resources",
    "href": "weeks/week_03_sched.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n3\nSimple Linear Regression\ncontinued\n\n\n\n\n\n\n4\nSLR: Inference and Prediction\n\n\n\n\n\n\n\n5\nSLR: More Inference\n\n\n\n\n\n\n\nPoll Everywhere Questions\n\n\nQuiz 1 Information\n\nWe will be in RLSB 3A003 B!!!\nGeneral structure\n\nIt will be a maximum of 15 questions\n\n~ 10 multiple choice questions (including T/F)\n~ 3 free response questions\n\n\nWhat will it cover?\n\nLesson 2 (Data Management) to Lesson 4 (SLR: Inference, except the mean response)\n\nSo up to what we covered on Monday 1/22\n\nHW 0 - 1\n\nWhat can you expect?\n\nMostly concept questions\nYou may need to recognize what certain, important functions do\nYou may need to recognize a number from R output (like the regression table on slide 4 in Lesson 4 slides)\n\nInstructions that will be on the quiz:\n\n\nI have written a “30 minute” quiz. However, you have 50 minutes from 2:00 - 2:50pm.\nThe quiz is open book and open notes. You may use books other than the class textbook, you may use anything on our course webpage, and you may use reference websites (like Wikipedia, Googling expected value of specific distribution, etc.).\nNo cheating will be tolerated. Cheating includes:\n\nUsing ChatGPT\nUsing question and answer threads typically seen on sites like StackExchange, WikiHow, Quora, Reddit, StackOverflow, Chegg, etc.\nAsking other students in the room or looking at other students’ quiz work."
  },
  {
    "objectID": "weeks/week_03_sched.html#on-the-horizon",
    "href": "weeks/week_03_sched.html#on-the-horizon",
    "title": "Week 3",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 1 due 1/25\nQuiz 1 on 1/29!\n\nWill cover up through today and HW1\n\nHomework 2 due 2/1"
  },
  {
    "objectID": "weeks/week_03_sched.html#class-exit-tickets",
    "href": "weeks/week_03_sched.html#class-exit-tickets",
    "title": "Week 3",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (1/22)\n Wednesday (1/24)"
  },
  {
    "objectID": "weeks/week_03_sched.html#additional-information",
    "href": "weeks/week_03_sched.html#additional-information",
    "title": "Week 3",
    "section": "Additional Information",
    "text": "Additional Information\n\nAnnouncements for 10/9\n\nHomework 1 solutions are posted!\n\n\n\nAnnouncements for 10/11\n\nHomework 1 released! Here are some overarching notes:\n\nIf you start with “the probability that …” you must end with a value between 0 and 1.\n\nIf you start with “the percent chance…” then you can have a value between 0% and 100%\n\n\\(P(A)\\) is not the same as probability of A only. Event A can overlap with B and C, so we need to find the part of A that does not overlap with B and C\n\nHence we get \\(P(A \\cap B^c \\cap C^c)\\)\n\n\nGroup advising tomorrow at 5pm in Vanport 515!\n\nIf you can’t get in, you can also Slack me\n\nRising Voices Retreat"
  },
  {
    "objectID": "weeks/week_03_sched.html#statistician-of-the-week-david-blackwell",
    "href": "weeks/week_03_sched.html#statistician-of-the-week-david-blackwell",
    "title": "Week 3",
    "section": "Statistician of the Week: David Blackwell",
    "text": "Statistician of the Week: David Blackwell\n\n\n\n\n\n\n\nDavid Blackwell\n\n\n\n\n\n\nBlackwell was the first black person to receive a PhD in statistics (from University of Illinois at Urbana-Champaign, in 1941 at the age of 22) in the US and the first black scholar to be admitted to the National Academy of Sciences. He was a statistician at UC Berkeley for more than 50 years. He was hired in 1954 after the department almost made him an offer in 1942 (but declined to do so when one faculty member’s wife said she didn’t want Blackwell hired because she wouldn’t feel comfortable having faculty events in her home with a black man). Hear Blackwell tell the story in his own words.\n\n\n\nTopics covered\nBlackwell contributed to game theory, probability theory, information science, and Bayesian statistics. The Rao-Blackwell theorem (you’ll likely see it in BSTA 551) is named after him.\n\n\nRelevant work\n\nBlackwell, D. (1947). “Conditional expectation and unbiased sequential estimation”. Annals of Mathematical Statistics. 18 (1): 105–110. doi:10.1214/aoms/1177730497.\n\n\n\nOutside links\n\nWikipedia\nMAD\n\nPlease note the statisticians of the week are taken directly from the CURV project by Jo Hardin."
  },
  {
    "objectID": "weeks/week_03_sched.html#muddiest-points",
    "href": "weeks/week_03_sched.html#muddiest-points",
    "title": "Week 3",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. The lecture materials don’t always feel like they apply to the homework. If asked to “state the linear regression models,” are we just running lm()?\nStating the linear regression model is asking us to show the population model that we are fitting. This is just to make sure we are aware of the model that we plan to fit. So the generic form of this is: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\).\nRunning lm() is the equivalent of fitting the model.\nKeep letting me know what feels disconnected in the class! Sometimes I purposefully say things in different ways to build our understanding, but sometimes that fails!\n\n\n2. “all of the different manifestations of t”\nI love the way this person said it!\nSo I’ve sorted this out:\n\nWe say \\(T\\) follows a t-distribution\n\n\\(T\\) is the general name for the variable (like \\(X\\) or \\(Y\\))\n\nWe calculate a given \\(t\\)-value and call that \\(t\\)\n\nWe also call this the test statistic\n\nThe critical value that corresponds to a specific confidence interval and \\(\\alpha\\) is labelled \\(t^*\\)\n\n\n\n3. What’s the difference between SD and variance?\nSD (standard deviation) is the square root of the variance. That’s why I sometimes write \\(\\sigma\\) (standard deviation) or \\(\\sigma^2\\) (variance) when I’m talking about the distribution of residuals.\n\\[\n\\sigma = \\sqrt{\\sigma^2}\n\\]\nVariance is usually easier to work with mathematically, but standard deviation is in the units that match a variable. For example, the variance of 10 height measurements are in square inches, but the standard deviation are in inches.\n\n\n4. Why is it important to test if \\(\\beta_1\\) is equal to zero? Is \\(\\beta_1=0\\) the same as the x and y variables having no correlation?\nLet’s answer the second question: Yes! It is the same in simple linear regression. When we get to multiple linear regression, and have several variables/coefficients in our model, testing \\(\\beta_1=0\\) won’t be the same as testing the correlation.\nIn simple linear regression, it is important to test \\(\\beta_1\\) mostly for pedagogical reasons. It’s just helpful to establish the process in a simpler setting.\n\n\n5. SSE and sigma\nWe were looking at the relationship between SSE and \\(\\widehat\\sigma^2\\):\n\\[ \\widehat{\\sigma}^2 = \\frac{1}{n-2}SSE \\]\nThe sum of square errors is \\(SSE = \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n \\epsilon_i^2\\)\n\n\n\n\n\n\nAn aside on variance\n\n\n\nThe definition of variance is the sum of the squared differences between values and their mean.\nSo if I had a variable \\(S\\), with 100 observations, the mean of \\(S\\), which we call \\(\\overline{S}\\), would be \\(\\frac{\\sum_{i=1}^{100} S_i}{100}\\). The variance of \\(S\\) would be \\(\\sum_{i=1}^{100} (S_i - \\overline{S})^2\\).\n\n\nNow, let’s get back to the sum of square errors: \\(SSE = \\sum_{i=1}^n \\epsilon_i^2\\)\nThe variance of the residuals would be \\(\\sum_{i=1}^n (\\epsilon_i - \\overline{\\epsilon})^2\\). The mean of \\(\\epsilon\\), \\(\\overline\\epsilon\\), should be 0 by our assumptions. So the variance of the residuals is \\(\\sum_{i=1}^n \\epsilon_i^2\\) which is our SSE!\nThere is some more complicated math that goes into why our variance is divided by n-2 to get the estimated variance of the residuals, but that’s basically it!\n\n\n6. It would be helpful to get some clarification on the notations that we need to use in this class. Perhaps, a chart or the types of notations for this class could be helpful to organize this?\nI can certainly work on this! In all honesty, I am a little overwhelmed with work this week, so I don’t think this is something I can produce by the quiz. If you want to get one started, I can share it! I think would be a really good thing to help you study, too!\n\n\n7. What is the relationship between the ANOVA for linear regression and ANOVA for group differences?\n\n\n8. The limitations of the different tests, like an F statistic vs. t-test\n\n\n9. unexplained vs the explained\n\n\n10. Changing the confidence level in tidy()\nHere is a good site about the input! Looks like we would use conf.level to change 95% confidence interval to some other percent."
  },
  {
    "objectID": "weeks/week_03_sched.html#clearest-points",
    "href": "weeks/week_03_sched.html#clearest-points",
    "title": "Week 3",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_11_sched.html#resources",
    "href": "weeks/week_11_sched.html#resources",
    "title": "Week 11",
    "section": "Resources",
    "text": "Resources\nFinal exam week!"
  },
  {
    "objectID": "weeks/week_11_sched.html#on-the-horizon",
    "href": "weeks/week_11_sched.html#on-the-horizon",
    "title": "Week 11",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "weeks/week_11_sched.html#end-of-quarter-feedback",
    "href": "weeks/week_11_sched.html#end-of-quarter-feedback",
    "title": "Week 11",
    "section": "End of quarter feedback!",
    "text": "End of quarter feedback!"
  },
  {
    "objectID": "weeks/week_11_sched.html#additional-information",
    "href": "weeks/week_11_sched.html#additional-information",
    "title": "Week 11",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_11_sched.html#statistician-of-the-week-maricela-cruz",
    "href": "weeks/week_11_sched.html#statistician-of-the-week-maricela-cruz",
    "title": "Week 11",
    "section": "Statistician of the Week: Maricela Cruz",
    "text": "Statistician of the Week: Maricela Cruz\n\n\n\n\n\n\n\nMaricela Cruz\n\n\n\n\n\n\nDr. Cruz did her undergraduate work at Pomona College, majoring in mathematics. Her PhD in Statistics is from University of California, Irvine. She is now Assistant Biostatistics Investigator, Kaiser Permanente Washington Health Research Institute (KPWHRI) and Affiliate Assistant Investigator, Department of Biostatistics, University of Washington.\nIn an interview done by Lathisms, Dr. Cruz reminds us:\n\nNavigating institutions that have systematically excluded groups of people can be taxing, especially for people from one or more of these groups. Surround yourself with those who believe in you, give you the space to ask ‘silly’ questions, value your input, and/or understand your struggles. Do activities that will help you blow off steam. No one person or activity will meet all your support needs, so find the right group and balance for you.\n\n\n\n\nTopics covered\nDr. Cruz’s dissertation work was on interrupted time series models used to determine intervention timing. Indeed, her primary research questions are to understand complex health interventions. At KPWHRI she has continued to work on epidemiological methods, particularly those appropriate for longitudinal and multilevel data.\n\n\nRelevant work\n\nCruz M, Ombao H, Gillen DL, A Generalized Interrupted Time Series Model for Assessing Complex Health Care Interventions. Statistics in Biosciences, 2022.\nCruz M, Gillen DL, Bender M, & Ombao H, Assessing health care interventions via an interrupted time series model: study power and design considerations. Statistics in Medicine, 2019.\n\n\n\nOutside links\n\nLathisms\nLinkedin\nKaiser Permanente"
  },
  {
    "objectID": "weeks/week_11_sched.html#muddiest-points",
    "href": "weeks/week_11_sched.html#muddiest-points",
    "title": "Week 11",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_11_sched.html#clearest-points",
    "href": "weeks/week_11_sched.html#clearest-points",
    "title": "Week 11",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_06_sched.html#resources",
    "href": "weeks/week_06_sched.html#resources",
    "title": "Week 6",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n10\nCategorical Covariates\n\n\n\n\n\n\n11\nInteractions"
  },
  {
    "objectID": "weeks/week_06_sched.html#on-the-horizon",
    "href": "weeks/week_06_sched.html#on-the-horizon",
    "title": "Week 6",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 3 due 2/15\nPresident’s Day on 2/19 (no class)\nQuiz 2 due 2/21\nHomework 4 due 2/22"
  },
  {
    "objectID": "weeks/week_06_sched.html#class-exit-tickets",
    "href": "weeks/week_06_sched.html#class-exit-tickets",
    "title": "Week 6",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (2/12)\n Wednesday (2/14)"
  },
  {
    "objectID": "weeks/week_06_sched.html#additional-information",
    "href": "weeks/week_06_sched.html#additional-information",
    "title": "Week 6",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_06_sched.html#statistician-of-the-week-lester-mackey",
    "href": "weeks/week_06_sched.html#statistician-of-the-week-lester-mackey",
    "title": "Week 6",
    "section": "Statistician of the Week: Lester Mackey",
    "text": "Statistician of the Week: Lester Mackey\n\n\n\n\n\n\n\nLester Mackey\n\n\n\n\n\n\nDr. Mackey is a machine learning researcher at Microsoft Research New England and an adjunct professor at Stanford University. His PhD (Computer Science 2012) and MA (Statistics 2011) are both from University of California, Berkeley, while his undergraduate degree (Computer Science 2007) is from Princeton University.\nHe is involved in Stanford’s initiative of Statistics for Social Good and has the following quote on his website:\n\nQuixotic though it may sound, I hope to use computer science and statistics to change the world for the better.\n\n\n\n\nTopics covered\nFrom Dr. Mackey’s personal website his areas of research are:\n\nstatistical machine learning\nscalable algorithms\nhigh-dimensional statistics\napproximate inference\nprobability\n\n\n\nRelevant work\n\nKoulik Khamaru, Yash Deshpande, Lester Mackey, and Martin J. Wainwright, Near-optimal inference in adaptive linear regression\n\n\nWhen data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals…\n\n\nPierre Bayle, Alexandre Bayle, Lucas Janson, and Lester Mackey, Cross-validation Confidence Intervals for Test Error Advances in Neural Information Processing Systems (NeurIPS), December 2020.\n\n\nThis work develops central limit theorems for cross-validation and consistent estimators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our real-data experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature…\n\n\n\nOutside links\n\nMathematically Gifted & Black\nLinkedin\npersonal\n\n\n\nOther\nThe precursor to kaggle was a $1 million prize given by Netflix to the most accurate prediction of ratings that people give to the movies they watch. As undergraduates, Dr. Mackey and two friends led the competition for a few hours in its first year. Later, groups merged and Dr. Mackey’s group merged with a few others, forming The Ensemble. Their final analysis came in second with the exact same error rates as the winning entry. The winning entry, however, had been submitted 20 minutes prior. Sigh."
  },
  {
    "objectID": "weeks/week_06_sched.html#muddiest-points",
    "href": "weeks/week_06_sched.html#muddiest-points",
    "title": "Week 6",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why do we need to create a new variable for ordinal / scoring?\nOtherwise R will treat income as non-ordinal, and use the default reference cell coding. So if we want our variables to be scored (and numeric) then we must put it in a form R can recognize.\n\n\n2. I’m a little confused on how the R code works for recoding/reordering our variables, specifically 1) why we use the mutate function but then use the same name for the variable/how that works and 2) why you need to include the list of each variable name in a vector. Basically, what each piece of that code does exactly and why it’s needed.\n\nMutate is just a function to create/change a variable. So if we are not fundamentally changing any aspect of the variable, we can call it by the same name. Helps keep our data frame neat by not tacking on additional variables.\nWhen I am including the list of levels I am giving R the exact order to read each level. So if I want to go from high income to low income, I would reset the levels to the below code. Then R would read high income as the first level.\n\ngapm2 = gapm2 %&gt;%\n mutate(income_levels = factor(income_levels, \n            ordered = T, \n            levels = c(\"High income\", \n                       \"Upper middle income\", \n                       \"Lower middle income\", \n                       \"Low income\")))\n\n\n\n\n3. Is there a rationale or strategy in choosing the most appropriate reference group?\nOften no, not if the groups are not ordered. Things that you may consider:\n\nIs there a central group that you want to make comparisons to?\nIs there any social consequences of continually centering comparisons to one group? We may be consequentially centering the narrative around that group.\nWhen we interpret the coefficients, is there one group as the reference that makes it a little easier to interpret? (this has more of an effect in 513)\n\n\n\n4. How do we build the regression indicators?\nIn R, we don’t need to build the indicators. If we have a variable that is a facotr with mutually exclusive groups, then R will automatically create the indicators within the lm() function.\n\n\n5. Interpretations!\n\n5.1 Interpreting a confounder\nWe model a confounder by adding it into the model (without an interaction, just the main effect).\n\nThus, our interpretation follows the interpretations that were presented in Lesson 8: Intro to MLR\n\n\n\n5.2 Interpreting the main effects when there is an interaction\nComing soon!!\n\n\n5.3 Interpreting the interaction (effect modifier)\nFirst, we model an effect modifier with an interaction!\nComing soon!!\n\n\n\n6. Can we use continuous covariates in an interaction model?\nYes! Here are the four types of interactions we’ll discuss:\n\nbinary categorical and continuous\nmulti-level categorical and continuous\nbinary categorical and multi-level categorical\ncontinuous and continuous\n\n\n\n7. Synergerism vs. antagonism: how does \\(\\beta_3\\) relate to each?\n\nSynergerism means the sign of interaction’s coefficient (\\(\\beta_3\\)) matches that of main effect of \\(X_1\\), so the effect of \\(X_1\\) is strengthened as \\(X_2\\) increases\n\nIn the case that we’re looking at \\(X_2\\) as an effect modifier of \\(X_1\\)\nIt’s a little hard to think about this when we’ve only discussed \\(X_2\\) as a binary covariate, but our “increase” for an indicator is going from 0 to 1.\n\nAntagonism means the sign of interaction’s coefficient (\\(\\beta_3\\)) is flipped from that of main effect of \\(X_1\\), so the effect of \\(X_1\\) is weakened as \\(X_2\\) increases\n\nIn the case that we’re looking at \\(X_2\\) as an effect modifier of \\(X_1\\)\nIt’s a little hard to think about this when we’ve only discussed \\(X_2\\) as a binary covariate, but our “increase” for an indicator is going from 0 to 1\n\n\n\n\n8. The red and green lines example. I’m not totally sure why the lines would be parallel if an interaction affects the slope of a line?\nThe lines should not be parallel if there is an interaction. Let me show the equation for each of those examples:\n\nHere is the plot and equation when \\(X_2\\) is a confounder:\n\nHere is the plot and equation when \\(X_2\\) is an effect modifier:\n\nHere is the plot and equation when \\(X_2\\) is a effect modifier:\n\nHere is the plot and equation when \\(X_2\\) should not be in the model:"
  },
  {
    "objectID": "weeks/week_06_sched.html#clearest-points",
    "href": "weeks/week_06_sched.html#clearest-points",
    "title": "Week 6",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_07_sched.html#resources",
    "href": "weeks/week_07_sched.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\nOn Wednesday, 2/21, we will continue to look at the Interactions slides.\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n11\nInteractions"
  },
  {
    "objectID": "weeks/week_07_sched.html#on-the-horizon",
    "href": "weeks/week_07_sched.html#on-the-horizon",
    "title": "Week 7",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHW 4 is due on Sunday (2/25)\nLab 3 is due Sunday (3/3)"
  },
  {
    "objectID": "weeks/week_07_sched.html#class-exit-tickets",
    "href": "weeks/week_07_sched.html#class-exit-tickets",
    "title": "Week 7",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Wednesday (2/21)"
  },
  {
    "objectID": "weeks/week_07_sched.html#additional-information",
    "href": "weeks/week_07_sched.html#additional-information",
    "title": "Week 7",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_07_sched.html#statistician-of-the-week-w.e.b.-du-bois",
    "href": "weeks/week_07_sched.html#statistician-of-the-week-w.e.b.-du-bois",
    "title": "Week 7",
    "section": "Statistician of the Week: W.E.B. Du Bois",
    "text": "Statistician of the Week: W.E.B. Du Bois\n\n\n\n\n\n\n\nW.E.B. Du Bois\n\n\n\n\n\n\nDu Bois was a sociologist and among the earliest data scientists. As Battle-Baptiste and Rusert say, his work can be thought of as\n\nthe rendering of information in a visual format to help communicate data while also generating new patterns and knoweldge throughout the act of visualization itselt.1\n\n\n\n\nTopics covered\nDu Bois was a sociologist who contributed to the field of data visualization through infographics related to the African American in the early twentieth century.\n\n\nRelevant work\n\nRusert, B., and Battle-Baptiste, W. “W. E. B. Du Bois’s Data Portraits: Visualizing Black America”, Princeton Architectural Press, 2018. https://papress.com/products/w-e-b-du-boiss-data-portraits-visualizing-black-america\n\n\n\nOutside links\n\nWikipedia\nTidyTuesday data viz and TidyTuesday challenge provided the data needed to re-create most of Du Bois’s original graphs (his originals were drawn by hand).\nData Journalism in the study of W.E.B. Du Bois\nW.E.B. Du Bois: retracing his attempt to challenge racism with data\nW.E.B. Du Bois’ Visionary Infographics Come Together for the First Time in Full Color\n\n\n\nOther\nIn 1900 Du Bois contributed approximately 60 data visualizations to an exhibit at the Exposition Universelle in Paris, an exhibit designed to illustrate the progress made by African Americans since the end of slavery (only 37 years prior, in 1863).\nAt their core, the data visualizations advocate for African American progress. They not only speak to the progress that had been made, but they centered many of the challenges that continued to exist at the time. The set of visualizations demonstrate how powerfully a picture can tell 1000 words, as the information Du Bois used was primarily available from public records (e.g., census and other government reports).\nWhitney Battle-Baptiste and Britt Rusert have reproduced and narrated the images from the exhibit in W.E.B. Du Bois’s Data Portraits: Visualizing Black America, the color line at the turn of the twentieth century."
  },
  {
    "objectID": "weeks/week_07_sched.html#muddiest-points",
    "href": "weeks/week_07_sched.html#muddiest-points",
    "title": "Week 7",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Interactions in general\n\n1.1 Why is it not an effect modifier if p-value is 0.4? Or why is it not significant? Is it because it is not = 0?\n\n\n1.2 What are some reasons why we would think an effect modifier might exist in our data? or are we testing indiscriminately/ based on our own perspective of the associations in the data\n\n\n1.3 I’m still not entirely sure I know how to do all the steps of interaction models, but I think part of that may just be me needing to look at my previous notes.\n\n\n\n2. Centering the continuous covariate\n\n2.1 How does centering help with interpretation?\nI think this blog post has a nice explanation.\nBut for an example from our class… centering female literacy rate can help us interpret the\n\n\n2.2 Why do some values of \\(\\beta\\) change after centering the mean and others don’t?"
  },
  {
    "objectID": "weeks/week_07_sched.html#clearest-points",
    "href": "weeks/week_07_sched.html#clearest-points",
    "title": "Week 7",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_07_sched.html#footnotes",
    "href": "weeks/week_07_sched.html#footnotes",
    "title": "Week 7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBattle-Baptiste and Rusert, W.E.B. Du Bois’s Data Portraits: Visualizing Black America, the color line at the turn of the twentieth century, 2018, page 8.↩︎"
  },
  {
    "objectID": "weeks/week_08_sched.html#resources",
    "href": "weeks/week_08_sched.html#resources",
    "title": "Week 8",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n11.1\nInteractions\n\n\n\n\n\n\n\n11.2\nInteractions continued"
  },
  {
    "objectID": "weeks/week_08_sched.html#on-the-horizon",
    "href": "weeks/week_08_sched.html#on-the-horizon",
    "title": "Week 8",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 3 due 3/3 at 11pm\nHW 5 (last homework!!) due 3/7"
  },
  {
    "objectID": "weeks/week_08_sched.html#class-exit-tickets",
    "href": "weeks/week_08_sched.html#class-exit-tickets",
    "title": "Week 8",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (2/26)\n Wednesday (2/28)"
  },
  {
    "objectID": "weeks/week_08_sched.html#additional-information",
    "href": "weeks/week_08_sched.html#additional-information",
    "title": "Week 8",
    "section": "Additional Information",
    "text": "Additional Information\n\nExtra Practice/Learning"
  },
  {
    "objectID": "weeks/week_08_sched.html#statistician-of-the-week-desi-small-rodriguez",
    "href": "weeks/week_08_sched.html#statistician-of-the-week-desi-small-rodriguez",
    "title": "Week 8",
    "section": "Statistician of the Week: Desi Small-Rodriguez",
    "text": "Statistician of the Week: Desi Small-Rodriguez\n\n\n\n\n\n\n\nDesi Small-Rodriguez\n\n\n\n\n\n\nDr. Small-Rodriguez is a social demographer and an Assistant Professor of Sociology and American Indian Studies at UCLA. She received a PhD in Sociology from the University of Arizona and a PhD in Demography from the University of Waikato. Dr. Small-Rodriguez is Northern Cheyenne and Chicana and grounds her work in Indigenous studies, sociology of race and ethnicity, critical demography, and health policy research. She directs the Data Warriors Lab (a mobile data sovereignty lab serving Indigenous communities) and was previously a member of the Collaboratory for Indigenous Data Governance. She is a founding member of the Global Indigenous Data Alliance.\n\n\n\nTopics covered\nDr. Small-Rodriguez is passionate about Indigenous data sovereignty and Indigenous data governance. Using networks of Indigenous scholars and survey methods, she works toward the following two goals: (1) better collection and use of data on Indigenous people that has been gathered by external sources such as the census and other federal entities; (2) development of data methods and practitioners within the Indigenous community. Dr. Small-Rodriguez also works for health and economic justice on Indian Reservations.\n\n\nRelevant work\n\nS.R. Carroll, D. Rodriguez-Lonebear, A. Martinez, “Indigenous Data Governance: Strategies from United States Native Nations.” Data Science Journal, 2019. https://datascience.codata.org/article/10.5334/dsj-2019-031/\n\n\n“Indigenous data sovereignty is the right of each Native nation to govern the collection, ownership, and application of the tribe’s data.”\n\n\nRodriguez-Lonebear D, Barceló NE, Akee R, Carroll SR. “American Indian Reservations and COVID-19: Correlates of Early Infection Rates in the Pandemic.” J Public Health Manag Pract. 2020. doi: 10.1097/PHH.0000000000001206.\n\n\n\nOutside links\n\nacademic\nLinkedin\npersonal"
  },
  {
    "objectID": "weeks/week_08_sched.html#muddiest-points",
    "href": "weeks/week_08_sched.html#muddiest-points",
    "title": "Week 8",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses.\n\n1. Understanding when you can and cannot use a t-test\nWe cannot use a t-test when we are testing more than one coefficient. “Testing more than one coefficient” might be testing an interaction involving a multi-level variable, testing the main effect of a multi-level variable, or testing a group of variables at the same time.\n\n\nThere was a drawing about fans being EMM and the parallel lines being confounding. Could you explain that?\n\n\nWhen does centering change the slope not the intercept?\n\n\nDeciding between confounder and effect modifier. I have always been confused about these topics.\n\n\nI’m still unsure about the number of interactions and how that is determined for multi-level covariates.\n\n\nFor income level as an effect modifier for World Region on Life Expectancy, when we talk about the intercept when everything is 0, what does that mean when world region is 0?\n\n\nGetting a little mixed up on swapping terminology between effect modifier and interaction, these two mean the same things and ARE NOT the same as a confounder (i.e. confounder and effect modifiers are mutually exclusive)?\n\n\nI still don’t really understand how you determine if something is a confounder or not. Is it that if a variable affects the main effect but the interaction isn’t significant then it’s a confounder? i.e. if in R you compare Y~X1 to Y~X1+X2 and find a 10+% change in the effect, but an ANOVA comparing Y~X1+X2 to Y~X1*X2 is not significant then X2 is a confounder? This came up on HW4 as well – can something be a confounder if the main effect is insignificant?\n\n\nTesting for a confounder: I thought the difference between a confounder & effect modifier was that B_1 doesn’t change with a confounder. Why is it that we’re testing for a &gt;10% change in B_1 when introducing a confounding variable? Shouldn’t we be testing whether B_2 adds significantly to B_0? That is, a confounder shifts the coefficient for our variable of interest, B_1, significantly?\n\n\nCan you explain the third criterion/thing you would look at when deciding whether to include a variable as a confounder when not all coefficients change &gt;10%?"
  },
  {
    "objectID": "weeks/week_08_sched.html#clearest-points",
    "href": "weeks/week_08_sched.html#clearest-points",
    "title": "Week 8",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_05_sched.html#resources",
    "href": "weeks/week_05_sched.html#resources",
    "title": "Week 5",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in orange mean there is an available file link.\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n\nA word on Lab 1 and Quiz 1\n\n\n\n\n\n8\nIntroduction to Multiple Linear Regression\n\n\n\n\n\n9\nMLR: Inference"
  },
  {
    "objectID": "weeks/week_05_sched.html#on-the-horizon",
    "href": "weeks/week_05_sched.html#on-the-horizon",
    "title": "Week 5",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 2 due 2/8\nHomework 3 due 2/15"
  },
  {
    "objectID": "weeks/week_05_sched.html#class-exit-tickets",
    "href": "weeks/week_05_sched.html#class-exit-tickets",
    "title": "Week 5",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (2/5)\n Wednesday (2/7)"
  },
  {
    "objectID": "weeks/week_05_sched.html#additional-information",
    "href": "weeks/week_05_sched.html#additional-information",
    "title": "Week 5",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_05_sched.html#statistician-of-the-week-liz-hare",
    "href": "weeks/week_05_sched.html#statistician-of-the-week-liz-hare",
    "title": "Week 5",
    "section": "Statistician of the Week: Liz Hare",
    "text": "Statistician of the Week: Liz Hare\n\n\n\n\n\n\n\nLiz Hare\n\n\n\n\n\n\nDr. Hare got her BA from Bryn Mawr College and her PhD in Genetics (1998) from The George Washington University. She works primarily in dog / animal genetics; although, as a quantitative geneticist her statistical and computational methodology is quite sophisticated.\nDr. Hare is active in the MiR (Minorities in R) Community which aims to support historically underrepresented R users around the world.\n\n\n\nTopics covered\nHer computational language of choice is R, and much of her work has focused on open science with an eye toward inclusion and equity. In many software programs, the user has the ability to include alt text: text descriptions that convey the content and meaning to blind and low-vision readers.\n\nyou really need to tell us what the data is saying and why you included it.\n\n\nWhat kind of graph or chart is it?\nWhat variables are on the axes?\nWhat are the ranges of the variables?\nWhat does the appearance tell you about the relationships between the variables?\n\n\n\nRelevant work\n\nL. Hare, Writing Alt Text to Communicate the Meaning in Data Visualizations, Do No Harm Guide: centering accessibility in data visualization, eds Schwabish, Popkin, Feng, Chapter 4, 2022.\n\n\n\nOutside links\n\nGoogle Scholar\nLinkedin\nprofessional\nfosstodon"
  },
  {
    "objectID": "weeks/week_05_sched.html#muddiest-points",
    "href": "weeks/week_05_sched.html#muddiest-points",
    "title": "Week 5",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why is it not called multivariate?\nMultivariate models refer to models that have multiple outcomes. In multiple/multivariable models, we still only have one outcome, \\(Y\\), but multiple predictors.\n\n\n2. When adjusting for another variable, how do we calculate the new slope and intercept?\nFirst, I want to clarify one thing: in the case of two covariates, we only have the best fit plane. The intercept is really just a placeholder for when the covariates are 0. And the coefficients for each covariate are no longer stand alone slopes, unless we examine a specific instance when one covariate takes on a realized value. (Okay, that was kinda a lot of vague language. Let’s go to our example.)\nFor \\[\\widehat{\\text{LE}} = 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\\], we have a regression plane.\nWhen we derive the regression lines for either variable, FLR or FS, we can think of the other variable as part of the intercept for the line:\nFor FLR: \\[\\widehat{\\text{LE}} = [33.595 + 0.008\\ \\text{FS}] + 0.157\\ \\text{FLR} \\] For FS: \\[\\widehat{\\text{LE}} = [33.595 + 0.157\\ \\text{FLR}]\n+ 0.008\\ \\text{FS}\\]\nFor FLR, any given FS value will result in the same slope, but the intercept will change. That’s why we say “holding FS constant,” “adjusting for FS,” or “controlling for FS” when we discuss the \\(\\widehat\\beta_1=0.157\\) estimate. Depending on the FS value, the intercept will change. So we can write: \\[(\\widehat{\\text{LE}}|FS=3000) = [33.595 + 0.008\\cdot 3000] + 0.157\\ \\text{FLR}= 57.595 + 0.157\\ \\text{FLR}\\]\nTry going through the same process for FS when FLR is 30%.\n\n\n3. I know we can’t really do visualizations past 3 coefficients but I still can’t really wrap my head around how this will work once we add a third covariate to the model.\nYeah… this is a tough one! You can try to think of the fitted Y \\(\\widehat{Y}|X\\) as being built from all the covariate values, and really just the equation for the best-fit line that we estimate. At three covariates, we need to let go of some of the visualizations.\n\n\n4. Still a little unsure about OLS\nI was going to write out an explanation to this, but then I wrote my explanation to the below question. I honestly think it helps bring context to OLS and talk about it in a new way that might help if it’s been confusing so far.\n\n\n5. We keep getting back to \\(\\widehat{Y}\\), \\(Y_i\\), and \\(\\overline{Y}\\) and their relationship to the population parameter estimates. Can you clarify this?\nI think it’ll be helpful to use the dataset I created from our quiz. I still think this relationship is best communicated with simple linear regression. What you didn’t see on the quiz was that I simulated the data:\n\nset.seed(444) # Set the seed so that every time I run this I get the same results\nx = runif(n=200, min = 40, max = 85) # I am sampling 200 points from a uniform distribution with minimum value 40 and maximum value 85\ny = rnorm(n=200, 215 - 0.85*x, 13) # Then I can construct my y-observations based on x. Notice that 215 is the true, underlying intercept and -0.85 is the true underlying slope\ndf = data.frame(Age = x, HR = y) # Then I combine these into a dataframe\n\nThen we can look at the scatterplot:\n\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11))\n\n\n\n\n\n\n\n\n\nEach point represents an observation \\((X_i, Y_i)\\). That is where we get \\(Y_i\\) from\nThe red line represents \\(\\widehat{Y}\\). We can look at each \\(\\widehat{Y}|X\\), so we look at the expected \\(Y\\) at a specific age like 70 years old.\nNow we need to find \\(\\overline{Y}\\). This does not take \\(X\\) into account. So we can look at the observed \\(Y\\)’s and find the mean\n\nggplot(df, aes(HR)) + geom_histogram()\n\n\n\n\n\n\n\nmean(df$HR)\n\n[1] 162.8725\n\n\n\nThen we can draw a line on the scatterplot for \\(\\overline{Y}\\):\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1, colour=\"#F14124\") +\n  labs(x = \"Age (years)\", \n       y = \"Peak exercise heart rate (bpm)\",\n       title = \"Peak exercise heart rate vs. Age\") +\n    theme(axis.title = element_text(size = 11), \n        axis.text = element_text(size = 11), \n        title = element_text(size = 11)) +\n  geom_hline(yintercept = mean(df$HR), linewidth = 1, colour=\"green\")\n\n\n\n\n\n\n\n\nWhen we talk about SSY (total variation), we can think of the histogram of the Y’s\n\nggplot(df, aes(HR)) + geom_histogram() + xlim(100, 225)\n\n\n\n\n\n\n\n\nThen the total variation of these observed values is related to the \\(\\sum_{i=1}^n (Y_i - \\overline{Y})^2\\). Let’s plot \\(Y_i - \\overline{Y}\\):\n\ndf = df %&gt;% mutate(y_center = HR - mean(HR))\nggplot(df, aes(y_center)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n\n\n\n\n\n\n\n\nHowever, we can fit a regression line to show the relationship between Y and X. For every observation \\(X_i\\) there is a specific \\(\\widehat{Y}\\) from the regression line. So if we take the difference between the mean Y and the fitted Y, then we get the variation that is explained by the regression.\n\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %&gt;% mutate(fitted_y = aug1$.fitted, \n                   diff_mean_fit = fitted_y - mean(HR))\nggplot(df, aes(diff_mean_fit)) + geom_histogram() + xlim(-60, 50)+ylim(0, 35)\n\n\n\n\n\n\n\n\nIn the plot above, there is variation! And it means that some of the variation in the plot of Y alone is actually coming from this variation explained by the regression model!!\nBut there is left over variation that is not explained by the model… What is that? It’s related to our residuals: \\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\nSo we’ll calculate the residuals (or more appropriately, use the calculation of the residuals that R gave us)\n\nmod1 = lm(HR ~ Age, data = df)\naug1 = augment(mod1)\ndf = df %&gt;% mutate(diff_y_fitted = aug1$.resid)\nggplot(df, aes(diff_y_fitted)) + geom_histogram() + xlim(-60, 50) +ylim(0, 35)\n\n\n\n\n\n\n\n\nOur aim in regression (through ordinary least squares) is to minimize the variance in the above plot. The more variance our model can explain, the less variance in the residuals. In SLR, we can only explain so much variance with a single predictor. As we include more predictors in our model, the model has the opportunity to explain even MORE variance.\n\n\n6. I feel that I am understanding and beginning to memorize the “processes” but failing to understand the “how/when/why” we apply certain models. Like, if you let me loose into the world tomorrow, I feel I would not be able to implement anything that I have learned thus far out in the wild.\nI feel you. Today was meant to establish the tools and the process for the hypothesis tests. In the next few classes we are going to shape the how/when/why. There’s so many that we can’t cover them in one class. So I thought it would be best to introduce the tools on their own, and then discuss how we use each one.\n\n\n7. I’m struggling with how to use the SSE, SSR, and SSY graphs but I think I just need to spend more time with them.\nWe’re going to keep talking about this! I think I have a good visual explanation that will help us connect some ideas!\n\n\n8. Using R to conduct each F-test\nI show this a little bit for the overall test, but let’s explicitly write it out for the example with the group of covariates.\nLet’s say our proposed model is:\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon\\]\nAnd we want to see if it fits the data significantly better than:\n\\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\nWe need to fit both models first:\n\n# Reduced model\nmod_red3 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub2)\n\n# Full model\nmod_full3 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct,\n               data = gapm_sub2)\n\nAnd then all we need to do is call each model into the anova() function! The order of the models in the function will not matter for the F-test.\n\nanova(mod_red3, mod_full3) %&gt;% tidy() %&gt;% gt() %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.00\n2,654.87\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n68.00\n1,517.92\n2.00\n1,136.96\n25.47\n0.00\n  \n  \n  \n\n\n\n\nAnd then we have all the information we need for our conclusion! Because the statistic is 25.47 and its corresponding p-value &lt; 0.001, we reject the null!\n\n\n9. Calculating the F-statistic\nWe will not need to know exactly how to calculate the F-statistic! We just need to understand the context of the calculation. With that poll everywhere question, I just wanted you to have a moment to interact with the fact the the F-test is measuring the difference in the sum of squares of the error.\n\n\n10. Why is SSE of the reduced model always greater than or equal to SSE of the full model?\nThe more variables that we have in the model, the more variation in our outcome we can explain. The worst case scenario is that the added variable does not help explain variation, then we are left with the same SSE as a model without the variable. If the variable add any information about our outcome, then we decrease the SSE.\n\n\n11. How many variables would be too many to include in MLR? What if they all help explain the regression?\nWhat a fun question! The maximum number of variables that you can have in a model is the number of observations that you have. At that point, you can have a variable that is an indicator of each observation. Basically, a singular mapping from each individual observation to its respective outcome.\nAnd this will explain the variation in Y perfectly!! But it won’t illuminate useful information. We cannot generalize it to the population. So with the number of variables that we include in the model, we need to balance generalizability and reduction of error."
  },
  {
    "objectID": "weeks/week_05_sched.html#clearest-points",
    "href": "weeks/week_05_sched.html#clearest-points",
    "title": "Week 5",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_09_sched.html#resources",
    "href": "weeks/week_09_sched.html#resources",
    "title": "Week 9",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n\nQuiz, Lab, Mid-quarter feedback\n\n\n\n\n\n12\nModel selection 1\n\n\n\n\n\n\n13\nPurposeful Model Selection"
  },
  {
    "objectID": "weeks/week_09_sched.html#on-the-horizon",
    "href": "weeks/week_09_sched.html#on-the-horizon",
    "title": "Week 9",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 3 was due yesterday\nHW 5 is due this Thursday\nQuiz 3 next Monday!!"
  },
  {
    "objectID": "weeks/week_09_sched.html#class-exit-tickets",
    "href": "weeks/week_09_sched.html#class-exit-tickets",
    "title": "Week 9",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (3/4)\n Wednesday (3/6)"
  },
  {
    "objectID": "weeks/week_09_sched.html#additional-information",
    "href": "weeks/week_09_sched.html#additional-information",
    "title": "Week 9",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_09_sched.html#statistician-of-the-week-mike-dairyko",
    "href": "weeks/week_09_sched.html#statistician-of-the-week-mike-dairyko",
    "title": "Week 9",
    "section": "Statistician of the Week: Mike Dairyko",
    "text": "Statistician of the Week: Mike Dairyko\n\n\n\n\n\n\n\nMike Dairyko\n\n\n\n\n\n\nDr. Dairyko was a Posse Scholar at Pomona College where a linear algebra class set him on a career path centered around mathematics. Through that class he found his way to two different summer REU programs and eventually to a PhD in Applied Mathematics from Iowa State University (2018). While initially believing that he would stay in academia after his graduate work, being introduced to machine learning methods caused him to pursue data science jobs after graduation.\nDr. Dairyko served as a Senior Manager of Data Science at the Milwaukee Brewers and is now the Director of Ticketing Analytics at the Milwaukee Bucks. Helping the organization get the most out of budgeting, revenue, and ticket sales allows him to fully use his training in mathematics and data science.\n\n\n\nTopics covered\nDr. Dairyko’s graduate work is in graph theory, in particular, exponential domination. In a graph, exponential domination is the extent to which a particular vertex influences the remaining vertices in a graph. His published work falls very much within the realm of mathematics, proving that particular properties of graphs exist. However, graph theory is intimately related to machine learning; for example, it is the foundational structure of a neural network. Understanding properties of graphs help data scientists develop even more powerful models to harness information from data.\n\n\nRelevant work\n\nM. Dairyko, A linear programming method for exponential domination. The Golden Anniversary Celebration of the National Association of Mathematicians, Volume 759 of Contemporary mathematics. Eds O. Ortega, E. Lawrence, E. Goins (2020).\nM. Dairyko, L.Hogben, J. Lin, J. Lockhart, D. Roberson, S. Severini, M. Young, Note on von Neumann and Rényi entropies of a graph. Linear Algebra and its Applications, 2017.\n\n\n\nOutside links\n\nMAD\nLinkedin\nGoogle Scholar\n\n\n\nOther\nDr. Dairyko’s path from mathematics to data science has been written about in SIAM and in the Iowa State University newsletter Math Matters."
  },
  {
    "objectID": "weeks/week_09_sched.html#muddiest-points",
    "href": "weeks/week_09_sched.html#muddiest-points",
    "title": "Week 9",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses.\n\n1. In CIE, why this assumes a 10% difference indicates confounding and not effect measure modification\nA 10% difference can also indicate an interaction, but we would need to see the difference when we include the interaction. I think I was specifically talking about including variables, which typically means I am only including their main effects. With main effects, we can test for confounding, but not interactions. That’s all! We can separately test for interactions using the same 10% criterion.\n\n\n2. “Any variables not selected for the final model have still been adjusted for, since they had a chance to be in the model” How are they adjusted for when they aren’t?\nI feel your confusion. These variables have not been explicitly adjusted for in the model, more implicitly. If a variable is not selected that means it does not predict our outcome well and/or it does not affect other variables’ relationship with the outcome. That means we tried to adjust for it, but it has no effect, so we have technically adjusted for something that does not change our model.\nAn example is probably best. Let’s say we’re looking at treatment effect. We measured the weather on the day that someone went in for treatment. We have a variable on whether it was cloudy or not. That variable was not selected in the model because the cloudiness has no effect on the treatment, but we allowed it to be a potential covariate. This is different than a variable that may not have an effect but was not measured. We cannot say we adjusted for something like shirt color during treatment because we haven’t actually tested it.\n\n\n3. Difference between stepwise and change in estimate approach?\nStepwise is an automatic selection process that only requires us to put our dataset in a function which will return the “optimal” model. It also based only on the p-values of coefficients in the model. In CIE, we are manually including and excluding variables, and checking for a change in a coefficient estimate, instead of a significant p-value. A big change in a coefficient estimate is not necessarily accompanied by a significant p-value.\n\n\n4. How does having fewer covariates cause a more biased estimate? (and what does it mean for \\(\\widehat\\beta\\) to be biased for \\(\\beta\\)?)\nFewer covariates in our model means we likely not capturing the complex relationship between our outcome and our variables. If we leave out a variable that is an important predictor of the outcome, then the coefficients of all the variables that made it into the model will be a little biased. (Because we are not capturing the true, underlying model).\nFor example, let’s say I am analyzing data for a study on dementia. Dementia is my outcome and I include a few variables in my model, such as whether or not you live with someone, depression, and physical activity. However, I leave out age, which is known to have high association with dementia. I have left out an important variable that may be a confounder or effect modifier of the variables in the model. Thus, the estimates of coefficients in the model will be biased.\nThe less variables in the model, the more likely we are leaving out a variable that would help predict our outcome. We can counter this by trying to select the best model!\nSecond part of the question: \\(\\widehat\\beta\\) is a biased estimate for \\(\\beta\\) means that the estimated value, \\(\\widehat\\beta\\), is not close to the true, underlying \\(\\beta\\). We work under the assumption that there is some true relationship between our covariates and our outcome, and we are trying to uncover the true value by estimating it. However, our estimate may not be close to the true value. We can try to get it as close as possible given our research aims and model.\n\n\n5. All the new approaches for model selection!\nJust to be clear, the main intention for the overview was that you can identify and recognize some of the key characteristics of different model selection strategies. We can’t cover all of them in detail, but I just want you to know what’s out there, and what other people might use.\n\n\n6. Assessing change in coefficients\nI highly recommend going back to the slides with interactions (effect modifiers) and confounders (Lesson 11.2: Interactions continued). On slide 18, we get into the change in coefficients. This is just one way to measure if a variable might be important in our model.\n\n\n7. General feelings of uncertainty when it comes to picking a model based on some of these more subjective measures\nFair enough! It takes time to build that trust in your instincts when building a model. This is mostly why there are a few concrete rules within purposeful model selection. I don’t think your model can go horribly wrong in the subjective choices, but sticking with the more concrete rules (when there are some) will be important.\n\n\n8. All of the different F-tests and p-values we are using in the early steps of model building\nYeah… definitely hard to keep organized when we’re seeing different uses so close together\n\nIn step 2, we use the F-test to see if a single variable (potentially with many coeffiicents) explains enough variation in our outcome\n\nThis is the F-test in simple linear regression with\n\nReduced / null model: \\(Y = \\beta_0 + \\epsilon\\)\nFull / alternative model: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nThis will be different for multi-level covariates\nWe can use anova( full_model ) to get the F-statistic and p-value\n\n\nIn step 3, we use the F-test to see if a single variable (potentially with many coeffiicents) explains enough variation in our outcome, given the other variables in the model\n\nThis is the F-test in simple linear regression with\n\nReduced / null model: \\(Y = \\beta_0 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\epsilon\\)\nFull / alternative model: \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\epsilon\\)\nThis will be different for multi-level covariates: more coefficients removed between full and reduced\nWe can use anova( full_model , reduced_model ) to get the F-statistic and p-value\n\n\n\n\n\n9. Why didn’t we use those packages earlier?\nYou got me! Partially because I wanted us to practice plotting in ggplot and be able to create more detailed plots. I think those functions (skim() and ggpairs()) super helpful in big picture, but if we don’t know what to look for or identify oversights in the output, then we can miss important information about the data. More detailed plots, and more practice with variable types (like making factors) is needed to approach skim() and ggpairs()."
  },
  {
    "objectID": "weeks/week_09_sched.html#clearest-points",
    "href": "weeks/week_09_sched.html#clearest-points",
    "title": "Week 9",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_10_sched.html#resources",
    "href": "weeks/week_10_sched.html#resources",
    "title": "Week 10",
    "section": "Resources",
    "text": "Resources\n\n\n\nChapter\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n\nLab 3 Feedback\n\n\n\n\n\n13\nPurposeful Model Selection\n\n\n\n\n\n\n\n14\nMLR Diagnostics"
  },
  {
    "objectID": "weeks/week_10_sched.html#on-the-horizon",
    "href": "weeks/week_10_sched.html#on-the-horizon",
    "title": "Week 10",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "weeks/week_10_sched.html#class-exit-tickets",
    "href": "weeks/week_10_sched.html#class-exit-tickets",
    "title": "Week 10",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (3/11)\n Wednesday (3/13)"
  },
  {
    "objectID": "weeks/week_10_sched.html#additional-information",
    "href": "weeks/week_10_sched.html#additional-information",
    "title": "Week 10",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_10_sched.html#statistician-of-the-week-florence-nightingale",
    "href": "weeks/week_10_sched.html#statistician-of-the-week-florence-nightingale",
    "title": "Week 10",
    "section": "Statistician of the Week: Florence Nightingale",
    "text": "Statistician of the Week: Florence Nightingale\n\n\n\n\n\n\n\nFlorence Nightingale\n\n\n\n\n\n\nNightingale was a nurse who is considered to be the founder of modern nursing. In particular, she was admant about the importance of hygiene and sanitary conditions. She was born into a wealthy and well-connected family and had a large amount of privilege. Educated by her father, she showed an ability toward making analytic arguments at an early age. For her work, she was awarded the Royal Red Cross, the Lady of Grace of the Order of St John, and the Order of Merit.\n\n\n\nTopics covered\nNightingale was a nurse and statistician who used her analytic abilities to better understand and improve public health. She served in the Crimean War where Britain and France fought against the Russian invasion of the Ottoman Empire. Nightingale worked to convince Queen Victoria that poor sanitation and overcrowding were causing unnecessary death. She was able to show, for example, that peacetime soldiers (who lived in poorly kept barracks) were dying in much higher number than comparable civilian men. Her genius was to collect data meticulously and to display it in ways that were accessible to the general public. Her visualizations are lauded as pioneering and the first of their kind to tell effective stories of important issues.\n\n\nRelevant work\n\nRJ Andrews, “Florence Nightingale’s Data Revolution” in Scientific American 327, 2, 78-85, 2022. doi:10.1038/scientificamerican0822-78,\n\n\n\n\n\n\n“Diagram of the causes of mortality in the army in the East” was published in Notes on Matters Affecting the Health, Efficiency, and Hospital Administration of the British Army and sent to Queen Victoria in 1858.\n\n\n\n\n\nS Julious, “On the battlefields of the Crimean War and in the hills of Sheffield, Florence Nightingale’s legacy lives on”, The Tribune, Feb 17, 2023.\n\n\n\nOutside links\n\nWikipedia"
  },
  {
    "objectID": "weeks/week_10_sched.html#muddiest-points",
    "href": "weeks/week_10_sched.html#muddiest-points",
    "title": "Week 10",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. What models or values are we comparing in VIF?\nMmm good question! VIFs work for continuous and binary variables. So if your model only has continuous or binary covariates, then the VIFs and GVIFs are the same, and you can use either. The GVIFs are needed for multi-level covariates.\n\n\n2. Still a little confused on the context of when we use a centered value vs not in our model.\nYou can always center a value! There are two scenarios where centering is really helpful:\n\nWhen we have an interaction. Centering makes coefficients more interpretable\nWhen we have a transformation of the variable. Centering avoids issues with multicollinearity.\n\n\n\n3. What is the difference between multicollinearity vs confounding vs effect modification?\nHere’s a pretty good video about the differences! About 8 minutes long, but easily played at 1.25/1.5 speed.\n\n\n4. Why we would have both age and age squared in a model\nWe would only have age and age-squared if we noticed the relationship between age and our outcome was not linear. For example, our plot could look like this:\n\nggplot(df, aes(x = age, y = y)) + geom_point() + geom_smooth()\n\n\n\n\n\n\n\n\nAnd let’s say we see the following plot for age-squared:\n\nggplot(df, aes(x = age_sq, y = y)) + geom_point() + geom_smooth()\n\n\n\n\n\n\n\n\nThen we would make the transformation of age for our model. When we include age-squared in the model, we still need to include age. We can run the model with both:\n\nmod = lm(y ~ age + age_sq, data = df)\n\nAnd we can look at the regression table. Notice that the standard error of age and age-squared’s coefficients are okay, but the intercept’s standard error is really big.\n\ntidy(mod, conf.int = T) %&gt;% gt() %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−56.66\n34.92\n−1.62\n0.11\n−125.27\n11.96\n    age\n2.29\n1.54\n1.49\n0.14\n−0.74\n5.31\n    age_sq\n0.58\n0.02\n37.80\n0.00\n0.55\n0.61\n  \n  \n  \n\n\n\n\nWe can also look at the VIF:\n\nrms::vif(mod)\n\n     age   age_sq \n37.01798 37.01798 \n\ncar::vif(mod) # will only give us GVIF if there is a multi-level covariate in the model\n\n     age   age_sq \n37.01798 37.01798 \n\n\nThe VIFs are really big, so centering age will help the multicolinearity of the model.\n\nmod2 = lm(y ~ age_c + age_c_sq, data = df)\n\nWhere age_c is age centered at the mean, and age_c_sq is the centered age squared.\n\ntidy(mod2, conf.int = T) %&gt;% gt() %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n1,447.76\n6.69\n216.39\n0.00\n1,434.62\n1,460.91\n    age_c\n59.31\n0.25\n234.40\n0.00\n58.81\n59.81\n    age_c_sq\n0.58\n0.02\n37.80\n0.00\n0.55\n0.61\n  \n  \n  \n\n\n\n\n\ncar::vif(mod2) # will only give us GVIF if there is a multi-level covariate in the model\n\n   age_c age_c_sq \n 1.00124  1.00124 \n\n\nYay! The VIFs are much better now! And the intercept and age coefficient estimate have better standard error!"
  },
  {
    "objectID": "weeks/week_10_sched.html#clearest-points",
    "href": "weeks/week_10_sched.html#clearest-points",
    "title": "Week 10",
    "section": "Clearest Points",
    "text": "Clearest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "slides/22_Counting_Intro.html",
    "href": "slides/22_Counting_Intro.html",
    "title": "Chapter 22: Introduction to Counting",
    "section": "",
    "text": "Example 1\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\nHow many possible ways are there to order them?\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\nHow many ways to order them without replacement and only need 6?\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?\n\n\n\n\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.1\n\n\nHow many possible ways are there to order them?\n\n\n \n\n\nExample 1.2\n\n\nHow many ways to order them if we can reuse the same subject and\n\nneed 10 total?\nneed 6 total?\n\n\n\n\n\n\n\n\nSuppose we have 10 (distinguishable) subjects for study.\n\n\n\n\nExample 1.3\n\n\nHow many ways to order them without replacement and only need 6?\n\n\n \n\n\nExample 1.4\n\n\nHow many ways to choose 6 subjects without replacement if the order doesn’t matter?"
  },
  {
    "objectID": "homework/HW2.html#directions",
    "href": "homework/HW2.html#directions",
    "title": "Homework 2",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "slides/3_IndependentEvents.html#revisiting-our-coin-toss",
    "href": "slides/3_IndependentEvents.html#revisiting-our-coin-toss",
    "title": "Chapter 3: Independent Events",
    "section": "Revisiting our coin toss",
    "text": "Revisiting our coin toss\nQuestion: Which of the following sequences of coin tosses of heads (\\(H\\)) and tails (\\(T\\)) is more likely to happen, assuming the coin is fair?\n\\[HTTHHHTHTHHTTTH\\] or \\[HTTTTTTTTHTTTTT\\]"
  },
  {
    "objectID": "slides/3_IndependentEvents.html#independent-events",
    "href": "slides/3_IndependentEvents.html#independent-events",
    "title": "Chapter 3: Independent Events",
    "section": "Independent Events",
    "text": "Independent Events\n\n\nDefinition: Independence\n\n\nEvents \\(A\\) and \\(B\\) are independent if \\[\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot  \\mathbb{P}(B).\\]\n\n\nNotation: For shorthand, we sometimes write \\[A \\mathrel{\\unicode{x2AEB}} B,\\] to denote that \\(A\\) and \\(B\\) are independent events."
  },
  {
    "objectID": "slides/3_IndependentEvents.html#example-of-two-dice",
    "href": "slides/3_IndependentEvents.html#example-of-two-dice",
    "title": "Chapter 3: Independent Events",
    "section": "Example of two dice",
    "text": "Example of two dice\n\n\nExample 1\n\n\nTwo dice (red and blue) are rolled. Let \\(A =\\) event a total of 7 appears, and \\(B =\\) event red die is a six. Are events \\(A\\) and \\(B\\) independent?"
  },
  {
    "objectID": "slides/3_IndependentEvents.html#independence-of-3-events",
    "href": "slides/3_IndependentEvents.html#independence-of-3-events",
    "title": "Chapter 3: Independent Events",
    "section": "Independence of 3 Events",
    "text": "Independence of 3 Events\n\n\nDefinition: Independence of 3 Events\n\n\nEvents \\(A\\), \\(B\\), and \\(C\\) are mutually independent if\n\n\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B)\\)\n\\(\\mathbb{P}(A \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(C)\\)\n\\(\\mathbb{P}(B \\cap C) = \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\\(\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A) \\cdot \\mathbb{P}(B) \\cdot \\mathbb{P}(C)\\)\n\n\n\nRemark:\nOn your homework you will show that \\((1) \\not \\Rightarrow (2)\\) and \\((2) \\not \\Rightarrow (1)\\)."
  },
  {
    "objectID": "slides/3_IndependentEvents.html#probability-at-least-one-smoker",
    "href": "slides/3_IndependentEvents.html#probability-at-least-one-smoker",
    "title": "Chapter 3: Independent Events",
    "section": "Probability at least one smoker",
    "text": "Probability at least one smoker\n\n\n\n\nExample 2\n\n\nSuppose you take a random sample of \\(n\\) people, of which people are smokers and non-smokers independently of each other. Let\n\n\\(A_i =\\) event person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\), and\n\\(p_i =\\) probability person \\(i\\) is a smoker, for \\(i=1, \\ldots ,n\\).\n\nFind the probability that at least one person in the random sample is a smoker."
  },
  {
    "objectID": "slides/3_IndependentEvents.html#three-people-toss-a-coin",
    "href": "slides/3_IndependentEvents.html#three-people-toss-a-coin",
    "title": "Chapter 3: Independent Events",
    "section": "Three people toss a coin",
    "text": "Three people toss a coin\n\n\nExample 3\n\n\n\\(A, B,\\) and \\(C\\) toss a fair coin in order. The first to throw heads wins. What are their respective chances of winning?\n\n\nLet\n\n\\(A_H\\) and \\(A_T\\) be the events player A tosses heads and tails, respectively.\nSimilarly define \\(B_H\\), \\(B_T\\), \\(C_H\\), and \\(C_T\\).\n\n\n\nChapter 3 Slides"
  },
  {
    "objectID": "slides/1_Outcomes_Events_Sample.html#learning-objectives",
    "href": "slides/1_Outcomes_Events_Sample.html#learning-objectives",
    "title": "Chapter 1: Outcomes, Events, and Sample Spaces",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine basic terms related to events such as events, outcomes, and sample space.\nUse proper set notation for events\nCharacterize possible outcomes, when something random occurs\nDescribe events into which outcomes can be grouped\nDefine important terms and rules within set theory such as unions, intersections, complements, mutually exclusive, and De Morgan’s Laws"
  },
  {
    "objectID": "slides/2_Probability.html#learning-objectives",
    "href": "slides/2_Probability.html#learning-objectives",
    "title": "Chapter 2: Probability",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine basic axioms and propositions in probability\nAssign probabilities to events, and perform manipulations on probabilities to make calculations easier"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#learning-objectives",
    "href": "slides/22_Counting_Intro.html#learning-objectives",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nDefine permutations and combinations\nCharacterize difference between sampling with and without replacement\nCharacterize difference between sampling when order matters and when order does not matter\nCalculate the probability of sampling any combination of the following: with or without replacement and order does or does not matter"
  },
  {
    "objectID": "slides/3_IndependentEvents.html#learning-objectives",
    "href": "slides/3_IndependentEvents.html#learning-objectives",
    "title": "Chapter 3: Independent Events",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nDefine independence of 2-3 events given probability notation\nCalculate whether two or more events are independent"
  },
  {
    "objectID": "slides/3_IndependentEvents.html#building-geometric-series",
    "href": "slides/3_IndependentEvents.html#building-geometric-series",
    "title": "Chapter 3: Independent Events",
    "section": "Building geometric series",
    "text": "Building geometric series\n\n\nExample 3\n\n\n\\(A, B,\\) and \\(C\\) toss a fair coin in order. The first to throw heads wins. What are their respective chances of winning?\n\n\nLet\n\n\\(A_H\\) and \\(A_T\\) be the events player A tosses heads and tails, respectively.\nSimilarly define \\(B_H\\), \\(B_T\\), \\(C_H\\), and \\(C_T\\)."
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#learning-objectives",
    "href": "slides/4_Conditional_Probability.html#learning-objectives",
    "title": "Chapter 4: Conditional Probability",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUse set process to calculate probability of event of interest\nCalculate the probability of an event occurring, given that another event occurred.\nDefine keys facts for conditional probabilities using notation."
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#lets-revisit-our-deck-of-cards",
    "href": "slides/4_Conditional_Probability.html#lets-revisit-our-deck-of-cards",
    "title": "Chapter 4: Conditional Probability",
    "section": "Let’s revisit our deck of cards",
    "text": "Let’s revisit our deck of cards\n\n\n\n\nExample 1\n\n\nSuppose we randomly draw 2 cards from a standard deck of cards. What is the probability that we draw a spade then a heart?\n\n\nLet\n\nLet \\(A =\\) event \\(1^{st}\\) card is spades\nLet \\(B =\\) event \\(2^{nd}\\) card is heart"
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#conditional-probability-with-two-dice",
    "href": "slides/4_Conditional_Probability.html#conditional-probability-with-two-dice",
    "title": "Chapter 4: Conditional Probability",
    "section": "Conditional probability with two dice",
    "text": "Conditional probability with two dice\n\n\n\n\nExample 2\n\n\nTwo dice (red and blue) are rolled. If the dice do not show the same face, what is the probability that one of the dice is a 1?\n\n\n\n\n\n\nChapter 4 Slides"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#learning-objectives",
    "href": "slides/5_Bayes_Theorem.html#learning-objectives",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCalculate conditional probability of an event using Bayes’ Theorem\nUtilize additional probability rules in probability calculations, specifically the Higher Order Multiplication Rule and the Law of Total Probabilities"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculating-probability-with-higher-order-multiplication-rule",
    "href": "slides/5_Bayes_Theorem.html#calculating-probability-with-higher-order-multiplication-rule",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with Higher Order Multiplication Rule",
    "text": "Calculating probability with Higher Order Multiplication Rule\n\n\n\n\nExample 1\n\n\nSuppose we draw 5 cards from a standard shuffled deck of 52 cards. What is the probability of a flush, that is all the cards are of the same suit (including straight flushes)?\n\n\n\n\n\nHigher Order Multiplication Rule\n\n\n\\[\\mathbb{P}(A_1\\cap A_2 \\cap  \\ldots \\cap A_n)=\\mathbb{P}(A_1)\\cdot\\mathbb{P}(A_2|A_1) \\cdot \\\\\n\\mathbb{P}(A_3|A_1A_2)\\ldots \\cdot\\mathbb{P}(A_n|A_1A_2\\ldots A_{n-1})\\]"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculating-probability-with-law-of-total-probability",
    "href": "slides/5_Bayes_Theorem.html#calculating-probability-with-law-of-total-probability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with Law of Total Probability",
    "text": "Calculating probability with Law of Total Probability\n\n\n\n\nExample 2\n\n\nSuppose 1% of people assigned female at birth (AFAB) and 5% of people assigned male at birth (AMAB) are color-blind. Assume person born is equally likely AFAB or AMAB (not including intersex). What is the probability that a person chosen at random is color-blind?\n\n\n\n\n\nLaw of Total Probability for 2 Events\n\n\nFor events \\(A\\) and \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=&\\mathbb{P}(B \\cap A) + \\mathbb{P}(B \\cap A^C)\\\\\n           &=& \\mathbb{P}(B|A) \\cdot \\mathbb{P}(A)+ \\mathbb{P}(B | A^C)\\cdot \\mathbb{P}(A^C)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-version-of-law-of-total-probability",
    "href": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-version-of-law-of-total-probability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized version of Law of Total Probability",
    "text": "Calculating probability with generalized version of Law of Total Probability\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?\n\n\n\n\nLaw of Total Probability (general)\n\n\nIf \\(\\{A_i\\}_{i=1}^{n} = \\{A_1, A_2, \\ldots, A_n\\}\\) form a partition of the sample space, then for event \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=& \\sum_{i=1}^{n} \\mathbb{P}(B \\cap A_i)\\\\\n           &=& \\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability",
    "href": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability",
    "text": "Calculating probability with generalized Law of Total Probability"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#lets-revisit-the-color-blind-example",
    "href": "slides/5_Bayes_Theorem.html#lets-revisit-the-color-blind-example",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Let’s revisit the color-blind example",
    "text": "Let’s revisit the color-blind example\n\n\n\n\nExample 4\n\n\nRecall the color-blind example (Example 2), where\n\na person is AMAB with probability 0.5,\nAMAB people are color-blind with probability 0.05, and\nall people are color-blind with probability 0.03.\n\nAssuming people are AMAB or AFAB, find the probability that a color-blind person is AMAB."
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculate-probability-with-both-rules",
    "href": "slides/5_Bayes_Theorem.html#calculate-probability-with-both-rules",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculate probability with both rules",
    "text": "Calculate probability with both rules\n\n\n\n\nExample 5\n\n\nSuppose\n\n1% of women aged 40-50 years have breast cancer,\na woman with breast cancer has a 90% chance of a positive test from a mammogram, and\na woman has a 10% chance of a false-positive result from a mammogram.\n\nWhat is the probability that a woman has breast cancer given that she just had a positive test?"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#general-law-of-total-proability",
    "href": "slides/5_Bayes_Theorem.html#general-law-of-total-proability",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "General Law of Total Proability",
    "text": "General Law of Total Proability\n\n\nLaw of Total Probability (general)\n\n\nIf \\(\\{A_i\\}_{i=1}^{n} = \\{A_1, A_2, \\ldots, A_n\\}\\) form a partition of the sample space, then for event \\(B\\),\n\\[%\\left(\n\\begin{array}{ccl}\n\\mathbb{P}(B)&=& \\sum_{i=1}^{n} \\mathbb{P}(B \\cap A_i)\\\\\n           &=& \\sum_{i=1}^{n} \\mathbb{P}(B|A_i) \\cdot \\mathbb{P}(A_i)\n\\end{array}\n%\\right)\\]"
  },
  {
    "objectID": "slides/4_Conditional_Probability.html#general-process-for-probability-word-problems",
    "href": "slides/4_Conditional_Probability.html#general-process-for-probability-word-problems",
    "title": "Chapter 4: Conditional Probability",
    "section": "General Process for Probability Word Problems",
    "text": "General Process for Probability Word Problems\n\nClearly define your events of interest\nTranslate question to probability using defined events OR Venn Diagram\nAsk yourself:\n\nAre we sampling with or without replacement?\nDoes order matter?\n\nUse axioms, properties, partitions, facts, etc. to define the end probability calculation into smaller parts\n\nIf probabilities are given to you, Venn Diagrams may help you parse out the events and probability calculations\nIf you need to find probabilities with counting, pictures or diagrams might help here\n\nWrite out a concluding statement that gives the probability context\n(For own check) Make sure the calculated probability follows the axioms. Is is between 0 and 1?"
  },
  {
    "objectID": "slides/22_Counting_Intro.html#enumerating-events-and-sample-space",
    "href": "slides/22_Counting_Intro.html#enumerating-events-and-sample-space",
    "title": "Chapter 22: Introduction to Counting",
    "section": "Enumerating Events and Sample Space",
    "text": "Enumerating Events and Sample Space\n\nRecall, \\(P(A) = \\dfrac{|A|}{|S|}\\). And within combinatorics, we can use the previous equations to help enumerate the event and sample space.\nI left something out though… the enumeration of the event is not just one of the above formulas.\nFor example in the example of the spades when order does not matter, we actually need to enumerate the other cards that were NOT spades. So the event is choosing 2 spades out of 13 AND choosing 0 other cards of 39 cards (13 hearts + 13 clubs + 13 diamonds).\nThus the probability is actually:\n\n\\[ P(\\text{two spades}) = \\dfrac{{13 \\choose 2}{39 \\choose 0}}{{52 \\choose 2}} \\]\n\nNote that \\(13 + 39 = 52\\) and \\(2+ 0 = 2\\). So the numerator’s \\(n\\)’s add up to the denominator’s \\(n\\) and the numerator’s \\(r\\)’s add up to the denominator’s \\(r\\)’s\n\n\n\nChapter 22 Slides"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-.smaller-.hidden",
    "href": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-.smaller-.hidden",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability {.smaller, .hidden}",
    "text": "Calculating probability with generalized Law of Total Probability {.smaller, .hidden}\n\n\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-.smaller",
    "href": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-.smaller",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability {.smaller}",
    "text": "Calculating probability with generalized Law of Total Probability {.smaller}\n\n\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?"
  },
  {
    "objectID": "slides/7_Random_Variables.html#what-is-a-random-variable",
    "href": "slides/7_Random_Variables.html#what-is-a-random-variable",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "What is a random variable?",
    "text": "What is a random variable?\n\n\nDefinition: Random Variable\n\n\nFor a given sample space \\(S\\), a random variable (r.v.) is a function whose domain is \\(S\\) and whose range is the set of real numbers \\(\\mathbb{R}\\). A random variable assigns a real number to each outcome in the sample space."
  },
  {
    "objectID": "slides/7_Random_Variables.html#learning-objectives",
    "href": "slides/7_Random_Variables.html#learning-objectives",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nMap the sample space to the set of real numbers using a discrete and continuous random variable\nDistinguish between discrete and continuous random variables from a written description"
  },
  {
    "objectID": "slides/7_Random_Variables.html#lets-demonstrate-this-definition-with-our-coin-toss",
    "href": "slides/7_Random_Variables.html#lets-demonstrate-this-definition-with-our-coin-toss",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Let’s demonstrate this definition with our coin toss",
    "text": "Let’s demonstrate this definition with our coin toss\n\n\n\n\nExample 1\n\n\nSuppose we toss 3 fair coins.\n\nWhat is the sample space?\nWhat are the probabilities for each of the elements in the sample space?\nWhat are the probabilities that you get 0, 1, 2, or 3 tails?"
  },
  {
    "objectID": "slides/7_Random_Variables.html#lets-stretch-our-definition-of-random-variables",
    "href": "slides/7_Random_Variables.html#lets-stretch-our-definition-of-random-variables",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Let’s stretch our definition of random variables",
    "text": "Let’s stretch our definition of random variables\n\n\n\n\nExample 2\n\n\nWhat are some other random variables we could consider in Example 1?"
  },
  {
    "objectID": "slides/7_Random_Variables.html#some-remarks-on-random-variables",
    "href": "slides/7_Random_Variables.html#some-remarks-on-random-variables",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Some remarks on random variables",
    "text": "Some remarks on random variables\n\nA random variable’s value is completely determined by the outcome \\(\\omega\\), where \\(\\omega \\in S\\)\n\nWhat is random is the outcome \\(\\omega\\)\n\nA random variable is a function from the sample space (with outcomes \\(\\omega\\)) to the set of real numbers\n\nWe typically write \\(X\\) instead of \\(X(\\omega)\\), where \\(X\\) is our random variable\n\nFor example, if we roll three dice, there are \\(6^3 = 216\\) possible outcomes (which is \\(\\omega\\))\n\nWe can define a random variable as the sum of the of the three dice\nIf our outcome is the set of numbers the dice landed on ( \\(\\omega=(a,b,c)\\) ), then \\[ X(\\omega) = X = a + b + c \\]"
  },
  {
    "objectID": "slides/7_Random_Variables.html#lets-look-at-sample-space",
    "href": "slides/7_Random_Variables.html#lets-look-at-sample-space",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Let’s look at sample space,",
    "text": "Let’s look at sample space,\n\n\n\n\nExample 4\n\n\nLet \\(X =\\) how many hours you slept last night.\n\nWhat is the sample space \\(S\\)?\nWhat is the range of possible values for \\(X\\)?\nWhat is \\(X(\\omega)\\)?"
  },
  {
    "objectID": "slides/7_Random_Variables.html#lets-look-at-a-continuous-r.v.",
    "href": "slides/7_Random_Variables.html#lets-look-at-a-continuous-r.v.",
    "title": "Chapter 7: Discrete vs. Continuous Random Variables",
    "section": "Let’s look at a continuous R.V.",
    "text": "Let’s look at a continuous R.V.\n\n\n\n\nExample 3\n\n\nLet \\(X =\\) how many hours you slept last night.\n\nWhat is the sample space \\(S\\)?\nWhat is the range of possible values for \\(X\\)?\nWhat is \\(X(\\omega)\\)?"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#learning-objectives",
    "href": "slides/8_pmfs_and_cdfs.html#learning-objectives",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCalculate probabilities for discrete random variables\nCalculate and graph a probability mass function (pmf)\nCalculate and graph a cumulative distribution function (CDF)"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#what-is-a-random-variable",
    "href": "slides/8_pmfs_and_cdfs.html#what-is-a-random-variable",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "What is a random variable?",
    "text": "What is a random variable?\n\n\nDefinition: probability distribution or probability mass function (pmf)\n\n\nThe probability distribution or probability mass function (pmf) of a discrete r.v.* \\(X\\) is defined for every number \\(x\\) by \\[p_X(x) = \\mathbb{P}(X=x) = \\mathbb{P}(\\mathrm{all }\\ \\omega\\in S:X(\\omega) = x)\\]"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#lets-demonstrate-this-definition-with-our-coin-toss",
    "href": "slides/8_pmfs_and_cdfs.html#lets-demonstrate-this-definition-with-our-coin-toss",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Let’s demonstrate this definition with our coin toss",
    "text": "Let’s demonstrate this definition with our coin toss\n\n\n\n\nExample 1\n\n\nSuppose we toss 3 coins with probability of tails \\(p\\). If \\(X\\) is the random variable counting the number of tails, what are the probabilities of each value of \\(X\\)?"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#what-is-a-probability-mass-function",
    "href": "slides/8_pmfs_and_cdfs.html#what-is-a-probability-mass-function",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "What is a probability mass function?",
    "text": "What is a probability mass function?\n\n\nDefinition: probability distribution or probability mass function (pmf)\n\n\nThe probability distribution or probability mass function (pmf) of a discrete r.v. \\(X\\) is defined for every number \\(x\\) by \\[p_X(x) = \\mathbb{P}(X=x) = \\mathbb{P}(\\mathrm{all }\\ \\omega\\in S:X(\\omega) = x)\\]"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#remarks-on-the-pmf",
    "href": "slides/8_pmfs_and_cdfs.html#remarks-on-the-pmf",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Remarks on the pmf",
    "text": "Remarks on the pmf\n\n\nProperties of pmf\n\n\nA pmf \\(p_X(x)\\) must satisfy the following properties:\n\n\\(0 \\leq p_X(x) \\leq 1\\) for all \\(x\\).\n\\(\\sum \\limits_{\\{all\\ x\\}}p_X(x)=1\\).\n\n\n\n\nSome distributions depend on parameters\n\nEach value of a parameter gives a different pmf\nIn previous example, the number of coins tossed was a parameter\n\nWe tossed 3 coins\nIf we tossed 4 coins, we’d get a different pmf!\n\nThe collection of all pmf’s for different values of the parameters is called a family of pmf’s"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#binomial-family-of-rvs",
    "href": "slides/8_pmfs_and_cdfs.html#binomial-family-of-rvs",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Binomial family of RVs",
    "text": "Binomial family of RVs\n\n\n\n\nExample 2\n\n\nSuppose you toss \\(n\\) coins, each with probability of tails \\(p\\). If \\(X\\) is the number of tails, what is the pmf of \\(X\\)?"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#bernoulli-family-of-rvs",
    "href": "slides/8_pmfs_and_cdfs.html#bernoulli-family-of-rvs",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Bernoulli family of RVs",
    "text": "Bernoulli family of RVs\n\n\n\n\nExample 3\n\n\nSuppose you toss 1 coin, with probability of tails \\(p\\). If \\(X\\) is the number of tails, what is the pmf of \\(X\\)?"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size",
    "href": "slides/8_pmfs_and_cdfs.html#household-size",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size",
    "text": "Household size\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nWhat is the sample space for household sizes?\nDefine the random variable for household sizes.\nDo the values in the table create a pmf? Why or why not?\nMake a plot of the pmf"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#what-is-a-cumulative-distribution-function",
    "href": "slides/8_pmfs_and_cdfs.html#what-is-a-cumulative-distribution-function",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "What is a cumulative distribution function?",
    "text": "What is a cumulative distribution function?\n\n\nDefinition: cumulative distribution function (CDF)\n\n\nThe cumulative distribution function (cdf) of a discrete r.v. \\(X\\) with pmf \\(p_X(x)\\), is defined for every value \\(x\\) by \\[F_X(x) = \\mathbb{P}(X \\leq x) = \\sum \\limits_{\\{all\\ y:\\ y\\leq x\\}}p_X(y)\\]"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-continued",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-continued",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size continued",
    "text": "Household size continued\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nGraph the cdf of household sizes in 2019.\nWrite the cdf as a function."
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#properties-of-discrete-cdfs",
    "href": "slides/8_pmfs_and_cdfs.html#properties-of-discrete-cdfs",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Properties of discrete CDFs",
    "text": "Properties of discrete CDFs\n\n\\(F(x)\\) is increasing or flat (never decreasing)\n\\(\\min\\limits_x F(x) = 0\\)\n\\(\\max\\limits_xF(x)=1\\)\nCDF is a step function\n\n\n\nChapter 8 Slides"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-14",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-14",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (1/4)",
    "text": "Household size (1/4)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nWhat is the sample space for household sizes?\nDefine the random variable for household sizes.\nDo the values in the table create a pmf? Why or why not?\nMake a plot of the pmf.\nGraph the cdf of household sizes in 2019.\nWrite the cdf as a function."
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-24",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-24",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (2/4)",
    "text": "Household size (2/4)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nWhat is the sample space for household sizes?\nDefine the random variable for household sizes."
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-34",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-34",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (3/4)",
    "text": "Household size (3/4)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nDo the values in the table create a pmf? Why or why not?\nMake a plot of the pmf"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-44",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-44",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (4/4)",
    "text": "Household size (4/4)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nGraph the cdf of household sizes in 2019.\nWrite the cdf as a function."
  },
  {
    "objectID": "slides/9_joint_distributions.html#learning-objectives",
    "href": "slides/9_joint_distributions.html#learning-objectives",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCalculate probabilities for a pair of discrete random variables\nCalculate and graph a joint, marginal, and conditional probability mass function (pmf)\nCalculate and graph a joint, marginal, and conditional cumulative distribution function (CDF)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#what-is-a-joint-pmf",
    "href": "slides/9_joint_distributions.html#what-is-a-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint pmf?",
    "text": "What is a joint pmf?\n\n\nDefinition: joint pmf\n\n\nThe joint pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[p_{X,Y}(x,y) = \\mathbb{P}(X=x\\ and\\ Y=y) = \\mathbb{P}(X=x, Y=y)\\]"
  },
  {
    "objectID": "slides/9_joint_distributions.html#example",
    "href": "slides/9_joint_distributions.html#example",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Example",
    "text": "Example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nHints:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counterexample\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#remarks-on-the-join-pmf",
    "href": "slides/9_joint_distributions.html#remarks-on-the-join-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the join pmf",
    "text": "Remarks on the join pmf\nSome properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#what-is-a-join-cdf",
    "href": "slides/9_joint_distributions.html#what-is-a-join-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a join CDF?",
    "text": "What is a join CDF?\n\n\nDefinition: joint CDF\n\n\nThe joint CDF of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]"
  },
  {
    "objectID": "slides/9_joint_distributions.html#example-1",
    "href": "slides/9_joint_distributions.html#example-1",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Example",
    "text": "Example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#example-2",
    "href": "slides/9_joint_distributions.html#example-2",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Example",
    "text": "Example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nHints:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counterexample\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#remarks-on-the-joint-cdf",
    "href": "slides/9_joint_distributions.html#remarks-on-the-joint-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint CDF",
    "text": "Remarks on the joint CDF\n\n\\(F_X(x)\\): right most columns of the CDf table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#independence-and-conditioning",
    "href": "slides/9_joint_distributions.html#independence-and-conditioning",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Independence and Conditioning",
    "text": "Independence and Conditioning\nRecall that for events \\(A\\) and \\(B\\),\n\n\\(\\mathbb{P}(A|B) = \\frac{\\mathbb{P}(A \\cap B)}{\\mathbb{P}(B)}\\)\n\\(A\\) and \\(B\\) are independent if and only if\n\n\\(\\mathbb{P}(A|B) = \\mathbb{P}(A)\\)\n\\(\\mathbb{P}(A \\cap B) = \\mathbb{P}(A)\\cdot\\mathbb{P}(B)\\)\n\n\nIndependence and conditioning are defined similarly for r.v.’s, since \\[p_X(x) = \\mathbb{P}(X=x)\\ \\mathrm{and}\\ \\ p_{X,Y}(x,y) = \\mathbb{P}(X = x ,Y = y).\\]"
  },
  {
    "objectID": "slides/9_joint_distributions.html#what-is-the-conditional-pmf",
    "href": "slides/9_joint_distributions.html#what-is-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is the conditional pmf?",
    "text": "What is the conditional pmf?\n\n\nDefinition: conditional pmf\n\n\nThe conditional pmf of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is defined as \\[p_{X|Y}(x|y) = \\mathbb{P}(X = x |Y = y) = \\frac{\\mathbb{P}(X = x\\ and\\ Y = y)}{\\mathbb{P}(Y = y)}\n=\\frac{p_{X,Y}(x,y) }{p_{Y}(y) }\\] if \\(p_{Y}(y) &gt; 0\\)."
  },
  {
    "objectID": "slides/9_joint_distributions.html#remarks-on-teh-conditional-pmf",
    "href": "slides/9_joint_distributions.html#remarks-on-teh-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on teh conditional pmf",
    "text": "Remarks on teh conditional pmf\nRemark: The following properties follow from the conditional pmf definition:"
  },
  {
    "objectID": "slides/9_joint_distributions.html#example-3",
    "href": "slides/9_joint_distributions.html#example-3",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Example",
    "text": "Example\n\nExample 7. Using \\(X\\) and \\(Y\\) from Example 2:\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\nHints:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counterexample.\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "slides/9_joint_distributions.html#hypothetical-4-sided-die",
    "href": "slides/9_joint_distributions.html#hypothetical-4-sided-die",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Hypothetical 4-sided die",
    "text": "Hypothetical 4-sided die\n\n\nExample 3\n\n\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\).\n\n\n\n\n\nChapter 9 Slides"
  },
  {
    "objectID": "slides/9_joint_distributions.html#remarks-on-the-joint-and-marginal-cdf",
    "href": "slides/9_joint_distributions.html#remarks-on-the-joint-and-marginal-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint and marginal CDF",
    "text": "Remarks on the joint and marginal CDF\n\n\\(F_X(x)\\): right most columns of the CDf table (where the \\(Y\\) values are largest)\n\\(F_Y(y)\\): bottom row of the table (where X values are largest)\n\\(F_X(x)=\\lim\\limits_{y\\rightarrow\\infty}F_{X, Y}(x,y)\\)\n\\(F_Y(y)=\\lim\\limits_{x\\rightarrow\\infty}F_{X, Y}(x,y)\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#remarks-on-the-conditional-pmf",
    "href": "slides/9_joint_distributions.html#remarks-on-the-conditional-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the conditional pmf",
    "text": "Remarks on the conditional pmf\nThe following properties follow from the conditional pmf definition:\n\nIf \\(X \\perp Y\\) (independent)\n\n\\(p_{X|Y}(x|y) = p_X(x)\\) for all \\(x\\) and \\(y\\)\n\\(p_{X,Y}(x,y) = p_X(x)p_Y(y)\\) for all \\(x\\) and \\(y\\)\nWhich also implies (\\(\\Rightarrow\\)): \\(F_{X,Y}(x,y) = F_X(x)F_Y(y)\\) for all \\(x\\) and \\(y\\)\n\nIf \\(X_1, X_2, …, X_n\\) are independent\n\n\\[p_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1=x_1, X_2=x_2, …, X_n=x_n)=\\prod\\limits_{i=1}^np_{X_i}(x_i)\\]\n\\[F_{X_1, X_2, …, X_n}(x_1, x_2, …, x_n) = P(X_1\\leq x_1, X_2\\leq x_2, …, X_n\\leq x_n)=\\prod\\limits_{i=1}^nP(X_i \\leq x_i) = \\prod\\limits_{i=1}^nF_{X_i}(x_i)\\]"
  },
  {
    "objectID": "slides/9_joint_distributions.html#remarks-on-the-joint-pmf",
    "href": "slides/9_joint_distributions.html#remarks-on-the-joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Remarks on the joint pmf",
    "text": "Remarks on the joint pmf\nSome properties of joint pmf’s:\n\nA joint pmf \\(p_{X,Y}(x,y)\\) must satisfy the following properties:\n\n\\(p_{X,Y}(x,y)\\geq 0\\) for all \\(x, y\\).\n\\(\\sum \\limits_{\\{all\\ x\\}} \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)=1\\).\n\nMarginal pmf’s:\n\n\\(p_X(x) = \\sum \\limits_{\\{all\\ y\\}} p_{X,Y}(x,y)\\)\n\\(p_Y(y) = \\sum \\limits_{\\{all\\ x\\}} p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#what-is-a-joint-cdf",
    "href": "slides/9_joint_distributions.html#what-is-a-joint-cdf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "What is a joint CDF?",
    "text": "What is a joint CDF?\n\n\nDefinition: joint CDF\n\n\nThe joint CDF of a pair of discrete r.v.’s \\(X\\) and \\(Y\\) is \\[F_{X,Y}(x,y) = \\mathbb{P}(X \\leq x\\ and\\ Y \\leq y) = \\mathbb{P}(X \\leq x, Y \\leq y)\\]"
  },
  {
    "objectID": "slides/9_joint_distributions.html#this-chapters-main-example",
    "href": "slides/9_joint_distributions.html#this-chapters-main-example",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "This chapter’s main example",
    "text": "This chapter’s main example\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?"
  },
  {
    "objectID": "slides/9_joint_distributions.html#joint-pmf",
    "href": "slides/9_joint_distributions.html#joint-pmf",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint pmf",
    "text": "Joint pmf\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X,Y}(x,y)\\).\nFind \\(\\mathbb{P}(X+Y=3).\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#marginal-pmfs",
    "href": "slides/9_joint_distributions.html#marginal-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal pmf’s",
    "text": "Marginal pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(\\mathbb{P}(Y = 1).\\)\nFind \\(\\mathbb{P}(Y \\leq 2).\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#joint-and-marginal-cdfs",
    "href": "slides/9_joint_distributions.html#joint-and-marginal-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint and marginal CDFs",
    "text": "Joint and marginal CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#hypothetical-4-sided-die-.visabilityhidden",
    "href": "slides/9_joint_distributions.html#hypothetical-4-sided-die-.visabilityhidden",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Hypothetical 4-sided die {.visability=“hidden”}",
    "text": "Hypothetical 4-sided die {.visability=“hidden”}\n\n\nExample 3\n\n\n\nSuppose you have a 4-sided die, and you roll the 4-sided die until the first 4 appears.\nLet \\(X\\) be the number of rolls required until (and including) the first 4.\nAfter the first 4, you keep rolling it again until you roll a 3.\nLet \\(Y\\) be the number of rolls, after the first 4, required until (and including) the 3.\n\n\nFind \\(p_{X,Y}(x,y)\\).\nUsing \\(p_{X,Y}(x,y)\\), find \\(p_{Y}(y)\\).\nFind \\(p_{X}(x)\\).\nAre \\(X\\) and \\(Y\\) are independent? Why or why not?\nFind \\(F_{X,Y}(x,y)\\).\n\n\n\n\n\nChapter 9 Slides"
  },
  {
    "objectID": "slides/9_joint_distributions.html#conditional-pmfs",
    "href": "slides/9_joint_distributions.html#conditional-pmfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Conditional pmf’s",
    "text": "Conditional pmf’s\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind \\(p_{X|Y}(x|y)\\).\nAre \\(X\\) and \\(Y\\) independent? Why or why not?\n\n\n\n\n\nRemark:\n\nTo show that \\(X\\) and \\(Y\\) are not independent, we just need to find one counter example\nHowever, to show that they are independent, we need to verify this for all possible pairs of \\(x\\) and \\(y\\)"
  },
  {
    "objectID": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-.smaller-visibilityhidden",
    "href": "slides/5_Bayes_Theorem.html#calculating-probability-with-generalized-law-of-total-probability-.smaller-visibilityhidden",
    "title": "Chapter 5: Bayes’ Theorem",
    "section": "Calculating probability with generalized Law of Total Probability {.smaller, visibility=“hidden”}",
    "text": "Calculating probability with generalized Law of Total Probability {.smaller, visibility=“hidden”}\n\n\n\n\nExample 3\n\n\nIndividuals are diagnosed with a particular type of cancer that can take on three different disease forms,* \\(D_1\\), \\(D_2\\), and \\(D_3\\). It is known that amongst people diagnosed with this particular type of cancer,\n\n20% of people will eventually be diagnosed with form \\(D_1\\),\n30% with form \\(D_2\\), and\n50% with form \\(D_3\\).\n\nThe probability of requiring chemotherapy (\\(C\\)) differs among the three forms of disease:\n\n80% with \\(D_1\\),\n30% with \\(D_2\\), and\n10% with \\(D_3\\).\n\nBased solely on the preliminary test of being diagnosed with the cancer, what is the probability of requiring chemotherapy (the event C)?\n\n\n\nSkipping in class! Let me know if you would like me to post solutions to this if you work through it!"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-34-1",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-34-1",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (3/4)",
    "text": "Household size (3/4)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nMake a plot of the pmf"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-15",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-15",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (1/5)",
    "text": "Household size (1/5)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nWhat is the sample space for household sizes?\nDefine the random variable for household sizes.\nDo the values in the table create a pmf? Why or why not?\nMake a plot of the pmf.\nWrite the cdf as a function.\nGraph the cdf of household sizes in 2019."
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-25",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-25",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (2/5)",
    "text": "Household size (2/5)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nWhat is the sample space for household sizes?\nDefine the random variable for household sizes."
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-35",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-35",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (3/5)",
    "text": "Household size (3/5)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nDo the values in the table create a pmf? Why or why not?\nMake a plot of the pmf"
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-45",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-45",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (4/5)",
    "text": "Household size (4/5)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nWrite the cdf as a function."
  },
  {
    "objectID": "slides/8_pmfs_and_cdfs.html#household-size-55",
    "href": "slides/8_pmfs_and_cdfs.html#household-size-55",
    "title": "Chapter 8: Probability Mass Functions (pmf’s) and Cumulative Distribution Functions (cdf’s)",
    "section": "Household size (5/5)",
    "text": "Household size (5/5)\n\n\n\n\nExample 4\n\n\nThe table below shows household sizes in 2019. Data are from the U.S. Census.\n\n\n\n\nSize\n1\n2\n3\n4\n5 or more\n\n\n\n\nPercent\n28%\n35%\n15%\n13%\n9%\n\n\n\n\n\nGraph the cdf of household sizes in 2019."
  },
  {
    "objectID": "slides/9_joint_distributions.html#joint-cdfs",
    "href": "slides/9_joint_distributions.html#joint-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Joint CDFs",
    "text": "Joint CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the joint CDF \\(F_{X,Y}(x,y)\\) for the joint pmf \\(p_{X,Y}(x,y)\\)"
  },
  {
    "objectID": "slides/9_joint_distributions.html#marginal-cdfs",
    "href": "slides/9_joint_distributions.html#marginal-cdfs",
    "title": "Chapter 9: Independence and Conditioning (Joint Distributions)",
    "section": "Marginal CDFs",
    "text": "Marginal CDFs\n\n\n\n\nExample 1\n\n\nLet \\(X\\) and \\(Y\\) be two random draws from a box containing balls labelled 1, 2, and 3 without replacement.\n\nFind the marginal CDFs \\(F_{X}(x)\\) and \\(F_{Y}(y)\\)"
  },
  {
    "objectID": "slides/10_Expected_Values.html#our-good-friend-the-6-sided-die",
    "href": "slides/10_Expected_Values.html#our-good-friend-the-6-sided-die",
    "title": "Chapter 10: Expected Values of Discrete r.v.’s",
    "section": "Our good friend, the 6-sided die",
    "text": "Our good friend, the 6-sided die\n\n\nExample 1.\n\n\nSuppose you roll a fair 6-sided die. What value do you expect to get?"
  },
  {
    "objectID": "slides/10_Expected_Values.html#our-good-and-fair-friend-the-6-sided-die",
    "href": "slides/10_Expected_Values.html#our-good-and-fair-friend-the-6-sided-die",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Our good and fair friend, the 6-sided die",
    "text": "Our good and fair friend, the 6-sided die\n\n\n\n\nExample 1\n\n\nSuppose you roll a fair 6-sided die. What value do you expect to get?"
  },
  {
    "objectID": "slides/10_Expected_Values.html#learning-objectives",
    "href": "slides/10_Expected_Values.html#learning-objectives",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCalculate the mean (expected value) of discrete random variables"
  },
  {
    "objectID": "slides/10_Expected_Values.html#our-good-and-not-so-fair-friend-the-6-sided-die",
    "href": "slides/10_Expected_Values.html#our-good-and-not-so-fair-friend-the-6-sided-die",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Our good and not-so-fair friend, the 6-sided die",
    "text": "Our good and not-so-fair friend, the 6-sided die\n\n\n\n\nExample 2\n\n\nSuppose the die is 6-sided, but not fair. And the probabilities of each side is distributed as:\n\n\n\n\\(x\\)\n\\(p_X(x)\\)\n\n\n\n\n1\n0.10\n\n\n2\n0.05\n\n\n3\n0.02\n\n\n4\n0.30\n\n\n5\n0.50\n\n\n6\n0.03\n\n\n\nWhat value do you expect to get on a roll?"
  },
  {
    "objectID": "slides/10_Expected_Values.html#quick-remark-on-expected-values",
    "href": "slides/10_Expected_Values.html#quick-remark-on-expected-values",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Quick remark on expected values",
    "text": "Quick remark on expected values\n\nLet’s think about expected values vs. actual outcomes\nExpected values are not necessarily an actual outcome\n\nIt could be that our expected value is not in the sample space (\\(E(X) \\notin S\\))"
  },
  {
    "objectID": "slides/10_Expected_Values.html#what-is-an-expected-value",
    "href": "slides/10_Expected_Values.html#what-is-an-expected-value",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "What is an expected value?",
    "text": "What is an expected value?\n\n\nDefinition: Expected value\n\n\nThe expected value of a discrete r.v. \\(X\\) that takes on values \\(x_1, x_2, \\ldots, x_n\\) is \\[\\mathbb{E}[X] = \\sum_{i=1}^n x_ip_X(x_i).\\]\n\n\n\nExpected values are not necessarily an actual outcome\n\nIn previous example, we cannot roll a 3.5\nIt could be that our expected value is not in the sample space (\\(E(X) \\notin S\\))\n\nDefinition holds when \\(X\\) takes on countably infinitely many values (think \\(n=\\infty\\))"
  },
  {
    "objectID": "slides/10_Expected_Values.html#remarks-on-the-expected-value",
    "href": "slides/10_Expected_Values.html#remarks-on-the-expected-value",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Remarks on the expected value",
    "text": "Remarks on the expected value\n\nThe definition holds if the r.v. \\(X\\) takes on countably infinitely many values \\(x_1, x_2, \\ldots\\), as well: \\[\\mathbb{E}[X] = \\sum_{i=1}^{\\infty} x_ip_X(x_i).\\]\nAnother way to define the expected value of a discrete r.v. is to do so at the \\(\\omega\\) level, where the \\(\\omega\\)’s are outcomes in the sample space:\n\nSuppose \\(\\omega_1, \\omega_2, \\ldots, \\omega_n\\) are the possible outcomes of a random phenomenon. If outcome \\(\\omega_i\\) causes the r.v. X to take on value \\(x_i\\) (meaning \\(X(\\omega_i)=x_i\\)), then \\[\\mathbb{E}[X] = \\sum_{i=1}^{\\infty} x_i\\mathbb{P}(\\{\\omega_i\\}).\\]"
  },
  {
    "objectID": "slides/10_Expected_Values.html#expected-value-of-a-bernoulli-distribution",
    "href": "slides/10_Expected_Values.html#expected-value-of-a-bernoulli-distribution",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Expected value of a Bernoulli distribution",
    "text": "Expected value of a Bernoulli distribution\n\n\n\n\nExample 3\n\n\nSuppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\quad\\mathrm{(success)}\\\\\n            0 & \\quad \\mathrm{with\\ probability}\\ 1-p \\quad\\mathrm{(failure)}\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\)."
  },
  {
    "objectID": "slides/10_Expected_Values.html#lets-slightly-change-our-random-variable",
    "href": "slides/10_Expected_Values.html#lets-slightly-change-our-random-variable",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Let’s slightly change our random variable",
    "text": "Let’s slightly change our random variable\n\n\n\n\nExample 5\n\n\nSuppose \\[X = \\left\\{\n        \\begin{array}{ll}\n            1 & \\quad \\mathrm{with\\ probability}\\ p \\\\\n            -1 & \\quad \\mathrm{with\\ probability}\\ 1-p\n        \\end{array}\n    \\right.\\] Find the expected value of \\(X\\)."
  },
  {
    "objectID": "slides/10_Expected_Values.html#dartboard",
    "href": "slides/10_Expected_Values.html#dartboard",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Dartboard",
    "text": "Dartboard\n\nExample 6. Suppose I throw darts at a dartboard until I hit the bullseye, and that my probability of hitting the bullseye is \\(p\\). Suppose further that all of my throws are independent, and that the probability of a bullseye never changes, no matter how many times I throw a dart. How many times should I expect to have to throw the dart until I hit the bullseye?"
  },
  {
    "objectID": "slides/10_Expected_Values.html#ghost",
    "href": "slides/10_Expected_Values.html#ghost",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Ghost! 👻",
    "text": "Ghost! 👻\n\n\n\n\nExample 6\n\n\nA ghost is trick-or-treating. It comes to a house where it is known that there are 30 candies in the bag and only one is a watermelon Jolly Rancher, which is the ghost’s favorite. The ghost takes pieces of candy without replacement until it gets the watermelon Jolly Rancher. How many pieces of candy do we expect the ghost to take?"
  },
  {
    "objectID": "slides/10_Expected_Values.html#some-more-remarks",
    "href": "slides/10_Expected_Values.html#some-more-remarks",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Some more remarks",
    "text": "Some more remarks\nRemark: Both examples are repeated random processes. They are fundamentally different though:\n\nThe bullseye example (6) is “with replacement” since the probability of success remains constant.\nThe ghost trick-or-treating example (7) is without replacement, and thus the probability of success changes with each trial.\n\n\n\nChapter 10 Slides"
  },
  {
    "objectID": "slides/10_Expected_Values.html#bullseye",
    "href": "slides/10_Expected_Values.html#bullseye",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Bullseye! 🎯",
    "text": "Bullseye! 🎯\n\n\n\n\nExample 5\n\n\nSuppose I throw darts at a dartboard until I hit the bullseye, and that my probability of hitting the bullseye is \\(p\\). Suppose further that all of my throws are independent, and that the probability of a bullseye never changes, no matter how many times I throw a dart. How many times should I expect to have to throw the dart until I hit the bullseye?"
  },
  {
    "objectID": "slides/10_Expected_Values.html#some-remarks-on-last-two-examples",
    "href": "slides/10_Expected_Values.html#some-remarks-on-last-two-examples",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Some remarks on last two examples",
    "text": "Some remarks on last two examples\nBoth examples are repeated random processes. They are fundamentally different though:\n\nThe bullseye example is “with replacement” since the probability of success remains constant.\nThe ghost trick-or-treating example is without replacement, and thus the probability of success changes with each trial.\n\n\n\nChapter 10 Slides"
  },
  {
    "objectID": "homework/HW4.html#directions",
    "href": "homework/HW4.html#directions",
    "title": "Homework 4",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW4.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW3.html#directions",
    "href": "homework/HW3.html#directions",
    "title": "Homework 3",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW7.html#directions",
    "href": "homework/HW7.html#directions",
    "title": "Homework 7",
    "section": "",
    "text": "Please turn in this homework on Sakai. Please submit your homework in pdf format. You can type your work on your computer or submit a photo of your written work or any other method that can be turned into a pdf. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in homework.\nTry to complete all of the problems listed below at some point this quarter! You may want to save some of them for studying later! Only turn in the ones listed in the “Turn In” column. Please submit problems in the order they are listed.\nThe more work you include that shows your thought process, the more I can give you feedback.\n\n\n\n\n\n\n\n\n\nChapter\nTurn In\nExtra Problems\n\n\n\n\n28\nTB # 18\nTB # 1, 10\n\n\n291\nTB # 26, NTB # 1, 3\nTB # 10, 14, 23, 11, 13, 32\n\n\n30\n\nTB # 4, 7-12\n\n\n31\nTB # 18\nTB # 13, 14, 17\n\n\n32\nTB # 8\nTB # 3, 5, 102, 15\n\n\n33\nNTB # 4\nTB # 3, 9, 10\n\n\n35\nTB # 10, NTB # 5\nTB # 6, 9, 24\n\n\n43\nTB # 93, 104, 11, 125, NTB # 6, 7, 8\nTB # 1-4, NTB # 9\n\n\n36\nTB # 126, 14\nTB # 4, 11, 13, 15, 16\n\n\n37\nTB # 24, 30\nTB # 2, 4, 13, 20, 29"
  },
  {
    "objectID": "homework/HW6.html#directions",
    "href": "homework/HW6.html#directions",
    "title": "Homework 6",
    "section": "",
    "text": "Please turn in this homework on Sakai. Please submit your homework in pdf format. You can type your work on your computer or submit a photo of your written work or any other method that can be turned into a pdf. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in homework.\nTry to complete all of the problems listed below at some point this quarter! You may want to save some of them for studying later! Only turn in the ones listed in the “Turn In” column. Please submit problems in the order they are listed.\nThe more work you include that shows your thought process, the more I can give you feedback.\n\n\n\n\nChapter\nTurn In\nExtra Problems\n\n\n\n\n19\nTB # 6\n# 1, 18, 19\n\n\n18\nTB # 24\n# 1, 26, 27\n\n\nCalculus Review\n\nNTB # 1\n\n\n24\nTB # 19, 20*\n# 2, 3, 7, 17, 18, 22, 23\n\n\n25\nTB # 18, NTB # 2\n# 1, 4, 8, 17, 23, 24\n\n\n26**\nTB # 12, NTB # 3, 4\n# 7, 9, 19, 20\n\n\n27\nTB # 12***\n# 6, 8, 13, 17\n\n\n\n\n* (Ch 24) Also find the cdf \\(F_X(x)\\)\n** Although within Chapter 26, these exercises are primarily practicing the material from Chapter 25.\n** For Ch 27 # 12, in order to find the conditional densities in parts (a) and (b), you will need to calculate \\(f_Y(y)\\) for the specific regions of \\(y\\) specified. After finding the conditional densities in parts (a) and (b), also calculate the conditional probabilities below. Please submit these together with your other work in parts (a) and (b):\n\nFind \\(\\mathbb{P}[0.5 &lt; X &lt; 3 | Y = 4]\\).\nFind \\(\\mathbb{P}[0.5 &lt; X &lt; 3 | Y = 7]\\).\n\n\nNon-textbook problems (NTB):\n\nCalculus Review\n\n\\[\\int_0^yc(x+y)dx\\]\n\\[\\frac{d}{dx}\\bigg(\\frac{4}{9}x^2y^2+\\frac{5}{9}xy^4\\bigg)\\]\n\\[\\frac{d}{dy}\\bigg(\\frac{4}{9}x^2y^2+\\frac{5}{9}xy^4\\bigg)\\]\n\\[\\int_0^y2e^{-x}e^{-y}dx\\]\n\\[\\int_0^\\infty xye^{-(x+y)}dy\\]\n\\[\\int_x^{2x} 2e^{-(x+3y)}dy\\]\nFind the area of the region bounded by the graphs of \\(f(x)=2-x^2\\) and \\(g(x)=x\\) by integrating with respect to \\(x\\).\nFind the area of the region bounded by the graphs of \\(f(x)=2-x^2\\) and \\(g(x)=x\\) by integrating with respect to \\(y\\).\nFind the area of the region bounded by the graphs of \\(x=3-y^2\\) and \\(y=x-1\\) by integrating with respect to \\(x\\).\nFind the area of the region bounded by the graphs of \\(x=3-y^2\\) and \\(y=x-1\\) by integrating with respect to \\(y\\).\n\nLet \\(X_1, X_2, \\ldots, X_n\\) be i.i.d. random variables with common pdf \\(f_X(x)\\) and cdf \\(F_X(x)\\). Find the pdf for the random variable \\(Z\\), where \\(Z = max(X_1, X_2, \\ldots, X_n)\\).\nLet \\(X\\) and \\(Y\\) be independent random variables with respective pdf’s \\(f_X(x)=\\frac{1}{5}\\), for \\(0\\leq x\\leq 5\\), and \\(f_Y(y)=2e^{-2y}\\), for \\(y&gt;0\\).\n\nFind the joint distribution \\(f_{X,Y}(x,y)\\).\nFind the probability that \\(X\\) is less than \\(Y\\).\nLet \\(Z\\) be the random variable that is the smaller of \\(X\\) and \\(Y\\). Find the cumulative distribution function for \\(Z\\).\nFind the pdf for Z.\n\nSuppose that the random variables \\(X\\) and \\(Y\\) have joint density \\(f_{X,Y}(x,y)\\), for \\(0&lt;x&lt;1\\), and \\(\\frac{1}{2}&lt;y&lt;1\\). Set up the equation for the cdf of \\(Z\\), where \\(Z=X/Y\\).\nHint: First determine what the possible values for \\(Z\\) are. Then make a sketch of the domain of the joint pdf and shade in the region representing the cdf of Z for different values of \\(z\\). Make sure to pay close attention to how the region we need to integrate over changes as \\(z\\) changes. The cdf has two different cases depending on the value of \\(z\\). Plug in specific values of \\(z\\) and shade in the region representing the cdf to see why two different cases are needed."
  },
  {
    "objectID": "homework/HW5.html#directions",
    "href": "homework/HW5.html#directions",
    "title": "Homework 5",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW5.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Revisiting our two card draw",
    "text": "Revisiting our two card draw\n\n\n\n\nExample 1\n\n\nSuppose you draw 2 cards from a standard deck of cards with replacement. Let \\(X\\) be the number of hearts you draw. Find \\(\\mathbb{E}[X]\\).\n\n\n\n\nRecall Binomial RV with \\(n=2\\):\n\\[p_X(x) = {2 \\choose x}p^x(1-p)^{2-x} \\text{  for } x = 0, 1, 2\\]"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw-1",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-two-card-draw-1",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Revisiting our two card draw",
    "text": "Revisiting our two card draw\n\n\n\n\nExample 2\n\n\nWhat is the expected number of hearts in Example 1 if you draw 200 cards?"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#sum-of-discrete-rvs",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#sum-of-discrete-rvs",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Sum of discrete RVs",
    "text": "Sum of discrete RVs\n\n\nTheorem 11.1: Sum of discrete RVs\n\n\nFor discrete r.v.’s \\(X_i\\) and constants \\(a_i\\), \\(i=1,2,\\dots, n\\), \\[\\mathbb{E}\\Bigg[\\sum_{i=1}^n a_iX_i\\Bigg] = \\sum_{i=1}^n a_i\\mathbb{E}[X_i] .\\] Remark: The theorem holds for infinitely r.v.’s \\(X_i\\) as well.\n\n\n\nFor two RVs, \\(X\\) and \\(Y\\):\n\nWe can say \\(E[X+Y] = E[X] + E[Y]\\)\n… and constant numbers \\(a\\) and \\(b\\), we can also say \\(E[aX+bY] = aE[X] + bE[Y]\\)\nWe can also also say \\(E[X-Y] = E[X] - E[Y]\\), since \\(b=-1\\)"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#corollaries",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#corollaries",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Corollaries",
    "text": "Corollaries\n\n\n\n\nCorollary 11.1.1\n\n\nFor a discrete r.v. \\(X\\), and constants \\(a\\) and \\(b\\), \\[\\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b.\\]\n\n\n\n\n\nCorollary 11.1.2\n\n\nIf \\(X_i\\), \\(i=1,2,\\dots, n\\), are i.i.d. r.v.’s, then \\[\\mathbb{E}[\\sum_{i=1}^n X_i] = n\\mathbb{E}[X_1] .\\]"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-ghost",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#revisiting-our-ghost",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Revisiting our ghost! 👻",
    "text": "Revisiting our ghost! 👻\n\n\n\n\nExample 3\n\n\nThe ghost is trick-or-treating at a different house now. In this case it is known that the bag of candy has 10 chocolates, 20 lollipops, and 30 laffy taffies. The ghost takes five pieces of candy without replacement. How many pieces of chocolate do we expect the ghost to take?"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#hotels",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#hotels",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Hotels",
    "text": "Hotels\n\n\n\n\nExample 4\n\n\nA tour group is planning a visit to the city of Landport and needs to book 30 hotel rooms. The average price of a room is $200. In addition, there is a 10% tourism tax for each room. What is the expected cost for the 30 hotel rooms?\n\n\n\n\n\n\nChapter 11 Slides"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#cost-of-hotel-rooms",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#cost-of-hotel-rooms",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Cost of hotel rooms",
    "text": "Cost of hotel rooms\n\n\n\n\nExample 4\n\n\nA tour group is planning a visit to the city of Minneapolis and needs to book 30 hotel rooms. The average price of a room is $200. In addition, there is a 10% tourism tax for each room. What is the expected cost for the 30 hotel rooms?\n\n\n\n\n\n\nChapter 11 Slides"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#corollaries-from-thm-11.1",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#corollaries-from-thm-11.1",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Corollaries from Thm 11.1",
    "text": "Corollaries from Thm 11.1\n\n\n\n\nCorollary 11.1.1\n\n\nFor a discrete r.v. \\(X\\), and constants \\(a\\) and \\(b\\), \\[\\mathbb{E}[aX+b] = a\\mathbb{E}[X] + b.\\]\n\n\n\n\n\nCorollary 11.1.2\n\n\nIf \\(X_i\\), \\(i=1,2,\\dots, n\\), are i.i.d. r.v.’s, then \\[\\mathbb{E}\\bigg[\\sum_{i=1}^n X_i\\bigg] = n\\mathbb{E}[X_1] .\\]"
  },
  {
    "objectID": "slides/10_Expected_Values.html#bullseye-.visibilityhidden",
    "href": "slides/10_Expected_Values.html#bullseye-.visibilityhidden",
    "title": "Chapter 10: Expected Values of Discrete RVs",
    "section": "Bullseye! 🎯 {.visibility=“hidden”}",
    "text": "Bullseye! 🎯 {.visibility=“hidden”}\n\n\n\n\nExample 5\n\n\nSuppose I throw darts at a dartboard until I hit the bullseye, and that my probability of hitting the bullseye is \\(p\\). Suppose further that all of my throws are independent, and that the probability of a bullseye never changes, no matter how many times I throw a dart. How many times should I expect to have to throw the dart until I hit the bullseye?"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#what-if-we-draw-a-lot-of-cards",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#what-if-we-draw-a-lot-of-cards",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "What if we draw A LOT of cards?",
    "text": "What if we draw A LOT of cards?\n\n\n\n\nExample 2\n\n\nWhat is the expected number of hearts in Example 1 if you draw 200 cards?\n\n\n\n\nRecall Binomial RV with \\(n=200\\):\n\\[p_X(x) = {200 \\choose x}p^x(1-p)^{200-x}\\] \\[\\text{  for } x = 0, 1, 2, ..., 200\\]"
  },
  {
    "objectID": "slides/11_Expected_Values_of_Sums_of_rvs.html#learning-objectives",
    "href": "slides/11_Expected_Values_of_Sums_of_rvs.html#learning-objectives",
    "title": "Chapter 11: Expected Values of Sums of Discrete RVs",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nCalculate the mean (expected value) of sums of discrete random variables"
  },
  {
    "objectID": "slides/01_SLR.html#nicky-wakim-sheher",
    "href": "slides/01_SLR.html#nicky-wakim-sheher",
    "title": "Simple Linear Regression (SLR)",
    "section": "Nicky Wakim (she/her)",
    "text": "Nicky Wakim (she/her)\n\n\n\nCall me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, spikeball, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "slides/01_SLR.html#some-important-tasks",
    "href": "slides/01_SLR.html#some-important-tasks",
    "title": "Simple Linear Regression (SLR)",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nJoin the Slack page!\nStar the class website: https://nwakim.github.io/F2023_BSTA_550/\nComplete the WhenIsGood for office hours\nComplete Homework 0 by this Thursday at 11pm!\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!"
  },
  {
    "objectID": "slides/01_SLR.html#lets-visit-the-website",
    "href": "slides/01_SLR.html#lets-visit-the-website",
    "title": "Simple Linear Regression (SLR)",
    "section": "Let’s visit the website",
    "text": "Let’s visit the website\n\nHomepage\n\nGitHub\n\nSyllabus\nSchedule\n\nWeeks, class info, exams, homeworks\n\nSearch\n\n\n\nImportant Note\n\n\nThis is my first time teaching the course. I will work hard to answer your questions in class, but I will often need some time outside of class to make sure I give you the best answer possible! Also, many of the examples are not my own. I will work to improve examples, but if you have feedback or suggestions, I am happy to hear them!"
  },
  {
    "objectID": "slides/01_SLR.html#lets-go-through-the-syllabus",
    "href": "slides/01_SLR.html#lets-go-through-the-syllabus",
    "title": "Simple Linear Regression (SLR)",
    "section": "Let’s go through the syllabus!",
    "text": "Let’s go through the syllabus!\nSyllabus page\n\n\nIntro"
  },
  {
    "objectID": "slides/01_SLR.html#simple-linear-regression-model",
    "href": "slides/01_SLR.html#simple-linear-regression-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nThe (population) regression line is denoted by:\n\n\\[\\begin{aligned}\nY & =  \\beta_0 + \\beta_1X + \\epsilon \\nonumber\n\\end{aligned}\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\n\nOur goal is to estimate \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)\n\nThe point estimates based on a sample are denoted by \\(\\hat{\\beta_0}\\), \\(\\hat{\\beta_1}\\), and \\(\\hat{\\sigma^2}\\)"
  },
  {
    "objectID": "slides/01_SLR.html#linear-models",
    "href": "slides/01_SLR.html#linear-models",
    "title": "Simple Linear Regression (SLR)",
    "section": "“Linear” Models",
    "text": "“Linear” Models"
  },
  {
    "objectID": "slides/01_SLR.html#model-components",
    "href": "slides/01_SLR.html#model-components",
    "title": "Simple Linear Regression (SLR)",
    "section": "Model Components",
    "text": "Model Components"
  },
  {
    "objectID": "slides/01_SLR.html#interpretations",
    "href": "slides/01_SLR.html#interpretations",
    "title": "Simple Linear Regression (SLR)",
    "section": "Interpretations",
    "text": "Interpretations"
  },
  {
    "objectID": "slides/01_SLR.html#parameter-estimation-best-fit-line",
    "href": "slides/01_SLR.html#parameter-estimation-best-fit-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "Parameter estimation: best fit line",
    "text": "Parameter estimation: best fit line"
  },
  {
    "objectID": "slides/01_SLR.html#least-squares-model-assumptions",
    "href": "slides/01_SLR.html#least-squares-model-assumptions",
    "title": "Simple Linear Regression (SLR)",
    "section": "Least squares model assumptions",
    "text": "Least squares model assumptions"
  },
  {
    "objectID": "slides/01_SLR.html#estimate-of-variance",
    "href": "slides/01_SLR.html#estimate-of-variance",
    "title": "Simple Linear Regression (SLR)",
    "section": "Estimate of variance??",
    "text": "Estimate of variance??\n\n\nIntro"
  },
  {
    "objectID": "slides/00_Intro.html",
    "href": "slides/00_Intro.html",
    "title": "Welcome to BSTA 512/612!",
    "section": "",
    "text": "Call me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "slides/00_Intro.html#nicky-wakim-sheher",
    "href": "slides/00_Intro.html#nicky-wakim-sheher",
    "title": "Welcome to BSTA 512/612!",
    "section": "Nicky Wakim (she/her)",
    "text": "Nicky Wakim (she/her)\n\n\n\nCall me “Nicky,” “Dr. W,” “Professor Wakim,” or any combo!\nAssistant Professor of Biostatistics\n \nOriginally from DC area (Virginia side!)\nTwo kitties\nVolleyball, biking, pickleball\nBut also sleeping, TV, and reading\nJust started taking a couple classes at PCC (French, ceramics, yoga)\nSlowly regrowing my plant collection after moving from Michigan\n\n\n\n\n Video"
  },
  {
    "objectID": "slides/00_Intro.html#some-important-tasks",
    "href": "slides/00_Intro.html#some-important-tasks",
    "title": "Welcome to BSTA 512/612!",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nJoin the Slack page!\nStar the class website: https://nwakim.github.io/W2024_BSTA_512/\nComplete the WhenIsGood for office hours\nComplete Homework 0 by this Thursday at 11pm!\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!"
  },
  {
    "objectID": "slides/00_Intro.html#lets-visit-the-website",
    "href": "slides/00_Intro.html#lets-visit-the-website",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s visit the website",
    "text": "Let’s visit the website\n\nHomepage\n\nGitHub\n\nSyllabus\nSchedule\n\nWeeks, class info, quizzes, homeworks, projects\n\nSearch\n\n\n\nImportant Note\n\n\nThis is my first time teaching the course. I will work hard to answer your questions in class, but I will often need some time outside of class to make sure I give you the best answer possible! Also, many of the examples are not my own. I will work to improve examples, but if you have feedback or suggestions, I am happy to hear them!"
  },
  {
    "objectID": "slides/00_Intro.html#lets-go-through-the-syllabus",
    "href": "slides/00_Intro.html#lets-go-through-the-syllabus",
    "title": "Welcome to BSTA 512/612!",
    "section": "Let’s go through the syllabus!",
    "text": "Let’s go through the syllabus!\nSyllabus page\n\n\nIntro"
  },
  {
    "objectID": "weeks/week_02_sched.html",
    "href": "weeks/week_02_sched.html",
    "title": "Week 2",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #006a4e; background-color: #fff; padding: 10px; }\n.description{ font-size: 20px; color: #fff; background-color: #006a4e; padding: 10px; } ```"
  },
  {
    "objectID": "weeks/week_02_sched.html#statistician-of-the-week",
    "href": "weeks/week_02_sched.html#statistician-of-the-week",
    "title": "Week 2",
    "section": "Statistician of the Week",
    "text": "Statistician of the Week"
  },
  {
    "objectID": "slides/Module_A.html",
    "href": "slides/Module_A.html",
    "title": "Linear Regression",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\n\n\n\nModule A\n\n\n\n\n\n\n\n\nBIOSTAT 650 Theory and Application of Linear Regression Module A: Introduction\n\n\n\n\nOutline Syllabus Module A Topics: Introduction to linear regression Overview of specific topics in BIOSTAT 650 Review of basic material\n\n\n\n\nCourse Description:  This is the first course in applied statistics for both incoming MS, MPH and PhD students. It is taught at the MS level.\n\n\nBoth theoretical and applied aspects of linear regression modeling will be covered in this course, including model building, model refinement, model diagnostics, hypothesis testing, parameter interpretation and scientific interpretation of results.\n\n\nStudents are expected to use R or SAS, when necessary, for homework assignments.\n\n\nTopics to be covered include simple linear regression, multiple regression, analysis of variance, residual and influence diagnostics, variable transformations, multicollinearity, model selection and validation.\n\n\n\n\nCourse Objectives:  The overall objective of this course is to help the student integrate and apply linear regression methods to scientific studies.\n\n\nThe student will learn to identify the scientific goals of a study\n\n\ndevelop a statistical strategy appropriate for those goals\n\n\nplan strategies for linear regression analysis and to implement these strategies\n\n\nbe aware of problems that arise in study design, power and data collection\n\n\ninterpret the results of linear regression analysis and convert them into a language understandable to the broad scientific community\n\n\n\n\nSchedule\n\n\nOnline class: M/W 1:00pm - 3:00pm Online lectures will be recorded No class on Sep 6 (Labor Day), Oct 18 (Fall Study Break)\n\n\nOffice hours (OH): Office hours will NOT be recorded Monday TBD (GSI) Wednesday 3:00pm - 5:00pm (instructor) Mousumi: please change to yours Note: if you are in a very different time zone, please email the instructor to schdule a separate OH when needed Exams and project Midterm I    (20%): Oct 6 (in class) Midterm II   (20%): Nov 22 (in class) Final project (30%): Group presentation on Dec 6 and 8 Final project (30%): Final report due on Dec 10\n\n\n\n\n Homework 30% Approximately weekly homework due on Mondays Suggestions: Begin working on homework questions shortly after they are assigned Attempt all homework questions alone (thoughtfully), before consulting others\n\n\nEthics: Collaborating on homework is OK, but homework turned in should reflect each student’s understanding Regrade policy: Please carefully review the homework/exam solution. To request a revised grade, please see the instructor or GSI during office hours. If no misgrading is found after examination of your case, your grade will be further lowered by the same amount (X points). A grade is considered “final” two weeks after it has been posted\n\n\n\n\nEmailing\n\n\nWe strongly encourage you to come to the office hour if you have technical questions such as how to approach a particular homework problem. Discussion on technical issues over email is inefficient.\n\n\nWhen emailing about this course, please put “BIOSTAT650” (no space) into the subject line\n\n\n\n\nOther suggestions Always try to think about “Why is this material important?” (if unclear, then ask) explain concepts/methods/results in layman language and/or in a graph\n\n\nHelp me, your classmates, and yourself learn better by Asking questions in class (raise your hand or type in the chat box) Turning your video on (not required but recommended)\n\n\nSpend time: 4 credit hour class means In class: 4 hours per week Outside of class: 8 to 12 hours per week (=2 to 3 hours for each credit)\n\n\n\n\nIntroduction to Linear Regression\n\n\n\n\nWhat is Linear Regression? Regression: a technique to study the association between two variables Response variable (outcome, dependent variable) Blood pressure Grouping variable (predictor, independent variable, explanatory variable) Male vs female – within each group, the value of the grouping variable is constant Drug dosage – continuous predictor of interest, infinitely many groups Adjustment for other variables\n\n\nLinear model: for our purposes, refers to linearity w.r.t. the parameters\n\n\nResponse variable is a linear function of parameters\n\n\nRegression models describe association, not causality\n\n\n\n\nWhy should I care? Most widely used and most developed method in statistics Appropriate in many practical settings Important to understand limitations Easy to interpret\n\n\nFun fact: one of the most frequently asked data scientist interview question What are the assumptions underlying linear regression?\n\n\n\n\nWhy should I care?\n\n\nMost importantly, it serves as a building block Essential concepts and ideas extend well to other regression methods and other areas e.g. you will see the following generic equation frequently: \\(\\sum_{i=1}^nX_i\\trans(Y_i-\\mu_i)=0\\) where \\(\\mu_i=X_i\\trans\\beta\\) in linear regression It says “the residuals are orthogonal to the covariates”\n\n\nTechniques used in linear regression apply to other regression methods e.g. estimation, hypothesis testing, model diagnosis\n\n\n\n\nLinear Regression: Objectives\n\n\nObjectives of any data analysis can generally be categorized as either inference or prediction\n\n\nrare that only one objective of interest in practice, both are usually of various degrees of importance\n\n\nInference:\n\n\nEstimation Hypothesis testing\n\n\n\n\nInference: Estimation \n\n\nIncludes both point and interval estimation\n\n\nSign of regression coefficient may be of interest; or, both sign and magnitude e.g., estimate mean change in serum cholesterol per unit increase in BMI model parameters must have clear interpretation (limits complexity of model)\n\n\n\n\nInference: Hypothesis Testing\n\n\nWe observe the data in the study sample; what can we infer about the underlying population of interest?\n\n\nHypothesis testing reduces results of study down to a sequence of yes/no answers\n\n\nModel parameters must be interpretable for inference to be meaningful\n\n\ne.g., on average, does serum cholesterol change as BMI (body mass index) increases? what about each of the following: age, gender, race, SBP (systolic blood pressure)?\n\n\n\n\nPrediction\n\n\nUsing regression model to predict response for yet unobserved subjects Accuracy and precision of predictions take precedence over interpretability of regression parameters\n\n\ne.g., develop a linear regression model to predict serum cholesterol given a set of patient characteristics (age, gender, race, BMI, SBP, etc)\n\n\n\n\n Data Analysis Process: Overview\n\n\nDescriptive analysis often skipped or done carelessly\n\n\nVERY important first step\n\n\nPropose model often done in close consultation with investigators\n\n\nEstimate model parameters\n\n\nAssess underlying assumptions of model → return to (1)?\n\n\nHypothesis testing and/or prediction\n\n\nInterpretation, conclusions\n\n\n\n\nWhat will be covered in BIOSTAT 650?\n\n\n\n\nSimple Linear Regression: one predictor SLR features one response variable and a single covariate\n\n\nModel: Yi = β0 + β1Xi + ϵi\n\n\nYi: response β0, β1: parameters Xi: covariate ϵi: error\n\n\n“Simple”: one covariate only\n\n\nRather restrictive since only one covariate is involved\n\n\noften, interest lies in several covariates if only interested in one certain covariate, may need to adjust for other covariates\n\n\n\n\nMultiple Linear Regression: multiple predictors\n\n\nMultiple Regression: q(&gt;1) covariates Model Yi = β0 + β1Xi1 + β2Xi2 + … + βqXiq + ϵi\n\n\n\n\n0.01\n\n\n\n0.75\n\n\nFor compactness, often use matrix notation Compared to simple linear regression βj’s have much different interpretation\n\n\nMuch more complicated than SLR; Much greater chance to mis-model the data set\n\n\n\n0.25 \n\n\n\n\n\n\nResidual Diagnostics  Most of the assumptions underlying linear regression model are about the error term: \\[\\begin{split}\n            \\epsilon_i &amp; \\sim  N(0,\\sigma^2)  \\\\\n            \\epsilon_i &amp; \\ind \\epsilon_j,\\; \\forall i, \\;j,\\; i\\neq\nj\n        \\end{split}\\] Failure of any of these assumptions to hold could invalidate various aspects of the analysis\n\n\nResiduals = estimated errors: ϵ̂i\n\n\nResidual diagnostics aims to check validity of error assumptions after model fitting\n\n\nresults of residual diagnostics could imply that modifications to model are required\n\n\n\n\nTransformations of the outcome Consider the following SLR model: Yi = β0 + β1Xi + ϵi It is possible that, instead, some function of Yi, say h(Yi), should be modeled: h(Yi) = β0 + β1Xi + ϵi Possible choices for h(⋅): $$\n\\[\\begin{aligned}\n            h(Y_i) &amp; = &amp; \\log(Y_i) \\nonumber \\\\\n            h(Y_i) &amp; = &amp; \\log_{10}(Y_i) \\nonumber \\\\\n            h(Y_i) &amp; = &amp; \\sqrt{Y_i} \\nonumber \\\\\n            h(Y_i) &amp; = &amp; 1/Y_i \\nonumber\n            \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt; Transformations of the covariate(s)&lt;/span&gt; Consider the\nfollowing SLR model: &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n        Y_i &amp; = &amp; \\beta_0 + \\beta_1X_{i} +  \\beta_2 X^2_{i} +\n\\epsilon_i \\nonumber\n        \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;This is still a linear model! “Linear\" refers to being linear in the\ncoefficients Transformation of the covariate can include &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n            &amp;   &amp;  \\log(X_i) \\nonumber \\\\\n            &amp;   &amp;X^2_{i}, \\cdots, X^q_{i}   \\nonumber \\\\\n            &amp;   &amp;\\sqrt{X_i} \\nonumber \\\\\n            &amp;   &amp; \\mbox{splines:} ~~~ (X_i-a)_+\\nonumber\n            \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Transformations&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Need to be very careful when using transformations&lt;/p&gt;\n&lt;p&gt;interpretation of parameters changes completely transforming the\noutcome alters assumptions regarding error structure&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Outliers/Influence Diagnostics&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Consider the fitted model: &lt;span\nclass=\"math display\"&gt;&lt;em&gt;Ŷ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt; + &lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;What impact does &lt;span\nclass=\"math inline\"&gt;(&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;,&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;\nhave on &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Ŷ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;? How much\ndoes the &lt;span class=\"math inline\"&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt;’th subject\ninfluence &lt;span class=\"math inline\"&gt;&lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;0&lt;/sub&gt;&lt;/span&gt;? By\nwhat amount would &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt; change if the &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt;’th subject were deleted? If the\n&lt;span class=\"math inline\"&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt;’th observation heavily\ninfluences &lt;span class=\"math inline\"&gt;&lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;/span&gt;,\nshould it be removed?&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Multicollinearity&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Consider: linear regression model with covariates &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, …, &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;q&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nCovariates are not assumed to be independent (e.g., age, height, weight,\netc) However, extreme correlations among the covariates interferes with\nthe model fitting:&lt;/p&gt;\n&lt;p&gt;may be impossible to compute &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;’s computed\n&lt;span class=\"math inline\"&gt;&lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;’s may\nhave extremely large standard errors fitted model may be quite\nunreliable&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Model Selection&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Consider: state-wide survey, wherein information on 30 covariates\ncollected&lt;/p&gt;\n&lt;p&gt;When objective is to find the ‘best’ model, several algorithms may\nsimplify the process&lt;/p&gt;\n&lt;p&gt;Forward Selection: start with 0 covariates add (most significant)\ncovariates one at a time until further addition does not improve the\nmodel’s fit&lt;/p&gt;\n&lt;p&gt;Backward Elimination: start with full model successively delete\n(least significant) covariates, until such deletions results in marked\ndecrease in model’s adequacy&lt;/p&gt;\n&lt;p&gt;Stepwise Selection: combination of forward selection and backward\nelimination&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Model Validation &lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Concept: how accurate is the fitted model on new data (not used to\ncompute the parameter estimates)&lt;/p&gt;\n&lt;p&gt;e.g., data splitting:&lt;/p&gt;\n&lt;p&gt;split data set in two fit model to first half of data set, &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt;, …, &lt;em&gt;β̂&lt;/em&gt;&lt;sub&gt;&lt;em&gt;p&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;\ncompare &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nand &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Ŷ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;,\nin second half of data set&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Weighted Regression&lt;/span&gt; Units &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt; are not always sampled completely\nat random Surveys may oversample certain subpopulations&lt;/p&gt;\n&lt;p&gt;Some units are given more weight during the estimation process&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Linear vs. &lt;span&gt;&lt;em&gt;Generalized&lt;/em&gt;&lt;/span&gt; Linear\nModels&lt;/span&gt; Linear regression: &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; assumed to be\ncontinuous&lt;br /&gt;\n(and &lt;span class=\"math inline\"&gt;&lt;em&gt;ϵ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; is\nnormally distributed)&lt;/p&gt;\n&lt;p&gt;Suppose &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; is an\nindicator variable (e.g., 1=cancer; 0=cancer-free)&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nfollows a Bernoulli distribution assumptions of linear regression\nblatantly violated&lt;/p&gt;\n&lt;p&gt;Generalized Linear Model (GLM): used to model non-Normal responses;\ne.g.,&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;∼&lt;/span&gt;\nBernoulli/Binomial: logistic regression &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; is a count:\nPoisson regression&lt;/p&gt;\n&lt;p&gt;Will study GLM extensively in BIOSTAT 651&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Multiple vs.\n&lt;span&gt;&lt;em&gt;Multivari&lt;span&gt;&lt;u&gt;ate&lt;/u&gt;&lt;/span&gt;&lt;/em&gt;&lt;/span&gt;\nregression&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Simple linear regression (this course): one outcome &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;, one predictor variable &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Multiple linear regression (this course): one outcome &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;, multiple predictor variables\n&lt;span class=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;&lt;em&gt;Multivari&lt;span&gt;&lt;u&gt;ate&lt;/u&gt;&lt;/span&gt;&lt;/em&gt; regression: multiple\noutcome variables &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;, e.g. same\noutcome measured repeatedly over time multiple related outcomes (e.g.,\nheight and weight) considered simultaneously as outcome can be of any\ndistribution, i.e., multivariate GLM&lt;/p&gt;\n&lt;p&gt;Will study multivariate regression extensively in BIOSTAT 653&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;&lt;span style=\"color: royalblue\"&gt;&lt;strong&gt;In class\nexercise&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;\n&lt;span&gt;&lt;span style=\"color: royalblue\"&gt;&lt;strong&gt;Maternal Bone Lead as an\nIndependent Risk Factor for Fetal Neurotoxicity: A Prospective\nStudy&lt;/strong&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Background&lt;/span&gt; Lead (Pb) is a neurotoxicant It is associated\nwith decreased mental development (in children), and accelerated mental\ndecline in the elderly.&lt;/p&gt;\n&lt;p&gt;Studies typically measure lead concentrations in blood, and correlate\nthem (e.g., linear regression) with measurements of intelligence (e.g.,\nBAYLEY mental development index, IQ tests)&lt;/p&gt;\n&lt;p&gt;Prenatal exposure measurements typically assessed by amount of lead\nin umbilical cord&lt;/p&gt;\n&lt;p&gt;Novel biomarker is bone lead concentration—premise is that lead\nleaches out of the mother’s bones throughout pregnancy&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Gomaa et al. Study Objective and Design&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Prospective Study: Children-mothers recruited at birth, followed\nuntil children are 24 months old&lt;/p&gt;\n&lt;p&gt;Research question: is lead exposure associated with mental\ndevelopment?&lt;/p&gt;\n&lt;p&gt;Lead exposure is measured by: Lead concentration in umbilical blood\nLead concentration in maternal bones&lt;/p&gt;\n&lt;p&gt;Mental development Is measured by: BAYLEY’s index of mental\ndevelopment (MDI)&lt;/p&gt;\n&lt;p&gt;Statistical question: Test if umbilical cord blood lead and bone lead\nconcentration are predictors of BAYLEY’s MDI&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Confounding Variables &lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;What are confounders? A confounder can make the observed association\nappear stronger than the true association, weaker than the true\nassociation, or even the reverse of the true association&lt;/p&gt;\n&lt;p&gt;Confounders in this case:&lt;/p&gt;\n&lt;p&gt;demographics (age, gender, education, marital status) Breastfeeding\nduration Maternal IQ&lt;/p&gt;\n&lt;p&gt;Sometimes confounders and predictor variables are called\n“covariates”&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Acknowledgement&lt;/span&gt;&lt;/p&gt;\n&lt;table&gt;\n&lt;tbody&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;img src=\"pic/mousumibanerjee\"\nstyle=\"height:30mm\" alt=\"image\" /&gt;&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;&lt;img src=\"pic/brisa\" style=\"height:30mm\"\nalt=\"image\" /&gt;&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"even\"&gt;\n&lt;td style=\"text-align: center;\"&gt;Mousumi Banerjee&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;Brisa N. Sánchez&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr class=\"odd\"&gt;\n&lt;td style=\"text-align: center;\"&gt;University of Michigan&lt;/td&gt;\n&lt;td style=\"text-align: center;\"&gt;Drexel University&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;p&gt;Thank you for your slides!&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span style=\"color: royalblue\"&gt;&lt;strong&gt;Brief Review of Basic\nStatistics&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Basic Statistics&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Random variable &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt; Sample\n&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;i&lt;/em&gt; = 1, …, &lt;em&gt;n&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Summation: &lt;span class=\"math inline\"&gt;$\\sum_{i=1}^n Y_i =Y_1 + Y_2 +\n\\ldots + Y_n$&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Product: &lt;span class=\"math inline\"&gt;$\\prod_{i=1}^n Y_i = Y_1 \\times\nY_2 \\times \\ldots \\times Y_n$&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Expected Value (or mean): &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;E&lt;/em&gt;[&lt;em&gt;Y&lt;/em&gt;] = ∫&lt;sub&gt;−∞&lt;/sub&gt;&lt;sup&gt;∞&lt;/sup&gt;&lt;em&gt;y&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;y&lt;/em&gt;)&lt;em&gt;d&lt;/em&gt;&lt;em&gt;y&lt;/em&gt;&lt;/span&gt;\nreflects ‘center’ of &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;’s\ndistribution&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Rules of Expected Values &lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt; Expectation of sum: &lt;span\nclass=\"math inline\"&gt;$E\\left[\\sum_{i=1}^n Y_i \\right] = \\sum_{i=1}^n\nE[Y_i]\n        \\nonumber$&lt;/span&gt; No assumption of independence required&lt;/p&gt;\n&lt;p&gt;Let &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, …, &lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nbe constants &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;] = &lt;em&gt;a&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;]&lt;/span&gt;&lt;br /&gt;\n&lt;span class=\"math inline\"&gt;$E\\left[\\sum_{i=1}^n a_iY_i\n        \\right] = \\sum_{i=1}^n a_iE[Y_i]$&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Expectation of product: &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;] = &lt;em&gt;E&lt;/em&gt;[&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;]&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;]&lt;/span&gt;&lt;br /&gt;\nif &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; and\n&lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; are\nindependent&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Variance and standard deviation&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Variance: &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{split}\n        &amp;Var(Y) =  E[(Y-\\mu_Y)^2]\\\\\n        = &amp;\\int_{-\\infty}^\\infty (y-\\mu_Y)^2 f(y) dy\n        %=  E[Y^2] - \\mu_Y^2\n        \\end{split}\\]\n\\[&lt;/span&gt; reflects spread of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;’s distribution units: (units of\n&lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;)&lt;span\nclass=\"math inline\"&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Standard deviation: &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{split}\n        SD(Y)  = &amp;\\; \\sqrt{Var(Y)}  \\\\\n        SD(aY)  = &amp;\\; aSD(Y)\n        \\end{split}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;reflects dispersion in &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;’s\ndistribution measured in same unit as &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Rules of Variances&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt; Variance&lt;br /&gt;\n&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;V&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;r&lt;/em&gt;(&lt;em&gt;Y&lt;/em&gt;) = &lt;em&gt;E&lt;/em&gt;[(&lt;em&gt;Y&lt;/em&gt;−&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/sub&gt;)&lt;sup&gt;2&lt;/sup&gt;]&lt;/span&gt;&lt;br /&gt;\n&lt;span class=\"math inline\"&gt;${\\color{white}{Var(Y)}} =E[Y^2] -\n\\mu_Y^2$&lt;/span&gt; Variance of linear combination: &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{split}\n        &amp;~Var(aY+b)\\\\\n         = &amp;~ Var(aY) \\\\\n         = &amp;~ E[(aY-E[aY])^2]\\\\\n         = &amp;~ a^2 E[(Y-E[Y])^2]\\\\\n         = &amp;~ a^2 Var(Y)\n        \\end{split}\\]\n\\[&lt;/span&gt; &lt;img src=\"pic/shift\" style=\"height:0.7in\"\nalt=\"image\" /&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Covariance&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.6&lt;/span&gt; &lt;span class=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt;, &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;: random variables Covariance:\n&lt;span\nclass=\"math inline\"&gt;cov(&lt;em&gt;Y&lt;/em&gt;,&lt;em&gt;X&lt;/em&gt;) = &lt;em&gt;E&lt;/em&gt;[(&lt;em&gt;Y&lt;/em&gt;−&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/sub&gt;)(&lt;em&gt;X&lt;/em&gt;−&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;X&lt;/em&gt;&lt;/sub&gt;)]&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Measures (linear) association between &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt;, &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;&lt;br /&gt;\n&lt;span&gt;&lt;span class=\"math inline\"&gt; &gt; 0&lt;/span&gt;, large values of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt; tend to occur with large values of\n&lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;&lt;br /&gt;\n&lt;span class=\"math inline\"&gt; &lt; 0&lt;/span&gt;, large values of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt; tend to coincide with small values\nof &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;&lt;br /&gt;\n&lt;span class=\"math inline\"&gt; = 0&lt;/span&gt;, size of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt; provides no information on size of\n&lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;&lt;/span&gt; When the covariance\nis calculated, the data are not standardized Not scale-invariant: can\ninterpret direction but not magnitude&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.4&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Correlation&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt; &lt;span class=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt;, &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;: random variables Correlation:\n&lt;span class=\"math inline\"&gt;$\\mbox{corr}(X,Y) =\n\\frac{\\mbox{cov}(X,Y)}{SD(X)SD(Y)}$&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Scaled measure of linear association,&lt;/p&gt;\n&lt;p&gt;&lt;span\nclass=\"math inline\"&gt; − 1 ≤ corr(&lt;em&gt;X&lt;/em&gt;,&lt;em&gt;Y&lt;/em&gt;) ≤ 1&lt;/span&gt; easier\nto interpret than covariance&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.5&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Rules of covariance &lt;/span&gt; &lt;span\nclass=\"math inline\"&gt;cov(&lt;em&gt;Y&lt;/em&gt;,&lt;em&gt;X&lt;/em&gt;) = &lt;em&gt;E&lt;/em&gt;[(&lt;em&gt;Y&lt;/em&gt;−&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/sub&gt;)(&lt;em&gt;X&lt;/em&gt;−&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;X&lt;/em&gt;&lt;/sub&gt;)] = &lt;em&gt;E&lt;/em&gt;[&lt;em&gt;X&lt;/em&gt;&lt;em&gt;Y&lt;/em&gt;] − &lt;em&gt;E&lt;/em&gt;[&lt;em&gt;X&lt;/em&gt;]&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;Y&lt;/em&gt;]&lt;/span&gt;\n&lt;span\nclass=\"math inline\"&gt;cov(&lt;em&gt;Y&lt;/em&gt;,&lt;em&gt;Y&lt;/em&gt;) = var(&lt;em&gt;Y&lt;/em&gt;)&lt;/span&gt;\nIndependent &lt;span\nclass=\"math inline\"&gt;$\\stackrel{\\Rightarrow}{\\not\\Leftarrow}$&lt;/span&gt;\nuncorrelated If &lt;span class=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt; are independent, &lt;span\nclass=\"math inline\"&gt;cov(&lt;em&gt;X&lt;/em&gt;,&lt;em&gt;Y&lt;/em&gt;) = 0&lt;/span&gt; If &lt;span\nclass=\"math inline\"&gt;cov(&lt;em&gt;X&lt;/em&gt;,&lt;em&gt;Y&lt;/em&gt;) = 0&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;(&lt;em&gt;X&lt;/em&gt;,&lt;em&gt;Y&lt;/em&gt;) ∼ Bivariate Normal&lt;/span&gt;,\nthen &lt;span class=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt; are independent Covariance is\nsymmetric, additive, and scale preserving &lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n\\mbox{cov} (X,Y) &amp; = &amp; \\mbox{cov} (Y,X) \\nonumber \\\\\n\\mbox{cov} (X,Y_1+Y_2) &amp; = &amp; \\mbox{cov} (X,Y_1)+\\mbox{cov}\n(X,Y_2) \\nonumber \\\\\n\\mbox{cov} (X,aY) &amp; = &amp; a\\;\\mbox{cov} (X,Y) \\nonumber\n%\\mbox{cov} (Y,Y) &amp; = &amp; \\mbox{var} (Y) \\nonumber\n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Rules of variance&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.6&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Variance of sum: &lt;span&gt;&lt;span\nclass=\"math display\"&gt;\\]\n\\[\\begin{split}\n        &amp;Var\\left(\\sum_{i=1}^n Y_i\\right)\n        =  \\sum_{i=1}^n\\sum_{j=1}^n\n        \\mbox{cov}(Y_i,Y_j )  \\\\\n        = &amp; \\sum_{i=1}^n Var(Y_i)   + \\sum_{i=1}^n\\sum_{j=1}^n\nI(j\\neq i) \\mbox{cov}(Y_i, Y_j )  \\\\\n        = &amp; \\sum_{i=1}^n Var(Y_i) + 2\\sum_{i=1}^n \\sum_{j=i+1}^n\n        \\mbox{cov}(Y_i, Y_j )\n        \\end{split}\\]\n\\[&lt;/span&gt;&lt;/span&gt; if &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, …, &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nare mutually independent, then &lt;span\nclass=\"math inline\"&gt;$Var\\left(\\sum_{i=1}^n Y_i\\right) = \\sum_{i=1}^n\nVar(Y_i)$&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.4&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Estimator of Mean&lt;/span&gt; Suppose we obtained a simple random\nsample from some underlying population, then we can derive sample\nestimates of each of the population quantities defined previously\nSuppose &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, …, &lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nare &lt;span&gt;&lt;em&gt;iid&lt;/em&gt;&lt;/span&gt; with mean &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;μ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; and variance\n&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;σ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.6&lt;/span&gt; Estimator of mean: &lt;span\nclass=\"math display\"&gt;\\]Y = {i=1}^n Y_i = \\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math inline\"&gt;$E[\\overline{Y}]= \\frac{1}{n} \\sum_{i=1}^n\nE[Y_i] = \\mu_Y$&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math inline\"&gt;$Var(\\overline{Y})= n^{-2} \\sum_{i=1}^n\nVar(Y_i) = \\sigma_Y^2/n$&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.37&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Variance and Covariance Estimator&lt;/span&gt;&lt;/p&gt;\n&lt;div class=\"columns\"&gt;\n&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.6&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Estimators of variance:&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math inline\"&gt;$\\widehat{\\sigma}^2_Y =  \\frac{1}{n}\n\\sum_{i=1}^n (Y_i-E[Y_i])^2$&lt;/span&gt;&lt;br /&gt;\nif population mean is known &lt;span\nclass=\"math inline\"&gt;$\\widehat{\\sigma}^2_Y = \\frac{1}{n-1} \\sum_{i=1}^n\n                (Y_i-\\overline{Y})^2$&lt;/span&gt;&lt;br /&gt;\nif population mean is unknown&lt;/p&gt;\n&lt;p&gt;Estimator of covariance:&lt;br /&gt;\nSuppose pairs &lt;span\nclass=\"math inline\"&gt;(&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;,&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;), …, (&lt;em&gt;Y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;,&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;\nare &lt;span&gt;&lt;em&gt;iid&lt;/em&gt;&lt;/span&gt;. &lt;span\nclass=\"math inline\"&gt;$\\widehat{\\mbox{cov}} (X,Y) = \\frac{1}{n-1}\n\\sum_{i=1}^n\n                (Y_i-\\overline{Y})(X_i-\\overline{X})$&lt;/span&gt; &lt;span\nclass=\"math inline\"&gt;$\\widehat{\\mbox{corr}} (X,Y) =$&lt;/span&gt; ?&lt;/p&gt;\n&lt;/div&gt;&lt;div class=\"column\"&gt;\n&lt;p&gt;&lt;span&gt;0.4&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Distributions that will be used in this class&lt;/span&gt; Normal\ndistribution Chi-square distribution t distribution F distribution&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Normal Distribution&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Density: &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt; ∼ &lt;em&gt;N&lt;/em&gt;(&lt;em&gt;μ&lt;/em&gt;,&lt;em&gt;σ&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;)&lt;/span&gt;,\n&lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n        f_Y(y) &amp; = &amp;\n        \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{\\frac{-1}{2}\\left(\\frac{y-\\mu}{\\sigma}\n        \\right)^2\\right\\} \\nonumber\n        \n\\end{aligned}\\]\n\\[&lt;/span&gt; If we know &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;E&lt;/em&gt;(&lt;em&gt;Y&lt;/em&gt;) = &lt;em&gt;μ&lt;/em&gt;&lt;/span&gt;, &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;V&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;r&lt;/em&gt;(&lt;em&gt;Y&lt;/em&gt;) = &lt;em&gt;σ&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;\nthen&lt;/p&gt;\n&lt;p&gt;/3 of &lt;span class=\"math inline\"&gt;&lt;em&gt;Y&lt;/em&gt;&lt;/span&gt;’s distribution lies\nwithin 1 &lt;span class=\"math inline\"&gt;&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; of &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;μ&lt;/em&gt;&lt;/span&gt; % &lt;span\nclass=\"math inline\"&gt;…&lt;/span&gt; &lt;span class=\"math inline\"&gt;…&lt;/span&gt; is\nwithin &lt;span class=\"math inline\"&gt;&lt;em&gt;μ&lt;/em&gt; ± 2&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; &lt;span\nclass=\"math inline\"&gt; &gt; 99&lt;/span&gt;% &lt;span class=\"math inline\"&gt;…&lt;/span&gt;\n&lt;span class=\"math inline\"&gt;…&lt;/span&gt; lies within &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;μ&lt;/em&gt; ± 3&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Arguably, the most important distribution in statistics&lt;/p&gt;\n&lt;p&gt;Linear combinations of Normals are Normal&lt;br /&gt;\ne.g., &lt;span\nclass=\"math inline\"&gt;(&lt;em&gt;a&lt;/em&gt;&lt;em&gt;Y&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;) ∼ N(&lt;em&gt;a&lt;/em&gt;&lt;em&gt;μ&lt;/em&gt;+&lt;em&gt;b&lt;/em&gt;, &lt;em&gt;a&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;em&gt;σ&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;)&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Standard normal: &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n        Z=\\frac{Y-\\mu}{\\sigma} \\sim \\mbox{N}(0,1) \\nonumber\n        \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;Chi-square Distribution&lt;/span&gt; Notation: &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt; ∼ &lt;em&gt;χ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;\n&lt;span class=\"math inline\"&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;=&lt;/span&gt; degrees of\nfreedom &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;X&lt;/em&gt;] = &lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;/span&gt;\n&lt;span class=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;/span&gt; takes on only positive\nvalues If &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; ∼ N(0,1)&lt;/span&gt;,\nthen &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; ∼ &lt;em&gt;χ&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;If &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, …, &lt;em&gt;Z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nare independent, with &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; ∼ N(0,1)&lt;/span&gt;,\nthen &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n        \\sum_{i=1}^n Z_i^2 &amp; \\sim &amp; \\chi^2_n \\nonumber\n        \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Used in hypothesis testing and CI’s involving variance&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;t Distribution&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;If &lt;span class=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt; ∼ N(0,1)&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;S&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt; ∼ &lt;em&gt;χ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;\nand &lt;span class=\"math inline\"&gt;&lt;em&gt;Z&lt;/em&gt;&lt;/span&gt; and &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;S&lt;/em&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt; are independent,&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n        \\frac{Z}{S/\\sqrt{df}} &amp; \\sim &amp; t_{df} \\nonumber\n        \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;Symmetric, bell-shaped; tails heavier than Normal &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;t&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;/sub&gt;] = 0&lt;/span&gt;;\n&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;V&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;r&lt;/em&gt;(&lt;em&gt;t&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;\ngreater than 1 &lt;span\nclass=\"math inline\"&gt;lim&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt; → ∞&lt;/sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;/sub&gt; → N(0,1)&lt;/span&gt;\nfor &lt;span class=\"math inline\"&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt; &gt; 30&lt;/span&gt;, the\n&lt;span\nclass=\"math inline\"&gt;&lt;em&gt;t&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;\nclosely resembles the &lt;span class=\"math inline\"&gt;N(0,1)&lt;/span&gt;\ndistribution&lt;/p&gt;\n&lt;p&gt;In linear modeling, used for inference on individual regression\nparameters&lt;/p&gt;\n&lt;/div&gt;\n&lt;div class=\"frame\"&gt;\n&lt;p&gt;&lt;span&gt;F Distribution&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;If &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; ∼ &lt;em&gt;χ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;\nand &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; ∼ &lt;em&gt;χ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;d&lt;/em&gt;&lt;em&gt;f&lt;/em&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;,\nwhere &lt;span\nclass=\"math inline\"&gt;&lt;em&gt;X&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; ⊥ &lt;em&gt;X&lt;/em&gt;&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/span&gt;,\nthen: &lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n        \\frac{X_1^2/df1}{X_2^2/df2} &amp; \\sim &amp; F_{df1,df2}\n\\nonumber\n        \n\\end{aligned}\\]\n\\[&lt;/span&gt;&lt;/p&gt;\n&lt;p&gt;only takes on positive values&lt;/p&gt;\n&lt;p&gt;connection to &lt;span class=\"math inline\"&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;\ndistribution:&lt;/p&gt;\n&lt;p&gt;&lt;span class=\"math display\"&gt;\\]\n\\[\\begin{aligned}\n            \\{ t_{df} \\}^2 &amp; \\stackrel{{\\cal D}}{=} &amp; F_{1,df}\n\\nonumber\n            \n\\end{aligned}\\]\n$$\n\n\nUsed extensively in linear regression (hypothesis testing)\n\n\n\n\nQuestions?"
  },
  {
    "objectID": "slides/Module_A.html#outline",
    "href": "slides/Module_A.html#outline",
    "title": "Module A",
    "section": "Outline",
    "text": "Outline\nSyllabus Module A Topics: Introduction to linear regression Overview of specific topics in BIOSTAT 650 Review of basic material\n\nCourse Description: This is the first course in applied statistics for both incoming MS, MPH and PhD students. It is taught at the MS level.\nBoth theoretical and applied aspects of linear regression modeling will be covered in this course, including model building, model refinement, model diagnostics, hypothesis testing, parameter interpretation and scientific interpretation of results.\nStudents are expected to use R or SAS, when necessary, for homework assignments.\nTopics to be covered include simple linear regression, multiple regression, analysis of variance, residual and influence diagnostics, variable transformations, multicollinearity, model selection and validation.\n\n\nCourse Objectives: The overall objective of this course is to help the student integrate and apply linear regression methods to scientific studies.\nThe student will learn to identify the scientific goals of a study\ndevelop a statistical strategy appropriate for those goals\nplan strategies for linear regression analysis and to implement these strategies\nbe aware of problems that arise in study design, power and data collection\ninterpret the results of linear regression analysis and convert them into a language understandable to the broad scientific community"
  },
  {
    "objectID": "slides/Module_A.html#schedule",
    "href": "slides/Module_A.html#schedule",
    "title": "Module A",
    "section": "Schedule",
    "text": "Schedule\nOnline class: M/W 1:00pm - 3:00pm Online lectures will be recorded No class on Sep 6 (Labor Day), Oct 18 (Fall Study Break)\nOffice hours (OH): Office hours will NOT be recorded Monday TBD (GSI) Wednesday 3:00pm - 5:00pm (instructor) Mousumi: please change to yours Note: if you are in a very different time zone, please email the instructor to schdule a separate OH when needed Exams and project Midterm I    (20%): Oct 6 (in class) Midterm II   (20%): Nov 22 (in class) Final project (30%): Group presentation on Dec 6 and 8\nFinal project (30%):Final report due on Dec 10 :::\n\nHomework 30% Approximately weekly homework due on Mondays Suggestions: Begin working on homework questions shortly after they are assigned Attempt all homework questions alone (thoughtfully), before consulting others\nEthics: Collaborating on homework is OK, but homework turned in should reflect each student’s understanding Regrade policy: Please carefully review the homework/exam solution. To request a revised grade, please see the instructor or GSI during office hours. If no misgrading is found after examination of your case, your grade will be further lowered by the same amount (X points). A grade is considered “final” two weeks after it has been posted\n\n\nEmailing\nWe strongly encourage you to come to the office hour if you have technical questions such as how to approach a particular homework problem. Discussion on technical issues over email is inefficient.\nWhen emailing about this course, please put “BIOSTAT650” (no space) into the subject line\n\n\nOther suggestions Always try to think about “Why is this material important?” (if unclear, then ask) explain concepts/methods/results in layman language and/or in a graph\nHelp me, your classmates, and yourself learn better by Asking questions in class (raise your hand or type in the chat box) Turning your video on (not required but recommended)\nSpend time: 4 credit hour class means In class: 4 hours per week Outside of class: 8 to 12 hours per week (=2 to 3 hours for each credit)\n\n\nIntroduction to Linear Regression\n\n\nWhat is Linear Regression? Regression: a technique to study the association between two variables Response variable (outcome, dependent variable) Blood pressure Grouping variable (predictor, independent variable, explanatory variable) Male vs female – within each group, the value of the grouping variable is constant Drug dosage – continuous predictor of interest, infinitely many groups Adjustment for other variables\nLinear model: for our purposes, refers to linearity w.r.t. the parameters\nResponse variable is a linear function of parameters\nRegression models describe association, not causality\n\n\nWhy should I care? Most widely used and most developed method in statistics Appropriate in many practical settings Important to understand limitations Easy to interpret\nFun fact: one of the most frequently asked data scientist interview question What are the assumptions underlying linear regression?\n\n\nWhy should I care?\nMost importantly, it serves as a building block Essential concepts and ideas extend well to other regression methods and other areas e.g. you will see the following generic equation frequently:\n\\(\\sum_{i=1}^nX_i\\trans(Y_i-\\mu_i)=0\\) where \\(\\mu_i=X_i\\trans\\beta\\) in linear regression\nIt says “the residuals are orthogonal to the covariates\"\nTechniques used in linear regression apply to other regression methods e.g. estimation, hypothesis testing, model diagnosis\n\n\nLinear Regression: Objectives\nObjectives of any data analysis can generally be categorized as either inference or prediction\nrare that only one objective of interest in practice, both are usually of various degrees of importance\nInference:\nEstimation Hypothesis testing\n\n\nInference: Estimation\nIncludes both point and interval estimation\nSign of regression coefficient may be of interest; or, both sign and magnitude e.g., estimate mean change in serum cholesterol per unit increase in BMI model parameters must have clear interpretation (limits complexity of model)\n\n\nInference: Hypothesis Testing\nWe observe the data in the study sample; what can we infer about the underlying population of interest?\nHypothesis testing reduces results of study down to a sequence of yes/no answers\nModel parameters must be interpretable for inference to be meaningful\ne.g., on average, does serum cholesterol change as BMI (body mass index) increases? what about each of the following: age, gender, race, SBP (systolic blood pressure)?\n\n\nPrediction\nUsing regression model to predict response for yet unobserved subjects Accuracy and precision of predictions take precedence over interpretability of regression parameters\ne.g., develop a linear regression model to predict serum cholesterol given a set of patient characteristics (age, gender, race, BMI, SBP, etc)\n\n\nData Analysis Process: Overview\nDescriptive analysis often skipped or done carelessly\nVERY important first step\nPropose model often done in close consultation with investigators\nEstimate model parameters\nAssess underlying assumptions of model \\(\\longrightarrow\\) return to (1)?\nHypothesis testing and/or prediction\nInterpretation, conclusions\n\n\nWhat will be covered in BIOSTAT 650?\n\n\nSimple Linear Regression: one predictor SLR features one response variable and a single covariate\nModel: \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\nonumber\\]\n\\(Y_i\\): response \\(\\beta_0\\), \\(\\beta_1\\): parameters \\(X_i\\): covariate \\(\\epsilon_i\\): error\n“Simple”: one covariate only\nRather restrictive since only one covariate is involved\noften, interest lies in several covariates if only interested in one certain covariate, may need to adjust for other covariates\n\n\nMultiple Linear Regression: multiple predictors\nMultiple Regression: \\(q(&gt;1)\\) covariates Model \\[Y_i  =  \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} +\\ldots +\n        \\beta_qX_{iq} +  \\epsilon_i\\]\n\n\n0.01\n\n0.75\nFor compactness, often use matrix notation Compared to simple linear regression \\(\\beta_j\\)’s have much different interpretation\nMuch more complicated than SLR;\nMuch greater chance to mis-model the data set\n\n0.25 \n\n\n\n\nResidual Diagnostics Most of the assumptions underlying linear regression model are about the error term: \\[\\begin{split}\n            \\epsilon_i & \\sim  N(0,\\sigma^2)  \\\\\n            \\epsilon_i & \\ind \\epsilon_j,\\; \\forall i, \\;j,\\; i\\neq j\n        \\end{split}\\] Failure of any of these assumptions to hold could invalidate various aspects of the analysis\nResiduals = estimated errors: \\(\\widehat{\\epsilon}_i\\)\nResidual diagnostics aims to check validity of error assumptions after model fitting\nresults of residual diagnostics could imply that modifications to model are required\n\n\nTransformations of the outcome Consider the following SLR model: \\[Y_i = \\beta_0 + \\beta_1X_{i} +  \\epsilon_i\\] It is possible that, instead, some function of \\(Y_i\\), say \\(h(Y_i)\\), should be modeled: \\[h(Y_i) =\\beta_0 + \\beta_1X_{i} +  \\epsilon_i\\] Possible choices for \\(h(\\cdot)\\): $$\n\\[\\begin{aligned}\n            h(Y_i) & = & \\log(Y_i) \\nonumber \\\\\n            h(Y_i) & = & \\log_{10}(Y_i) \\nonumber \\\\\n            h(Y_i) & = & \\sqrt{Y_i} \\nonumber \\\\\n            h(Y_i) & = & 1/Y_i \\nonumber\n            \n\\end{aligned}\\]\n$$\n\n\nTransformations of the covariate(s) Consider the following SLR model: $$\n\\[\\begin{aligned}\n        Y_i & = & \\beta_0 + \\beta_1X_{i} +  \\beta_2 X^2_{i} + \\epsilon_i \\nonumber\n        \n\\end{aligned}\\]\n$$\nThis is still a linear model! “Linear\" refers to being linear in the coefficients Transformation of the covariate can include $$\n\\[\\begin{aligned}\n            &   &  \\log(X_i) \\nonumber \\\\\n            &   &X^2_{i}, \\cdots, X^q_{i}   \\nonumber \\\\\n            &   &\\sqrt{X_i} \\nonumber \\\\\n            &   & \\mbox{splines:} ~~~ (X_i-a)_+\\nonumber\n            \n\\end{aligned}\\]\n$$\n\n\nTransformations\nNeed to be very careful when using transformations\ninterpretation of parameters changes completely transforming the outcome alters assumptions regarding error structure\n\n\nOutliers/Influence Diagnostics\nConsider the fitted model: \\[\\widehat{Y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X_{i}\\]\nWhat impact does \\((X_i,Y_i)\\) have on \\(\\widehat{Y}_i\\)? How much does the \\(i\\)’th subject influence \\(\\widehat{\\beta}_0\\)? By what amount would \\(\\widehat{\\beta}_1\\) change if the \\(i\\)’th subject were deleted? If the \\(i\\)’th observation heavily influences \\(\\widehat{\\beta}_1\\), should it be removed?\n\n\nMulticollinearity\nConsider: linear regression model with covariates \\(X_1,\\ldots,X_q\\) Covariates are not assumed to be independent (e.g., age, height, weight, etc) However, extreme correlations among the covariates interferes with the model fitting:\nmay be impossible to compute \\(\\widehat{\\beta}_j\\)’s computed \\(\\widehat{\\beta}_j\\)’s may have extremely large standard errors fitted model may be quite unreliable\n\n\nModel Selection\nConsider: state-wide survey, wherein information on 30 covariates collected\nWhen objective is to find the ‘best’ model, several algorithms may simplify the process\nForward Selection: start with 0 covariates add (most significant) covariates one at a time until further addition does not improve the model’s fit\nBackward Elimination: start with full model successively delete (least significant) covariates, until such deletions results in marked decrease in model’s adequacy\nStepwise Selection: combination of forward selection and backward elimination\n\n\nModel Validation\nConcept: how accurate is the fitted model on new data (not used to compute the parameter estimates)\ne.g., data splitting:\nsplit data set in two fit model to first half of data set, \\(\\widehat{\\beta}_1^*,\\dots,\\widehat{\\beta}_p^*\\) compare \\(Y_i\\) and \\(\\widehat{Y}_i^*\\), in second half of data set\n\n\nWeighted Regression Units \\(i\\) are not always sampled completely at random Surveys may oversample certain subpopulations\nSome units are given more weight during the estimation process\n\n\nLinear vs. Generalized Linear Models Linear regression: \\(Y_i\\) assumed to be continuous\n(and \\(\\epsilon_i\\) is normally distributed)\nSuppose \\(Y_i\\) is an indicator variable (e.g., 1=cancer; 0=cancer-free)\n\\(Y_i\\) follows a Bernoulli distribution assumptions of linear regression blatantly violated\nGeneralized Linear Model (GLM): used to model non-Normal responses; e.g.,\n\\(Y_i\\sim\\) Bernoulli/Binomial: logistic regression \\(Y_i\\) is a count: Poisson regression\nWill study GLM extensively in BIOSTAT 651\n\n\nMultiple vs. Multivariate regression\nSimple linear regression (this course): one outcome \\(Y\\), one predictor variable \\(X\\)\nMultiple linear regression (this course): one outcome \\(Y\\), multiple predictor variables \\(X\\)\nMultivariate regression: multiple outcome variables \\(Y\\), e.g. same outcome measured repeatedly over time multiple related outcomes (e.g., height and weight) considered simultaneously as outcome can be of any distribution, i.e., multivariate GLM\nWill study multivariate regression extensively in BIOSTAT 653\n\n\nIn class exercise\nMaternal Bone Lead as an Independent Risk Factor for Fetal Neurotoxicity: A Prospective Study\n\n\nBackground Lead (Pb) is a neurotoxicant It is associated with decreased mental development (in children), and accelerated mental decline in the elderly.\nStudies typically measure lead concentrations in blood, and correlate them (e.g., linear regression) with measurements of intelligence (e.g., BAYLEY mental development index, IQ tests)\nPrenatal exposure measurements typically assessed by amount of lead in umbilical cord\nNovel biomarker is bone lead concentration—premise is that lead leaches out of the mother’s bones throughout pregnancy\n\n\nGomaa et al. Study Objective and Design\nProspective Study: Children-mothers recruited at birth, followed until children are 24 months old\nResearch question: is lead exposure associated with mental development?\nLead exposure is measured by: Lead concentration in umbilical blood Lead concentration in maternal bones\nMental development Is measured by: BAYLEY’s index of mental development (MDI)\nStatistical question: Test if umbilical cord blood lead and bone lead concentration are predictors of BAYLEY’s MDI\n\n\nConfounding Variables\nWhat are confounders? A confounder can make the observed association appear stronger than the true association, weaker than the true association, or even the reverse of the true association\nConfounders in this case:\ndemographics (age, gender, education, marital status) Breastfeeding duration Maternal IQ\nSometimes confounders and predictor variables are called “covariates”\n\n\nAcknowledgement\n\n\n\n\n\n\n\nMousumi Banerjee\nBrisa N. Sánchez\n\n\nUniversity of Michigan\nDrexel University\n\n\n\nThank you for your slides!\n\n\nBrief Review of Basic Statistics\n\n\nBasic Statistics\n\n\n0.5\nRandom variable \\(Y\\) Sample \\(Y_i, i=1,\\dots, n\\)\nSummation: \\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\nProduct: \\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)\nExpected Value (or mean): \\(\\mu_Y= E[Y] = \\int_{-\\infty}^\\infty y f(y) dy\\) reflects ‘center’ of \\(Y\\)’s distribution\n\n0.5\n\n\n\n\nRules of Expected Values\n\n\n0.5 Expectation of sum: \\(E\\left[\\sum_{i=1}^n Y_i \\right] = \\sum_{i=1}^n E[Y_i]  \\nonumber\\) No assumption of independence required\nLet \\(a_1,\\ldots,a_n\\) be constants \\(E[a_iY_i] = a_iE[Y_i]\\)\n\\(E\\left[\\sum_{i=1}^n a_iY_i  \\right] = \\sum_{i=1}^n a_iE[Y_i]\\)\nExpectation of product: \\(E[Y_iY_j] = E[Y_i]E[Y_j]\\)\nif \\(Y_i\\) and \\(Y_j\\) are independent\n\n0.5\n\n\n\n\nVariance and standard deviation\n\n\n0.5\nVariance: \\[\\begin{split}\n        &Var(Y) =  E[(Y-\\mu_Y)^2]\\\\\n        = &\\int_{-\\infty}^\\infty (y-\\mu_Y)^2 f(y) dy\n        %=  E[Y^2] - \\mu_Y^2\n        \\end{split}\\] reflects spread of \\(Y\\)’s distribution units: (units of \\(Y\\))\\(^2\\)\nStandard deviation: \\[\\begin{split}\n        SD(Y)  = &\\; \\sqrt{Var(Y)}  \\\\\n        SD(aY)  = &\\; aSD(Y)\n        \\end{split}\\]\nreflects dispersion in \\(Y\\)’s distribution measured in same unit as \\(Y\\)\n\n0.5\n\n\n\n\nRules of Variances\n\n\n0.5 Variance\n\\(Var(Y)=E[(Y-\\mu_Y)^2]\\)\n\\({\\color{white}{Var(Y)}} =E[Y^2] - \\mu_Y^2\\) Variance of linear combination: \\[\\begin{split}\n        &~Var(aY+b)\\\\\n         = &~ Var(aY) \\\\\n         = &~ E[(aY-E[aY])^2]\\\\\n         = &~ a^2 E[(Y-E[Y])^2]\\\\\n         = &~ a^2 Var(Y)\n        \\end{split}\\] \n\n0.5\n\n\n\n\nCovariance\n\n\n0.6 \\(X\\), \\(Y\\): random variables Covariance: \\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]\\)\nMeasures (linear) association between \\(X\\), \\(Y\\)\n\\(&gt;0\\), large values of \\(X\\) tend to occur with large values of \\(Y\\)\n\\(&lt;0\\), large values of \\(X\\) tend to coincide with small values of \\(Y\\)\n\\(=0\\), size of \\(X\\) provides no information on size of \\(Y\\) When the covariance is calculated, the data are not standardized Not scale-invariant: can interpret direction but not magnitude\n\n0.4\n\n\n\n\nCorrelation\n\n\n0.5 \\(X\\), \\(Y\\): random variables Correlation: \\(\\mbox{corr}(X,Y) = \\frac{\\mbox{cov}(X,Y)}{SD(X)SD(Y)}\\)\nScaled measure of linear association,\n\\(-1 \\leq \\mbox{corr}(X,Y) \\leq 1\\) easier to interpret than covariance\n\n0.5\n\n\n\n\nRules of covariance \\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]= E[XY]-E[X]E[Y]\\) \\(\\mbox{cov} (Y,Y) = \\mbox{var} (Y)\\) Independent \\(\\stackrel{\\Rightarrow}{\\not\\Leftarrow}\\) uncorrelated If \\(X\\) and \\(Y\\) are independent, \\(\\mbox{cov}(X,Y)=0\\) If \\(\\mbox{cov}(X,Y)=0\\) and \\((X,Y)\\sim \\text{Bivariate Normal}\\), then \\(X\\) and \\(Y\\) are independent Covariance is symmetric, additive, and scale preserving \\[\\begin{aligned}\n\\mbox{cov} (X,Y) & = & \\mbox{cov} (Y,X) \\nonumber \\\\\n\\mbox{cov} (X,Y_1+Y_2) & = & \\mbox{cov} (X,Y_1)+\\mbox{cov} (X,Y_2) \\nonumber \\\\\n\\mbox{cov} (X,aY) & = & a\\;\\mbox{cov} (X,Y) \\nonumber\n%\\mbox{cov} (Y,Y) & = & \\mbox{var} (Y) \\nonumber\n\\end{aligned}\\]\n\n\nRules of variance\n\n\n0.6\nVariance of sum: \\[\\setlength{\\jot}{1pt}\n        \\begin{split}\n        &Var\\left(\\sum_{i=1}^n Y_i\\right)\n        =  \\sum_{i=1}^n\\sum_{j=1}^n\n        \\mbox{cov}(Y_i,Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i)   + \\sum_{i=1}^n\\sum_{j=1}^n I(j\\neq i) \\mbox{cov}(Y_i, Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i) + 2\\sum_{i=1}^n \\sum_{j=i+1}^n\n        \\mbox{cov}(Y_i, Y_j )\n        \\end{split}\\] if \\(Y_1,\\ldots,Y_n\\) are mutually independent, then \\(Var\\left(\\sum_{i=1}^n Y_i\\right) = \\sum_{i=1}^n Var(Y_i)\\)\n\n0.4\n\n\n\n\nEstimator of Mean Suppose we obtained a simple random sample from some underlying population, then we can derive sample estimates of each of the population quantities defined previously Suppose \\(Y_1,\\ldots,Y_n\\) are iid with mean \\(\\mu_Y\\) and variance \\(\\sigma^2_Y\\)\n\n\n0.6 Estimator of mean: \\[\\widehat{\\mu}_Y = \\frac{1}{n} \\sum_{i=1}^n Y_i =\n            \\overline{Y}\\]\n\\(E[\\overline{Y}]= \\frac{1}{n} \\sum_{i=1}^n E[Y_i] = \\mu_Y\\)\n\\(Var(\\overline{Y})= n^{-2} \\sum_{i=1}^n Var(Y_i) = \\sigma_Y^2/n\\)\n\n0.37\n\n\n\n\nVariance and Covariance Estimator\n\n\n0.6\nEstimators of variance:\n\\(\\widehat{\\sigma}^2_Y = \\frac{1}{n} \\sum_{i=1}^n (Y_i-E[Y_i])^2\\)\nif population mean is known \\(\\widehat{\\sigma}^2_Y = \\frac{1}{n-1} \\sum_{i=1}^n  (Y_i-\\overline{Y})^2\\)\nif population mean is unknown\nEstimator of covariance:\nSuppose pairs \\((Y_1,X_1),\\ldots,(Y_n,X_n)\\) are iid. \\(\\widehat{\\mbox{cov}} (X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n  (Y_i-\\overline{Y})(X_i-\\overline{X})\\) \\(\\widehat{\\mbox{corr}} (X,Y) =\\) ?\n\n0.4\n\n\n\n\nDistributions that will be used in this class Normal distribution Chi-square distribution t distribution F distribution\n\n\nNormal Distribution\nDensity: \\(Y\\sim N(\\mu,\\sigma^2)\\), $$\n\\[\\begin{aligned}\n        f_Y(y) & = &\n        \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{\\frac{-1}{2}\\left(\\frac{y-\\mu}{\\sigma}\n        \\right)^2\\right\\} \\nonumber\n        \n\\end{aligned}\\]\n$$ If we know \\(E(Y)=\\mu\\), \\(Var(Y)=\\sigma^2\\) then\n/3 of \\(Y\\)’s distribution lies within 1 \\(\\sigma\\) of \\(\\mu\\) % \\(\\ldots\\) \\(\\ldots\\) is within \\(\\mu\\pm 2\\sigma\\) \\(&gt;99\\)% \\(\\ldots\\) \\(\\ldots\\) lies within \\(\\mu\\pm 3\\sigma\\)\nArguably, the most important distribution in statistics\nLinear combinations of Normals are Normal\ne.g., \\((aY+b)\\sim \\mbox{N}(a\\mu+b,\\;a^2\\sigma^2)\\)\nStandard normal: $$\n\\[\\begin{aligned}\n        Z=\\frac{Y-\\mu}{\\sigma} \\sim \\mbox{N}(0,1) \\nonumber\n        \n\\end{aligned}\\]\n$$\n\n\nChi-square Distribution Notation: \\(X \\sim \\chi^2_{df}\\) \\(df=\\) degrees of freedom \\(E[X]=df\\) \\(X\\) takes on only positive values If \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\nIf \\(Z_1,\\ldots,Z_n\\) are independent, with \\(Z_i\\sim\\mbox{N}(0,1)\\), then $$\n\\[\\begin{aligned}\n        \\sum_{i=1}^n Z_i^2 & \\sim & \\chi^2_n \\nonumber\n        \n\\end{aligned}\\]\n$$\nUsed in hypothesis testing and CI’s involving variance\n\n\nt Distribution\nIf \\(Z\\sim \\mbox{N}(0,1)\\) and \\(S^2\\sim \\chi^2_{df}\\) and \\(Z\\) and \\(S^2\\) are independent,\n$$\n\\[\\begin{aligned}\n        \\frac{Z}{S/\\sqrt{df}} & \\sim & t_{df} \\nonumber\n        \n\\end{aligned}\\]\n$$\nSymmetric, bell-shaped; tails heavier than Normal \\(E[t_{df}]=0\\); \\(Var(t_{df})\\) greater than 1 \\(\\lim_{df\\rightarrow \\infty}t_{df} \\rightarrow \\mbox{N}(0,1)\\) for \\(df&gt;30\\), the \\(t_{df}\\) closely resembles the \\(\\mbox{N}(0,1)\\) distribution\nIn linear modeling, used for inference on individual regression parameters\n\n\nF Distribution\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then: $$\n\\[\\begin{aligned}\n        \\frac{X_1^2/df1}{X_2^2/df2} & \\sim & F_{df1,df2} \\nonumber\n        \n\\end{aligned}\\]\n$$\nonly takes on positive values\nconnection to \\(t\\) distribution:\n$$\n\\[\\begin{aligned}\n            \\{ t_{df} \\}^2 & \\stackrel{{\\cal D}}{=} & F_{1,df} \\nonumber\n            \n\\end{aligned}\\]\n$$\nUsed extensively in linear regression (hypothesis testing)\n\n\nQuestions?\n\n\n\nIntro"
  },
  {
    "objectID": "slides/Module_A.html#what-is-linear-regression-regression-a-technique-to",
    "href": "slides/Module_A.html#what-is-linear-regression-regression-a-technique-to",
    "title": "Module A",
    "section": "What is Linear Regression? Regression: a technique to",
    "text": "What is Linear Regression? Regression: a technique to\nstudy the association between two variables Response variable (outcome, dependent variable) Blood pressure Grouping variable (predictor, independent variable, explanatory variable) Male vs female – within each group, the value of the grouping variable is constant Drug dosage – continuous predictor of interest, infinitely many groups Adjustment for other variables\nLinear model: for our purposes, refers to linearity w.r.t. the parameters\nResponse variable is a linear function of parameters\nRegression models describe association, not causality"
  },
  {
    "objectID": "slides/Module_A.html#why-should-i-care",
    "href": "slides/Module_A.html#why-should-i-care",
    "title": "Module A",
    "section": "Why should I care?",
    "text": "Why should I care?\nMost widely used and most developed method in statistics Appropriate in many practical settings Important to understand limitations Easy to interpret\nFun fact: one of the most frequently asked data scientist interview question What are the assumptions underlying linear regression?"
  },
  {
    "objectID": "slides/Module_A.html#why-should-i-care-1",
    "href": "slides/Module_A.html#why-should-i-care-1",
    "title": "Module A",
    "section": "Why should I care?",
    "text": "Why should I care?\nMost importantly, it serves as a building block Essential concepts and ideas extend well to other regression methods and other areas e.g. you will see the following generic equation frequently:\n\\(\\sum_{i=1}^nX_i\\trans(Y_i-\\mu_i)=0\\) where \\(\\mu_i=X_i\\trans\\beta\\) in linear regression\nIt says “the residuals are orthogonal to the covariates”\nTechniques used in linear regression apply to other regression methods e.g. estimation, hypothesis testing, model diagnosis"
  },
  {
    "objectID": "slides/Module_A.html#linear-regression-objectives",
    "href": "slides/Module_A.html#linear-regression-objectives",
    "title": "Module A",
    "section": "Linear Regression: Objectives",
    "text": "Linear Regression: Objectives\nObjectives of any data analysis can generally be categorized as either inference or prediction\nrare that only one objective of interest in practice, both are usually of various degrees of importance\nInference:\nEstimation Hypothesis testing"
  },
  {
    "objectID": "slides/Module_A.html#inference-estimation",
    "href": "slides/Module_A.html#inference-estimation",
    "title": "Module A",
    "section": "Inference: Estimation",
    "text": "Inference: Estimation\nIncludes both point and interval estimation\nSign of regression coefficient may be of interest; or, both sign and magnitude e.g., estimate mean change in serum cholesterol per unit increase in BMI model parameters must have clear interpretation (limits complexity of model)"
  },
  {
    "objectID": "slides/Module_A.html#inference-hypothesis-testing",
    "href": "slides/Module_A.html#inference-hypothesis-testing",
    "title": "Module A",
    "section": "Inference: Hypothesis Testing",
    "text": "Inference: Hypothesis Testing\nWe observe the data in the study sample; what can we infer about the underlying population of interest?\nHypothesis testing reduces results of study down to a sequence of yes/no answers\nModel parameters must be interpretable for inference to be meaningful\ne.g., on average, does serum cholesterol change as BMI (body mass index) increases? what about each of the following: age, gender, race, SBP (systolic blood pressure)?"
  },
  {
    "objectID": "slides/Module_A.html#prediction",
    "href": "slides/Module_A.html#prediction",
    "title": "Module A",
    "section": "Prediction",
    "text": "Prediction\nUsing regression model to predict response for yet unobserved subjects Accuracy and precision of predictions take precedence over interpretability of regression parameters\ne.g., develop a linear regression model to predict serum cholesterol given a set of patient characteristics (age, gender, race, BMI, SBP, etc)"
  },
  {
    "objectID": "slides/Module_A.html#data-analysis-process-overview",
    "href": "slides/Module_A.html#data-analysis-process-overview",
    "title": "Module A",
    "section": "Data Analysis Process: Overview",
    "text": "Data Analysis Process: Overview\nDescriptive analysis often skipped or done carelessly\nVERY important first step\nPropose model often done in close consultation with investigators\nEstimate model parameters\nAssess underlying assumptions of model \\(\\longrightarrow\\) return to (1)?\nHypothesis testing and/or prediction\nInterpretation, conclusions"
  },
  {
    "objectID": "slides/Module_A.html#simple-linear-regression-one-predictor",
    "href": "slides/Module_A.html#simple-linear-regression-one-predictor",
    "title": "Module A",
    "section": "Simple Linear Regression: one predictor",
    "text": "Simple Linear Regression: one predictor\nSLR features one response variable and a single covariate\nModel: \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \\nonumber\\]\n\\(Y_i\\): response \\(\\beta_0\\), \\(\\beta_1\\): parameters \\(X_i\\): covariate \\(\\epsilon_i\\): error\n“Simple”: one covariate only\nRather restrictive since only one covariate is involved\noften, interest lies in several covariates if only interested in one certain covariate, may need to adjust for other covariates"
  },
  {
    "objectID": "slides/Module_A.html#multiple-linear-regression-multiple-predictors",
    "href": "slides/Module_A.html#multiple-linear-regression-multiple-predictors",
    "title": "Module A",
    "section": "Multiple Linear Regression: multiple predictors",
    "text": "Multiple Linear Regression: multiple predictors\nMultiple Regression: \\(q(&gt;1)\\) covariates Model \\[Y_i  =  \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} +\\ldots +\n        \\beta_qX_{iq} +  \\epsilon_i\\]\n\n\n\nFor compactness, often use matrix notation Compared to simple linear regression \\(\\beta_j\\)’s have much different interpretation\nMuch more complicated than SLR;\nMuch greater chance to mis-model the data set\n\n\n\n\n\nimage"
  },
  {
    "objectID": "slides/Module_A.html#residual-diagnostics",
    "href": "slides/Module_A.html#residual-diagnostics",
    "title": "Module A",
    "section": "Residual Diagnostics",
    "text": "Residual Diagnostics\nMost of the assumptions underlying linear regression model are about the error term: \\[\\begin{split}\n            \\epsilon_i & \\sim  N(0,\\sigma^2)  \\\\\n            \\epsilon_i & \\perp \\epsilon_j,\\; \\forall i, \\;j,\\; i\\neq j\n        \\end{split}\\] Failure of any of these assumptions to hold could invalidate various aspects of the analysis\nResiduals = estimated errors: \\(\\widehat{\\epsilon}_i\\)\nResidual diagnostics aims to check validity of error assumptions after model fitting\nresults of residual diagnostics could imply that modifications to model are required"
  },
  {
    "objectID": "slides/Module_A.html#transformations-of-the-outcome",
    "href": "slides/Module_A.html#transformations-of-the-outcome",
    "title": "Module A",
    "section": "Transformations of the outcome",
    "text": "Transformations of the outcome\nConsider the following SLR model: \\[Y_i = \\beta_0 + \\beta_1X_{i} +  \\epsilon_i\\] It is possible that, instead, some function of \\(Y_i\\), say \\(h(Y_i)\\), should be modeled: \\[h(Y_i) =\\beta_0 + \\beta_1X_{i} +  \\epsilon_i\\] Possible choices for \\(h(\\cdot)\\): $$\n\\[\\begin{aligned}\n            h(Y_i) & = & \\log(Y_i) \\nonumber \\\\\n            h(Y_i) & = & \\log_{10}(Y_i) \\nonumber \\\\\n            h(Y_i) & = & \\sqrt{Y_i} \\nonumber \\\\\n            h(Y_i) & = & 1/Y_i \\nonumber\n            \n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/Module_A.html#transformations-of-the-covariates",
    "href": "slides/Module_A.html#transformations-of-the-covariates",
    "title": "Module A",
    "section": "Transformations of the covariate(s)",
    "text": "Transformations of the covariate(s)\nConsider the following SLR model: $$\n\\[\\begin{aligned}\n        Y_i & = & \\beta_0 + \\beta_1X_{i} +  \\beta_2 X^2_{i} + \\epsilon_i \\nonumber\n        \n\\end{aligned}\\]\n$$\nThis is still a linear model! “Linear” refers to being linear in the coefficients Transformation of the covariate can include $$\n\\[\\begin{aligned}\n            &   &  \\log(X_i) \\nonumber \\\\\n            &   &X^2_{i}, \\cdots, X^q_{i}   \\nonumber \\\\\n            &   &\\sqrt{X_i} \\nonumber \\\\\n            &   & \\mbox{splines:} ~~~ (X_i-a)_+\\nonumber\n            \n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/Module_A.html#transformations",
    "href": "slides/Module_A.html#transformations",
    "title": "Module A",
    "section": "Transformations",
    "text": "Transformations\nNeed to be very careful when using transformations\ninterpretation of parameters changes completely transforming the outcome alters assumptions regarding error structure"
  },
  {
    "objectID": "slides/Module_A.html#outliersinfluence-diagnostics",
    "href": "slides/Module_A.html#outliersinfluence-diagnostics",
    "title": "Module A",
    "section": "Outliers/Influence Diagnostics",
    "text": "Outliers/Influence Diagnostics\nConsider the fitted model: \\[\\widehat{Y}_i = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X_{i}\\]\nWhat impact does \\((X_i,Y_i)\\) have on \\(\\widehat{Y}_i\\)? How much does the \\(i\\)’th subject influence \\(\\widehat{\\beta}_0\\)? By what amount would \\(\\widehat{\\beta}_1\\) change if the \\(i\\)’th subject were deleted? If the \\(i\\)’th observation heavily influences \\(\\widehat{\\beta}_1\\), should it be removed?"
  },
  {
    "objectID": "slides/Module_A.html#multicollinearity",
    "href": "slides/Module_A.html#multicollinearity",
    "title": "Module A",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nConsider: linear regression model with covariates \\(X_1,\\ldots,X_q\\) Covariates are not assumed to be independent (e.g., age, height, weight, etc) However, extreme correlations among the covariates interferes with the model fitting:\nmay be impossible to compute \\(\\widehat{\\beta}_j\\)’s computed \\(\\widehat{\\beta}_j\\)’s may have extremely large standard errors fitted model may be quite unreliable"
  },
  {
    "objectID": "slides/Module_A.html#model-selection",
    "href": "slides/Module_A.html#model-selection",
    "title": "Module A",
    "section": "Model Selection",
    "text": "Model Selection\nConsider: state-wide survey, wherein information on 30 covariates collected\nWhen objective is to find the ‘best’ model, several algorithms may simplify the process\nForward Selection: start with 0 covariates add (most significant) covariates one at a time until further addition does not improve the model’s fit\nBackward Elimination: start with full model successively delete (least significant) covariates, until such deletions results in marked decrease in model’s adequacy\nStepwise Selection: combination of forward selection and backward elimination"
  },
  {
    "objectID": "slides/Module_A.html#model-validation",
    "href": "slides/Module_A.html#model-validation",
    "title": "Module A",
    "section": "Model Validation",
    "text": "Model Validation\nConcept: how accurate is the fitted model on new data (not used to compute the parameter estimates)\ne.g., data splitting:\nsplit data set in two fit model to first half of data set, \\(\\widehat{\\beta}_1^*,\\dots,\\widehat{\\beta}_p^*\\) compare \\(Y_i\\) and \\(\\widehat{Y}_i^*\\), in second half of data set"
  },
  {
    "objectID": "slides/Module_A.html#weighted-regression-units-i-are-not-always-sampled-completely-at",
    "href": "slides/Module_A.html#weighted-regression-units-i-are-not-always-sampled-completely-at",
    "title": "Module A",
    "section": "Weighted Regression Units \\(i\\) are not always sampled completely at",
    "text": "Weighted Regression Units \\(i\\) are not always sampled completely at\nrandom Surveys may oversample certain subpopulations\nSome units are given more weight during the estimation process"
  },
  {
    "objectID": "slides/Module_A.html#linear-vs.-generalized-linear-models",
    "href": "slides/Module_A.html#linear-vs.-generalized-linear-models",
    "title": "Module A",
    "section": "Linear vs. Generalized Linear Models",
    "text": "Linear vs. Generalized Linear Models\nLinear regression: \\(Y_i\\) assumed to be continuous\n(and \\(\\epsilon_i\\) is normally distributed)\nSuppose \\(Y_i\\) is an indicator variable (e.g., 1=cancer; 0=cancer-free)\n\\(Y_i\\) follows a Bernoulli distribution assumptions of linear regression blatantly violated\nGeneralized Linear Model (GLM): used to model non-Normal responses; e.g.,\n\\(Y_i\\sim\\) Bernoulli/Binomial: logistic regression \\(Y_i\\) is a count: Poisson regression\nWill study GLM extensively in BIOSTAT 651"
  },
  {
    "objectID": "slides/Module_A.html#multiple-vs.-multivariate-regression",
    "href": "slides/Module_A.html#multiple-vs.-multivariate-regression",
    "title": "Module A",
    "section": "Multiple vs. Multivariate regression",
    "text": "Multiple vs. Multivariate regression\nSimple linear regression (this course): one outcome \\(Y\\), one predictor variable \\(X\\)\nMultiple linear regression (this course): one outcome \\(Y\\), multiple predictor variables \\(X\\)\nMultivariate regression: multiple outcome variables \\(Y\\), e.g. same outcome measured repeatedly over time multiple related outcomes (e.g., height and weight) considered simultaneously as outcome can be of any distribution, i.e., multivariate GLM\nWill study multivariate regression extensively in BIOSTAT 653"
  },
  {
    "objectID": "slides/Module_A.html#background",
    "href": "slides/Module_A.html#background",
    "title": "Module A",
    "section": "Background",
    "text": "Background\nLead (Pb) is a neurotoxicant It is associated with decreased mental development (in children), and accelerated mental decline in the elderly.\nStudies typically measure lead concentrations in blood, and correlate them (e.g., linear regression) with measurements of intelligence (e.g., BAYLEY mental development index, IQ tests)\nPrenatal exposure measurements typically assessed by amount of lead in umbilical cord\nNovel biomarker is bone lead concentration—premise is that lead leaches out of the mother’s bones throughout pregnancy"
  },
  {
    "objectID": "slides/Module_A.html#gomaa-et-al.-study-objective-and-design",
    "href": "slides/Module_A.html#gomaa-et-al.-study-objective-and-design",
    "title": "Module A",
    "section": "Gomaa et al. Study Objective and Design",
    "text": "Gomaa et al. Study Objective and Design\nProspective Study: Children-mothers recruited at birth, followed until children are 24 months old\nResearch question: is lead exposure associated with mental development?\nLead exposure is measured by: Lead concentration in umbilical blood Lead concentration in maternal bones\nMental development Is measured by: BAYLEY’s index of mental development (MDI)\nStatistical question: Test if umbilical cord blood lead and bone lead concentration are predictors of BAYLEY’s MDI"
  },
  {
    "objectID": "slides/Module_A.html#confounding-variables",
    "href": "slides/Module_A.html#confounding-variables",
    "title": "Module A",
    "section": "Confounding Variables",
    "text": "Confounding Variables\nWhat are confounders? A confounder can make the observed association appear stronger than the true association, weaker than the true association, or even the reverse of the true association\nConfounders in this case:\ndemographics (age, gender, education, marital status) Breastfeeding duration Maternal IQ\nSometimes confounders and predictor variables are called “covariates”"
  },
  {
    "objectID": "slides/Module_A.html#acknowledgement",
    "href": "slides/Module_A.html#acknowledgement",
    "title": "Module A",
    "section": "Acknowledgement",
    "text": "Acknowledgement\n\n\n\n\n\n\n\n\n\n\n\nMousumi Banerjee\nBrisa N. Sánchez\n\n\nUniversity of Michigan\nDrexel University\n\n\n\nThank you for your slides!"
  },
  {
    "objectID": "slides/Module_A.html#basic-statistics",
    "href": "slides/Module_A.html#basic-statistics",
    "title": "Module A",
    "section": "Basic Statistics",
    "text": "Basic Statistics\n\n\n0.5\nRandom variable \\(Y\\) Sample \\(Y_i, i=1,\\dots, n\\)\nSummation: \\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\nProduct: \\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)\nExpected Value (or mean): \\(\\mu_Y= E[Y] = \\int_{-\\infty}^\\infty y f(y) dy\\) reflects ‘center’ of \\(Y\\)’s distribution\n\n0.5"
  },
  {
    "objectID": "slides/Module_A.html#rules-of-expected-values",
    "href": "slides/Module_A.html#rules-of-expected-values",
    "title": "Module A",
    "section": "Rules of Expected Values",
    "text": "Rules of Expected Values\n\n\n0.5 Expectation of sum: \\(E\\left[\\sum_{i=1}^n Y_i \\right] = \\sum_{i=1}^n E[Y_i] \\nonumber\\) No assumption of independence required\nLet \\(a_1,\\ldots,a_n\\) be constants \\(E[a_iY_i] = a_iE[Y_i]\\)\n\\(E\\left[\\sum_{i=1}^n a_iY_i \\right] = \\sum_{i=1}^n a_iE[Y_i]\\)\nExpectation of product: \\(E[Y_iY_j] = E[Y_i]E[Y_j]\\)\nif \\(Y_i\\) and \\(Y_j\\) are independent\n\n0.5\n\n\n:::"
  },
  {
    "objectID": "slides/Module_A.html#variance-and-standard-deviation",
    "href": "slides/Module_A.html#variance-and-standard-deviation",
    "title": "Module A",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\n\n\n0.5\nVariance: \\[\\begin{split}\n        &Var(Y) =  E[(Y-\\mu_Y)^2]\\\\\n        = &\\int_{-\\infty}^\\infty (y-\\mu_Y)^2 f(y) dy\n        %=  E[Y^2] - \\mu_Y^2\n        \\end{split}\\] reflects spread of \\(Y\\)’s distribution units: (units of \\(Y\\))\\(^2\\)\nStandard deviation: \\[\\begin{split}\n        SD(Y)  = &\\; \\sqrt{Var(Y)}  \\\\\n        SD(aY)  = &\\; aSD(Y)\n        \\end{split}\\]\nreflects dispersion in \\(Y\\)’s distribution measured in same unit as \\(Y\\)\n\n0.5\n\n\n:::"
  },
  {
    "objectID": "slides/Module_A.html#rules-of-variances",
    "href": "slides/Module_A.html#rules-of-variances",
    "title": "Module A",
    "section": "Rules of Variances",
    "text": "Rules of Variances\n\n\n0.5 Variance\n\\(Var(Y)=E[(Y-\\mu_Y)^2]\\)\n\\({\\color{white}{Var(Y)}} =E[Y^2] - \\mu_Y^2\\) Variance of linear combination: \\[\\begin{split}\n        &~Var(aY+b)\\\\\n         = &~ Var(aY) \\\\\n         = &~ E[(aY-E[aY])^2]\\\\\n         = &~ a^2 E[(Y-E[Y])^2]\\\\\n         = &~ a^2 Var(Y)\n        \\end{split}\\] \n\n0.5\n\n\n:::"
  },
  {
    "objectID": "slides/Module_A.html#covariance",
    "href": "slides/Module_A.html#covariance",
    "title": "Module A",
    "section": "Covariance",
    "text": "Covariance\n\n\n0.6 \\(X\\), \\(Y\\): random variables Covariance: \\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]\\)\nMeasures (linear) association between \\(X\\), \\(Y\\)\n\\(&gt;0\\), large values of \\(X\\) tend to occur with large values of \\(Y\\)\n\\(&lt;0\\), large values of \\(X\\) tend to coincide with small values of \\(Y\\)\n\\(=0\\), size of \\(X\\) provides no information on size of \\(Y\\) When the covariance is calculated, the data are not standardized Not scale-invariant: can interpret direction but not magnitude\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/Module_A.html#correlation",
    "href": "slides/Module_A.html#correlation",
    "title": "Module A",
    "section": "Correlation",
    "text": "Correlation\n\n\n0.5 \\(X\\), \\(Y\\): random variables Correlation: \\(\\mbox{corr}(X,Y) = \\frac{\\mbox{cov}(X,Y)}{SD(X)SD(Y)}\\)\nScaled measure of linear association,\n\\(-1 \\leq \\mbox{corr}(X,Y) \\leq 1\\) easier to interpret than covariance\n\n0.5"
  },
  {
    "objectID": "slides/Module_A.html#rules-of-covariance",
    "href": "slides/Module_A.html#rules-of-covariance",
    "title": "Module A",
    "section": "Rules of covariance",
    "text": "Rules of covariance\n\\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]= E[XY]-E[X]E[Y]\\) \\(\\mbox{cov} (Y,Y) = \\mbox{var} (Y)\\) Independent \\(\\stackrel{\\Rightarrow}{\\not\\Leftarrow}\\) uncorrelated If \\(X\\) and \\(Y\\) are independent, \\(\\mbox{cov}(X,Y)=0\\) If \\(\\mbox{cov}(X,Y)=0\\) and \\((X,Y)\\sim \\text{Bivariate Normal}\\), then \\(X\\) and \\(Y\\) are independent Covariance is symmetric, additive, and scale preserving \\[\\begin{aligned}\n\\mbox{cov} (X,Y) & = & \\mbox{cov} (Y,X) \\nonumber \\\\\n\\mbox{cov} (X,Y_1+Y_2) & = & \\mbox{cov} (X,Y_1)+\\mbox{cov} (X,Y_2) \\nonumber \\\\\n\\mbox{cov} (X,aY) & = & a\\;\\mbox{cov} (X,Y) \\nonumber\n%\\mbox{cov} (Y,Y) & = & \\mbox{var} (Y) \\nonumber\n\\end{aligned}\\] :::"
  },
  {
    "objectID": "slides/Module_A.html#rules-of-variance",
    "href": "slides/Module_A.html#rules-of-variance",
    "title": "Module A",
    "section": "Rules of variance",
    "text": "Rules of variance\n\n\n0.6\nVariance of sum: \\[\\setlength{\\jot}{1pt}\n        \\begin{split}\n        &Var\\left(\\sum_{i=1}^n Y_i\\right)\n        =  \\sum_{i=1}^n\\sum_{j=1}^n\n        \\mbox{cov}(Y_i,Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i)   + \\sum_{i=1}^n\\sum_{j=1}^n I(j\\neq i) \\mbox{cov}(Y_i, Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i) + 2\\sum_{i=1}^n \\sum_{j=i+1}^n\n        \\mbox{cov}(Y_i, Y_j )\n        \\end{split}\\] if \\(Y_1,\\ldots,Y_n\\) are mutually independent, then \\(Var\\left(\\sum_{i=1}^n Y_i\\right) = \\sum_{i=1}^n Var(Y_i)\\)\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/Module_A.html#estimator-of-mean",
    "href": "slides/Module_A.html#estimator-of-mean",
    "title": "Module A",
    "section": "Estimator of Mean",
    "text": "Estimator of Mean\nSuppose we obtained a simple random sample from some underlying population, then we can derive sample estimates of each of the population quantities defined previously Suppose \\(Y_1,\\ldots,Y_n\\) are iid with mean \\(\\mu_Y\\) and variance \\(\\sigma^2_Y\\)\n\n\n0.6 Estimator of mean: \\[\\widehat{\\mu}_Y = \\frac{1}{n} \\sum_{i=1}^n Y_i =\n            \\overline{Y}\\]\n\\(E[\\overline{Y}]= \\frac{1}{n} \\sum_{i=1}^n E[Y_i] = \\mu_Y\\)\n\\(Var(\\overline{Y})= n^{-2} \\sum_{i=1}^n Var(Y_i) = \\sigma_Y^2/n\\)\n\n0.37\n\n\n:::"
  },
  {
    "objectID": "slides/Module_A.html#variance-and-covariance-estimator",
    "href": "slides/Module_A.html#variance-and-covariance-estimator",
    "title": "Module A",
    "section": "Variance and Covariance Estimator",
    "text": "Variance and Covariance Estimator\n\n\n0.6\nEstimators of variance:\n\\(\\widehat{\\sigma}^2_Y = \\frac{1}{n} \\sum_{i=1}^n (Y_i-E[Y_i])^2\\)\nif population mean is known \\(\\widehat{\\sigma}^2_Y = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i-\\overline{Y})^2\\)\nif population mean is unknown\nEstimator of covariance:\nSuppose pairs \\((Y_1,X_1),\\ldots,(Y_n,X_n)\\) are iid. \\(\\widehat{\\mbox{cov}} (X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i-\\overline{Y})(X_i-\\overline{X})\\) \\(\\widehat{\\mbox{corr}} (X,Y) =\\) ?\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/Module_A.html#distributions-that-will-be-used-in-this-class",
    "href": "slides/Module_A.html#distributions-that-will-be-used-in-this-class",
    "title": "Module A",
    "section": "Distributions that will be used in this class",
    "text": "Distributions that will be used in this class\nNormal distribution Chi-square distribution t distribution F distribution"
  },
  {
    "objectID": "slides/Module_A.html#normal-distribution",
    "href": "slides/Module_A.html#normal-distribution",
    "title": "Module A",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nDensity: \\(Y\\sim N(\\mu,\\sigma^2)\\), $$\n\\[\\begin{aligned}\n        f_Y(y) & = &\n        \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{\\frac{-1}{2}\\left(\\frac{y-\\mu}{\\sigma}\n        \\right)^2\\right\\} \\nonumber\n        \n\\end{aligned}\\]\n$$ If we know \\(E(Y)=\\mu\\), \\(Var(Y)=\\sigma^2\\) then\n/3 of \\(Y\\)’s distribution lies within 1 \\(\\sigma\\) of \\(\\mu\\) % \\(\\ldots\\) \\(\\ldots\\) is within \\(\\mu\\pm 2\\sigma\\) \\(&gt;99\\)% \\(\\ldots\\) \\(\\ldots\\) lies within \\(\\mu\\pm 3\\sigma\\)\nArguably, the most important distribution in statistics\nLinear combinations of Normals are Normal\ne.g., \\((aY+b)\\sim \\mbox{N}(a\\mu+b,\\;a^2\\sigma^2)\\)\nStandard normal: $$\n\\[\\begin{aligned}\n        Z=\\frac{Y-\\mu}{\\sigma} \\sim \\mbox{N}(0,1) \\nonumber\n        \n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/Module_A.html#chi-square-distribution",
    "href": "slides/Module_A.html#chi-square-distribution",
    "title": "Module A",
    "section": "Chi-square Distribution",
    "text": "Chi-square Distribution\nNotation: \\(X \\sim \\chi^2_{df}\\) \\(df=\\) degrees of freedom \\(E[X]=df\\) \\(X\\) takes on only positive values If \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\nIf \\(Z_1,\\ldots,Z_n\\) are independent, with \\(Z_i\\sim\\mbox{N}(0,1)\\), then $$\n\\[\\begin{aligned}\n        \\sum_{i=1}^n Z_i^2 & \\sim & \\chi^2_n \\nonumber\n        \n\\end{aligned}\\]\n$$\nUsed in hypothesis testing and CI’s involving variance"
  },
  {
    "objectID": "slides/Module_A.html#t-distribution",
    "href": "slides/Module_A.html#t-distribution",
    "title": "Module A",
    "section": "t Distribution",
    "text": "t Distribution\nIf \\(Z\\sim \\mbox{N}(0,1)\\) and \\(S^2\\sim \\chi^2_{df}\\) and \\(Z\\) and \\(S^2\\) are independent,\n$$\n\\[\\begin{aligned}\n        \\frac{Z}{S/\\sqrt{df}} & \\sim & t_{df} \\nonumber\n        \n\\end{aligned}\\]\n$$\nSymmetric, bell-shaped; tails heavier than Normal \\(E[t_{df}]=0\\); \\(Var(t_{df})\\) greater than 1 \\(\\lim_{df\\rightarrow \\infty}t_{df} \\rightarrow \\mbox{N}(0,1)\\) for \\(df&gt;30\\), the \\(t_{df}\\) closely resembles the \\(\\mbox{N}(0,1)\\) distribution\nIn linear modeling, used for inference on individual regression parameters"
  },
  {
    "objectID": "slides/Module_A.html#f-distribution",
    "href": "slides/Module_A.html#f-distribution",
    "title": "Module A",
    "section": "F Distribution",
    "text": "F Distribution\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then: $$\n\\[\\begin{aligned}\n        \\frac{X_1^2/df1}{X_2^2/df2} & \\sim & F_{df1,df2} \\nonumber\n        \n\\end{aligned}\\]\n$$\nonly takes on positive values\nconnection to \\(t\\) distribution:\n$$\n\\[\\begin{aligned}\n            \\{ t_{df} \\}^2 & \\stackrel{{\\cal D}}{=} & F_{1,df} \\nonumber\n            \n\\end{aligned}\\]\n$$\nUsed extensively in linear regression (hypothesis testing)\n\n\nClass 1 Slides"
  },
  {
    "objectID": "slides/Module_A.html#what-is-linear-regression",
    "href": "slides/Module_A.html#what-is-linear-regression",
    "title": "Module A",
    "section": "What is Linear Regression?",
    "text": "What is Linear Regression?\n\nRegression: a technique to study the association between two variables Response variable (outcome, dependent variable) Blood pressure Grouping variable (predictor, independent variable, explanatory variable) Male vs female – within each group, the value of the grouping variable is constant Drug dosage – continuous predictor of interest, infinitely many groups Adjustment for other variables\nLinear model: for our purposes, refers to linearity w.r.t. the parameters\nResponse variable is a linear function of parameters\nRegression models describe association, not causality"
  },
  {
    "objectID": "slides/Module_A.html#learning-objectives",
    "href": "slides/Module_A.html#learning-objectives",
    "title": "Module A",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nLearning 1\nLearning 2\n\n\n\nLearning 3\n\n\n\nLearning 4"
  },
  {
    "objectID": "slides/Module_N.html",
    "href": "slides/Module_N.html",
    "title": "Linear Regression",
    "section": "",
    "text": "BIOSTAT 650\nTheory and Application of Linear Regression\nModule N: Model Selection\n\n\nOutline\nTopics:\nprediction\nmaximum model\ncriteria for comparing models\nautomated model selection\nexamples\nText (MPV): Chapter 10 Reading: An Introduction to Statistical Learning with Applications in R (ISLR) Chapter 6\n\n\nModel Selection for Inference vs Prediction Model selection for Inference: we want \\(\\widehat{\\beta}\\) to be close to \\(\\beta\\) requires pre-specified scientific knowledge about the known or hypothesized causal/biological mechanisms unbiased point estimation & valid inference (interval estimation and hypothesis testing) Model selection for Prediction: we want the best model for predicting future responses The focus is more on the fitted model than on the individual parameters: \\(\\widehat{\\beta}\\) can even be biased If \\(p\\) is too large, then there can be overfitting (model follows the noise in current observations too closely) and consequently poor predictions on future observations not used in model training\n\n\nModel Complexity vs Parsimony Suppose we have p = 30 covariates (in the true model) and n = 50 observations. We could consider the following two alternatives: We could fit a model using all of the covariates. In this case, \\(\\widehat{\\beta}\\) is unbiased for \\({\\beta}\\), but it has very high variance. We could fit a model using the five strongest predictors. In this case, \\(\\widehat{\\beta}\\) will be biased for \\({\\beta}\\), but it will have lower variance. For prediction, either approach 1 or approach 2 could perform better, depending on the circumstances\n\n\nTraining vs Test error\n\n\n\n\nimage\n\n\n\n\n\nModel Selection for Prediction: Big Picture\n\\(\\bullet~~\\)Model selection methods generally consist of the same essential steps:\nSpecify maximum/full model construct a list of all candidate covariates Specify criterion for comparing competing models how well the model fits current data (training error) how well the model predicts future responses (test error) Select best (sequence of) model(s) where “best\" is defined by (2) All-Possible-subsets Regression (APR) forward, backward, hybrid Test error estimation and model validation Accuracy of the predictions when we apply our model to previously unseen test data not used to train the model indirectly estimate test error: criteria in (2) directly estimate test error: cross-validation\n\n\nStep 1: Specifying Maximum Model Maximum model: all covariates which are candidates for inclusion in final model\nNecessary to inspect data carefully before choosing candidate covariates: covariates must be scored on a meaningful scale covariates must have sufficient variability no covariate should be highly predicted by remaining covariates (i.e., beware of potential high \\(VIF_j=1/(1-R^2_j)\\) ahead of time)\n“Size” of maximum model (\\(p\\)) can be constrained by sample size (e.g., \\(\\geq\\)5 subjects per parameter)\nStatisticians and investigators should work closely together in constructing maximum model\n\n\nStep 3: Select Model Given a Criterion\nWe will introduce the approaches taking \\(p\\)-value as a criterion Selection by \\(p\\)-value is a commonly used and traditional method We will introduce other criteria (Step 2) in detail later\nAll-Possible-subsets Regression (APR) having chosen criterion for comparing models, fit models using all possible combinations of covariates final model is the one judged to be the best based on the specified criterion\nThere are many (\\(2^p-1\\)) possible combinations of covariates we want to avoid fitting and comparing all possible models desirable to apply computationally efficient selection methods\n\n\nStep 3: Classical Automated Selection Three “classical\" variations on theme: Forward Selection: start with null model (\\(\\beta_0\\) only); add covariates one at a time\nBackward Elimination: start with full model; delete covariates one at a time\nStepwise Regression (hybrid): combination of forward selection and backward elimination\nOther methods (not covered in 650), e.g., the LASSO\n\n\nAlgorithm 1: Forward Selection Pre-specify p-value threshold for inclusion: \\(p_{\\scriptscriptstyle\\text{$I$}}\\) (e.g., 0.05)\nBegin with null model (\\(\\beta_0\\) only) \\(\\mathcal{M}_0\\)\nAt each step, add the most significant covariate, provided \\(p&lt;p_{\\scriptscriptstyle\\text{$I$}}\\) for \\(k\\!=\\!0,\\dots,p\\!-\\!1\\), fit \\(p\\!-\\!k\\) models that add 1 predictor to \\(\\mathcal{M}_k\\) e.g., in the first step, \\(k=0\\), fit \\(p\\) SLRs covariate with lowest \\(p\\)-value (\\(p&lt;p_{\\scriptscriptstyle\\text{$I$}}\\)) gets entered into model \\(\\mathcal{M}_{k\\!+\\!1}\\)\nProcedure terminates when no more covariate meets the \\(p_{\\scriptscriptstyle\\text{$I$}}\\)-rule (\\(p&lt;p_{\\scriptscriptstyle\\text{$I$}}\\)) or all covariates have been added\n\n\nAlgorithm 2: Backward Elimination Pre-specify p-value for omitting: \\(p_{\\scriptscriptstyle\\text{$O$}}\\) (e.g., 0.05)\nBegin with maximum model \\(\\mathcal{M}_p\\)\nAt each step, eliminate least significant covariate, provided \\(p\\!&gt;\\!p_{\\scriptscriptstyle\\text{$O$}}\\) for \\(k\\!=\\!p,\\dots,1\\), fit \\(k\\) models that remove 1 predictor from \\(\\mathcal{M}_k\\) e.g., in the first step, \\(k=p\\), fit \\(p\\) models each w/ \\(p-1\\) predictors covariate with highest \\(p\\)-value (\\(p\\!&gt;\\!p_{\\scriptscriptstyle\\text{$O$}}\\)) gets removed to obtain \\(\\mathcal{M}_{k\\!-\\!1}\\)\nProcedure terminates when no more covariate meets the \\(p_{\\scriptscriptstyle\\text{$O$}}\\)-rule (\\(p&gt;p_{\\scriptscriptstyle\\text{$O$}}\\)), i.e., all covariates are significant in the sense that \\(p&lt;p_{\\scriptscriptstyle\\text{$O$}}\\)\n\n\nAlgorithm 3: Stepwise Regression Pre-specify \\(p_{\\scriptscriptstyle\\text{$I$}}\\) and \\(p_{\\scriptscriptstyle\\text{$O$}}\\) satisfying \\(p_O\\geq p_I\\)\nBegin with null model (\\(\\beta_0\\) only)\nProceed as in forward selection. At each step:\nadd most significant covariate, provided \\(p&lt;p_{\\scriptscriptstyle\\text{$I$}}\\)\nthen, remove least significant covariate, if \\(p&gt;p_{\\scriptscriptstyle\\text{$O$}}\\)\nProcedure terminates when no more covariates added or deleted\nIdea: After adding each new variable, seek to remove one variable that no longer provide an improvement in the model fit Attempts to more closely mimic APR while retaining the computational advantages\n\n\nAlgorithm 3: Stepwise Regression (continued) Opinion quite varied regarding appropriate \\(p_{\\scriptscriptstyle\\text{$O$}}\\) e.g., \\(p_{\\scriptscriptstyle\\text{$O$}}=0.10\\) or \\(p_O=0.25\\) \\(p_{\\scriptscriptstyle\\text{$I$}}\\) often set to 0.05\nWhy higher \\(p_{\\scriptscriptstyle\\text{$O$}}\\): test statistics involve \\(\\widehat{\\sigma}^2\\) in current model, which may be an overestimate if too few covariates in the model consequently, \\(p\\)-values may be inflated and artificially exceed \\(p_{\\scriptscriptstyle\\text{$O$}}\\)\n\n\nLimitations of Classical Automated Selection Greedy search and not necessarily find the optimal: e.g., best \\(\\mathcal{M}_1\\) includes \\(\\{X_1\\}\\) but \\(\\mathcal{M}_2\\) includes \\(\\{X_2,X_3\\}\\) sometimes, covariate is only significant in the presence of others Tend to break down when: \\(p\\) is very large (e.g., \\(p&gt;n\\), can’t do backward elimination) multicollinearity: large VIFs thus questionable \\(p\\)-values\nF-tests are problematic both during and after selection: In forward selection \\(F\\)-tests are conservative for early stages at each step, \\(\\widehat{\\sigma}^2\\) in denominator of F-statistic is not \\(\\widehat{\\sigma}_{full}^2\\) Multiple comparison problem during selection we ask the same dataset too many questions (hypotheses) and can get a small \\(p\\)-value simply by chance Post-selection inference: F-tests in the final model are not valid theoretical results for F-test assume \\(p\\) is fixed, but with automated selection we are also “estimating” \\(p\\)\n\n\nImplementation of Automated Selection Straightforward in SAS PROC REG are specified with the SELECTION= option in the MODEL statement In R The olsrr package, e.g., ols_step_forward_p(), ols_step_backward_p()\nFunction stepwise() writen by Dr. Paul A. Rubin: https://rubin.msu.domains/code/stepwise_demo.nb.html\n(demo.Rmd uploaded to Canvas, helpful for optional hw11)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTraining vs Test error\n\n\n\n\nimage\n\n\n\n\n\nModel Selection for Prediction: Big Picture\n\\(\\bullet~~\\)Model selection methods for the purpose of prediction generally consist of the same essential steps:\nSpecify maximum/full model construct a list of all candidate covariates Specify criterion for comparing competing models how well the model fits current data (training error) how well the model predicts future responses (test error) Select best (sequence of) model(s) where “best\" is defined by (2) All-Possible-subsets Regression (APR) forward, backward, hybrid Test error estimation and model validation Accuracy of the predictions when we apply our model to previously unseen test data not used to train the model indirectly estimate test error directly estimate test error: cross-validation\n\n\nStep 2: Criteria for Comparing Models \\(R^2\\)\\(=\\widehat{corr}(\\widehat{Y},Y)^2\\): nondecreasing when covariates are added\n\\(R_a^2\\)\\(= 1-\\frac{\\widehat{\\sigma}^2}{SSY/(n-1)}\\): increase or decrease when covariates added \\(R_a^2\\)-based model selection equivalent to MSE-based (\\(\\widehat{\\sigma}^2\\)) \\(\\widehat{\\sigma}^2\\): when we include \\(X_{p+1}\\), \\(\\widehat{\\sigma}^2_p=\\frac{SSE_p}{n-p}\\) vs \\(\\widehat{\\sigma}^2_{p+1}=\\frac{SSE_{p+1}\\downarrow}{n-p-1\\downarrow}\\) decrease in \\(SSE\\) is not offset by corresponding loss of \\(df_E\\)\n\\(PRESS\\)\nMallows \\(C_p\\) Others: e.g., Akaike information criterion (AIC), Bayesian information criterion (BIC)\n\n\nStep 2: Criteria for Comparing Models\n\\(R^2=\\widehat{corr}(\\widehat{Y},Y)^2\\): nondecreasing when covariates are added\n\\(R_a^2\\)\\(= 1-\\frac{\\widehat{\\sigma}^2}{SSY/(n-1)}\\): increase or decrease when covariates added \\(R_a^2\\)-based model selection equivalent to MSE-based (\\(\\widehat{\\sigma}^2\\)) \\(\\widehat{\\sigma}^2\\): when we include \\(X_{p+1}\\), \\(\\widehat{\\sigma}^2_p=\\frac{SSE_p}{n-p}\\) vs \\(\\widehat{\\sigma}^2_{p+1}=\\frac{SSE_{p+1}\\downarrow}{n-p-1\\downarrow}\\) decrease in \\(SSE\\) is not offset by corresponding loss of \\(df_E\\)\n\\(PRESS\\)\nMallows \\(C_p\\) Others: e.g., Akaike information criterion (AIC), Bayesian information criterion (BIC)\n\n\nStep 2: Criteria for Comparing Models\n\\(R^2=\\widehat{corr}(\\widehat{Y},Y)^2\\): nondecreasing when covariates are added\n\\(R_a^2= 1-\\frac{\\widehat{\\sigma}^2}{SSY/(n-1)}\\): increase or decrease when covariates added \\(R_a^2\\)-based model selection equivalent to MSE-based (\\(\\widehat{\\sigma}^2\\)) \\(\\widehat{\\sigma}^2\\): when we include \\(X_{p+1}\\), \\(\\widehat{\\sigma}^2_p=\\frac{SSE_p}{n-p}\\) vs \\(\\widehat{\\sigma}^2_{p+1}=\\frac{SSE_{p+1}\\downarrow}{n-p-1\\downarrow}\\) decrease in \\(SSE\\) is not offset by corresponding loss of \\(df_E\\)\n\\(PRESS\\)\nMallows \\(C_p\\) Others: e.g., Akaike information criterion (AIC), Bayesian information criterion (BIC)\n\n\nStep 2: Criteria for Comparing Models\n\\(R^2=\\widehat{corr}(\\widehat{Y},Y)^2\\): nondecreasing when covariates are added\n\\(R_a^2= 1-\\frac{\\widehat{\\sigma}^2}{SSY/(n-1)}\\): increase or decrease when covariates added \\(R_a^2\\)-based model selection equivalent to \\(\\widehat{\\sigma}^2\\)-based \\(\\widehat{\\sigma}^2\\): when we include \\(X_{p+1}\\), \\(\\widehat{\\sigma}^2_p=\\frac{SSE_p}{n-p}\\) vs \\(\\widehat{\\sigma}^2_{p+1}=\\frac{SSE_{p+1}\\downarrow}{n-p-1\\downarrow}\\) decrease in \\(SSE\\) is not offset by corresponding loss of \\(df_E\\)\n\\(PRESS\\)\nMallows \\(C_p\\) Others: e.g., Akaike information criterion (AIC), Bayesian information criterion (BIC)\n\n\nFit v.s. Predictive Criteria \\(R^2\\), \\(R^2_a\\), \\(\\widehat{\\sigma}^2\\): all are functions of \\(\\widehat{\\epsilon}_i\\)’s: reflect how well the model fits current data (training error) do not reflect how well the model predicts future responses (test error)\nResiduals \\(\\widehat{\\epsilon}_i^2\\)’s may underestimate true errors \\(\\epsilon_i^2\\)’s \\(\\widehat{Y}_i\\) is not independent of \\(Y_i\\) and may be artificially drawn towards it (particularly for high influence points) In fact, one can show that \\(SSE=\\|\\widehat{\\boldepsilon}\\|^2\\leq \\|\\boldepsilon\\|^2\\): “The training error is often overly optimistic – it underestimates the true error\"\nResiduals \\(\\widehat{\\epsilon}_i^2\\)’s (training error) also underestimate the test errors\n\n\nModel Selection for Prediction: Big Picture\n\\(\\bullet~~\\)Model selection methods for the purpose of prediction generally consist of the same essential steps:\nSpecify maximum/full model Specify criterion for comparing competing models how well the model fits current data (training error) \\(R^2,R^2_{adj},\\widehat{\\sigma}^2\\) how well the model predicts future responses (test error) Select best (sequence of) model(s) where “best\" is defined by (2) Test error estimation and model validation indirectly estimate test error directly estimate test error: cross-validation\n\n\nModel Selection for Prediction: Big Picture\n\\(\\bullet~~\\)Model selection methods for the purpose of prediction generally consist of the same essential steps:\nSpecify maximum/full model Specify criterion for comparing competing models how well the model fits current data (training error) \\(R^2,R^2_{adj},\\widehat{\\sigma}^2\\) how well the model predicts future responses (test error) Select best (sequence of) model(s) where “best\" is defined by (2) Test error estimation and model validation indirectly estimate test error directly estimate test error: cross-validation\n\n\nPrediction error residuals Deleted residuals (or prediction error residuals)\n_i(-i)=Y_i-_i(-i)= where \\(\\widehat{Y}_{i(-i)}\\) is the predicted value for the \\(i^{th}\\) observation from a fitted model where the \\(i^{th}\\) observation was left out Note: now \\(\\widehat{Y}_{i(-i)}\\) is independent of \\(Y_i\\)\n\\(\\widehat{\\epsilon}_{i(-i)}\\) also known as “\\(PRESS\\)\" residual, denoted as \\(PRESS_i\\) Rationale: [leave-one-out] systematically remove each observation, pretend it is future data and measure performance of prediction Instead of defining criteria (\\(\\widehat{\\sigma}^2\\) and \\(R^2\\)) based on \\(\\widehat{\\epsilon_i}^2\\), we can use the prediction error residual \\(\\widehat{\\epsilon}_{i(-i)}\\) Prediction counterparts of \\(SSE\\) and \\(R^2\\): the \\(PRESS\\) and \\(R^2_{pred}\\)\n\n\nCriterion 4. Prediction Sum of Squares (PRESS) Define the \\(PRESS\\) (prediction sum of squares) statistic: PRESS & = & _i=1^n _i(-i)^2= _i=1^n {}^2 Not necessary to re-fit model Accounted for high leverage \\(h_{ii}\\) but not high residual Reasonable to select model with lowest \\(PRESS\\)\nAnother measure, prediction \\(R^2\\): R^2_pred & = & 1 - \\(R^2_{pred}\\) measures ability to predict future responses \\(R^2\\) and \\(R_a^2\\) relate to model’s ability to predict current data\nThis is also known as “leave one out” cross-validation!\n\n\nResidual taxonomy\\(^1\\)\n\n\n\n\n\nCriterion 5. Mallows’ \\(C_p\\) Total variation in prediction \\(\\widehat{Y}\\) is due to bias and variance: _p =_i=1^n { _ + _} \\(\\Gamma_p\\) corresponds to a specific model (with \\(p\\) parameters) Mallows’ \\(C_p\\) is an estimate of \\(\\Gamma_p\\): C_p = _p =(_+_) \\(SSE_p\\): from the model being evaluated, underestimates test error \\(\\widehat{\\sigma}_{full}^2=SSE_{\\scriptscriptstyle full} /(n-p_{\\scriptscriptstyle full})\\): from full (maximum) model Mallows’ \\(C_p\\) strikes a balance between bias and variance Recall: \\(p\\uparrow\\), Var(\\(\\widehat{Y}\\))\\(\\uparrow\\). In \\(C_p\\): when \\(p\\uparrow\\), \\(SSE_p\\downarrow\\) but \\(2p\\widehat{\\sigma}_{full}^2\\uparrow\\) (penalty) The model with the smallest \\(C_p\\) is selected\n\n\nOther Criterion: e.g. AIC and BIC\nRecall Mallows’ \\(C_p\\)\n\\(C_p = \\widehat{\\Gamma}_p =\\frac{1}{n}(\\underbracket{SSE_p}_\\text{training error of current model}+\\underbracket{2p\\widehat{\\sigma}_{full}^2}_\\text{adjustment for underestimation})\\)\nAkaike information criterion (AIC): \\(2\\log \\mathcal{L}+2p\\)\n\\(AIC = \\frac{1}{n{\\color{royalblue}\\widehat{\\sigma}_{full}^2}}(\\underbracket{SSE_p}_\\text{training error of current model}+\\underbracket{2p\\widehat{\\sigma}_{full}^2}_\\text{adjustment for underestimation})+c\\) Equivalent to Mallows’ \\(C_p\\) in MLR with Normal data Bayesian information criterion (BIC)\n\\(BIC = \\frac{1}{n{\\color{royalblue}\\widehat{\\sigma}_{full}^2}}(\\underbracket{SSE_p}_\\text{training error of current model}+\\underbracket{{\\color{royalblue}\\log(n)}p\\widehat{\\sigma}_{full}^2}_\\text{adjustment for underestimation})+c %\\nonumber\\ee\\) Places a heavier penalty on models with large \\(p\\) because \\(\\log(n)&gt;2\\) for any \\(n&gt;7\\)\n\n\nModel Selection for Prediction: Big Picture\n\\(\\bullet~~\\)Model selection methods for the purpose of prediction generally consist of the same essential steps:\nSpecify maximum/full model Specify criterion for comparing competing models how well the model fits current data (training error) \\(R^2,R^2_{adj},\\widehat{\\sigma}^2\\) how well the model predicts future responses (test error) \\(C_p\\), AIC, BIC, etc. Select best (sequence of) model(s) where “best\" is defined by (2) Test error estimation and model validation indirectly estimate test error via an adjustment to the training error to account for overfitting directly estimate test error: cross-validation\n\n\nUse of Criteria: Example True model: \\(Y=1 + 0.1 *X_1 + 0.2*X_2 + 0.01*X_3 + 0.4*X_4 + \\epsilon\\), \\(\\epsilon \\sim N(0,0.2)\\) Suppose true model is unknown, select best predictive model Measures for 3 possible models:\n\n\n\n\n\\(X\\)’s\n\\(R^2\\)\n\\(\\widehat{\\sigma}^2\\)\n\\(R^2_a\\)\n\\(R^2_{pred}\\)\n\\(C_p\\)\n\n\n\n\n2,4\n0.2896\n0.04186\n0.2882\n0.2852\n34.4\n\n\n2,4,1\n0.3123\n0.04057\n0.3102\n0.3065\n3.3\n\n\n2,4,1,3\n0.3125\n0.04059\n0.3098\n0.3055\n5.0\n\n\n\n\nNote that even though \\(X_3\\) is in the true model, its effect is not large enough to offset the ‘costs’ to estimate it\n\n\nStep 4: Model Validation Ultimately, desirable to test final model on external data set or newly collected data Use existing data to develop model Assess model’s predictive ability in new data is a useful alternative\n\n\nStep 4: Model Validation: K-fold cross-validation Split data into \\(K\\) equally sized data sets called \\(\\boldD_1,\\dots,\\boldD_K\\) For \\(j=1,\\dots,K\\), train model on \\(\\boldD_{(-j)}\\) data; test on \\(\\boldD_j\\); Average the measures of predictive ability across the \\(K\\) pieces\n\n\n\n\nimage\n\n\n\n\\(K=n\\): “leave one out” cross-validation, e.g., \\(PRESS\\) statistic \\(K=2\\): “data spiting\": half training and half testing data\n\n\nModel Selection for Prediction: Big Picture\n\\(\\bullet~~\\)Model selection methods for the purpose of prediction generally consist of the same essential steps:\nSpecify maximum/full model\nSpecify criterion for comparing competing models how well the model fits current data (training error) \\(R^2,R^2_{adj},\\widehat{\\sigma}^2\\)\nhow well the model predicts future responses (test error) \\(PRESS,~C_p\\), AIC, BIC Select best (sequence of) model(s) where “best\" is defined by (2) Test error estimation and model validation indirectly estimate test error via an adjustment to the training error to account for overfitting \\(C_p\\), AIC, BIC, etc. directly estimate test error: cross-validation \\(PRESS\\) & k-fold cross validation\n\n\n\n\n\nimage\n\n\n\n\nOptional Readings: True Errors vs. Residuals Claim: \\(||\\widehat{\\boldepsilon} ||^2 \\leq ||{\\boldepsilon} ||^2\\) Proof: & = & (I - )\n& = & (I - ) (+ )\n& = & - + (I - )\n& = & (I - ) Therefore, || ||^2 & = & {(I - ) }^T (I - )\n& = & ^T (I - )\n& = & || ||^2 - ^T Note: \\({\\boldepsilon}^T {\\boldH}{\\boldepsilon}={\\boldepsilon}^T {\\boldH^T\\boldH}{\\boldepsilon} = ({\\boldH}{\\boldepsilon )}^T ({\\boldH}{\\boldepsilon}) ={(\\boldepsilon^*)}^T (\\boldepsilon^*) \\geq 0\\) where \\(\\boldepsilon^* = {\\boldH} \\boldepsilon\\)\n\n\nOptional Readings: Justification of Mallows’ \\(C_p\\) _p &= & { _i=1^n [E(_i - Y_i)]^2 + _i=1^n Var( _i)}\n&= &{_i=1^n [E(_i)]^2 + ^2 p}\nRecall \\(\\frac{SSE_p}{\\sigma^2}\\sim \\chi^2_{n-p,\\lambda}\\) with mean = \\(n-p + \\lambda\\), i.e., \\[E\\left[\\frac{SSE_p}{\\sigma^2}\\right] = (n-p) + \\frac{\\sum_{i=1}^n [E(\\widehat{\\epsilon}_i)]^2}{\\sigma^2}\\]\nThus, we can re-write: _p &= &E- (n-p) + p\n\n\nOptional Readings: Justification of Mallows’ \\(C_p\\) (continued) _p &= &E- (n-p) + p\nMallows’ \\(C_p\\) is a plug-in estimator: C_p _p&=& -(n-p)+ p\n&= &-(n-p)+ p\n& = & p + { -1 }(n-p)\n\n\nQuestions?"
  },
  {
    "objectID": "slides/Module_M_example.html",
    "href": "slides/Module_M_example.html",
    "title": "Module M",
    "section": "",
    "text": "A data set is assembled from (n=17) hospitals in order to construct a model to predict manpower requirements (MANPOWER) as a function of the following factors: average number of patients admitted per day (PAT-DAYS), number of x- rays taken (X-RAYS), total number of days patients spent in bed (BED-DAYS), and average number of days stay per admission (AVG-STAY). One month’s worth of experience is recorded for each hospital.\nRead in the data file (systolic1.csv).\n\n\nrm(list=ls())\ndata = read.csv(file=\"hosp1.csv\",header=T)\nattach(data)\nhead(data)\n\n  X pat_days x_rays bed_days avg_stay manpower\n1 1    15.57   2463   472.92     4.45   566.52\n2 2    44.02   2048  1339.75     6.92   696.82\n3 3    20.42   3940   620.25     4.28  1033.15\n4 4    18.74   6505   568.33     3.90  1603.62\n5 5    49.20   5723  1497.60     5.50  1611.37\n6 6    44.92  11520  1365.83     4.60  1613.27"
  },
  {
    "objectID": "slides/Module_M_example.html#example",
    "href": "slides/Module_M_example.html#example",
    "title": "Module M",
    "section": "",
    "text": "A data set is assembled from (n=17) hospitals in order to construct a model to predict manpower requirements (MANPOWER) as a function of the following factors: average number of patients admitted per day (PAT-DAYS), number of x- rays taken (X-RAYS), total number of days patients spent in bed (BED-DAYS), and average number of days stay per admission (AVG-STAY). One month’s worth of experience is recorded for each hospital.\nRead in the data file (systolic1.csv).\n\n\nrm(list=ls())\ndata = read.csv(file=\"hosp1.csv\",header=T)\nattach(data)\nhead(data)\n\n  X pat_days x_rays bed_days avg_stay manpower\n1 1    15.57   2463   472.92     4.45   566.52\n2 2    44.02   2048  1339.75     6.92   696.82\n3 3    20.42   3940   620.25     4.28  1033.15\n4 4    18.74   6505   568.33     3.90  1603.62\n5 5    49.20   5723  1497.60     5.50  1611.37\n6 6    44.92  11520  1365.83     4.60  1613.27"
  },
  {
    "objectID": "slides/Module_M_example.html#compute-the-correlation-matrix-of-the-four-covariates.",
    "href": "slides/Module_M_example.html#compute-the-correlation-matrix-of-the-four-covariates.",
    "title": "Module M",
    "section": "Compute the correlation matrix of the four covariates.",
    "text": "Compute the correlation matrix of the four covariates.\n\nDo you have any concerns? Which pair-wise correlation appears to be the most problematic?\n\n\ncor(data[c(\"pat_days\", \"x_rays\", \"bed_days\", \"avg_stay\")])\n\n          pat_days    x_rays  bed_days  avg_stay\npat_days 1.0000000 0.9073795 0.9999039 0.6711974\nx_rays   0.9073795 1.0000000 0.9071492 0.4466496\nbed_days 0.9999039 0.9071492 1.0000000 0.6711098\navg_stay 0.6711974 0.4466496 0.6711098 1.0000000\n\n# psych::pairs.panels(data[,\n#     c(\"pat_days\", \"x_rays\", \"bed_days\", \"avg_stay\")])"
  },
  {
    "objectID": "slides/Module_M_example.html#what-are-the-limitations-of-your-assessment-via-bivariate-analysis",
    "href": "slides/Module_M_example.html#what-are-the-limitations-of-your-assessment-via-bivariate-analysis",
    "title": "Module M",
    "section": "What are the limitations of your assessment via bivariate analysis?",
    "text": "What are the limitations of your assessment via bivariate analysis?"
  },
  {
    "objectID": "slides/Module_M_example.html#fit-a-model-containing-each-of-the-afore-listed-factors.-compute-the-vifs.",
    "href": "slides/Module_M_example.html#fit-a-model-containing-each-of-the-afore-listed-factors.-compute-the-vifs.",
    "title": "Module M",
    "section": "Fit a model containing each of the afore-listed factors. Compute the VIFs.",
    "text": "Fit a model containing each of the afore-listed factors. Compute the VIFs.\n\nm = lm(manpower~pat_days+x_rays+bed_days+avg_stay)\ncar::vif(m)\n\n   pat_days      x_rays    bed_days    avg_stay \n5234.696180    7.778477 5210.637545    2.499282 \n\n\n\nWhich covariate has the highest % of its variation explained by other covariates in the mode?"
  },
  {
    "objectID": "slides/Module_M_example.html#re-fit-the-model-without-bed-days.-compare-the-r2-to-that-of-the-original-model.",
    "href": "slides/Module_M_example.html#re-fit-the-model-without-bed-days.-compare-the-r2-to-that-of-the-original-model.",
    "title": "Module M",
    "section": "Re-fit the model without BED-DAYS. Compare the R\\(^2\\) to that of the original model.",
    "text": "Re-fit the model without BED-DAYS. Compare the R\\(^2\\) to that of the original model.\n\nm2 = lm(manpower~pat_days+x_rays+avg_stay)\nc(summary(m)$r.squared, summary(m)$adj.r.squared)\n\n[1] 0.9905451 0.9873935\n\nc(summary(m2)$r.squared, summary(m2)$adj.r.squared)\n\n[1] 0.9894044 0.9869592"
  },
  {
    "objectID": "slides/Module_M_example.html#re-fit-the-model-without-pat-days.-compare-the-r2-to-that-of-the-original-model.",
    "href": "slides/Module_M_example.html#re-fit-the-model-without-pat-days.-compare-the-r2-to-that-of-the-original-model.",
    "title": "Module M",
    "section": "Re-fit the model without PAT-DAYS. Compare the R\\(^2\\) to that of the original model.",
    "text": "Re-fit the model without PAT-DAYS. Compare the R\\(^2\\) to that of the original model.\n\nm3 = lm(manpower~bed_days+x_rays+avg_stay)\nc(summary(m)$r.squared, summary(m)$adj.r.squared)\n\n[1] 0.9905451 0.9873935\n\nc(summary(m2)$r.squared, summary(m2)$adj.r.squared)\n\n[1] 0.9894044 0.9869592\n\nc(summary(m3)$r.squared, summary(m3)$adj.r.squared)\n\n[1] 0.9900682 0.9877763"
  },
  {
    "objectID": "slides/Module_M_example.html#based-on-your-work-thus-far-what-is-a-solution-to-the-collinearity-issue-previously-identified",
    "href": "slides/Module_M_example.html#based-on-your-work-thus-far-what-is-a-solution-to-the-collinearity-issue-previously-identified",
    "title": "Module M",
    "section": "Based on your work thus far, what is a solution to the collinearity issue previously identified?",
    "text": "Based on your work thus far, what is a solution to the collinearity issue previously identified?\n\ncar::vif(m)\n\n   pat_days      x_rays    bed_days    avg_stay \n5234.696180    7.778477 5210.637545    2.499282 \n\ncar::vif(m2)\n\n pat_days    x_rays  avg_stay \n11.321381  7.771393  2.498503 \n\ncar::vif(m3)\n\n bed_days    x_rays  avg_stay \n11.269348  7.737331  2.492904"
  },
  {
    "objectID": "slides/Module_J.html",
    "href": "slides/Module_J.html",
    "title": "Linear Regression",
    "section": "",
    "text": "BIOSTAT 650\nTheory and Application of Linear Regression\nModule J: General Linear Hypothesis\n\n\nOutline\nTopics:\nGLH framework\nTesting equality of regression coefficients\nExamples\n\n\nGeneral Linear Hypothesis (GLH) To date, we have focused on testing hypotheses through the Extra SS principle. e.g.,\n\\({\\boldY }={\\boldX}\\boldbeta+{\\boldepsilon}\\) \\(\\boldbeta=(\\boldbeta_1^T,\\boldbeta_2^T)^T\\)\nAll null hypotheses took the form: \\(H_0:\\boldbeta_2={\\boldzero }\\) \\(F = \\frac{SS(\\boldbeta_2\\mid \\boldbeta_1)/p_2}{\\widehat{\\sigma}_{\\scriptscriptstyle \\text{$1$} } } \\sim F_{p_2,n-p}\\) \\(SS(\\boldbeta_2\\mid \\boldbeta_1)=SSR(\\boldbeta_1,\\boldbeta_2)-SSR(\\boldbeta_1)\\)\nOften, it is of interest to test more general hypotheses: e.g., \\(H_0: \\beta_{1}=\\beta_{2}=\\beta_{3}\\) (recall cell means coding) Using the GLH principle, we have more flexibility to specify hypotheses\n\n\nExample Example: consider a study designed to assess the relationship between resting heart rate (\\(HR\\)) and age (\\(A\\)) among 30-54 year olds.\nin order to avoid assuming that \\(E(HR)\\) is linear in age, the following model is specified: HR_i & = & _0 + _1AG_1i + _2AG_2i +_3AG_3i + _4AG_4i + _i where: AG_1i & = & I(30 A_i )\nAG_2i & = & I(35 A_i )\nAG_3i & = & I(40 A_i )\nAG_4i & = & I(45 A_i )\nAG_5i & = & I(50 A_i )\n\n\nExample HR_i & = & _0 + _1AG_1i + _2AG_2i +_3AG_3i + _4AG_4i + _i\nHowever, the investigator hypothesizes that only four age groups may be necessary, as mean heart rate in the 30-34 and 35-39 age groups are equal This implies the following hypotheses: \\(H_0:\\beta_1=\\beta_2\\), \\(H_1:\\beta_1 \\neq \\beta_2\\)\nThis test can easily be formulated as a linear function of \\(\\beta\\)’s \\(H_0:\\beta_1-\\beta_2=0\\), \\(H_1:\\beta_1 - \\beta_2\\neq 0\\)\n\n\nGLH method\nAny null hypothesis expressed as \\(H_0: {\\boldT}{\\boldbeta}={\\boldcc}\\) is a GLH Entries in \\({\\boldT}\\) and \\({\\boldcc}\\) are known constants \\({\\boldT}\\) is often referred to as the contrast matrix\nMain idea of GLH: Find \\({\\boldT\\ktimesp}\\) s.t. the linear functions \\({\\boldT\\ktimesp}{\\boldbeta\\ptimesone}=\\boldcc\\) gives the stated \\(H_0\\) \\(k\\) = number of equations simultaneously being tested \\(p\\) = dimension of \\(\\boldbeta\\) \\({\\boldT\\ktimesp}\\) is needed for constructing a test statistic\nIn the example \\(H_0:\\beta_1=\\beta_2\\), \\(H_1:\\beta_1 \\neq \\beta_2\\) equivalent to \\(H_0:\\beta_1-\\beta_2=0\\), \\(H_1:\\beta_1 - \\beta_2\\neq 0\\) \\({\\boldT\\onetimesp}=[\\begin{array}{ccccc}0 & 1 & -1 & 0 & 0  \\end{array}]\\), s.t. \\({\\boldT}{\\boldbeta}=\\beta_1-\\beta_2\\) and \\(\\boldcc=0\\)\n\n\nConstructing Test Statistic \\(H_0:\\boldT\\ktimesp\\boldbeta\\ptimesone=\\boldcc\\ktimesone\\) vs \\(H_1:\\boldT\\ktimesp\\boldbeta\\ptimesone\\neq \\boldcc\\ktimesone\\) F-test for GLH \\[\\boxed{F=\\frac{ ({\\boldT}\\widehat{\\boldbeta}-{\\boldcc})^T \\{{\\boldT}({\\boldX}^T{\\bf\n        X})^{-1}{\\boldT}^T\\}^{-1} ({\\boldT}\\widehat{\\boldbeta}-{\\boldcc})/rank(\\boldT) }{ \\widehat{\\sigma}^2_{\\scriptscriptstyle \\text{$1$}}}\n\\stackrel{H_0}{\\sim}  F_{rank({\\bf\n        T}),n-p}\n}\\] For a well defined \\(H_0\\), \\(rank(\\boldT)\\) = number of rows in \\(\\boldT\\), i.e., how many equations (at most \\(p\\)) are we simultaneously testing\n\\(\\widehat{\\sigma}^2_{\\scriptscriptstyle \\text{$1$}}\\): obtained from the unrestricted model, i.e., under \\(H_1\\)\nDerivation provided at the end of the slides Rationale: \\(\\underbrace{\\boldT \\widehat{\\boldbeta}-\\boldcc}_\\text{denoted as $\\boldZ$} \\stackrel{H_0}{\\sim} N\\{ \\;\\boldzero,\\;\\; \\underbrace{\\sigma^2 \\boldT \\xtx^{-1} \\boldT^T}_\\text{denoted as $\\boldSigma$} \\;\\}\\) Under the null, \\(\\boldZ^\\top \\boldSigma^{-1} \\boldZ\\) should be small\n\n\nExample 1 Suppose the investigators want to examine whether the two lowest and two highest age intervals could be combined HR_i & = & _0 + _1AG_1i + _2AG_2i +_3AG_3i + _4AG_4i + _i\nthe pertinent hypotheses can be listed as: \\(H_0: \\beta_1=\\beta_2\\), \\(\\beta_4=0\\) \\(H_1: \\beta_1 \\neq \\beta_2\\), or \\(\\beta_4 \\neq 0\\) re-casting \\(H_0\\) in terms of the GLH, _ & = &\n\\(H_0: {\\boldT_{\\!\\sf\\scriptscriptstyle{2\\times 5}}}\\boldbeta_{\\!\\sf\\scriptscriptstyle{5\\times1}}={\\boldzero }_2\\) vs. \\(H_1: {\\boldT}\\boldbeta\\neq {\\boldzero }_2\\) test statistic: F = ^2 F_2,n-5\n\n\nExample 2 \\(n=26\\) subjects selected to participate in a study designed to examine the effects of weight and activity level on cholesterol levels (HDL)\nSubjects were placed into one of three groups: Group 1: control (n=8) Group 2: running (n=8) Group 3: running and weight training (n=10) Response: HDL (mg/dl) Covariates: group, weight (lbs) Does the association between weight and HDL differ by group?\n\n\nExample 2, Stratified SLR models It is common in practice to fit “stratified” models: Group 1: \\(HDL_i, = \\beta_1 + \\beta_4 W_i + \\epsilon_i\\) Group 2: \\(HDL_i = \\beta_2 + \\beta_5W_i + \\epsilon_i\\) Group 3: \\(HDL_i = \\beta_3 + \\beta_6W_i + \\epsilon_i\\) Models are fitted separately to “three datasets\" (each group is treated as a separate dataset).\nSince there are three distinct regression lines, we have distinct slopes\ndistinct intercepts\n\n\nExample 2 (From 3 SLR to 1 MLR) separate SLR models can be fitted on three datasets\nUltimately we wish to see if the set of models can be simplified, i.e., are the 3 slopes the same? if the slopes are the same, are the intercepts also the same? Fitting models separately does not readily allow us to test differences in parameters\nWe can represent the previous set of group-specific models in a single, more compact MLR model: \\({\\boldY } = {\\boldX}\\boldbeta + {\\boldepsilon}\\) where \\(\\boldbeta^T=(\\beta_1,\\ldots,\\beta_6)\\). What should \\({\\boldX}\\) be?\n\n\nExample 2: From Group-specific SLRs to MLR Group 1: \\({\\boldY }_1 = {\\boldX}_1\\boldbeta_1 + {\\boldepsilon}_1\\) = +\nGroup 2: \\({\\boldY }_2 = {\\boldX}_2\\boldbeta_2 + {\\boldepsilon}_2\\) = +\nGroup 3: \\({\\boldY }_3 = {\\boldX}_3\\boldbeta_3 + {\\boldepsilon}_3\\) = +\n\n\nExample 2: From Group-specific SLRs to MLR Group 1: \\({\\boldY }_1 = {\\boldX}_1\\boldbeta_1 + {\\boldepsilon}_1\\) = +\nGroup 2: \\({\\boldY }_2 = {\\boldX}_2\\boldbeta_2 + {\\boldepsilon}_2\\) = +\nGroup 3: \\({\\boldY }_3 = {\\boldX}_3\\boldbeta_3 + {\\boldepsilon}_3\\) = +\n\n\nExample 2 (MLR model) Using the typical MLR \\({\\boldY } = {\\boldX}\\boldbeta + {\\bf \\epsilon}\\), where = = =\n= \\(HDL=\\beta_1G_{1}+\\beta_2G_{2}+\\beta_3G_{3}+\\beta_4G_{1}W+\\beta_5G_{2}W+\\beta_6G_{3}W+\\epsilon\\), where \\(G_j=I(\\text{group } j),j=1,\\dots,3\\) and \\(W=(W_1,\\dots,W_{26})\\) Note: the above \\(\\boldX\\) spans, but does not contain, an intercept\n\n\nExample 2: parallelism \\(HDL=\\underbrace{\\left(\\beta_1G_{1}+\\beta_2G_{2}+\\beta_3G_{3}\\right)}_\\text{intercept}+\\underbrace{\\left(\\beta_4G_{1}+\\beta_5G_{2}+\\beta_6G_{3}\\right)}_\\text{slope}W+\\epsilon\\) Using the GLH approach, we can test whether the slopes of the group-specific regression lines are equal \\(H_0: \\beta_4=\\beta_5=\\beta_6\\) (lines are parallel),\n\\(H_1:\\) at least one of \\(\\beta_j\\), \\(j=4,5,6\\), is different from the others\n(at least one line is not parallel to the others) : \\(H_0: \\beta_4-\\beta_5=0\\) and \\(\\beta_5-\\beta_6=0\\),\nEquivalently: \\(H_1:\\beta_4 - \\beta_5 \\neq 0\\) or \\(\\beta_5 - \\beta_6 \\neq 0\\) Note: third contrast is redundant Re-framing \\(H_0\\) in terms of the GLH, = \\(H_0: {\\boldT}\\boldbeta={\\boldzero }_2\\) vs. \\(H_1: {\\boldT}\\boldbeta\\neq {\\boldzero }_2\\)\n\n\nExample 2: parallelism Test statistic: F = & & F_2,20 see R code and output: \\(F=2.95\\) \\(p=P(F_{2,20}&gt;2.95)=0.075\\)\nThis data is consistent with the parallelism hypothesis (null).\nWe have insufficient evidence to conclude the associations between weight and HDL differ across exercise groups (p=0.075).\n\n\n\n\n\n\n\n\n\n\n\nHome Exercise: equality of intercepts\n\\(HDL=\\underbrace{\\left(\\beta_1G_{1}+\\beta_2G_{2}+\\beta_3G_{3}\\right)}_\\text{intercept}+\\underbrace{\\left(\\beta_4G_{1}+\\beta_5G_{2}+\\beta_6G_{3}\\right)}_\\text{slope}W+\\epsilon\\) We can also test the equality of intercepts of the group-specific regression lines \\(H_0: \\beta_1=\\beta_2=\\beta_3\\) Re-write hypotheses find \\({\\boldT}\\) in GLH \\(H_0: {\\boldT}\\boldbeta= {\\boldzero }_2\\) vs. \\(H_1: {\\boldT}\\boldbeta\\neq {\\boldzero }_2\\) =\nTest statistic:\n\\(F=~~~~~~~~~~~~~\\stackrel{H_0}{\\sim} F_{2,20}\\) \\(p=P(F_{2,20}&gt;~~~~~~~~~~~)=\\)\n\n\n\n\n\nHome Exercise Answer Key: equality of intercepts\n\\(HDL=\\underbrace{\\left(\\beta_1G_{1}+\\beta_2G_{2}+\\beta_3G_{3}\\right)}_\\text{intercept}+\\underbrace{\\left(\\beta_4G_{1}+\\beta_5G_{2}+\\beta_6G_{3}\\right)}_\\text{slope}W+\\epsilon\\) We can also test the equality of intercepts of the group-specific regression lines \\(H_0: \\beta_1=\\beta_2=\\beta_3\\) Re-write hypotheses \\(H_0: \\beta_1-\\beta_2=0, \\beta_2-\\beta_3=0\\)\n(many different ways, e.g.,) \\(H_0: \\beta_3-\\beta_1=0, \\beta_2-\\beta_1=0\\) find \\({\\boldT}\\) in GLH \\(H_0: {\\boldT}\\boldbeta= {\\boldzero }_2\\) vs. \\(H_1: {\\boldT}\\boldbeta\\neq {\\boldzero }_2\\) =\nTest statistic:\n\\(F=~~~3.12~~~~\\stackrel{H_0}{\\sim} F_{2,20}\\) \\(p=P(F_{2,20}&gt;~~~3.12~~~~)=0.066\\)\n\n\nExample 2: Connection to reference cell coding and SS \\(\\boxed{HDL=\\underbrace{\\left(\\beta_1G_{1}+\\beta_2G_{2}+\\beta_3G_{3}\\right)}_\\text{intercept}+\\underbrace{\\left(\\beta_4G_{1}+\\beta_5G_{2}+\\beta_6G_{3}\\right)}_\\text{slope}W+\\epsilon}\\) Cell means coding: \\(H_0: \\underbrace{\\beta_5-\\beta_4}_\\text{2 vs 1}=0; \\underbrace{\\beta_6-\\beta_4}_\\text{3 vs 1}=0\\) (GLH) Re-parameterize the model: Reference cell coding: directly parameterize \\(\\beta_5-\\beta_4\\) and \\(\\beta_6-\\beta_4\\) Slope of weight for each group: \\(H_0:\\) Extra SS test: \\(F~~=~~ \\frac{SS(~~~~~~~~~~~~~\\mid ~~~~~~~~~~~~~)}{\\widehat{\\sigma}^2_{\\scriptstyle 1}}\\stackrel{H_0}{\\sim} F_{~~~~,~~~~}\\)\n\n\nExample 2: Extra SS Test of interactions \\(HDL=\\gamma_1+\\gamma_2G_{2}+\\gamma_3G_{3}+\\gamma_4W+\\gamma_5G_{2}W+\\gamma_6G_{3}W+\\epsilon\\) Test the equality of slopes using extra SS & reference cell coding\nEquality of slopes: are interactions significant given that main effects are in the model?\nExtra SS test for equal slopes: F& = &   =  2.95     F_  ,  \ni.e., same result as GLH test of equal slopes\n\n\n\n\n\nExample 2: Extra SS Test of interactions\n\\(HDL=\\gamma_1+\\gamma_2G_{2}+\\gamma_3G_{3}+\\gamma_4W+\\gamma_5G_{2}W+\\gamma_6G_{3}W+\\epsilon\\) Test the equality of slopes using extra SS & reference cell coding\nEquality of slopes: are interactions significant given that main effects are in the model?\nExtra SS test for equal slopes: F   =      =   2.95     F_2,20\ni.e., same result as GLH test of equal slopes\n\n\nHome Exercise: Extra SS Test of Intercepts\n\\(HDL=\\gamma_1+\\gamma_2G_{2}+\\gamma_3G_{3}+\\gamma_4W+\\gamma_5G_{2}W+\\gamma_6G_{3}W+\\epsilon\\) Test the equality of intercepts using extra SS & reference cell coding\nEquality of slopes: are intercepts significant given that slopes are different?\nExtra SS test for equal intercepts: F & = &   =   3.12    F_  ,  \ni.e., same result as GLH test of equal intercepts Note: GLH gives partial test of intercepts, assuming slopes are different (i.e., assuming group x weight interactions are in the model).\n\n\n\n\n\nHome Exercise Answer Key: Extra SS Test of Intercepts\n\\(HDL=\\gamma_1+\\gamma_2G_{2}+\\gamma_3G_{3}+\\gamma_4W+\\gamma_5G_{2}W+\\gamma_6G_{3}W+\\epsilon\\) Test the equality of intercepts using extra SS & reference cell coding\nEquality of slopes: are intercepts significant given that slopes are different?\nExtra SS test for equal intercepts: F & = &   =   3.12  ~  F_2,20\ni.e., same result as GLH test of equal intercepts Note: GLH gives partial test of intercepts, assuming slopes are different (i.e., assuming group x weight interactions are in the model).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstructing Test Statistic (proof)\nRecall: \\(\\widehat{\\boldbeta} \\sim N(\\boldbeta, \\sigma^2 \\xtx^{-1} )\\)\ntherefore, \\(\\boldT \\widehat{\\boldbeta} \\sim N\\{ \\;E[\\boldT \\widehat{\\boldbeta}],\\;\\; Var(\\boldT \\widehat{\\boldbeta}) \\;\\}\\), where E[] & = &\nVar()& & Var() ^T\n& = & ^2 ^-1 ^T\nUnder the null hypothesis \\(E[\\boldT \\widehat{\\boldbeta} - {\\boldcc}] =\\boldT \\boldbeta - {\\boldcc}= {\\boldzero }\\)\nTherefore, \\(\\underbrace{\\boldT \\widehat{\\boldbeta}-\\boldcc}_\\text{$\\boldY$} \\stackrel{H_0}{\\sim} N\\{ \\;\\boldzero,\\;\\; \\underbrace{\\sigma^2 \\boldT \\xtx^{-1} \\boldT^T}_\\text{$\\boldSigma$} \\;\\}\\)\nApplying Results 1-2 in the footnote \\[\\boxed{\n    (\\boldT \\widehat{\\boldbeta}-{\\boldcc} )^T \\{\\sigma^2 \\boldT \\xtx^{-1} \\boldT^T \\}^{-1}   (\\boldT \\widehat{\\boldbeta}-{\\boldcc}) \\sim \\chi^2_{\\scriptstyle \\text{rank($\\boldT$), $\\lambda=0$}}\n}\\]\n\n\nConstructing Test Statistic (proof continued) We have \\((\\boldT \\widehat{\\boldbeta}-{\\boldcc} )^T \\{\\sigma^2 \\boldT \\xtx^{-1} \\boldT^T \\}^{-1} (\\boldT \\widehat{\\boldbeta}-{\\boldcc}) \\stackrel{H_0}{\\sim} \\chi^2_{\\scriptstyle \\text{rank($\\boldT$), $\\lambda=0$}}\\) \\(\\frac{SSE}{\\sigma^2 } \\sim \\chi^2_{\\text{$n\\!-\\!p,\\lambda \\stackrel{\\scriptscriptstyle \\text{LINE}}{=}0$} }\\) and \\(\\widehat{\\boldbeta}\\ind SSE\\) (Module F)\nTo derive a test statistic, we take their ratio (divide each by df)\nF & = & (SSE/^2)/(n-p)\n& = &\n& & F_rank( T),n-p\n\n\nQuestions?\n\n\nGroup project Group assignments available on Canvas “People\" page Presentation on December 6th and 8th Presentation order randomly assigned. Please email us if you are not in the US time zone and have time constraints Project report due at 11:59pm on December 10th Sample project report\n\n\nConfounding Definition: a pre-exposure variable that, conditional on other covariates, is associated with outcome conditional on exposure associated with exposure/treatment/POI\n\n\n0.5\n\n0.5\n\n\nWe adjust for confounders when studying the association between the predictor of interest (POI) and the outcome Crude treatment effect is biased unless treatment is randomized\n\n\nGroup project\nDetermine question of interest (X and Y) via literature review hypothesis generation via e.g., model selection is not necessary model selection for inference questions is not recommended\nDetermine confounders to adjust via literature review"
  },
  {
    "objectID": "slides/Module_K.html",
    "href": "slides/Module_K.html",
    "title": "Linear Regression",
    "section": "",
    "text": "BIOSTAT 650\nTheory and Application of Linear Regression\nModule K: Model Diagnosis\n\n\nOutline\nTopics:\nPurpose Residuals and Properties of residuals Review of model assumptions Impact of violating assumptions Use of residuals in model diagnostics Correction strategies Outliers Examples\nMPV 5th edition: Chapter 4 and 5\n\n\nRegression Diagnostics Regression diagnostics includes assessment of: Model assumptions (LINE) Influential observations/outliers Multicollinearity\n\n\nLinear Model: Assumptions\nModel: \\({\\boldY} = \\boldX\\boldbeta + \\boldepsilon\\) Assumptions: \\({\\boldY}\\sim N(\\boldX\\boldbeta,\\sigma^2{\\boldI})\\)\nLinearity: \\(E[Y_i]=\\beta_0 + \\beta_1X_{i1} + \\ldots + \\beta_{p-1}X_{i,p-1}\\) Linearity w.r.t. \\(\\beta\\)’s Functional form of \\(\\boldX\\): often the focus of diagnostics for linearity Independence: \\(\\epsilon_i \\perp \\epsilon_j\\), for \\(i\\neq j\\) Normality: \\(\\epsilon_i\\sim N(0,\\sigma^2)\\) Equal variance (homogeneity): \\(Var(\\epsilon_i)=\\sigma^2\\), for all \\(i\\) Assumptions mainly about \\(\\epsilon_i\\) and will be checked using residuals Since we cannot observe the true \\(\\epsilon_i\\)’s, we infer their behavior via the \\(\\widehat{\\epsilon}_i\\)’s Residual is a measure of the variability of the outcome not explained by the regression model\n\n\nViolation of Assumptions Possible consequences of assumptions being violated: \\(\\widehat{\\beta}_j\\) biased \\(SE(\\widehat{\\beta}_j)\\) inaccurate CI’s for \\(\\beta_j\\) inaccurate hypothesis tests invalid Impact depends on which assumption is violated and the extent\n\n\nRegression Diagnostics: Objectives Identify violation of model assumptions Identify observations that may have too much influence on the results Understand the impact of violating the assumption e.g., impact on confidence intervals or coefficients Assumptions will never hold perfectly: degree of validity of model results depends on degree to which assumptions hold\nBe able to suggest potential corrective action e.g., transformation of response\nRegression diagnostics can involve an “iterative process” because violation of one assumption can mask a violation of another assumption\n\n\nTypes of Residuals\n\n\nConventional Residuals: Properties (review) Assume: \\(\\boldepsilon\\sim N({\\boldzero},\\sigma^2{\\boldI})\\) Errors: \\(\\boldepsilon={\\boldY}-E[{\\boldY}]\\) Residuals: & = & -\n& = & -\n& = & (-)\n\nE[] & = & (-)E[]\n& = & (-)\n& = & -\n& = & - (^T)^-1^T\n& = &\n\n\nResiduals: Properties (continued) Var() & = & Var({-} )\n& = & (-) ^2 (-)^T\n& = & ^2(-)^2\n& = & ^2(-) Therefore, \\(\\widehat{\\boldepsilon}\\sim N({\\boldzero},\\sigma^2\\{{\\boldI}-{\\boldH}\\})\\), since \\(\\widehat{\\boldepsilon}\\) is a linear combination of Normals\n\n\nResiduals: Properties (cont’d) Therefore, if \\(\\boldepsilon \\sim\\) Normal, then \\(\\widehat{\\boldepsilon}\\sim\\) Normal if \\(E[{\\boldepsilon}]={\\boldzero}\\), then \\(E[\\widehat{\\boldepsilon}]={\\boldzero}\\) even if \\(\\{\\epsilon_1,\\ldots,\\epsilon_n\\}\\) are mutually independent,\n\\(\\{\\widehat{\\epsilon}_1,\\ldots,\\widehat{\\epsilon}_n\\}\\) are not independent cov\\((\\widehat{\\epsilon}_i,\\widehat{\\epsilon}_j)=-h_{ij}\\sigma^2\\) because \\(Var(\\widehat{\\boldepsilon})=\\sigma^2({\\boldI}-{\\boldH})\\) even if \\(Var(\\epsilon_i)=\\sigma^2\\) for all \\(i\\), \\(Var(\\widehat{\\epsilon}_i)\\neq \\sigma^2\\) for any \\(i\\) in fact \\(Var(\\widehat{\\epsilon}_i)=(1-h_{ii})\\sigma^2\\leq\\sigma^2\\) (\\(h_{ii}\\geq0\\) since \\(\\boldH\\) is positive definite)\n\n\nStandardized Residuals Standardized residuals (imperfect): _i & = & Rationale: \\(\\widehat{\\epsilon}_i\\)’s are unit-dependent (e.g., changing units of \\(Y_i\\) will change \\(\\widehat{\\epsilon}_i\\))\nStandardizing with \\(\\widehat{\\sigma}\\) is very easy to do.\nPatterns in \\(\\widehat{z}_i\\)’s will be the same as \\(\\widehat{\\epsilon}_i\\)’s, but easier to judge the magnitude of \\(\\widehat{\\epsilon}_i\\) because it is “standardized”\nMore meaningful to judge the magnitude of \\(\\widehat{\\epsilon}_i\\) relative to its standard deviation\n\n\nStandardized Residuals (continued)\n\\(\\widehat{z}_i\\)’s have mean \\(\\approx 0\\) and variance \\(\\approx 1\\)\nIf model assumptions are correct, \\(\\widehat{z}_i\\stackrel{\\cdot}{\\sim} N(0,1)\\).\nTherefore \\(\\widehat{z}_i\\)’s are more readily interpreted: P(|_i| ) & & 0.67\nP(|_i| ) & & 0.95\nP(|_i| ) & & 0.99 Advantage of \\(\\widehat{z}\\) over \\(\\widehat{\\epsilon}\\) is that we have a “rule of thumb\" to identify large value: \\(|\\widehat{z}_i| \\geq 2.5\\)\nHistogram, Q-Q (quantile-to-quantile) plot, box/whisker plot will have same pattern as for \\(\\widehat{\\epsilon}_i\\)’s (to be introduced soon)\n\n\nInternally Studentized Residuals Recall: \\(\\widehat{\\sigma}^2(1-h_{ii})\\), not \\(\\widehat{\\sigma}^2\\), is an unbiased estimator of \\(Var(\\widehat{\\epsilon}_i)\\) Internally studentized residuals: _i & = & where \\(h_{ii}=\\text{diag}(\\boldH)_i=\\) leverage for \\(i\\)’th individual (the \\(i\\)-th element of the diagonal of the hat matrix)\n\\(h_{ii}=\\boldX_i^T (\\boldX^T\\boldX)^{-1} \\boldX_i\\); Special case: in SLR h_ii & = & +\nReflects outlyingness in \\(\\boldX\\) space\n\\(E[\\widehat{r}_i]= 0\\), \\(Var(\\widehat{r}_i)\\approx 1\\), \\(\\widehat{r}_i\\sim t_{df SSE}\\). Note: when \\(df SSE\\) (or sample size) is large: \\(t_{df SSE}\\approx N(0,1)\\), \\(|t_{df SSE, 0.025}|\\approx 2\\)\n\n\nExternally Studentized Residuals\nExternally studentized residuals: _(-i) & = & \\(\\widehat{\\sigma}_{(-i)}\\) is an estimator of \\(\\sigma^2\\), with \\(i\\)’th individual deleted\nRationale:\nif \\(|\\widehat{\\epsilon}_i|\\) is large, \\(\\widehat{\\sigma}^2\\) may over-estimate \\(\\sigma^2\\) \\(\\widehat{\\epsilon}_i\\) can serve to mask its own outlyingness by inflating \\(\\widehat{\\sigma}^2\\) hence remove influence of \\(\\widehat{\\epsilon}_i\\) on \\(\\widehat{\\sigma}^2\\) by removing the data point\n\\(E[\\widehat{r}_{(-i)}]= 0\\), \\(Var(\\widehat{r}_{(-i)})\\approx 1\\), \\(\\widehat{r}_{(-i)}\\sim t_{dfSSE-1}\\)\nMost sensitive residuals for detecting outliers\nAlso known as “leave-one-out/Jackknife” residuals\n\n\nComparing Residual Types\nFor large samples, \\(\\widehat{\\epsilon}_i\\), \\(\\widehat{z}_i\\), \\(\\widehat{r}_i\\), \\(\\widehat{r}_{(-i)}\\) tend to provide similar information\nFor large samples, \\(\\widehat{r}_{(-i)}\\), \\(\\widehat{r}_i\\), \\(\\widehat{z}_i\\) follow, approximately, a standard normal distribution For small to moderate samples, a rank, in decreasing order of sensitivity: \\(\\widehat{r}_{(-i)}\\) \\(\\widehat{r}_i\\) \\(\\widehat{z}_i\\) \\(\\widehat{\\epsilon}_i\\)\n\n\nChecking functional form of continuous predictor (linearity)\n\n\nFunctional form of continuous predictor Continuous covariates may have nonlinear functional form of relationship with the outcome e.g. SBP vs age Several approaches to check Approach 1: Partial regression plots Approach 2: Categorizing predictor Approach 3: Testing and smoothing approaches (future lecture)\n\n\nResidual vs. covariate plots in SLR and MLR\nIn MLR: conditional association; in SLR, marginal=conditional. In simple linear regression, can plot \\(\\widehat{\\epsilon}\\) versus \\(X\\) (or \\(\\widehat{Y}\\)) to assess linearity of \\((X,Y)\\) relationship can detect departures from linearity, and nature of non-linear relationships If the current functional form has captured the effect of \\(X\\), then \\(\\widehat{\\epsilon}_i\\)’s should be randomly scattered and show no trend with \\(X\\) For multiple linear regression, situation is more complicated Plot of \\(\\widehat{\\epsilon}\\) versus \\(X_k\\) can be impacted other covariates\" If \\(X_k\\) is correlated with \\(X_j\\), and \\(X_j\\) has a non-linear association with \\(Y\\), then plot of \\(\\widehat{\\boldepsilon}\\) vs \\(X_k\\) will partially reflect non-linear association of \\(Y\\) and \\(X_j\\). Solution: Partial regression plots\n\n\nApproach 1: Partial Regression Plots Design matrix \\(\\boldX=[{\\boldone}_n, X_1,\\ldots, X_p]\\) \\(\\boldX_{-k}\\) contains all covariates, except \\(X_{k}\\)\nRegress \\({\\boldY}\\) on \\(\\boldX_{-k}\\) and get residual \\(\\widehat{\\boldepsilon}({\\boldY}|\\boldX_{-k})\\) \\(\\boldY=\\boldX_{-k}\\alpha%_{\\scriptscriptstyle {\\boldY}|\\boldX_{-k} }  +\\epsilon({\\boldY}|\\boldX_{-k})\\) Regress \\(X_{k}\\) on \\(\\boldX_{-k}\\) and get residual \\(\\widehat{\\boldepsilon}(\\boldX_{k}|\\boldX_{-k})\\) \\(X_{k}=\\boldX_{-k}  \\boldbeta%_{\\scriptscriptstyle\\boldX_{k}|\\boldX_{-k}}  +\\epsilon(\\boldX_{k} |\\boldX_{-k})\\) Plot \\(\\widehat{\\boldepsilon}(X_{k}|\\boldX_{-k})\\) vs. \\(\\widehat{\\boldepsilon}({\\boldY}|\\boldX_{-k})\\) both sets of residuals are covariate-adjusted (and hence model dependent): remove information/contribution from \\(\\boldX_{-k}\\) if linear, then shows the adjusted effect of \\(X_k\\); if nonlinear, then suggests a functional form of \\(X_k\\) (see also partial residual plot) can also help to spot outliers (extreme residuals in the Y direction) and “leverage points” (extreme residuals in the X direction)\n\n\n\n\nimage\n\n\n\n\n\nPartial Regression Plots–Example Example: Suppose we want to study association between Sytolic blood pressure (SBP) and covariates Age (\\(A_{i}\\)), Quetlet intex (\\(Q_i\\)), and smoking \\(S_{i}\\). \\(SBP_i = \\beta_0 + \\beta_A A_i + \\beta_Q Q_i + \\beta_S S_i + \\epsilon_i\\)\nWe want to assess functional relationship between \\(SBP\\) and \\(Age\\) Fit first model: \\(SBP_i = \\gamma_0 + \\gamma_1 Q_{i} + \\gamma_3 S_i +\\epsilon_{i(SBP|Q,S)}\\)\nobtain residuals, \\(\\widehat{\\epsilon}_{i(SBP|Q,S)}\\)’s Fit second model: \\(A_{i} = \\alpha_0 + \\alpha_1 Q_{i} + \\alpha_2 S_i +\\epsilon_{i(Age|Q,S)}\\) obtain residuals \\(\\widehat{\\epsilon}_{i(A|Q,S)}\\)’s Plot \\(\\widehat{\\epsilon}_{i(SBP|Q,S)}\\) vs. \\(\\widehat{\\epsilon}_{i(Age|Q,S)}\\) pattern in plot describes association between \\(Age\\) and \\(SBP\\), adjusting for \\(Q\\) and \\(S\\)\n\n\nApproach 2: Categorize Continuous Predictor\nReplace continuous with categorical covariates, then examine pattern in coefficients of indicator variables Functional form of indicator variables can’t be misspecified e.g., blood pressure data: \\(SBP_i = \\beta_0 + \\beta_A A_i + \\beta_Q Q_i + \\beta_S S_i + \\epsilon_i\\) We want to assess functional relationship between \\(SBP\\) and \\(Age\\) Eliminate the linearity assumption by treating age as categorical: Break age into groups containing (approximately) equal numbers of subjects and set up indicator covariates: AG_1i & = & I(41 A_i &lt; 48)\nAG_2i & = & I(48 A_i &lt; 54)\nAG_3i & = & I(54 A_i &lt; 59)\nAG_4i & = & I(59 A_i)\n\n\nCategorizing approach, continued (1) Revised model: E[SBP_i] & = & _0 + _A2AG_2i + _A3AG_3i +_A4AG_4i + _Q Q_i + _S S_i Now, plot \\(\\widehat{\\beta}_{A1}\\), \\(\\widehat{\\beta}_{A2}\\) \\(\\dots\\), \\(\\widehat{\\beta}_{A4}\\) against group-specific medians set \\(\\widehat{\\beta}_{A1}\\equiv0\\); \\(\\widehat{\\beta}_{A1}\\) \\(\\dots\\), \\(\\widehat{\\beta}_{A4}\\) are adjusted group mean differences Plot reflects true nature of association between \\(A_i\\) and \\(E[SBP_i]\\) linear? quadratic? U-shaped? threshold? If plot approximately linear: evidence in favor of linearity assumption\n\n\nViolation of Linearity Assumption If linearity fails \\(\\ldots\\) \\(\\widehat{\\boldbeta}\\): \\(\\widehat{\\boldY}\\): \\(\\widehat{\\sigma}^2\\): \\(\\widehat{Var}(\\widehat{\\boldbeta})\\): confidence intervals: hypothesis tests:\nPossible solutions: modify the model \\(\\ldots\\) additional covariates transform \\({\\boldY}\\); e.g., \\(log({\\boldY})\\), \\(\\sqrt{{\\boldY}}\\) transform certain \\(\\boldX_k\\)’s\n\n\nViolation of Linearity Assumption If linearity fails \\(\\ldots\\) \\(\\widehat{\\boldbeta}\\): biased \\(\\widehat{\\boldY}\\): biased \\(\\widehat{\\sigma}^2\\): biased \\(\\widehat{Var}(\\widehat{\\boldbeta})\\): biased confidence intervals: invalid hypothesis tests: wrong inference\nPossible solutions: modify the model additional covariates transform certain \\(\\boldX_k\\)’s based on the plots transform \\({\\boldY}\\); e.g., \\(log({\\boldY})\\), \\(\\sqrt{{\\boldY}}\\)\n\n\nIndependence Assumption\n\n\nIndependence Assumption Independence: (multivariate normal + ) \\(E[\\epsilon_i \\epsilon_j]=0\\) for \\(i\\neq j\\)\nimplies: \\(Y_i\\) independent of \\(Y_j\\), \\(i\\neq j\\)\nto assess validity of independence assumption, careful consideration of study design is essential\nviolations of independence often clear from sampling scheme\ne.g., if \\(Y_i\\)’s are time-ordered, may be serial correlation; \\(E[\\epsilon_i \\epsilon_{i+1}]\\neq 0\\)\ne.g., subjects in a study are clustered\ne.g., response measured repeatedly on each subject\nNote: recall that residuals are never independent\n\n\nIndependence Assumption: Autocorrelation : errors are correlated (sequenced) often occurs when response is measured over time \\(\\mbox{cov}(\\epsilon_i \\epsilon_{i+\\ell})\\neq 0\\), for some \\(\\ell\\) (lag) e.g., lag-2 autocorrelation: \\(E[\\epsilon_i \\epsilon_{i+1}]\\neq 0\\) \\(E[\\epsilon_i \\epsilon_{i+2}]\\neq 0\\) \\(E[\\epsilon_i \\epsilon_{i+k}]=0\\), for \\(k&gt;2\\)\n\n\nIndependence: Detecting Autocorrelation\nplot \\(\\widehat{\\epsilon}_i\\)’s versus \\(i\\) (index) positive autocorrelation: adjacent residuals have same sign negative autocorrelation: sign of residuals alternates Note: only if index \\(i\\) has a meaningful order, e.g. time plot \\(\\widehat{\\epsilon}_i\\) vs. \\(\\widehat{\\epsilon}_{i-1}\\) positive correlation: positive trend negative correlation: negative trend\n\n\nAutocorrelation: Durbin-Watson Test (Chap 14) consider: departure from independence in form of lag 1 autocorrelation (first order) \\(\\mbox{corr}(\\epsilon_i, \\epsilon_{i+1})=\\rho\\), \\(i=1,\\ldots,n-1\\) Durbin-Watson Test: hypotheses: \\(H_0: \\rho=0\\) vs. \\(H_1:\\rho &gt; 0\\) test statistic: DW & = & (1-) 2 Distribution depends on \\(\\boldX\\), but bounds are available (Table A.6) reject \\(H_0\\) if \\(DW &lt; DW_L\\) fail to reject \\(H_0\\) if \\(DW &gt; DW_U\\) inconclusive if \\(DW_L \\leq DW \\leq DW_U\\) to test \\(H_0\\) vs. \\(H_1:\\rho &lt;0\\), use \\((4-DW)\\) as test statistic\n\n\nImpact of Lack of Independence if \\(e_i\\) and \\(e_j\\) are correlated (\\(i\\neq j\\)): \\(\\widehat{\\boldbeta}:\\) \\(\\widehat{\\sigma}^2:\\) \\(Var({\\widehat{\\boldbeta}}):\\) CI’s, hypothesis tests:\n\n\nImpact of Lack of Independence if \\(e_i\\) and \\(e_j\\) are correlated (\\(i\\neq j\\)): \\(\\widehat{\\boldbeta}:\\) unbiased, normally distributed \\(\\widehat{\\sigma}^2:\\) biased The conclusion that \\(E[\\widehat{\\sigma}^2]=\\sigma^2\\) depends on independence (\\(n\\) is no longer the effective sample size) \\(Var({\\widehat{\\boldbeta}}):\\) \\(Var\\{(\\boldX^T\\boldX)^{-1}\\boldX^T\\boldY\\}=(\\boldX^T\\boldX)^{-1}\\boldX^T{\\color{royalblue}\\boldsymbol{\\Sigma}}\\boldX(\\boldX^T\\boldX)^{-1}\\) where \\(\\boldSigma\\) is not a diagonal matrix CI’s, hypothesis tests: invalid\n\n\nIndependence: Remedies In BIOSTAT 653 conditional approach: random effects, mixed models let \\(Y_{ij}=\\) response, \\(j\\)’th subject in \\(i\\)’th cluster e.g, \\(Y_{ij} = \\beta_{0i} + \\beta_1 X_{ij} + \\epsilon_{ij}\\), where \\(\\beta_{0i}\\sim N(\\beta_0,\\sigma^2_0)\\) dependence among subjects captured by \\(\\sigma^2_0\\) \\(Y_{ij}\\) conditionally independent of \\(Y_{ik}\\), given \\(\\beta_{0i}\\) marginal approach: Use a “working\" correlation structure model: unchanged from the independence case compute \\(\\widehat{\\boldbeta}\\) as if subjects were independent use adjusted (robust) version of variance estimator\n\n\nIndependence: Remedies (continued) e.g., time-ordered data: include time as covariate need to ensure that effect of time is modelled correctly may or may not solve problem; need to examine residuals may need to depart from linear regression altogether appeal to time series analysis (regression methods specially developed for time-ordered data)\n\n\nConstant Variance Assumption\n\n\nAssessing Constant Variance Plot \\(\\widehat{\\boldepsilon}\\) vs. \\(\\widehat{\\boldY}\\) to assess homogeneity assumption\nshould be a random scatter\ncommon departure: \\(|\\widehat{\\epsilon}_i|\\) increases with \\(\\widehat{Y}_i\\) (cone shape), implying that the variance is an increasing function of the mean common in linear modelling of count/rate data\nNote: residuals are heterogeneous, i.e., \\(Var(\\widehat{\\epsilon}_i)\\neq\\sigma^2\\), but shouldn’t have patterns\n\n\n\n\nimage\n\n\n\n\n\nModel Assumptions: Constant Variance If homogeneity fails \\(\\ldots\\) \\(\\widehat{\\boldbeta}\\): \\(\\widehat{\\sigma}^2\\): \\(\\widehat{Var}(\\widehat{\\boldbeta})\\): hypothesis tests, CI’s: Possible solutions: weighted least squares (Chapter 5.5.2 and 5.5.3) variance stabilizing transformations (Chapter 5.2) robust regression (Chapter 15.1) robust standard errors\n\n\nModel Assumptions: Constant Variance If homogeneity fails \\(\\ldots\\) \\(\\widehat{\\boldbeta}\\): remain valid \\(\\widehat{\\sigma}^2\\): not well defined \\(\\widehat{Var}(\\widehat{\\boldbeta})\\): invalid \\(Var\\{(\\boldX^T\\boldX)^{-1}\\boldX^T\\boldY\\}=(\\boldX^T\\boldX)^{-1}\\boldX^T{\\color{royalblue}Var(\\boldY)}\\boldX(\\boldX^T\\boldX)^{-1}\\) \\({\\color{white}{Var\\{(\\boldX^T\\boldX)^{-1}\\boldX^T\\boldY\\}}}\\neq (\\boldX^T\\boldX)^{-1}\\boldX^T{\\color{royalblue}\\boldsymbol{\\sigma^2\\boldI}}\\boldX(\\boldX^T\\boldX)^{-1}\\) hypothesis tests, CI’s: invalid because SE(\\(\\widehat{\\beta}\\)) invalid Possible solutions: weighted least squares (Chapter 5.5.2 and 5.5.3) variance stabilizing transformations (Chapter 5.2) robust regression (Chapter 15.1) robust standard errors\n\n\nWeighted Least Squares\nSuppose constant variance is violated, but the responses are uncorrelated, i.e., \\({\\boldepsilon}\\sim N({\\boldzero}, {\\boldW})\\), where \\({\\boldW}=\\mbox{diag}\\{\\boldsymbol{w}_1,\\ldots,\\boldsymbol{w}_n\\}\\) where \\(\\boldsymbol{w}_i\\neq \\boldsymbol{w}_j\\) for some \\(i\\neq j\\) \\(Var\\{(\\boldX^T\\boldX)^{-1}\\boldX^T\\boldY\\}=(\\boldX^T\\boldX)^{-1}\\boldX^T{\\color{royalblue}\\boldsymbol{\\boldW}}\\boldX(\\boldX^T\\boldX)^{-1}\\) We can estimate:\n_W &=& (^T^-1)^-1^T^-1\nVar(_W) & =& (^T^-1)^-1\nTypically, \\(\\boldW\\) is unknown and needs to be estimated from \\(\\widehat{\\epsilon}_i\\)’s\n\n\nVariance-Stabilizing Transformations\nVariance-stabilizing transformations (VST) can be used when strong evidence against the constant variance assumption exists\nIdea: often, when \\(Var(Y_i)\\) is not constant, it is a function of \\(E[Y_i]\\). Thus we: choose a function \\(g(\\cdot)\\) s.t. \\(Var(g(Y_i))\\) is not a function of \\(E[Y_i]\\) fit \\(g(\\boldY)=\\boldX\\boldgamma+\\bolddelta\\)\nCaveat: \\(\\boldgamma\\) has different interpretation than \\(\\boldbeta\\) in \\(\\boldY=\\boldX\\boldbeta+\\boldepsilon\\)\n\n\nNon-Constant Variance: Example\nSuppose \\(Y_i\\) follows a Poisson distribution From BIOS 601 we know \\(E[Y_i]=Var(Y_i)=\\boldX_i^T\\boldbeta\\) (non-constant!)\nSet \\(g(x)=\\sqrt{x}\\) due to the following approximation: \\[Var(g(Y_i)) \\approx  %Var\\{\\left.\\frac{\\partial g(x)}{\\partial x}\\right|_{x=E[Y_i]}   (Y_i)\\} =\n         \\left\\{ \\left.\\frac{\\partial g(x)}{\\partial\n            x} \\right|_{x=E[Y_i]} \\right\\}^2 Var(Y_i)\\]\nthis result is exact only when \\(g(x)\\) is linear here, \\(\\{\\partial g/\\partial x\\}^2=(4x)^{-1}\\), such that \\(Var(\\sqrt{Y_i})=1/4\\)\nTherefore, although \\(Var(Y_i)\\) is non-constant, \\(Var(\\sqrt{Y_i})\\) is a constant\n\n\nCommon VST’s\nExamples of variance-stabilizing transforms:\n\n\n\n\n\\(Var(Y_i)\\)\n\\(g(Y_i)\\)\n\n\n\n\n\\(\\sigma^2\\)\n\\(Y_i\\)\n\n\n\\(E[Y_i]\\)\n\\(\\sqrt{Y_i}\\)\n\n\n\\(E[Y_i]^2\\)\n\\(\\log(Y_i)\\)\n\n\n\\(E[Y_i]^3\\)\n\\(1/\\sqrt{Y_i}\\)\n\n\n\\(E[Y_i]^4\\)\n\\(1/Y_i\\)\n\n\n\n\nThese are ordered from weakest to strongest \\(\\ldots\\) We don’t know exactly which one to choose. So: Plot residuals vs fitted values Use prior experience Involves some trial and error\n\n\nRobust standard errors\nRobust standard errors correctly estimate variability of parameter estimates even under non-constant variance Use empirical estimates of the variance in \\(Y\\) at each \\(X\\) value rather than assuming this variance is the same for all \\(X\\) values Regression point estimates will be unchanged Robust or empirical standard errors will give correct confidence intervals and p-values In R: lmtest::coeftest()\n\n\nNormality Assumption\n\n\nNormality Assumption Several ways to check assumption that \\(\\epsilon_i\\sim N(0,\\sigma^2)\\) normality: histogram (stem & leaf plots)\nbox-whisker plots (boxplots)\nquantile-to-quantile (Q-Q) plots\nShapiro-Wilks Test (among other tests for normal distribution)\n\n\nApproach 1: Histogram\nHistogram of \\(\\widehat{\\epsilon}_i\\)’s: should approximate the shape of a zero-mean normal distribution; i.e., symmetric bell-shaped light tailed\nCaveat: Need a fairly large number of individuals to get reliable information from histogram Histograms of the same data but using different bin sizes (breaks) and/or different cut-points may look quite different\nQ: Why not simply plot histogram of \\(Y_i\\)’s? The distribution of \\(Y\\) might look very non-bell shaped yet normality of errors may still hold – e.g., predictor itself might be highly skewed\n\n\nApproach 2: Box-Whisker Plot Box-whisker plots (Boxplot): provides information on symmetry of distribution\nsymmetric distribution: terminology: \\(q\\%\\) of the \\(\\widehat{\\epsilon}_i\\)’s are \\(\\leq\\) the \\(q\\)’th percentile mean equals median 25th and 75th percentiles are equal distance from median median of \\(\\widehat{\\epsilon}_i\\)’s should be \\(\\approx\\) 0\n\n\n\n\nimage\n\n\n\n\n\nApproach 3: Q-Q plot Normal probability plot: rank the \\(\\widehat{z}_i\\)’s in ascending order: _(1) &lt; _(2) &lt; …&lt; _(n) if the \\(\\epsilon_i\\)’s are truly normal, then \\(\\widehat{z}_{(i)}\\)’s should resemble order statistics from a size \\(n\\) standard normal sample plot \\(\\widehat{z}_{(i)}\\)’s against the i-th quantile \\(\\Phi^{-1}(\\frac{i-1/2}{n})\\), where \\(\\Phi^{-1}(\\cdot)\\) is inverse of standard normal cumulative distribution function (cdf)\nA special case of the quantile-to-quantile (Q–Q) plot: for a normal distribution Plot should be straight line\n\n\nApproach 3: Q-Q plot \n\n\nShapiro-Wilks Test Shapiro-Wilks Test: formally examines correlation between \\(\\widehat{z}_{(i)}\\)’s and their expected values under normality \\(H_0: \\epsilon_i\\sim N(0,\\sigma^2)\\); \\(H_1:\\overline{H}_0\\) under \\(H_0\\), E[_(i)|H_0] & = & ^-1() set: \\(\\widehat{\\boldZ}_{(\\cdot)}=[\\widehat{z}_{(1)},\\ldots,\\widehat{z}_{(n)}]\\)\ntest statistic: \\(\\widehat{\\rho}^2(\\widehat{\\boldZ}_{(\\cdot)},E[\\widehat{\\boldZ}_{(\\cdot)}|H_0])\\), ^2(_(),E[_()|H_0]) & = & { }^2\nlow values: evidence against \\(H_0\\) In R: shapiro.test(). If \\(p&gt;0.05\\): we can assume normality\n\n\nModel Assumptions: Normality If Normality does not hold \\(\\ldots\\) \\(\\widehat{\\boldbeta}\\): \\(\\widehat{\\sigma}^2\\): \\(\\widehat{Var}(\\widehat{\\boldbeta})\\): hypothesis tests, CI’s: Possible solution: transform \\({\\boldY}\\) Note: OLS is quite robust to departures from normality By CLT: even if violation of normality, sample mean of variables tends towards a normal distribution as sample size grows\n\n\nModel Assumptions: Normality If Normality does not hold \\(\\ldots\\) \\(\\widehat{\\boldbeta}\\): remains unbiased (does not rely on normality) \\(\\widehat{\\sigma}^2\\): remains unbiased (does not rely on normality) \\(Var(\\widehat{\\boldbeta})=\\sigma^2(\\boldX^T\\boldX)^{-1}\\): still valid formula (only requires uncorrelated and homogeneity) \\(\\widehat{Var}(\\widehat{\\boldbeta})\\): unbiased hypothesis tests, CI’s: invalid under small sample size; valid if sample size is large Possible solution: transform \\({\\boldY}\\) (\\(\\sqrt{\\boldY}, \\log(\\boldY)\\)) Note: OLS is quite robust to departures from normality By CLT: even if violation of normality, sample mean of variables tends towards a normal distribution as sample size grows\n\n\nSummary\n\n\n\n\nCheck\nEstimates\nTests/CIs\nCorrect\n\n\n\n\nL\nPartial regression plot;\nProblematic\nProblematic\nTransform X or Y\n\n\n\nCategorize (step-wise)\n\n\n\n\n\nI\nDesign\nValid\nProblematic\nBIOS 653\n\n\nE\nPlot \\(\\widehat{{\\epsilon}}\\sim\\widehat{{Y}}\\)\nValid\nProblematic\nWeighted Least Squares\n\n\n\n\n\n\nTransform Y; Robust SE\n\n\nN\nHist; boxplot; QQ plot\nValid\nProblematic\nTransform Y\n\n\n\nShapiro-Wilks Test\n\nif \\(n\\) small\n\n\n\n\n\n\nAll models are wrong but some are useful \\(E(\\boldY)\\! =\\! \\boldX\\boldbeta\\) is a convenient way to measure the association that I’m interested in (“useful\"), but I don’t believe the model (”wrong\"). Since I have a random sample, I’m comfortable assuming that my errors are uncorrelated (by design). But why should I believe that my errors are homoscedastic? Maybe the instrument that measures the outcome is less accurate for larger outcome values. My errors are not really normal, but if my sample is large then I don’t need to worry about that. (And most of this class did not rely on normality anyway!)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutliers\n\n\nOutliers We observe data points of the form: (Y_i,X_i1,…,X_ik)\n“Outliers” \\(\\longrightarrow\\) extreme values Consider the \\(i\\)’th individual; can be extreme in several ways: \\(Y_i\\), i.e., “outlying in \\({\\boldY}\\)”\n\\(x_{ij}\\), outlying w.r.t. \\(\\boldX_j\\) row \\(\\boldX_i^T\\), outlying w.r.t \\(\\boldX\\)\n\\(\\widehat{\\epsilon}_i\\) outlying relative to \\(\\widehat{\\boldepsilon}\\): outliers\n\n\nImpact of Outliers Outliers can distort \\(\\widehat{\\beta}_j\\)’s recall: sample means are sensitive to outliers \\(\\widehat{\\beta}_j\\) is a weighted sample mean of \\(Y_i\\)’s hence can be distorted Outliers may also distort \\(\\widehat{\\sigma}^2\\) \\(\\widehat{\\epsilon}_i\\) outlier \\(\\Rightarrow \\widehat{\\sigma}^2=\\frac{1}{n-p}\\sum_{i=1}^n\\widehat{\\epsilon}_i^2\\) Often difficult to detect outliers in \\(\\boldX\\)-space visually (unless SLR)\n\n\nLeverage: Hat Diagonals \\(h_{ii}=\\) leverage for \\(i\\)’th individual \\((i,i)\\)’th element of the \\({\\boldH}\\) matrix Reflects outlyingness in \\(\\boldX\\) space measures distance between \\(\\boldX_i^T\\) and \\(\\overline\\boldX^T\\)\n\\(\\widehat{Y}_i\\) and hence \\(\\widehat{\\epsilon}_i\\) will be driven by the units with large \\(h_{ii}\\)\nCalculation: \\(h_{ii}=\\boldX_i^T (\\boldX^T\\boldX)^{-1} \\boldX_i\\)\nspecial case: SLR, h_ii & = & +\n\n\nLeverage: Interpretation\nIndividuals with large \\(h_{ii}\\) may have disproportionate impact on the \\(\\widehat{\\beta}_j\\)’s\nVarious criteria for “large” \\(h_{ii}&gt;2\\times \\overline{h}\\), where \\(\\overline{h}=\\mbox{\\# parameters}/n\\) \\(0.2 \\leq h_i \\leq 0.5\\): moderate; \\(h_i&gt;0.5\\): large\nOften useful to rank \\(h_{ii}\\)’s, in addition to applying the criteria\nNote: \\(0 \\leq h_{ii} \\leq 1\\)\n\n\nOutliers, summary\nImportant to detect outliers\nGenerally, not interested in formal test of outlyingness power of existing tests is driven by \\(n\\)\nbut, small samples are where outliers may have the greatest impact!\nRarely should outliers be discarded\nappropriate efforts should be made to ensure that the outliers really do represent the process under study\nnot the result of data collection or coding error\ndiscarding outliers may change the population to which the analysis conclusions can generalize\n\n\nQuestions?"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#basic-statistics",
    "href": "slides/01_Reviewqmd.html#basic-statistics",
    "title": "Review",
    "section": "Basic Statistics",
    "text": "Basic Statistics\n\n\n0.5\nRandom variable \\(Y\\) Sample \\(Y_i, i=1,\\dots, n\\)\nSummation: \\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\nProduct: \\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)\nExpected Value (or mean): \\(\\mu_Y= E[Y] = \\int_{-\\infty}^\\infty y f(y) dy\\) reflects ‘center’ of \\(Y\\)’s distribution\n\n0.5"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#rules-of-expected-values",
    "href": "slides/01_Reviewqmd.html#rules-of-expected-values",
    "title": "Review",
    "section": "Rules of Expected Values",
    "text": "Rules of Expected Values\n\n\n0.5 Expectation of sum: \\(E\\left[\\sum_{i=1}^n Y_i \\right] = \\sum_{i=1}^n E[Y_i] \\nonumber\\) No assumption of independence required\nLet \\(a_1,\\ldots,a_n\\) be constants \\(E[a_iY_i] = a_iE[Y_i]\\)\n\\(E\\left[\\sum_{i=1}^n a_iY_i \\right] = \\sum_{i=1}^n a_iE[Y_i]\\)\nExpectation of product: \\(E[Y_iY_j] = E[Y_i]E[Y_j]\\)\nif \\(Y_i\\) and \\(Y_j\\) are independent\n\n0.5\n\n\n:::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#variance-and-standard-deviation",
    "href": "slides/01_Reviewqmd.html#variance-and-standard-deviation",
    "title": "Review",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\n\n\n0.5\nVariance: \\[\\begin{split}\n        &Var(Y) =  E[(Y-\\mu_Y)^2]\\\\\n        = &\\int_{-\\infty}^\\infty (y-\\mu_Y)^2 f(y) dy\n        %=  E[Y^2] - \\mu_Y^2\n        \\end{split}\\] reflects spread of \\(Y\\)’s distribution units: (units of \\(Y\\))\\(^2\\)\nStandard deviation: \\[\\begin{split}\n        SD(Y)  = &\\; \\sqrt{Var(Y)}  \\\\\n        SD(aY)  = &\\; aSD(Y)\n        \\end{split}\\]\nreflects dispersion in \\(Y\\)’s distribution measured in same unit as \\(Y\\)\n\n0.5\n\n\n:::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#rules-of-variances",
    "href": "slides/01_Reviewqmd.html#rules-of-variances",
    "title": "Review",
    "section": "Rules of Variances",
    "text": "Rules of Variances\n\n\n0.5 Variance\n\\(Var(Y)=E[(Y-\\mu_Y)^2]\\)\n\\({\\color{white}{Var(Y)}} =E[Y^2] - \\mu_Y^2\\) Variance of linear combination: \\[\\begin{split}\n        &~Var(aY+b)\\\\\n         = &~ Var(aY) \\\\\n         = &~ E[(aY-E[aY])^2]\\\\\n         = &~ a^2 E[(Y-E[Y])^2]\\\\\n         = &~ a^2 Var(Y)\n        \\end{split}\\] \n\n0.5\n\n\n:::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#covariance",
    "href": "slides/01_Reviewqmd.html#covariance",
    "title": "Review",
    "section": "Covariance",
    "text": "Covariance\n\n\n0.6 \\(X\\), \\(Y\\): random variables Covariance: \\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]\\)\nMeasures (linear) association between \\(X\\), \\(Y\\)\n\\(&gt;0\\), large values of \\(X\\) tend to occur with large values of \\(Y\\)\n\\(&lt;0\\), large values of \\(X\\) tend to coincide with small values of \\(Y\\)\n\\(=0\\), size of \\(X\\) provides no information on size of \\(Y\\) When the covariance is calculated, the data are not standardized Not scale-invariant: can interpret direction but not magnitude\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#correlation",
    "href": "slides/01_Reviewqmd.html#correlation",
    "title": "Review",
    "section": "Correlation",
    "text": "Correlation\n\n\n0.5 \\(X\\), \\(Y\\): random variables Correlation: \\(\\mbox{corr}(X,Y) = \\frac{\\mbox{cov}(X,Y)}{SD(X)SD(Y)}\\)\nScaled measure of linear association,\n\\(-1 \\leq \\mbox{corr}(X,Y) \\leq 1\\) easier to interpret than covariance\n\n0.5"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#rules-of-covariance",
    "href": "slides/01_Reviewqmd.html#rules-of-covariance",
    "title": "Review",
    "section": "Rules of covariance",
    "text": "Rules of covariance\n\\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]= E[XY]-E[X]E[Y]\\) \\(\\mbox{cov} (Y,Y) = \\mbox{var} (Y)\\) Independent \\(\\stackrel{\\Rightarrow}{\\not\\Leftarrow}\\) uncorrelated If \\(X\\) and \\(Y\\) are independent, \\(\\mbox{cov}(X,Y)=0\\) If \\(\\mbox{cov}(X,Y)=0\\) and \\((X,Y)\\sim \\text{Bivariate Normal}\\), then \\(X\\) and \\(Y\\) are independent Covariance is symmetric, additive, and scale preserving \\[\\begin{aligned}\n\\mbox{cov} (X,Y) & = & \\mbox{cov} (Y,X) \\nonumber \\\\\n\\mbox{cov} (X,Y_1+Y_2) & = & \\mbox{cov} (X,Y_1)+\\mbox{cov} (X,Y_2) \\nonumber \\\\\n\\mbox{cov} (X,aY) & = & a\\;\\mbox{cov} (X,Y) \\nonumber\n%\\mbox{cov} (Y,Y) & = & \\mbox{var} (Y) \\nonumber\n\\end{aligned}\\] :::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#rules-of-variance",
    "href": "slides/01_Reviewqmd.html#rules-of-variance",
    "title": "Review",
    "section": "Rules of variance",
    "text": "Rules of variance\n\n\n0.6\nVariance of sum: \\[\\setlength{\\jot}{1pt}\n        \\begin{split}\n        &Var\\left(\\sum_{i=1}^n Y_i\\right)\n        =  \\sum_{i=1}^n\\sum_{j=1}^n\n        \\mbox{cov}(Y_i,Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i)   + \\sum_{i=1}^n\\sum_{j=1}^n I(j\\neq i) \\mbox{cov}(Y_i, Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i) + 2\\sum_{i=1}^n \\sum_{j=i+1}^n\n        \\mbox{cov}(Y_i, Y_j )\n        \\end{split}\\] if \\(Y_1,\\ldots,Y_n\\) are mutually independent, then \\(Var\\left(\\sum_{i=1}^n Y_i\\right) = \\sum_{i=1}^n Var(Y_i)\\)\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#estimator-of-mean",
    "href": "slides/01_Reviewqmd.html#estimator-of-mean",
    "title": "Review",
    "section": "Estimator of Mean",
    "text": "Estimator of Mean\nSuppose we obtained a simple random sample from some underlying population, then we can derive sample estimates of each of the population quantities defined previously Suppose \\(Y_1,\\ldots,Y_n\\) are iid with mean \\(\\mu_Y\\) and variance \\(\\sigma^2_Y\\)\n\n\n0.6 Estimator of mean: \\[\\widehat{\\mu}_Y = \\frac{1}{n} \\sum_{i=1}^n Y_i =\n            \\overline{Y}\\]\n\\(E[\\overline{Y}]= \\frac{1}{n} \\sum_{i=1}^n E[Y_i] = \\mu_Y\\)\n\\(Var(\\overline{Y})= n^{-2} \\sum_{i=1}^n Var(Y_i) = \\sigma_Y^2/n\\)\n\n0.37\n\n\n:::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#variance-and-covariance-estimator",
    "href": "slides/01_Reviewqmd.html#variance-and-covariance-estimator",
    "title": "Review",
    "section": "Variance and Covariance Estimator",
    "text": "Variance and Covariance Estimator\n\n\n0.6\nEstimators of variance:\n\\(\\widehat{\\sigma}^2_Y = \\frac{1}{n} \\sum_{i=1}^n (Y_i-E[Y_i])^2\\)\nif population mean is known \\(\\widehat{\\sigma}^2_Y = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i-\\overline{Y})^2\\)\nif population mean is unknown\nEstimator of covariance:\nSuppose pairs \\((Y_1,X_1),\\ldots,(Y_n,X_n)\\) are iid. \\(\\widehat{\\mbox{cov}} (X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i-\\overline{Y})(X_i-\\overline{X})\\) \\(\\widehat{\\mbox{corr}} (X,Y) =\\) ?\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#distributions-that-will-be-used-in-this-class",
    "href": "slides/01_Reviewqmd.html#distributions-that-will-be-used-in-this-class",
    "title": "Review",
    "section": "Distributions that will be used in this class",
    "text": "Distributions that will be used in this class\nNormal distribution Chi-square distribution t distribution F distribution"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#normal-distribution",
    "href": "slides/01_Reviewqmd.html#normal-distribution",
    "title": "Review",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nDensity: \\(Y\\sim N(\\mu,\\sigma^2)\\), $$\n\\[\\begin{aligned}\n        f_Y(y) & = &\n        \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{\\frac{-1}{2}\\left(\\frac{y-\\mu}{\\sigma}\n        \\right)^2\\right\\} \\nonumber\n        \n\\end{aligned}\\]\n$$ If we know \\(E(Y)=\\mu\\), \\(Var(Y)=\\sigma^2\\) then\n/3 of \\(Y\\)’s distribution lies within 1 \\(\\sigma\\) of \\(\\mu\\) % \\(\\ldots\\) \\(\\ldots\\) is within \\(\\mu\\pm 2\\sigma\\) \\(&gt;99\\)% \\(\\ldots\\) \\(\\ldots\\) lies within \\(\\mu\\pm 3\\sigma\\)\nArguably, the most important distribution in statistics\nLinear combinations of Normals are Normal\ne.g., \\((aY+b)\\sim \\mbox{N}(a\\mu+b,\\;a^2\\sigma^2)\\)\nStandard normal: $$\n\\[\\begin{aligned}\n        Z=\\frac{Y-\\mu}{\\sigma} \\sim \\mbox{N}(0,1) \\nonumber\n        \n\\end{aligned}\\]\n$$"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#chi-square-distribution",
    "href": "slides/01_Reviewqmd.html#chi-square-distribution",
    "title": "Review",
    "section": "Chi-square Distribution",
    "text": "Chi-square Distribution\nNotation: \\(X \\sim \\chi^2_{df}\\) \\(df=\\) degrees of freedom \\(E[X]=df\\) \\(X\\) takes on only positive values If \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\nIf \\(Z_1,\\ldots,Z_n\\) are independent, with \\(Z_i\\sim\\mbox{N}(0,1)\\), then $$\n\\[\\begin{aligned}\n        \\sum_{i=1}^n Z_i^2 & \\sim & \\chi^2_n \\nonumber\n        \n\\end{aligned}\\]\n$$\nUsed in hypothesis testing and CI’s involving variance"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#t-distribution",
    "href": "slides/01_Reviewqmd.html#t-distribution",
    "title": "Review",
    "section": "t Distribution",
    "text": "t Distribution\nIf \\(Z\\sim \\mbox{N}(0,1)\\) and \\(S^2\\sim \\chi^2_{df}\\) and \\(Z\\) and \\(S^2\\) are independent,\n$$\n\\[\\begin{aligned}\n        \\frac{Z}{S/\\sqrt{df}} & \\sim & t_{df} \\nonumber\n        \n\\end{aligned}\\]\n$$\nSymmetric, bell-shaped; tails heavier than Normal \\(E[t_{df}]=0\\); \\(Var(t_{df})\\) greater than 1 \\(\\lim_{df\\rightarrow \\infty}t_{df} \\rightarrow \\mbox{N}(0,1)\\) for \\(df&gt;30\\), the \\(t_{df}\\) closely resembles the \\(\\mbox{N}(0,1)\\) distribution\nIn linear modeling, used for inference on individual regression parameters"
  },
  {
    "objectID": "slides/01_Reviewqmd.html#f-distribution",
    "href": "slides/01_Reviewqmd.html#f-distribution",
    "title": "Review",
    "section": "F Distribution",
    "text": "F Distribution\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then: $$\n\\[\\begin{aligned}\n        \\frac{X_1^2/df1}{X_2^2/df2} & \\sim & F_{df1,df2} \\nonumber\n        \n\\end{aligned}\\]\n$$\nonly takes on positive values\nconnection to \\(t\\) distribution:\n$$\n\\[\\begin{aligned}\n            \\{ t_{df} \\}^2 & \\stackrel{{\\cal D}}{=} & F_{1,df} \\nonumber\n            \n\\end{aligned}\\]\n$$\nUsed extensively in linear regression (hypothesis testing)\n\n\nIntro"
  },
  {
    "objectID": "slides/01_Review.html#basic-statistics",
    "href": "slides/01_Review.html#basic-statistics",
    "title": "Review",
    "section": "Basic Statistics",
    "text": "Basic Statistics\n\n\n\nRandom variable \\(Y\\)\n\nSample \\(Y_i, i=1,\\dots, n\\)\n\nSummation:\n\\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\nProduct:\n\\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)\nSample mean:\n\\(\\mu_Y= E[Y] = \\int_{-\\infty}^\\infty y f(y) dy\\)\n\nReflects ‘center’ of \\(Y\\)’s distribution"
  },
  {
    "objectID": "slides/01_Review.html#rules-of-expected-values",
    "href": "slides/01_Review.html#rules-of-expected-values",
    "title": "Review",
    "section": "Rules of Expected Values",
    "text": "Rules of Expected Values\n\n\nExpectation of sum: \\(E\\left[\\sum_{i=1}^n Y_i \\right] = \\sum_{i=1}^n E[Y_i] \\nonumber\\) No assumption of independence required\nLet \\(a_1,\\ldots,a_n\\) be constants \\(E[a_iY_i] = a_iE[Y_i]\\)\n\\(E\\left[\\sum_{i=1}^n a_iY_i \\right] = \\sum_{i=1}^n a_iE[Y_i]\\)\nExpectation of product: \\(E[Y_iY_j] = E[Y_i]E[Y_j]\\)\nif \\(Y_i\\) and \\(Y_j\\) are independent"
  },
  {
    "objectID": "slides/01_Review.html#variance-and-standard-deviation",
    "href": "slides/01_Review.html#variance-and-standard-deviation",
    "title": "Review",
    "section": "Variance and standard deviation",
    "text": "Variance and standard deviation\n\n\nVariance: \\[\\begin{split}\n        &Var(Y) =  E[(Y-\\mu_Y)^2]\\\\\n        = &\\int_{-\\infty}^\\infty (y-\\mu_Y)^2 f(y) dy\n        %=  E[Y^2] - \\mu_Y^2\n        \\end{split}\\] reflects spread of \\(Y\\)’s distribution units: (units of \\(Y\\))\\(^2\\)\nStandard deviation: \\[\\begin{split}\n        SD(Y)  = &\\; \\sqrt{Var(Y)}  \\\\\n        SD(aY)  = &\\; aSD(Y)\n        \\end{split}\\]\nreflects dispersion in \\(Y\\)’s distribution measured in same unit as \\(Y\\)"
  },
  {
    "objectID": "slides/01_Review.html#rules-of-variances",
    "href": "slides/01_Review.html#rules-of-variances",
    "title": "Review",
    "section": "Rules of Variances",
    "text": "Rules of Variances\n\n\n0.5 Variance\n\\(Var(Y)=E[(Y-\\mu_Y)^2]\\)\n\\({\\color{white}{Var(Y)}} =E[Y^2] - \\mu_Y^2\\) Variance of linear combination: \\[\\begin{split}\n        &~Var(aY+b)\\\\\n         = &~ Var(aY) \\\\\n         = &~ E[(aY-E[aY])^2]\\\\\n         = &~ a^2 E[(Y-E[Y])^2]\\\\\n         = &~ a^2 Var(Y)\n        \\end{split}\\] \n\n0.5\n\n\n:::"
  },
  {
    "objectID": "slides/01_Review.html#covariance",
    "href": "slides/01_Review.html#covariance",
    "title": "Review",
    "section": "Covariance",
    "text": "Covariance\n\n\n\n\\(X\\), \\(Y\\): random variables\nCovariance: \\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]\\)\nMeasures (linear) association between \\(X\\), \\(Y\\)\n\n\\(&gt;0\\), large values of \\(X\\) tend to occur with large values of \\(Y\\)\n\\(&lt;0\\), large values of \\(X\\) tend to coincide with small values of \\(Y\\)\n\\(=0\\), size of \\(X\\) provides no information on size of \\(Y\\)\n\nWhen the covariance is calculated, the data are not standardized\nNot scale-invariant: can interpret direction but not magnitude"
  },
  {
    "objectID": "slides/01_Review.html#correlation",
    "href": "slides/01_Review.html#correlation",
    "title": "Review",
    "section": "Correlation",
    "text": "Correlation\n\n\n\n\\(X\\), \\(Y\\): random variables\nCorrelation: \\(\\mbox{corr}(X,Y) = \\frac{\\mbox{cov}(X,Y)}{SD(X)SD(Y)}\\)\nScaled measure of linear association,\n\n\\(-1 \\leq \\mbox{corr}(X,Y) \\leq 1\\)\nEasier to interpret than covariance\n\n\n\n0.5"
  },
  {
    "objectID": "slides/01_Review.html#rules-of-covariance",
    "href": "slides/01_Review.html#rules-of-covariance",
    "title": "Review",
    "section": "Rules of covariance",
    "text": "Rules of covariance\n\\(\\mbox{cov}(Y,X)= E[(Y-\\mu_Y)(X-\\mu_X)]= E[XY]-E[X]E[Y]\\) \\(\\mbox{cov} (Y,Y) = \\mbox{var} (Y)\\) Independent \\(\\stackrel{\\Rightarrow}{\\not\\Leftarrow}\\) uncorrelated If \\(X\\) and \\(Y\\) are independent, \\(\\mbox{cov}(X,Y)=0\\) If \\(\\mbox{cov}(X,Y)=0\\) and \\((X,Y)\\sim \\text{Bivariate Normal}\\), then \\(X\\) and \\(Y\\) are independent Covariance is symmetric, additive, and scale preserving \\[\\begin{aligned}\n\\mbox{cov} (X,Y) & = & \\mbox{cov} (Y,X) \\nonumber \\\\\n\\mbox{cov} (X,Y_1+Y_2) & = & \\mbox{cov} (X,Y_1)+\\mbox{cov} (X,Y_2) \\nonumber \\\\\n\\mbox{cov} (X,aY) & = & a\\;\\mbox{cov} (X,Y) \\nonumber\n%\\mbox{cov} (Y,Y) & = & \\mbox{var} (Y) \\nonumber\n\\end{aligned}\\] :::"
  },
  {
    "objectID": "slides/01_Review.html#rules-of-variance",
    "href": "slides/01_Review.html#rules-of-variance",
    "title": "Review",
    "section": "Rules of variance",
    "text": "Rules of variance\n\n\n0.6\nVariance of sum: \\[\\setlength{\\jot}{1pt}\n        \\begin{split}\n        &Var\\left(\\sum_{i=1}^n Y_i\\right)\n        =  \\sum_{i=1}^n\\sum_{j=1}^n\n        \\mbox{cov}(Y_i,Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i)   + \\sum_{i=1}^n\\sum_{j=1}^n I(j\\neq i) \\mbox{cov}(Y_i, Y_j )  \\\\\n        = & \\sum_{i=1}^n Var(Y_i) + 2\\sum_{i=1}^n \\sum_{j=i+1}^n\n        \\mbox{cov}(Y_i, Y_j )\n        \\end{split}\\] if \\(Y_1,\\ldots,Y_n\\) are mutually independent, then \\(Var\\left(\\sum_{i=1}^n Y_i\\right) = \\sum_{i=1}^n Var(Y_i)\\)\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/01_Review.html#estimator-of-mean",
    "href": "slides/01_Review.html#estimator-of-mean",
    "title": "Review",
    "section": "Estimator of Mean",
    "text": "Estimator of Mean\nSuppose we obtained a simple random sample from some underlying population, then we can derive sample estimates of each of the population quantities defined previously Suppose \\(Y_1,\\ldots,Y_n\\) are iid with mean \\(\\mu_Y\\) and variance \\(\\sigma^2_Y\\)\n\n\n0.6 Estimator of mean: \\[\\widehat{\\mu}_Y = \\frac{1}{n} \\sum_{i=1}^n Y_i =\n            \\overline{Y}\\]\n\\(E[\\overline{Y}]= \\frac{1}{n} \\sum_{i=1}^n E[Y_i] = \\mu_Y\\)\n\\(Var(\\overline{Y})= n^{-2} \\sum_{i=1}^n Var(Y_i) = \\sigma_Y^2/n\\)\n\n0.37\n\n\n:::"
  },
  {
    "objectID": "slides/01_Review.html#variance-and-covariance-estimator",
    "href": "slides/01_Review.html#variance-and-covariance-estimator",
    "title": "Review",
    "section": "Variance and Covariance Estimator",
    "text": "Variance and Covariance Estimator\n\n\n0.6\nEstimators of variance:\n\\(\\widehat{\\sigma}^2_Y = \\frac{1}{n} \\sum_{i=1}^n (Y_i-E[Y_i])^2\\)\nif population mean is known \\(\\widehat{\\sigma}^2_Y = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i-\\overline{Y})^2\\)\nif population mean is unknown\nEstimator of covariance:\nSuppose pairs \\((Y_1,X_1),\\ldots,(Y_n,X_n)\\) are iid. \\(\\widehat{\\mbox{cov}} (X,Y) = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i-\\overline{Y})(X_i-\\overline{X})\\) \\(\\widehat{\\mbox{corr}} (X,Y) =\\) ?\n\n0.4\n\n\n:::"
  },
  {
    "objectID": "slides/01_Review.html#distributions-that-will-be-used-in-this-class",
    "href": "slides/01_Review.html#distributions-that-will-be-used-in-this-class",
    "title": "Review",
    "section": "Distributions that will be used in this class",
    "text": "Distributions that will be used in this class\n\nNormal distribution\nChi-square distribution\nt distribution\nF distribution"
  },
  {
    "objectID": "slides/01_Review.html#normal-distribution",
    "href": "slides/01_Review.html#normal-distribution",
    "title": "Review",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\n\n\nNotation: \\(Y\\sim N(\\mu,\\sigma^2)\\)\nArguably, the most important distribution in statistics\nIf we know \\(E(Y)=\\mu\\), \\(Var(Y)=\\sigma^2\\) then\n\n2/3 of \\(Y\\)’s distribution lies within 1 \\(\\sigma\\) of \\(\\mu\\)\n95% \\(\\ldots\\) \\(\\ldots\\) is within \\(\\mu\\pm 2\\sigma\\)\n\\(&gt;99\\)% \\(\\ldots\\) \\(\\ldots\\) lies within \\(\\mu\\pm 3\\sigma\\)\n\nLinear combinations of Normal’s are Normal\ne.g., \\((aY+b)\\sim \\mbox{N}(a\\mu+b,\\;a^2\\sigma^2)\\)\nStandard normal: \\(Z=\\frac{Y-\\mu}{\\sigma} \\sim \\mbox{N}(0,1)\\)"
  },
  {
    "objectID": "slides/01_Review.html#chi-square-distribution",
    "href": "slides/01_Review.html#chi-square-distribution",
    "title": "Review",
    "section": "Chi-square Distribution",
    "text": "Chi-square Distribution\n\nNotation: \\(X \\sim \\chi^2_{df}\\)\n\n\\(df=\\) degrees of freedom\n\\(E[X]=df\\)\n\\(X\\) takes on only positive values\n\nIf \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\nIf \\(Z_1,\\ldots,Z_n\\) are independent, with \\(Z_i\\sim\\mbox{N}(0,1)\\), then\n\n\\[\\begin{aligned}\n        \\sum_{i=1}^n Z_i^2 & \\sim & \\chi^2_n \\nonumber\n        \n\\end{aligned}\\]\n\nUsed in hypothesis testing and CI’s involving variance"
  },
  {
    "objectID": "slides/01_Review.html#t-distribution",
    "href": "slides/01_Review.html#t-distribution",
    "title": "Review",
    "section": "t Distribution",
    "text": "t Distribution\n\n\n\nNotation: \\(T \\sim t_{df}\\) OR \\(T \\sim t_{n-1}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(T = \\dfrac{\\bar{x} - \\mu_x}{\\dfrac{s}{\\sqrt{n}}}\\sim t_{n-1}\\)\n\nIn linear modeling, used for inference on individual regression parameters\n\nThink: our estimated coefficients (\\(\\hat{\\beta}\\))"
  },
  {
    "objectID": "slides/01_Review.html#f-distribution",
    "href": "slides/01_Review.html#f-distribution",
    "title": "Review",
    "section": "F-Distribution",
    "text": "F-Distribution\n\n\n\nModel ratio of sample variances\n\nRatio of variances is important for hypothesis testing of regression models\n\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then:\n\n\\[\\dfrac{X_1^2/df1}{X_2^2/df2} \\sim F_{df1,df2}\\] - only takes on positive values\n\nImportant relationship with \\(t\\) distribution: \\(T^2 \\sim F_{1,\\nu}\\)\n\nThe square of a t-distribution with \\(df=\\nu\\)\nis an F-distribution with numerator df (\\(df_1 = 1\\)) and denominator df (\\(df_2 = \\nu\\))"
  },
  {
    "objectID": "Class_dictionary.html",
    "href": "Class_dictionary.html",
    "title": "Stat Talk",
    "section": "",
    "text": "Name\nNormal Speak Examples\nFormula\n\n\n\n\nRandom variable \\(Y\\)\n\n“If we look at Y…”\n\nSample \\(Y_i, i=1,\\dots, n\\)\n\n\nSummation\n\n“Sum over the Y’s”\n\n\\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\n\n\nProduct\n\n“Take the product of the Y’s”\n\n\\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)"
  },
  {
    "objectID": "slides/01_Review.html#some-basic-statistics-talk",
    "href": "slides/01_Review.html#some-basic-statistics-talk",
    "title": "Review",
    "section": "Some Basic Statistics “Talk”",
    "text": "Some Basic Statistics “Talk”\n\n\n\nRandom variable \\(Y\\)\n\nSample \\(Y_i, i=1,\\dots, n\\)\n\nSummation:\n\\(\\sum_{i=1}^n Y_i =Y_1 + Y_2 + \\ldots + Y_n\\)\nProduct:\n\\(\\prod_{i=1}^n Y_i = Y_1 \\times Y_2 \\times \\ldots \\times Y_n\\)"
  },
  {
    "objectID": "slides/01_Review.html#descriptive-statistics-continuous-variables",
    "href": "slides/01_Review.html#descriptive-statistics-continuous-variables",
    "title": "Review",
    "section": "Descriptive Statistics: continuous variables",
    "text": "Descriptive Statistics: continuous variables\n\n\nMeasures of central tendency\n\nSample mean\n\\[\n\\bar{x} = \\dfrac{x_1+x_2+...+x_n}{n}=\\dfrac{\\sum_{i=1}^nx_i}{n}\n\\]\nMedian\n\n\nMeasures of variability (or dispersion)\n\nSample variance\n\nAverage of the squared deviations from the sample mean\n\nSample standard deviation\n\\[\ns = \\sqrt{\\dfrac{(x_1-\\bar{x})^2+(x_2-\\bar{x})^2+...+(x_n-\\bar{x})^2}{n-1}}=\\sqrt{\\dfrac{\\sum_{i=1}^n(x_i-\\bar{x})^2}{n-1}}\n\\]\nIQR\n\nRange from 1st to 3rd quartile"
  },
  {
    "objectID": "slides/01_Review.html#data-visualization",
    "href": "slides/01_Review.html#data-visualization",
    "title": "Review",
    "section": "Data visualization",
    "text": "Data visualization\n\nUsing the library ggplot2 to visualize data\nWe will load the package:\n\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "slides/01_Review.html#where-are-we",
    "href": "slides/01_Review.html#where-are-we",
    "title": "Review",
    "section": "Where are we?",
    "text": "Where are we?\n\nReview\nIntro to SLR: estimation and testing\nIntro to MLR: estimation and testing\nDiving into our predictors: categorical variables, interactions between variable\nKey ingredients: model evaluation, diagnostics, selection, and building"
  },
  {
    "objectID": "slides/01_Review.html#confidence-interval-for-one-mean",
    "href": "slides/01_Review.html#confidence-interval-for-one-mean",
    "title": "Review",
    "section": "Confidence interval for one mean",
    "text": "Confidence interval for one mean\n\n\nThe confidence interval for population mean \\(\\mu\\):\n\\[\n\\bar{x} \\pm t^{*}\\dfrac{s}{\\sqrt{n}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n-1\\)\n\n\n\nWe can use R to find the critical t-value, \\(t^*\\)\n\n\nFor example the critical value for the 95% CI with \\(n=10\\) subjects is…\n\nqt(0.975, df=9)\n\n[1] 2.262157\n\n\n\nRecall, that as the \\(df\\) increases, the t-distribution converges towards the Normal distribution"
  },
  {
    "objectID": "slides/01_Review.html#confidence-interavl-for-two-independent-means",
    "href": "slides/01_Review.html#confidence-interavl-for-two-independent-means",
    "title": "Review",
    "section": "Confidence interavl for two independent means",
    "text": "Confidence interavl for two independent means"
  },
  {
    "objectID": "slides/01_Review.html#steps-in-hypothesis-testing",
    "href": "slides/01_Review.html#steps-in-hypothesis-testing",
    "title": "Review",
    "section": "Steps in hypothesis testing",
    "text": "Steps in hypothesis testing"
  },
  {
    "objectID": "slides/01_Review.html#example-one-sample-t-test-using-p-value-approach",
    "href": "slides/01_Review.html#example-one-sample-t-test-using-p-value-approach",
    "title": "Review",
    "section": "Example: one sample t-test using p-value approach",
    "text": "Example: one sample t-test using p-value approach\nWe want to see what the mean population body temperature is.\n\nState the null and alternative hypotheses:\n\n\n\n\n\n\n\n\\(H_0: \\mu = 98.6\\)\n\\(H_0\\): The population mean body temperature is 98.6 degrees F\n\n\n\\(H_A: \\mu \\neq 98.6\\)\n\\(H_A\\): The population mean body temperature is not 98.6 degrees F\n\n\n\nThe significance level is \\(\\alpha = 0.05\\)\nThe test statistic, \\(t_{\\bar{x}}\\) follows a student’s t-distribution with \\(df = n-1 = 129\\)\nThe test statistic is: \\(t_{\\bar{x}} = \\dfrac{\\bar{x}-\\mu_0}{\\dfrac{s}{\\sqrt{n}}}\\) and with the data: \\(t_{\\bar{x}} = \\dfrac{98.25-98.6}{\\dfrac{0.73}{\\sqrt{130}}} = -5.45\\)\nCalculate the p-value: \\(p-value = P(t \\leq -5.45) + P(t \\geq 5.45)\\)\n\n2*pt(-5.4548, df = 130-1, lower.tail=T)\n\n[1] 2.410889e-07\n\n\nConclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( \\(p-value &lt; 0.001\\))."
  },
  {
    "objectID": "slides/01_Review.html#example-one-sample-t-test-using-critical-values-approach",
    "href": "slides/01_Review.html#example-one-sample-t-test-using-critical-values-approach",
    "title": "Review",
    "section": "Example: one sample t-test using critical values approach",
    "text": "Example: one sample t-test using critical values approach\nWe want to see what the mean population body temperature is.\n\nState the null and alternative hypotheses:\n\n\n\n\n\n\n\n\\(H_0: \\mu = 98.6\\)\n\\(H_0\\): The population mean body temperature is 98.6 degrees F\n\n\n\\(H_A: \\mu \\neq 98.6\\)\n\\(H_A\\): The population mean body temperature is not 98.6 degrees F\n\n\n\nThe significance level is \\(\\alpha = 0.05\\)\nThe test statistic, \\(t_{\\bar{x}}\\) follows a student’s t-distribution with \\(df = n-1 = 129\\)\nDecision rule (critical value): For \\(\\alpha=0.05\\) , \\(2*P(t \\geq t^*) = 0.05\\)\n\nqt(0.05/2, df = 130-1, lower.tail=F)\n\n[1] 1.978524\n\n\nThe test statistic is: \\(t_{\\bar{x}} = \\dfrac{\\bar{x}-\\mu_0}{\\dfrac{s}{\\sqrt{n}}}\\) and with the data: \\(t_{\\bar{x}} = \\dfrac{98.25-98.6}{\\dfrac{0.73}{\\sqrt{130}}} = -5.45\\)\nConclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( 95% CI: \\(98.12, 98.38\\))."
  },
  {
    "objectID": "slides/01_Review.html#type-1-and-2-errors",
    "href": "slides/01_Review.html#type-1-and-2-errors",
    "title": "Review",
    "section": "Type 1 and 2 errors",
    "text": "Type 1 and 2 errors"
  },
  {
    "objectID": "slides/01_Review.html#power",
    "href": "slides/01_Review.html#power",
    "title": "Review",
    "section": "Power",
    "text": "Power\n\nPower is \\(1-\\beta\\)\n\nThe probability of correctly rejecting the null hypothesis\n\n\n\n\n\nReview"
  },
  {
    "objectID": "slides/01_Review.html#confidence-interval-for-two-independent-means",
    "href": "slides/01_Review.html#confidence-interval-for-two-independent-means",
    "title": "Review",
    "section": "Confidence interval for two independent means",
    "text": "Confidence interval for two independent means\n\n\nThe confidence interval for difference in independent population means, \\(\\mu_1\\) and \\(\\mu_2\\):\n\\[\n\\bar{x}_1 - \\bar{x}_2 \\pm t^{*}\\sqrt{\\dfrac{s_1^2}{n_1} + \\dfrac{s_2^2}{n_2}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n_1 + n_2 -2\\)"
  },
  {
    "objectID": "slides/01_Review.html#before-we-begin",
    "href": "slides/01_Review.html#before-we-begin",
    "title": "Review",
    "section": "Before we begin",
    "text": "Before we begin\n\nMeike has some really good online notes, code, and work on her BSTA 511 page"
  },
  {
    "objectID": "slides/Module_B.html",
    "href": "slides/Module_B.html",
    "title": "Linear Models",
    "section": "",
    "text": "BIOSTAT 650\nTheory and Application of Linear Regression\nModule B: Simple Linear Regression\n\n\nOutline Announcements Module B Topics: Simple linear regression (SLR) model\nInterpretation of parameters\nParameter estimation\nProperties of least squares estimators\nEstimation of variance\nRelevant readings from Textbook: Chapters 1 and 2\n\n\nSimple Linear Regression (SLR) Model\n\n\nSimple Linear Regression Model: \\[\\begin{aligned}\nY_i & = & \\beta_0 + \\beta_1X_i + \\epsilon_i \\nonumber\n\\end{aligned}\\] \\(Y_i\\): response, dependent variable\n\\(\\beta_0\\): intercept (fixed, unknown)\n\\(\\beta_1\\): slope (fixed, unknown)\n\\(X_i\\): covariate, predictor variable (fixed)\n\\(\\epsilon_i\\): error term (random, unobservable)\nObserved are the ordered pairs: (\\(X_i,Y_i\\)), for \\(i=1,\\ldots,n\\)\n\n\n“Linear” Models “Linearity” refers to the fact that the mean can be written as a weighted sum of parameters:\ni.e., \\(E[Y_i|X_i]=\\sum_{k=1}^q w_k\\beta_k\\)\nExamples of linear models (covered in more detail later): \\(E[Y_i|X_i] = \\beta_0+ \\beta_1X_i^2\\)\n\\(E[Y_i|X_i] = \\beta_0+ \\beta_1\\exp(X_i)\\)\n\\(E[Y_i|X_i] = \\beta_0+ \\beta_1\\log(1+X_i)\\)\nExamples of non-linear models (not covered in this class):\n\\(E[Y_i|X_i] = \\exp\\{\\beta_0+ \\beta_1X_i\\}\\)\n\\(E[Y_i|X_i] = [1+ \\exp\\{-(\\beta_0+ \\beta_1X_i)\\}]^{-1}\\)\n\\(E[Y_i|X_i] = \\beta_0+ \\exp\\{\\beta_1X_i\\}\\)\n\n\nAssumptions for estimation (not sufficient for inference)\nAssumptions about errors: First and second moment: \\(\\epsilon_i\\sim (0,\\sigma^2)\\) for all \\(i\\) Uncorrelated: \\(cov(\\epsilon_i,\\epsilon_j)=E[\\epsilon_i\\epsilon_j] =0\\), for all \\(i\\), \\(j\\), \\(i\\neq j\\) Uncorrelated \\(\\not\\Rightarrow\\) independence Later we will consider inference (hypothesis testing), and we will further assume that \\(\\epsilon_i\\stackrel{i.i.d}{\\sim} N(0,\\sigma^2)\\) (independence and normality)\nThese imply assumptions about distribution of \\(Y_i\\) at each \\(X_i\\) \\(E[Y_i\\mid X_i]=\\beta_0+\\beta_1X_i\\) (depends on \\(X_i\\)) \\(Var(Y_i\\mid X_i)=Var(Y_i)=\\sigma^2\\) (does not depend on \\(X_i\\)) \\(Y_i\\) and \\(Y_j\\) are uncorrelated, \\(i\\neq j\\)\n\n\nA full list of assumptions (LINE) for estimation and inference\n\\(Y_i=\\beta_0+\\beta_1X_i+\\epsilon_i\\) \"Linearity\": the model is correctly specified \\(E[Y_i|X_i]=\\beta_0+\\beta_1X_i\\)\n\\(\\epsilon_i \\ind \\epsilon_j\\), \\(i\\neq j\\) \"Independence\" hence \\(Y_i \\ind Y_j\\), \\(i\\neq j\\)\n\\(\\epsilon_i\\sim N(0,\\sigma^2)\\) \"Normality\": errors follow normal distribution w/ mean \\(0\\), variance \\(\\sigma^2\\) hence \\(Y_i\\mid X_i\\sim N(\\beta_0+\\beta_1X_i,\\sigma^2)\\)\n\\(\\sigma^2_i=Var[Y_i|X_i]=\\sigma^2&lt;\\infty\\) \"Equal variance\": errors have equal variance \\(\\sigma^2\\)\n\n\nFixed Design \\(X_i\\)’s treated as fixed constants, throughout this course Think of sampling \\(Y\\) at each level of \\(X\\) In reality, \\(X_i\\) may be random: measurement error (e.g., body weight)\nscale not perfectly balanced and/or natural fluctuation (e.g., body weight; water retention)\nPreferably, randomness in \\(X_i\\) is small (negligible)\n\n\nModel Components\nRelationship between \\(X\\) and mean of \\(Y\\) is described by a line: \\[E[Y_i|X_i]=\\beta_0+\\beta_1X_i \\;\\;(\\epsilon_i=Y_i-E[Y_i|X_i])\\]\nA line is determined by two numbers: intercept and slope\n\\(\\beta_0=E[Y_i|X_i=0]\\) = intercept of regression line “Mean weight for individuals at age zero\" \\(\\beta_1=\\Delta E[Y] /\\Delta X\\)= slope of regression line Change in mean of \\(Y\\) per unit increase in \\(X\\) \\[\\beta_1 = \\frac{E[Y_i|X_i=x_2]-E[Y_i|X_i=x_1]}{x_2-x_1}\\]\n\n\nModel Components\nSuppose two subjects (e.g., \\(i\\)=1, \\(i\\)=2) differ by 1 unit of \\(X_1\\) (age); the difference in \\(E[Y]\\) (mean weight) will be \\(\\beta_1\\), since e.g. \\(E[Y_1|X_1=7] = \\beta_0 + \\beta_1 7\\)\n\\(E[Y_1|X_1=6] = \\beta_0 + \\beta_1 6\\) subtracting the right and left sides gives \\(\\beta_1\\) Are the above results dependent on the specific \\(X_i\\) values used? Linearity assumes: “for every unit change in \\(X\\), the mean difference in \\(Y\\) is constant\", regardless of the value of \\(X\\)\n\n\nModel Components \\(\\beta_0\\), \\(\\beta_1\\) are scale-dependent when interpreting \\(\\beta_0\\) and \\(\\beta_1\\), units are considered \\(\\beta_1\\) reflects the magnitude of the (\\(X_i,Y_i\\)) association; often of much greater inherent interest than \\(\\beta_0\\) Notwithstanding, a useful interpretation of \\(\\beta_0\\) is preferable\n\n\nInterpretation example 1 \\(Y_i=\\) serum cholesterol (mg/dL)\n\\(X_i=\\) systolic blood pressure (SBP; mm Hg)\nModel: \\(Y_i = \\beta_0+\\beta_1X_i+\\epsilon_i\\)\n\\(\\beta_1=\\) mean difference in serum cholesterol (mg/dL) per one unit higher in SBP (mm Hg)\n\\(\\beta_0=\\) mean serum cholesterol (mg/dL) for patients with SBP=0 (is zero SBP possible for live persons?)\n\n\nInterpretation example 2 e.g., study of adult males age 20-39\n\\(Y_i=\\) weight (kg)\n\\(X_i=\\) age (years)\nModel: \\(Y_i = \\beta_0+\\beta_1X_i+\\epsilon_i\\)\n\\(\\beta_1=\\) mean change in weight (kg) per year increase in age\n\\(\\beta_0=\\)\n\n\nRegression: Extrapolation Generally, inference/conclusions should respect the range of observed covariate values\ne.g., return to the age/weight example\\(\\ldots\\)\nAge/weight model which applies to\n\\(20\\leq age \\leq 39\\) may be quite different from that which applies to someone age 65 Relationship may no longer be linear\nPredictions should generally not be made outside the observed range of the \\(X_i\\)’s \n\n\nCentering Covariate to make intercept interpretable Original model: \\(Y_i=\\beta_0+\\beta_1 X_i +\\epsilon_i\\) Define \\(X_i^*=X_i-\\overline{X}\\), where \\(\\overline{X}=n^{-1}\\sum_{i=1}^nX_i\\)\nRevised Model: \\(Y_i=\\beta_0^*+\\beta_1^*X_i^*+\\epsilon_i^*\\)\nWe still have: \\(\\epsilon_i^*\\sim N(0,\\sigma^2)\\) The error is unchanged, i.e., \\(\\epsilon_i^* = \\epsilon_i\\) Graphically we have a shift in axis:\n\n\nCentering Covariate: Interpretation of Intercept Recall: interpretation of \\(\\beta_0\\) in original model: \\(E[Y_i|X_i]=\\beta_0+\\beta_1X_i\\), thus, \\(\\beta_0=E[Y_i|X_i=0]\\) “Mean weight for individuals at age zero\" For the revised model: \\(E[Y_i|X_i^*]=\\beta_0^*+\\beta_1^*X_i^*\\), thus, \\(\\beta_0^*=E[Y_i|X_i^*=0]=E[Y_i|X_i=\\overline{X}]\\)\n“Mean weight for individuals with average age\"\n\n\nCentering Covariate: Slope Interpretation is unchanged Original model: \\[\\begin{aligned}\n\\beta_1 &  = & \\frac{E[Y_i|X_i=x_2 ]-E[Y_i|X_i=x_1]}{x_2-x_1 }\n\\nonumber\n\\end{aligned}\\]\nCentered model: \\[\\begin{aligned}\n\\beta_1^* &  = & \\frac{E[Y_i|X_i^*=x_2\n]-E[Y_i|X_i^*=x_1]}{x_2-x_1 }\n\\nonumber \\\\\n&  \\stackrel{X_i^*=X_i-\\overline{X}}{\\longeq} & \\frac{E[Y_i|X_i=\\overline{X}+x_2\n]-E[Y_i|X_i=\\overline{X}+x_1]}{x_2-x_1 }\n\\nonumber \\\\\n&  = & \\frac{E[Y_i|X_i=\\overline{X}+x_2\n    ]-E[Y_i|X_i=\\overline{X}+x_1]}{(\\overline{X}+x_2)-(\\overline{X}+x_1) }\n\\nonumber\n\\end{aligned}\\]\n\n\nParameter Estimation\n\n\nParameter Estimation: Preliminaries Truth \\(\\beta_0\\), \\(\\beta_1\\): true parameters, fixed, unknown \\(E[Y_i|X_i]=\\beta_0+\\beta_1X_i\\): true mean is not observable \\(\\epsilon_i=Y_i-E[Y_i|X_i]\\): true errors, unobservable Estimators \\(\\widehat{\\beta}_0\\), \\(\\widehat{\\beta}_1\\): estimators, computed using observables \\(\\widehat{Y}_i=\\widehat{E}[Y_i|X_i]=\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i\\), estimated mean (fitted value) \\(\\widehat{\\epsilon}_i=Y_i-\\widehat{Y}_i\\), estimated errors (residuals) Note: textbook uses \\(e_i\\) to denote estimated errors. I prefer using the hat notation, \\(\\widehat{\\epsilon}_i\\), to emphasize it is an \\(estimated\\) error.\n\n\nParameter Estimation: Deterministic Model Consider a model for temperature : \\(C=\\) degrees Celsius \\(F=\\) Fahrenheit \\(F_i = \\beta_0 + \\beta_1C_i\\) at day \\(i\\)\nIf I give you data about two days: \\((C=10, F=50)\\) and \\((C=20,F=68)\\), can you determine \\(\\beta_0\\) and \\(\\beta_1\\)?\n\n\nParameter Estimation when there is noise SLR: \\(Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\)\n\\(\\beta_0 + \\beta_1 X_i\\): mean component (deterministic)\n\\(\\epsilon_i\\): error component (random)\nImpact of noise If there was no \\(\\epsilon_i\\), we could use any two \\((X_i,Y_i)\\) pairs to estimate \\(\\beta_0\\) and \\(\\beta_1\\)\nDue to presence of \\(\\epsilon_i\\), estimation is more complicated\nRequire a method to estimate our parameters which uses all data points\n\n\nMethod of Estimation Scatter plot of data:\n\n\n\n\nimage\n\n\n\nAn infinite number of lines of the form \\(E[Y_i|X_i]=\\beta_0+\\beta_1X_i\\) are available Need to find the line which best fits the data \\(\\ldots\\) Need to select ‘best’ version of (\\(\\widehat{\\beta}_0,\\widehat{\\beta}_1\\))\n\n\nMethod of Estimation How to find a best-fitting line?\n\n\n\n\nimage\n\n\n\nMethod: Least Squares Estimation (or OLS: ordinary least squares) Idea: chooses the line that minimizes the sum of squares of the vertical distances from the observed points to the line.\n\n\nLeast Squares Method The Least Squares Estimators (LSEs) of \\(\\beta_0\\) and \\(\\beta_1\\) are the estimators which minimize the sum of squares error (SSE): \\[\\begin{aligned}\nSSE & = & \\sum_{i=1}^n \\widehat{\\epsilon}_i^2 ~~ = ~~~~~~~~~~~~~~\\nonumber \\\\\n&  & \\nonumber \\\\\n& = & \\nonumber\n\\end{aligned}\\] To minimize \\(SSE\\), find the values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) which solve the following system of equations: \\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial\\widehat{\\beta}_0} =0  & &\n\\frac{\\partial SSE}{\\partial\\widehat{\\beta}_1} =0 \\nonumber\n\\end{aligned}\\]\n\n\nLeast Squares Estimators\n\\[0=\\frac{\\partial SSE}{\\partial\\widehat{\\beta}_0} =\n-2\\sum_{i=1}^n (Y_i-\\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)\n\\label{eq:derbeta0}\\] \\[0=\\frac{\\partial SSE}{\\partial\\widehat{\\beta}_1} =\n-2\\sum_{i=1}^n X_i(Y_i-\\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)\n\\label{eq:derbeta1}\\]\n\n\nLeast Squares Estimators From ([eq:derbeta0]) we have: \\[\\boxed{\\widehat{\\beta}_0 =\\overline{Y}-\\widehat{\\beta}_1\\overline{X}}\\label{beta0}\\]\nSubstitute \\(\\widehat{\\beta}_0\\) into ([eq:derbeta1]) we get \\[\\sum_{i=1}^nX_i(Y_i {\\color{red}{ -\\overline{Y}+\\widehat{\\beta}_1\\overline{X}}} -\n\\widehat{\\beta}_1X_i) = 0\\] Thus \\[\\begin{aligned}\n\\sum_{i=1}^nX_i(Y_i-\\overline{Y}) & = & \\widehat{\\beta}_1\n\\sum_{i=1}^nX_i(X_i-\\overline{X}) \\nonumber\n\\end{aligned}\\] Which gives: \\[\\boxed{\\widehat{\\beta}_1 = \\frac{SSXY}{SSX}},\\label{beta1}\\] where \\(SSXY = {\\sum_{i=1}^n X_i(Y_i-\\overline{Y}), \\;\\;SSX = \\sum_{i=1}^n X_i(X_i-\\overline{X})}\\)\n\n\nA little more on \\(SSXY\\) and \\(SSX\\)\n\n\\(SSXY = {\\sum_{i=1}^n X_i(Y_i-\\overline{Y}), \\;\\;SSX = \\sum_{i=1}^n X_i(X_i-\\overline{X})}\\)\n\nHome exercise: Show that \\[\\begin{split}\nSSXY &= \\sum_{i=1}^n(Y_i-\\overline{Y})(X_i-\\overline{X})= n\\left\\{\\overline{X\\cdot Y}-\\overline{X}\\cdot\\overline{Y}\\right\\}\\\\\nSSX &= \\sum_{i=1}^n(X_i-\\overline{X})^2= n\\left\\{\\overline{X^2}-\\overline{X}^2\\right\\}\n\\end{split}\\] “The slope is the sample covariance of \\(X\\) and \\(Y\\), divided by the sample variance of \\(X\\)\" Recall \\(\\text{cov}(X,Y)=E[(X-E[X])(Y-E[Y])]=E[XY]-E[X]E[Y]\\) Recall\\(Var(X)=E[(X-E[X])^2]=E[X^2]-E[X]^2\\)\nTurns out this applies to sample variance and covariance as well. Helpful to keep in mind that”sum of centered variables is zero\":\n\n\nA little more on \\(SSXY\\) and \\(SSX\\)\nHome exercise: Show that\n“The slope is the sample covariance of \\(X\\) and \\(Y\\), divided by the sample variance of \\(X\\)\", i.e., \\(\\widehat{\\beta}_1=\\frac{\\frac{1}{n-1}SSXY}{\\frac{1}{n-1}SSX}\\), where \\(\\frac{1}{n-1}SSXY = \\frac{1}{n-1}\\sum_{i=1}^n(Y_i-\\overline{Y})(X_i-\\overline{X})\\) is the sample covariance \\(\\frac{1}{n-1}SSX = \\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\overline{X})^2\\) is the sample variance Recall \\(\\text{cov}(X)=E[(X-E[X])(Y-E[Y])]\\)\nRecall\\(Var(X)=E[(X-E[X])^2]\\) Helpful to keep in mind that”sum of centered variables is zero\": \\(\\sum_i (Y_i-\\overline{Y})=\\sum_i (X_i-\\overline{X})=0\\)\n\n\nReview and Preview Suppose we have \\(n\\) (sample size) pairs of data \\((X_i,Y_i),i=1\\dots,n\\) \\(X\\)’s are fixed values \\(Y\\)’s are randomly selected from the population at a given \\(X\\) value We assume a linear relationship \\(Y_i=\\beta_0+\\beta_1X_i+\\epsilon_i\\), \\(\\epsilon_i\\sim (0,\\sigma^2)\\) The errors \\(\\epsilon_i\\) are at least uncorrelated Given the data we can estimate \\(\\beta_0,\\beta_1\\) \\(\\widehat{\\beta}_0 =\\overline{Y}-\\widehat{\\beta}_1\\overline{X}\\) \\(\\widehat{\\beta}_1 = \\frac{SSXY}{SSX}\\) Because the values of \\(\\widehat{\\beta}_0,\\widehat{\\beta}_1\\) depend on the particular random sample drawn from the population, these quantities are considered random variables (estimators). Once we “plug in\" observed data, we have estimates.\n\n\nReview and Preview A typical goal of the analysis is to infer (draw inference on) the values of \\(\\beta_0,\\beta_1\\) supported by the data (point and interval estimates), as well as test if particular values of \\(\\beta_0,\\beta_1\\) are consistent with the data (hypothesis testing where e.g., \\(H_0:\\beta_1=b_1\\), and \\(b_1\\) is a given constant such as 0). To draw inference (interval estimates or tests) we need to know the sampling distribution of the estimators \\(\\widehat{\\beta}_0,\\widehat{\\beta}_1\\), i.e., What are the average values of \\(\\widehat{\\beta}_0,\\widehat{\\beta}_1\\) if we were able to take repeated samples of the population How much variation is there in the values of \\(\\widehat{\\beta}_0,\\widehat{\\beta}_1\\) from one random sample to another Given the model assumptions, we can derive Average values: \\(E[\\widehat{\\beta}_0]=\\beta_0\\), \\(E[\\widehat{\\beta}_1]=\\beta_1\\) Variation: \\(Var(\\widehat{\\beta}_0)=\\sigma^2(\\frac{1}{n}+\\frac{\\bar{X}^2}{SSX})\\), \\(Var(\\widehat{\\beta}_1)=\\frac{\\sigma^2}{SSX}\\)\n\n\nReview and Preview Since \\(E[\\widehat{\\beta}_0]=\\beta_0\\) and \\(E[\\widehat{\\beta}_1]=\\beta_1\\), we know that the estimators (formulas) will give estimates that are centered at the true values.\n\n\n0.3\n\n\n\n\nimage\n\n\n\n\n0.7 Histogram of observed values of \\(\\widehat{\\beta}_1\\) if we took many repeated samples from the population Since we have only one particular dataset (sample), we are only able to observe one value of the slope, not the whole distribution\n\n\nThe formulas for \\(Var(\\widehat{\\beta}_0),Var(\\widehat{\\beta}_1)\\) tell us how much spread there is in this histogram. However, the formulas also involve \\(\\sigma^2\\) (the variance of errors). So we further estimate \\(\\sigma^2\\) before drawing inference. We do this by “plug in\" an estimate of \\(\\sigma^2\\), specifically \\(\\widehat{\\sigma}^2=MSE=\\frac{\\sum_{i=1}^n(Y_i-\\widehat{Y}_i)^2}{n-2}\\) Hence \\(\\widehat{Var}(\\widehat{\\beta}_0)=\\widehat{\\sigma}^2(\\frac{1}{n}+\\frac{\\bar{X}^2}{SSX})=MSE(\\frac{1}{n}+\\frac{\\bar{X}^2}{SSX})\\) Hence \\(\\widehat{Var}(\\widehat{\\beta}_1)=\\frac{\\widehat{\\sigma}^2}{SSX}=\\frac{MSE}{SSX}\\)\n\n\nProperties of Least Squares Estimators\n\n\nProperties of Estimators (Definitions) Suppose \\(\\widehat{\\theta}\\) is an estimator of \\(\\theta\\) \\(\\widehat{\\theta}\\): estimator (random, depends on random sample of data);\n\\(\\theta\\): estimand (fixed, generally unknown true value)\nVarious criteria used to evaluate estimators: Bias: \\(\\mbox{bias}(\\widehat{\\theta})=E[\\widehat{\\theta}]-\\theta\\) We say \\(\\widehat{\\theta}\\) is an unbiased estimator if \\(E[\\widehat{\\theta}]=\\theta\\)\nSampling variance: \\(Var(\\widehat{\\theta})=E[\\widehat{\\theta}-E(\\widehat{\\theta})]^2\\)\nMean squared error: \\(mse(\\widehat{\\theta})=Var(\\widehat{\\theta}) + \\mbox{bias}(\\widehat{\\theta})^2\\) Often there is a trade-off between bias and sampling variance, which is measured by the MSE\nOur goal in the next few slides is to find: \\(E(\\widehat{\\beta}_0),~E(\\widehat{\\beta}_1),~Var(\\widehat{\\beta}_0),~Var(\\widehat{\\beta}_1)\\)\n\n\nRecall the expected value of sums of random variables, from Module A: Let \\(a_1,\\ldots,a_n\\) be constants. Recall:\n\\(E[a_iY_i]=a_iE[Y_i]\\) \\[E\\left[\\sum_{i=1}^n a_i Y_i \\right] = \\sum_{i=1}a_i E[Y_i]\\label{sumofrv}\\] Note: result does not require independence among the \\(Y_i\\)’s\n\n\nExpected value of \\(\\widehat{\\beta}_1\\) Claim: \\(\\widehat{\\beta}_1\\) is an unbiased estimator of \\(\\beta_1\\), i.e., \\(E(\\widehat{\\beta}_1)=\\beta_1\\) Proof:\n\n\nExpected value of \\(\\widehat{\\beta}_0\\) Claim: \\(\\widehat{\\beta}_0\\) is an unbiased estimator of \\(\\beta_0\\), i.e., \\(E(\\widehat{\\beta}_0)=\\beta_0\\) Proof: \\[\\begin{aligned}\nE[\\widehat{\\beta}_0] & \\stackrel{(\\ref{beta0})}{\\longeq} &\nE[\\overline{Y}-\\widehat{\\beta}_1\\overline{X}]\\stackrel{ \\text{E(sum)=sum(E)} }{\\longeq}E[\\overline{Y}] - E[\\widehat{\\beta}_1\\overline{X}]   \\\\\n& \\longeq &\nn^{-1}\\sum_{i=1}^nE[Y_i] - E[\\widehat{\\beta}_1\\overline{X}]  \\\\\n& \\stackrel{\\text{model def}}{\\longeq} &\nn^{-1}\\sum_{i=1}^n E[\\beta_0+\\beta_1 X_i+\\epsilon_i]-\\beta_1\\overline{X} \\\\\n& \\stackrel{E[\\epsilon_i]=0}{\\longeq} & n^{-1}\\sum_{i=1}^n\n(\\beta_0+\\beta_1X_i)-\\beta_1\\overline{X}  \\\\\n& \\longeq & \\beta_0\n\\end{aligned}\\]\n\n\nRecall the variance of sums of random variables, from Module A:\nLet \\(a_1,\\ldots,a_n\\) be constants. Recall: \\(Var(a_iY_i)=a_i^2Var(Y_i)\\) \\(\\mbox{cov}(X,aY)=a\\mbox{cov}(X,Y)\\)\n\\(\\mbox{cov}(X,Y_1+Y_2)=\\mbox{cov}(X,Y_1)+\\mbox{cov}(X,Y_2)\\) \\(Var\\left( \\sum_{i=1}^n a_iY_i \\right) = \\sum_{i=1}^n  \\sum_{j=1}^n a_i a_j \\mbox{cov}(Y_i,Y_j)\\)\n\\({\\color{white}{Var\\left( \\sum_{i=1}^n a_iY_i \\right)}}=\\sum_{i=1}^{n}a_i^2Var(Y_i) +2 \\sum_{i=1}^{n-1}  \\sum_{j=i+1}^n a_i a_j \\mbox{cov}(Y_i,Y_j)\\) In particular \\[\\boxed{\n        Var(a_1Y_1+a_2Y_2)=a_1^2Var(Y_1)+a_2^2Var(Y_2)+2a_1a_2\\text{cov}(Y_1,Y_2)\n    }\\label{varof2sum}\\]\nIf \\(Y_i\\)’s are uncorrelated, then \\[\\boxed{Var\\left(\\sum_{i=1}^n a_iY_i\\right) = \\sum_{i=1}^n a_i^2Var(Y_i)}\\label{varofsum}\\]\n\n\nVariance of \\(\\widehat{\\boldbeta}_1\\) Claim: \\(Var(\\widehat{\\beta}_1)=\\sigma^2/SSX\\) Proof:\n\\[\\begin{aligned}\nVar(\\widehat{\\beta_1}) & \\longeq & Var\\left(  \\frac{SSXY}{SSX} \\right) \\stackrel[\\text{constant}]{SSX\\text{ is a}}{\\longeq} SSX^{-2}Var(SSXY) \\\\\n& \\longeq & SSX^{-2}Var\\left\\{ \\sum_{i=1}^n Y_i (X_i-\\overline{X}) \\right\\}\\\\\n& \\stackrel{(\\ref{varofsum}) \\text{ w/ } a_i=X_i-\\overline{X}}{\\longeq} & SSX^{-2}\\sum_{i=1}^n \\left\\{(X_i-\\overline{X})^2 Var(Y_i) \\right\\}\\\\\n& \\longeq & SSX^{-1} \\sigma^2\n\\nonumber\n\\end{aligned}\\]\n\n\nVariance of \\(\\widehat{\\boldbeta}_0\\) Claim: \\(Var(\\widehat{\\beta}_0) = \\sigma^2 \\left\\{\\frac{1}{n} + \\frac{\\overline{X}^2}{SSX} \\right\\}\\) Proof: \\[\\begin{aligned}\nVar(\\widehat{\\beta}_0) & = & Var(\\overline{Y} - \\widehat{\\beta}_1\\overline{X} ) \\\\\n&& \\mbox{Here $\\overline{Y}$ and $\\widehat{\\beta}$ are random, $\\overline{X}$ is fixed}\\\\\n& \\stackrel{(\\ref{varof2sum})}{\\longeq} & Var(\\overline{Y}) + \\overline{X}^2 Var(\\widehat{\\beta}_1) - 2\\overline{X}\\mbox{cov}(\\overline{Y},\\widehat{\\beta}_1)\\\\\n&& \\mbox{Next slide shows that cov}(\\overline{Y},\\widehat{\\beta}_1) =0\\mbox{, thus,}\\\\\n& = & \\frac{\\sigma^2}{n} + \\overline{X}^2 \\frac{\\sigma^2}{SSX}\n\\end{aligned}\\]\n\n\nCovariance: \\(\\overline{Y},\\widehat{\\boldbeta}_1\\) Claim: \\(\\mbox{cov}(\\overline{Y},\\widehat{\\beta}_1)=0\\) Proof:\n$$\n\\[\\begin{aligned}\n%       &&\\mbox{cov}(\\overline{Y},\\widehat{\\beta}_1) \\\\\n        \\mbox{cov}(\\overline{Y},\\widehat{\\beta}_1) & \\stackrel{ (\\ref{beta1}) }{\\longeq} & \\mbox{cov}\\big(\\overline{Y},\\frac{\\sum_{i=1}^n (X_i-\\overline{X})Y_i }{SSX}\\big) \\\\\n        &\\stackrel[\\text{cov(a)=a(cov)}]{ \\text{cov(sum)=sum(cov)} }{\\longeq} & \\frac{1}{SSX}\n        \\sum_{i=1}^n    (X_i-\\overline{X})\\mbox{cov}(\\overline{Y}, Y_i)\\\\\n        &{\\longeq} & \\frac{1}{SSX}\n        \\sum_{i=1}^n    (X_i-\\overline{X})\\mbox{cov}(\\frac{1}{n}\\sum_{\\myi=1}^nY_\\myi, Y_i)\\\\\n        %%%\n        &\\stackrel[\\text{for all $i\\neq j$}]{\\text{cov$(Y_i,Y_j)=0$}}{\\longeq}&\n        \\frac{1}{SSX}\n        \\sum_{i=1}^n    (X_i-\\overline{X})\\underbrace{cov(\\frac{1}{n}Y_i,Y_i)}_{\\text{$\\sigma^2/n$}}\\\\\n        &\\stackrel{\\text{$\\sum_{i=1}^n  (X_i-\\overline{X})\\sigma^2=0$}}{\\longeq}&0\n    \n\\end{aligned}\\]\n$$\n\n\nEstimating the sampling variance\nof the regression coefficients\n\n\nEstimating \\(Var(\\widehat{\\beta}_0)\\) and \\(Var(\\widehat{\\beta}_1)\\) We have shown that the true sampling variances of the regression coefficients are: \\(Var(\\widehat{\\beta}_1)=\\sigma^2/SSX\\) \\(Var(\\widehat{\\beta}_0)= \\sigma^2 \\left\\{\\frac{1}{n} + \\frac{\\overline{X}^2}{SSX} \\right\\}\\) However, \\(\\sigma^2\\) is generally unknown. In SLR we use the estimator: \\[\\boxed{\\widehat{\\sigma}^2 =\\frac{1}{n-2} \\sum_{i=1}^n \\widehat{\\epsilon}_i^2}\\stackrel{\\text{def}}{\\longeq}\\frac{SSE}{n-2}\\stackrel{\\text{def}}{\\longeq}MSE\\]\nwhere \\(\\widehat{\\epsilon}_i=Y_i-\\widehat{Y}_i\\) is the residual (estimated errors)\n\\(\\widehat{\\sigma}^2\\) reflects average squared distance between \\(Y_i\\)’s and \\(\\widehat{Y}_i\\)’s\nLarge \\(\\widehat{\\sigma}^2\\) implies large amount of scatter in \\(Y_i\\)’s around estimated regression line\n\n\nEstimating \\(Var(\\widehat{\\beta}_0)\\) and \\(Var(\\widehat{\\beta}_1)\\) (continued) Recall sample variance when \\(\\mu\\) is unknown (module A) Sample: \\(Y_1,\\ldots,Y_n\\)\nEstimated mean: \\(\\widehat{E}[Y_i]=\\overline{Y}\\) for all \\(i\\)\nSample variance: \\[\\widehat{Var}(Y_i) = \\frac{1}{  {\\color{red}n-1}  } \\sum_{i=1}^n\n(Y_i-\\overline{Y})^2\\] Estimating sample variance of error \\(\\ldots\\) Sample: \\((X_1,Y_1),\\ldots,(X_n,Y_n)\\)\nEstimated mean: \\(\\widehat{E}[Y_i|X_i]=\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i\\)\nSample variance: \\[\\widehat{Var}(Y_i) =\\widehat{\\sigma}^2=  \\frac{1}{  {\\color{red}n-2}  } \\sum_{i=1}^n\n(Y_i-\\widehat{Y}_i)^2\\] Note: difference in divisor (Q: Why?)\n\n\nEstimator of \\(\\boldsigma^2\\) in SLR\nThe \\(-2\\) in the denominator comes from the number of degrees of freedom \\(df\\) in the \\(SSE\\) \\(df =\\) the number of observations that are free to vary\n\\(df\\) \\(=\\) number of contributions (\\(n\\))- number of constraints Example for \\(df\\) in SSE: Suppose we observe \\(Y_1,\\dots,Y_n\\) and \\(\\mu_Y\\) is known, then \\(df = n\\) \\(Y_1\\), \\(Y_2\\) (\\(n=2\\)) and \\(\\mu_Y\\) is unknown and estimated by \\(\\overline{Y}\\), then once \\(Y_1\\) and \\(\\overline{Y}\\) are known, \\(Y_2\\) is not free to vary. There is 1 df lost in SSE due to estimation of \\(\\overline{Y}\\), \\(df = n-1\\) \\((X_1,Y_1),\\ldots,(X_n,Y_n)\\) and estimated \\(\\widehat{\\beta}_0,\\widehat{\\beta}_1\\). There are 2 df lost in SSE due to estimation of \\(\\widehat{\\beta}_0,\\widehat{\\beta}_1\\), \\(df = n-2\\) For \\(\\widehat{\\sigma}^2\\): two constraints from the two estimated parameters \\(\\widehat{\\beta}_0\\), \\(\\widehat{\\beta}_1\\) It can be shown that \\(\\widehat{\\sigma}^2\\) as defined in previous slide is an unbiased estimator of \\(\\sigma^2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation Interpretation check list: Units, direction (for slope not intercept), magnitude, “average/mean\", “estimated\"\n\\(\\widehat{\\beta}_0 = 646.483\\)\n\\(\\widehat{\\beta}_1=-14.041\\)\n\n\nInterpretation Interpretation check list: Units, direction, magnitude, “estimated\", “average/mean\", in which population\n\\(\\widehat{\\beta}_0 = 646.483\\) The estimated mean sleep time for someone who is age 0 (a newborn) is 646.483 minutes\n\\(\\widehat{\\beta}_1=-14.041\\) The estimated mean difference in sleep time for one year increase in age is -14.041 minutes Comparing two children who differ in age by one year, the older individual has an estimated mean sleep time that is 14.041 minutes lower. We estimated that children who are one year older sleep, on average, 14.041 minutes less.\n\n\nAcknowledgement\n\n\n\n\n\n\nLan Luo\n\n\nUniversity of Iowa\n\n\n\nThank you for your notes!\n\n\nQuestions?"
  },
  {
    "objectID": "slides/01_SLR.html#lets-start-with-an-example",
    "href": "slides/01_SLR.html#lets-start-with-an-example",
    "title": "Simple Linear Regression (SLR)",
    "section": "Let’s start with an example",
    "text": "Let’s start with an example\n\n\n\n\n\n\n\n\nAverage life expectancy vs. female literacy rate\n\nEach point on the plot is for a different country\n\\(x\\) = country’s adult female literacy rate\n\\(y\\) = country’s average life expectancy (years)\nData are from Gapminder (2011)\n\n\n\n\n\\[\\begin{aligned}\n\\hat{\\text{life expectancy}} & =  50.9 + 0.232\\cdot\\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/01_SLR.html#questions-we-can-ask-with-this-model",
    "href": "slides/01_SLR.html#questions-we-can-ask-with-this-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Questions we can ask with this model",
    "text": "Questions we can ask with this model"
  },
  {
    "objectID": "slides/01_SLR.html#dependent-vs.-independent-variables",
    "href": "slides/01_SLR.html#dependent-vs.-independent-variables",
    "title": "Simple Linear Regression (SLR)",
    "section": "Dependent vs. Independent Variables",
    "text": "Dependent vs. Independent Variables"
  },
  {
    "objectID": "slides/01_SLR.html#association-vs.-prediction",
    "href": "slides/01_SLR.html#association-vs.-prediction",
    "title": "Simple Linear Regression (SLR)",
    "section": "Association vs. prediction",
    "text": "Association vs. prediction"
  },
  {
    "objectID": "slides/01_SLR.html#study-design",
    "href": "slides/01_SLR.html#study-design",
    "title": "Simple Linear Regression (SLR)",
    "section": "Study Design",
    "text": "Study Design"
  },
  {
    "objectID": "slides/data/NHANES_EDA.html",
    "href": "slides/data/NHANES_EDA.html",
    "title": "NHANES",
    "section": "",
    "text": "NHANES\n\nlibrary(NHANES)\nlibrary(skimr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\ndata(\"NHANES\")\n\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n# 16 Depressed             3327         0.667 FALSE          3 \"Non: 5246, Sev: 1009, Mos: 418\"            \n# 17 SleepTrouble          2228         0.777 FALSE          2 \"No: 5799, Yes: 1973\"                       \n# 18 PhysActive            1674         0.833 FALSE          2 \"Yes: 4649, No: 3677\"          \n\n\nNHANES18 &lt;- NHANES %&gt;% dplyr::filter(Age &gt;= 18)\nNHANES18 %&gt;% tabyl(Depressed, PhysActive)\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n      &lt;NA&gt;  423  385\n\nNHANES18 %&gt;% drop_na(Depressed) %&gt;% tabyl(Depressed, PhysActive)\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n\nNHANES18Dep &lt;- NHANES18 %&gt;% drop_na(Depressed)\nNHANES18Dep %&gt;% \n  tabyl(Depressed, PhysActive) %&gt;% \n  adorn_totals()\n\n Depressed   No  Yes\n      None 2297 2949\n   Several  538  471\n      Most  275  143\n     Total 3110 3563\n\nchisq_Dep_Phys&lt;- chisq.test(NHANES18Dep$Depressed, NHANES18Dep$PhysActive)\ntidy(chisq_Dep_Phys)\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                    \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      96.9 9.26e-22         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys$expected\n\n                     NHANES18Dep$PhysActive\nNHANES18Dep$Depressed        No       Yes\n              None    2444.9363 2801.0637\n              Several  470.2518  538.7482\n              Most     194.8119  223.1881\n\nchisq_Dep_Phys$observed\n\n                     NHANES18Dep$PhysActive\nNHANES18Dep$Depressed   No  Yes\n              None    2297 2949\n              Several  538  471\n              Most     275  143\n\nlibrary(moderndive)\nset.seed(5348)\n# 5347\nNHANES18Dep200 &lt;- NHANES18Dep %&gt;%\n  rep_sample_n(size = 200, reps = 1, replace = FALSE)\n\nNHANES18Dep200 %&gt;% \n  tabyl(Depressed, PhysActive) %&gt;% \n  adorn_totals()\n\n Depressed No Yes\n      None 63  79\n   Several 19  18\n      Most 10  11\n     Total 92 108\n\nchisq_Dep_Phys200&lt;- chisq.test(NHANES18Dep200$Depressed, NHANES18Dep200$PhysActive)\ntidy(chisq_Dep_Phys200)\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1     0.601   0.740         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys200$expected\n\n                        NHANES18Dep200$PhysActive\nNHANES18Dep200$Depressed    No   Yes\n                 None    65.32 76.68\n                 Several 17.02 19.98\n                 Most     9.66 11.34\n\nchisq_Dep_Phys200$observed\n\n                        NHANES18Dep200$PhysActive\nNHANES18Dep200$Depressed No Yes\n                 None    63  79\n                 Several 19  18\n                 Most    10  11\n\n#------------\nset.seed(5349)\nNHANES18Dep400 &lt;- NHANES18Dep %&gt;%\n  rep_sample_n(size = 400, reps = 1, replace = FALSE)\n\nNHANES18Dep400 %&gt;% \n  tabyl(PhysActive, Depressed) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  adorn_title \n\n            Depressed                   \n PhysActive      None Several Most Total\n         No       115      32   27   174\n        Yes       199      26    1   226\n      Total       314      58   28   400\n\nchisq_Dep_Phys400&lt;- chisq.test(NHANES18Dep400$Depressed, NHANES18Dep400$PhysActive)\ntidy(chisq_Dep_Phys400)\n\n# A tibble: 1 × 4\n  statistic       p.value parameter method                    \n      &lt;dbl&gt;         &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      41.2 0.00000000115         2 Pearson's Chi-squared test\n\nchisq_Dep_Phys400$observed\n\n                        NHANES18Dep400$PhysActive\nNHANES18Dep400$Depressed  No Yes\n                 None    115 199\n                 Several  32  26\n                 Most     27   1\n\nchisq_Dep_Phys400$expected\n\n                        NHANES18Dep400$PhysActive\nNHANES18Dep400$Depressed     No    Yes\n                 None    136.59 177.41\n                 Several  25.23  32.77\n                 Most     12.18  15.82\n\nset.seed(5349)\nNHANES18Dep_PAy100 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"Yes\") %&gt;% \n  rep_sample_n(size = 100, reps = 1, replace = FALSE)\nNHANES18Dep_PAy100 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 85    0.85\n   Several 12    0.12\n      Most  3    0.03\n\nNHANES18Dep_PAn100 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"No\") %&gt;% \n  rep_sample_n(size = 100, reps = 1, replace = FALSE)\nNHANES18Dep_PAn100 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 78    0.78\n   Several 17    0.17\n      Most  5    0.05\n\n(DepPA200_table &lt;- matrix(c(83, 12, 5, 78, 16, 6), nrow = 2, ncol = 3, byrow = T))\n\n     [,1] [,2] [,3]\n[1,]   83   12    5\n[2,]   78   16    6\n\ndimnames(DepPA200_table) &lt;- list(\"PA\" = c(\"Yes\", \"No\"),   # row names\n                              \"Depression\" = c(\"None\", \"Several\", \"Most\"))  # column names\nDepPA200_table\n\n     Depression\nPA    None Several Most\n  Yes   83      12    5\n  No    78      16    6\n\nchisq.test(DepPA200_table) \n\n\n    Pearson's Chi-squared test\n\ndata:  DepPA200_table\nX-squared = 0.81762, df = 2, p-value = 0.6644\n\nchisq.test(DepPA200_table)$expected\n\n     Depression\nPA    None Several Most\n  Yes 80.5      14  5.5\n  No  80.5      14  5.5\n\nset.seed(5349)\nNHANES18Dep_PAy50 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"Yes\") %&gt;% \n  rep_sample_n(size = 50, reps = 1, replace = FALSE)\nNHANES18Dep_PAy50 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 43    0.86\n   Several  6    0.12\n      Most  1    0.02\n\nNHANES18Dep_PAn50 &lt;- NHANES18Dep %&gt;% filter(PhysActive == \"No\") %&gt;% \n  rep_sample_n(size = 50, reps = 1, replace = FALSE)\nNHANES18Dep_PAn50 %&gt;% tabyl(Depressed)\n\n Depressed  n percent\n      None 30    0.60\n   Several 14    0.28\n      Most  6    0.12"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project Central",
    "section": "",
    "text": "{css, echo=FALSE} .title{   font-size: 40px;   color: #213c96;   background-color: #fff;   padding: 0px; }"
  },
  {
    "objectID": "project.html#labs",
    "href": "project.html#labs",
    "title": "Project Central",
    "section": "Labs",
    "text": "Labs\n\n\n\nLab\nDue Date\nTopics\n\n\n\n\nLab 1\n1/18\nExploring the question\n\n\nLab 2\n2/8\nExploring the data\n\n\nLab 3\n3/3\nA little more data exploration + Fitting and interpreting a model\n\n\nLab 4\n3/14\nBuilding a model\n\n\n\n\nHelp with BMI variable"
  },
  {
    "objectID": "project.html#report",
    "href": "project.html#report",
    "title": "Project Central",
    "section": "Report",
    "text": "Report\nReport Instructions\nDue 3/21/2024 at 11pm\n\nReading and listening sources\nIf you are interested in sources that discuss the social complexities of anti-fat bias, feel free to take a look at the following sources. Please be aware that these resources will discuss anti-fat bias and related histories, including racism and sexism.\n\nArticle: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nPodcast: Anti-Fat Bias by Maintenance Phase\nBook: Fearing the Black Body: The Racial Origins of Fat Phobia\n\nMultnomah County Library has unlimited loans for the audiobook\n\nBlog: Dances with Fat\n\nYou can subscribe to Ragen’s weekly newsletter for free\n\n\nIf you have additional sources that you would like to share, please send them to me!"
  },
  {
    "objectID": "labs/Lab_01.html",
    "href": "labs/Lab_01.html",
    "title": "Lab 1",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in the labs.\nYou can download the .qmd file for this lab here.\n\n\nThis lab will serve as an introduction to our quarter long project.\nThere will be no analysis in this lab. Instead, we are building our knowledge around the research question.\n\n\n\nEach lab will have a slightly different grading rubric. Since this lab does not include coding nor analysis, this portion of the rubric is excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nAnswers\nAnswers demonstrate completion and understanding of the needed activity*. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate completion and understanding of the needed activity*. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate completion and minimal understanding of the needed activity*. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate completion of needed activities*, although evidently rushed through. Answers seem rushed and with minimal thought.\nIt is evident that the needed activities* were not completed. Answers seem rushed and without thought.\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\n\n*Example of needed activity: if asked to read something, answers reflect the gained knowledge from the reading."
  },
  {
    "objectID": "labs/Lab_01.html#directions",
    "href": "labs/Lab_01.html#directions",
    "title": "Lab 1",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy. We can work out another way for you to turn in the labs.\nYou can download the .qmd file for this lab here.\n\n\nThis lab will serve as an introduction to our quarter long project.\nThere will be no analysis in this lab. Instead, we are building our knowledge around the research question.\n\n\n\nEach lab will have a slightly different grading rubric. Since this lab does not include coding nor analysis, this portion of the rubric is excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nAnswers\nAnswers demonstrate completion and understanding of the needed activity*. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate completion and understanding of the needed activity*. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate completion and minimal understanding of the needed activity*. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate completion of needed activities*, although evidently rushed through. Answers seem rushed and with minimal thought.\nIt is evident that the needed activities* were not completed. Answers seem rushed and without thought.\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\n\n*Example of needed activity: if asked to read something, answers reflect the gained knowledge from the reading."
  },
  {
    "objectID": "homework/HW1.html#section",
    "href": "homework/HW1.html#section",
    "title": "Homework 1",
    "section": "5.1",
    "text": "5.1\n\nLoad the dataset using the readxl package.\n\nThis readxl package was installed as a part of the tidyverse, however it does not get loaded when you load the tidyverse package and thus you need to do that separately.\nUse the command read_excel(), as shown below\n\n\n\nlibrary(readxl)\n# you might need to update the location of the data file\n# you can choose whatever name you like for the tibble when loading it into R's workspace \nch05q01 &lt;- read_excel(\"data/CH05Q01.xls\")\n\n\n(a)\nAdditional instructions: Create the scatterplots in R using ggplot, in addition to describing the relationships in the book’s instructions.\n\n\n(b)\nNote: This is asking for the regression models BEFORE you find the values of the coefficients.\n\n\n(c)\nNote: Now get the regression coefficients using R and plug them into the regression models from (b). You can get the coefficients from the R output - you don’t have to use the formulas.\n\n\n(d)\nNote: Instead of sketching the lines, create them using ggplot.\n\n\n(e)\nNote: You can get the CI’s from the R output - you don’t have to use the formulas.\n\n\n(f)\nNote: Instead of sketching, make the figure using ggplot. Get the asked for CI using R instead of estimating it based on the sketch."
  },
  {
    "objectID": "homework/HW1.html#section-1",
    "href": "homework/HW1.html#section-1",
    "title": "Homework 1",
    "section": "3.5",
    "text": "3.5"
  },
  {
    "objectID": "homework/HW1.html#section-2",
    "href": "homework/HW1.html#section-2",
    "title": "Homework 1",
    "section": "3.10",
    "text": "3.10"
  },
  {
    "objectID": "homework/HW1.html#section-3",
    "href": "homework/HW1.html#section-3",
    "title": "Homework 1",
    "section": "3.11",
    "text": "3.11"
  },
  {
    "objectID": "homework/HW1.html#section-4",
    "href": "homework/HW1.html#section-4",
    "title": "Homework 1",
    "section": "3.13",
    "text": "3.13"
  },
  {
    "objectID": "homework/HW1.html#section-5",
    "href": "homework/HW1.html#section-5",
    "title": "Homework 1",
    "section": "3.14",
    "text": "3.14"
  },
  {
    "objectID": "homework/HW1.html#section-6",
    "href": "homework/HW1.html#section-6",
    "title": "Homework 1",
    "section": "3.15",
    "text": "3.15\nAdditional instructions: Do the problem with both the p-value and critical value approaches."
  },
  {
    "objectID": "homework/HW1.html#section-7",
    "href": "homework/HW1.html#section-7",
    "title": "Homework 1",
    "section": "3.16",
    "text": "3.16\nAdditional instructions: You do not need to use the formula to do this problem. You can run it in R using the given data.\n\ngrp1 &lt;- c(132, 145, 124, 122, 165, 144, 151)\ngrp2 &lt;- c(141, 139, 172, 131, 150, 125)"
  },
  {
    "objectID": "homework/HW1.html#section-8",
    "href": "homework/HW1.html#section-8",
    "title": "Homework 1",
    "section": "3.17",
    "text": "3.17"
  },
  {
    "objectID": "homework/HW1.html#section-9",
    "href": "homework/HW1.html#section-9",
    "title": "Homework 1",
    "section": "3.20",
    "text": "3.20"
  },
  {
    "objectID": "homework/HW1.html#section-10",
    "href": "homework/HW1.html#section-10",
    "title": "Homework 1",
    "section": "3.21",
    "text": "3.21"
  },
  {
    "objectID": "homework/HW1.html#section-11",
    "href": "homework/HW1.html#section-11",
    "title": "Homework 1",
    "section": "3.22",
    "text": "3.22"
  },
  {
    "objectID": "homework/HW1.html#section-12",
    "href": "homework/HW1.html#section-12",
    "title": "Homework 1",
    "section": "3.23",
    "text": "3.23"
  },
  {
    "objectID": "homework/HW1.html#section-13",
    "href": "homework/HW1.html#section-13",
    "title": "Homework 1",
    "section": "3.25",
    "text": "3.25"
  },
  {
    "objectID": "homework/HW1.html#section-14",
    "href": "homework/HW1.html#section-14",
    "title": "Homework 1",
    "section": "5.1",
    "text": "5.1\n\nLoad the dataset using the readxl package.\n\nThis readxl package was installed as a part of the tidyverse, however it does not get loaded when you load the tidyverse package and thus you need to do that separately.\nUse the command read_excel(), as shown below\n\n\n\nlibrary(readxl)\n# you might need to update the location of the data file\n# you can choose whatever name you like for the tibble when loading it into R's workspace \nch05q01 &lt;- read_excel(\"data/CH05Q01.xls\")\n\n\n(a)\nAdditional instructions: Create the scatterplots in R using ggplot, in addition to describing the relationships in the book’s instructions.\n\n\n(b)\nNote: This is asking for the regression models BEFORE you find the values of the coefficients.\n\n\n(c)\nNote: Now get the regression coefficients using R and plug them into the regression models from (b). You can get the coefficients from the R output - you don’t have to use the formulas.\n\n\n(d)\nNote: Instead of sketching the lines, create them using ggplot.\n\n\n(e)\nNote: You can get the CI’s from the R output - you don’t have to use the formulas.\n\n\n(f)\nNote: Instead of sketching, make the figure using ggplot. Get the asked for CI using R instead of estimating it based on the sketch."
  },
  {
    "objectID": "homework/HW1.html#a---g",
    "href": "homework/HW1.html#a---g",
    "title": "Homework 1",
    "section": "5.4 (a - g)",
    "text": "5.4 (a - g)\n\nch05q04 &lt;- read_excel(\"./data/CH05Q04.xls\")\n\n\n(a)\nAdditional instructions: Run the model to get the regression coefficients and create a scatterplot with the regression line using ggplot.\n\n\n(b)\n\n\n(c)\nAdditional instructions: Calculate the CI using the formula. The problem gives the values for \\(S_{Y|X}\\) and \\(S_X\\). Verify these values using R.\n\n\n(d)\n\n\n(e)\nAdditional instructions: Create a new dataset without the outlier. Verify the regression coefficients using R and create a new scatterplot with regression line for the dataset without the outlier. Then “decide whether this outlier has any effect on your estimate of the IQ–DI relationship.”\n\n\n(f)\nAdditional instructions: Calculate the test statistic using the formula and find the p-value using the test statistic value. The problem gives the values for \\(S_{Y|X}\\) and \\(S_X\\). Verify these values using R. Check your work by comparing your answers to the linear model output. \n\n\n(g)"
  },
  {
    "objectID": "homework/HW1.html#questions",
    "href": "homework/HW1.html#questions",
    "title": "Homework 1",
    "section": "Questions",
    "text": "Questions\nThe following questions were adapted from this textbook.\n\nQuestion 1\nPlease use R code to determine the following answers. (adapted from problem 3.3 in Applied Regression Analysis and Other Multivariable Methods)\n\n\n\n\n\n\nType ?pnorm in the console to get some information on a potentially helpful function.\n\n\n\n\nPart a\nFrom a normal distribution with mean 4 and standard deviation 6, what is \\(P(X&gt;2)\\)?\n\n\nPart b\nFrom a normal distribution with mean 4 and standard deviation 6, for what value (in place of ??) would \\(P(X&gt;??) = 0.1\\)?\n\n\n\nQuestion 2\nSuppose that the height (\\(H\\)) of assigned-male-at-birth (AMAB) patients registered at a clinic has the normal distribution with mean 70 inches and variance 4. (adapted from problem 3.11 in Applied Regression Analysis and Other Multivariable Methods)\n\nPart a\nFor a random sample of patients of size \\(n = 25\\), the expression \\(P(\\bar{H} &lt; 65)\\), in which \\(\\bar{H}\\) denotes the sample mean height, is equivalent to saying \\(P(Z &lt; ?)\\)\n\n\n\n\n\n\n\\(Z\\) is a standard normal random variable.\n\n\n\n\n\nPart b\nUsing the pnorm function, show that the probability expressions in Part a are equal.\n\n\nPart c\nFind an interval \\((a, b)\\) such that \\(P(a&lt; \\bar{H} &lt;b) = 0.80\\) for the same random sample in Part a.\n\n\n\nQuestion 3\nTest the null hypothesis that the true population average height is the same for two independent groups from one hospital versus the alternative hypothesis that these two population averages are different, using the following data:\n\nGroup 1: [69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73]\nGroup 2: [67.54, 68.51, 71.84, 70.59, 71.52, 71.50]\n\nYou may assume that the populations from which the data come are each normally distributed, with equal population variances. What conclusion should be drawn, with \\(\\alpha = 0.05\\)?\n\n\n\n\n\n\nPlease attempt this problem using R. Take a look at the information for the t.test function. You will need to set x, y, alternative, and var.equal=T. You can use the below groups coded in R.\n\n\n\n\ngrp1 = c(69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73)\ngrp2 = c(67.54, 68.51, 71.84, 70.59, 71.52, 71.50)\n\n\n\nQuestion 4\nThe choice of an alternative hypothesis (\\(H_A\\) or \\(H_1\\)) should depend primarily on (choose all that apply). Explain your reasoning.\n\nthe data obtained from the study.\nwhat the investigator is interested in determining.\nthe critical region.\nthe significance level.\nthe power of the test.\n\n\n\nQuestion 5\nVisit this site on dplyr.\nFor one of the functions that we have not discussed in class, please use it on the dds.discr dataset. Please write in words what you would like to perform, then write the code.\nNote: the dds.discr dataset is an .Rda file. Instead of using read_csv() or read_excel(), you can use load().\n\n\nQuestion 6\nThe accompanying data CH05Q01 gives the dry weights (Y) of 11 chick embryos ranging in age from 6 to 16 days (X ). Also given in the data are the values of the common logarithms of the weights (Z).\n\nLoad the dataset using the readxl package.\n\nThis readxl package was installed as a part of the tidyverse, however it does not get loaded when you load the tidyverse package and thus you need to do that separately.\nUse the command read_excel(), as shown below\n\n\n\nlibrary(readxl)\n# you might need to update the location of the data file\n# you can choose whatever name you like for the tibble when loading it into R's workspace \nch05q01 &lt;- read_excel(\"./data/CH05Q01.xls\")\n\n\nPart a\nCreate two scatterplots in R using ggplot:\n\nBetween age (X) and dry weight (Y)\nBetween age (X) and log10 dry weight (Z)\n\nObserve the following two scatter diagrams. Describe the relationships between age (X) and dry weight (Y) and between age and log10 dry weight (Z).\n\n\nPart b\nState the population simple linear regression models for these two regressions: Y regressed on X and Z regressed on X.\n\n\n\n\n\n\nThis is asking for the regression models BEFORE you find the values of the coefficients.\n\n\n\n\n\nPart c\nDetermine the least-squares estimates of each of the regression lines in part (b).\n\n\n\n\n\n\nNow get the regression coefficients using R and plug them into the regression models from (b). You can get the coefficients from the R output!\n\n\n\n\n\nPart d\nUsing ggplot, create the respective best-fit lines on your plots. Which of the two regression lines has the better fit? Based on your answers to parts (a)–(c), is it more appropriate to run a linear regression of Y on X or of Z on X? Explain.\n\n\nPart e (to be covered on Monday 1/22)\nFor the regression that you chose as being more appropriate in part (d), find 95% confidence intervals for the true slope and intercept. Interpret each interval with regard to the null hypothesis that the true value is 0.\n\n\n\n\n\n\nYou can get the CI’s from the R output\n\n\n\n\n\n\n\n\n\nUpdate 1/21/2024\n\n\n\nI removed some of the instructions because they were a little confusing. No need to go through the formal process of a hypothesis test.\n\n\n\n\nPart f (to be covered on Monday 1/22)\nFor the regression that you chose as being more appropriate in part (d), add 95% confidence and prediction bands. Using your sketch, find and interpret an approximate 95% confidence interval for the mean response of an 8-day-old chick.\n\n\n\n\n\n\nUpdate 1/21/2024\n\n\n\nI removed prediction bands because we did not cover them!\n\n\n\n\n\n\n\n\nUpdate 1/22/2024\n\n\n\nYou do NOT need to do this problem (Question 6, part f). We did NOT get to it on Monday, so I don’t expect you to do it for the homework."
  },
  {
    "objectID": "homework/HW2.html#section",
    "href": "homework/HW2.html#section",
    "title": "Homework 2",
    "section": "(1)",
    "text": "(1)\nUse R to create the ANOVA table for the regression described in the exercise."
  },
  {
    "objectID": "homework/HW2.html#section-1",
    "href": "homework/HW2.html#section-1",
    "title": "Homework 2",
    "section": "(2)",
    "text": "(2)\nUsing the values in the Df and Sum Sq columns, show how the remaining values in the table are calculated."
  },
  {
    "objectID": "homework/HW2.html#section-2",
    "href": "homework/HW2.html#section-2",
    "title": "Homework 2",
    "section": "(3)",
    "text": "(3)\nCalculate the SSY and its degrees of freedom. Use these values to calculate the standard deviation of the outcome variable."
  },
  {
    "objectID": "homework/HW2.html#section-3",
    "href": "homework/HW2.html#section-3",
    "title": "Homework 2",
    "section": "(4)",
    "text": "(4)\nWhat are the hypotheses being tested in the ANOVA table? Make sure to include a description of the parameter being tested in the context of the research question."
  },
  {
    "objectID": "homework/HW2.html#section-4",
    "href": "homework/HW2.html#section-4",
    "title": "Homework 2",
    "section": "(5)",
    "text": "(5)\nUsing just the values in the ANOVA table, find the value of the t-distribution test statistic for testing the slope that one would find in the regression output of the linear model. What are the degrees of freedom for that t-distribution?"
  },
  {
    "objectID": "homework/HW2.html#section-5",
    "href": "homework/HW2.html#section-5",
    "title": "Homework 2",
    "section": "(6)",
    "text": "(6)\nShow the regression output for the linear model. Using just the regression output, calculate the SSR and SSE values in the ANOVA table."
  },
  {
    "objectID": "homework/HW2.html#section-6",
    "href": "homework/HW2.html#section-6",
    "title": "Homework 2",
    "section": "(1)",
    "text": "(1)\nCreate a scatterplot of the dependent and independent variables, and in words describe the their relationship. Is it reasonable to use a linear regression to model the relationship?"
  },
  {
    "objectID": "homework/HW2.html#section-7",
    "href": "homework/HW2.html#section-7",
    "title": "Homework 2",
    "section": "(2)",
    "text": "(2)\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in the previous question? Why or why not?"
  },
  {
    "objectID": "homework/HW2.html#section-8",
    "href": "homework/HW2.html#section-8",
    "title": "Homework 2",
    "section": "(3)",
    "text": "(3)\nTest whether the two variables are significantly correlated. Do this using the formula and then check your work with R’s test for correlations. Make sure to include the hypotheses and a conclusion."
  },
  {
    "objectID": "homework/HW2.html#section-9",
    "href": "homework/HW2.html#section-9",
    "title": "Homework 2",
    "section": "(4)",
    "text": "(4)\nCalculate the confidence interval for \\(\\rho\\) using the formula and verify that it matches the confidence interval in R’s test output. Include an interpretation of the confidence interval and also explain why the confidence interval is consistent with the p-value."
  },
  {
    "objectID": "homework/HW2.html#section-10",
    "href": "homework/HW2.html#section-10",
    "title": "Homework 2",
    "section": "(5)",
    "text": "(5)\nCalculate the coefficient of determination using the ANOVA table output, and confirm that it matches the value in the R output (what R output shows this and what is it labeled as?)."
  },
  {
    "objectID": "homework/HW2.html#section-11",
    "href": "homework/HW2.html#section-11",
    "title": "Homework 2",
    "section": "(6)",
    "text": "(6)\nWhat is another way to calculate the coefficient of determination? Do the calculation and verify that you have the same answer."
  },
  {
    "objectID": "homework/HW2.html#section-12",
    "href": "homework/HW2.html#section-12",
    "title": "Homework 2",
    "section": "(7)",
    "text": "(7)\nGive an interpretation of the coefficient of determination in the context of the study.\nNote: the question numbers below do not refer to questions from the textbook. Complete the problems below instead of the ones in the book."
  },
  {
    "objectID": "homework/HW2.html#section-13",
    "href": "homework/HW2.html#section-13",
    "title": "Homework 2",
    "section": "(7)",
    "text": "(7)\nDetermine whether there are any observations with high leverage. If there are observations with high leverage, identify their coordinates and describe how they relate to the other observations. Why would these points have high leverage compared to the other observations? Do you think removing the points would change the linear model much? (you do not need to remove the points and rerun the model, just comment on whether you think they are influential)"
  },
  {
    "objectID": "homework/HW2.html#section-14",
    "href": "homework/HW2.html#section-14",
    "title": "Homework 2",
    "section": "(8)",
    "text": "(8)\nDetermine whether there are any observations with high Cook’s distance. If there are observations with high Cook’s distance, identify their coordinates and describe how they relate to the other observations. Why would these points have high Cook’s distance compared to the other observations? Do you think removing the points would change the linear model much? (you do not need to remove the points and rerun the model, just comment on whether you think they are influential)"
  },
  {
    "objectID": "homework/HW2.html#section-15",
    "href": "homework/HW2.html#section-15",
    "title": "Homework 2",
    "section": "(9)",
    "text": "(9)\nCreate histograms and density plots of the dependent and independent variables and describe their distribution shapes."
  },
  {
    "objectID": "homework/HW2.html#section-16",
    "href": "homework/HW2.html#section-16",
    "title": "Homework 2",
    "section": "(10)",
    "text": "(10)\nUse Tukey’s ladder of transformations to choose two possible transformations for the dependent variable. Explain why you chose them. Note: questions below will ask about model fit with the transformations. For now, just explain why you chose the ones that you did."
  },
  {
    "objectID": "homework/HW2.html#section-17",
    "href": "homework/HW2.html#section-17",
    "title": "Homework 2",
    "section": "(11)",
    "text": "(11)\nUse Tukey’s ladder of transformations to choose two possible transformations for the independent variable. Explain why you chose them. Note: questions below will ask about model fit with the transformations. For now, just explain why you chose the ones that you did."
  },
  {
    "objectID": "homework/HW2.html#section-18",
    "href": "homework/HW2.html#section-18",
    "title": "Homework 2",
    "section": "(12)",
    "text": "(12)\nAdd the 4 transformations you chose above (2 for the dependent variable and 2 for the independent variable) to the dataset."
  },
  {
    "objectID": "homework/HW2.html#section-19",
    "href": "homework/HW2.html#section-19",
    "title": "Homework 2",
    "section": "(13)",
    "text": "(13)\nCreate scatterplots using the transformed variables and discuss whether any of the transformations improve the model fit and why (or why not). Include plots with just the x or y variables transformed, and at least one plot with both the x and y variables transformed."
  },
  {
    "objectID": "homework/HW2.html#section-20",
    "href": "homework/HW2.html#section-20",
    "title": "Homework 2",
    "section": "(14)",
    "text": "(14)\nRun the various transformed models and save the output to use for the diagnostic questions below."
  },
  {
    "objectID": "homework/HW2.html#section-21",
    "href": "homework/HW2.html#section-21",
    "title": "Homework 2",
    "section": "(15)",
    "text": "(15)\nCompare the normal QQ plots of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-22",
    "href": "homework/HW2.html#section-22",
    "title": "Homework 2",
    "section": "(16)",
    "text": "(16)\nCompare the residual plots of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-23",
    "href": "homework/HW2.html#section-23",
    "title": "Homework 2",
    "section": "(17)",
    "text": "(17)\nCompare the leverage & Cook’s distance of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-24",
    "href": "homework/HW2.html#section-24",
    "title": "Homework 2",
    "section": "(18)",
    "text": "(18)\nCompare the \\(R^2\\) values and F-test p-values of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-25",
    "href": "homework/HW2.html#section-25",
    "title": "Homework 2",
    "section": "(19)",
    "text": "(19)\nWhich of the models would you recommend using for analyses? Discuss why you chose the model and why you did not choose the other models."
  },
  {
    "objectID": "homework/HW2.html#section-26",
    "href": "homework/HW2.html#section-26",
    "title": "Homework 2",
    "section": "(14)",
    "text": "(14)\nRun the various transformed models and save the output to use for the diagnostic questions below."
  },
  {
    "objectID": "homework/HW2.html#section-27",
    "href": "homework/HW2.html#section-27",
    "title": "Homework 2",
    "section": "(15)",
    "text": "(15)\nCompare the normal QQ plots of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-28",
    "href": "homework/HW2.html#section-28",
    "title": "Homework 2",
    "section": "(16)",
    "text": "(16)\nCompare the residual plots of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-29",
    "href": "homework/HW2.html#section-29",
    "title": "Homework 2",
    "section": "(17)",
    "text": "(17)\nCompare the leverage & Cook’s distance of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-30",
    "href": "homework/HW2.html#section-30",
    "title": "Homework 2",
    "section": "(18)",
    "text": "(18)\nCompare the \\(R^2\\) values and F-test p-values of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW2.html#section-31",
    "href": "homework/HW2.html#section-31",
    "title": "Homework 2",
    "section": "(19)",
    "text": "(19)\nWhich of the models would you recommend using for analyses? Discuss why you chose the model and why you did not choose the other models."
  },
  {
    "objectID": "labs/Project_report_instructions.html",
    "href": "labs/Project_report_instructions.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "Important\n\n\n\nInstructions and rubric are completely done! (3/15/2024)"
  },
  {
    "objectID": "labs/Project_report_instructions.html#sections",
    "href": "labs/Project_report_instructions.html#sections",
    "title": "Project Report Instructions",
    "section": "2 Sections",
    "text": "2 Sections\n\n2.1 Title\n\nPurpose: Create an identifiable name for your research project that includes the main research question’s variables and gives some context to the analysis or results\n\n\n\n2.2 Introduction\n\nLength: 1-2 paragraphs\nPurpose: Introduce the research question and why it is important to study\nThis section is non-technical.\n\nBy reading just the introduction and conclusion, someone without a technical background should have an idea of what they study was about, why it is important, and what the main results are\n\nYou may start with the introduction written in Lab 1, but you should edit it and make sure it flows into your report well!\nShould contain some references\n\n\n\n2.3 Statistical Methods\n\nLength: 3-5 paragraphs\nPurpose: Describe the analyses that were conducted and methods used to select variables and check diagnostics\nImportant to keep in mind: methods typically describe your approach and process, not the results of that process\n\nFor example: I might say “We investigated the linearity of each continuous covariate visually. If continuous variables were not linear, then we divided the variable into categories using existing guidelines from &lt;insert reference here&gt; or creating quartiles.”\n\nIn the methods section, I would NOT say: “We investigated the linearity of each continuous covariate visually. We found that age was not linearly related to IAT scores. Thus, we categorized age into the following groups: ___, ____, ____, ____, and ____.”\n\nThe last two sentences about age would be more appropriate in the Results section\n\n\n\nSome important methods to discuss (You may divide these into your sections, not necessarily with these names)\n\nGeneral approach to the dataset\n\n3-5 sentences\nDid you need to do any quality control?\nMissing data: we performed complete case analysis\n\n1 sentence\nCan be included in the Exploratory data analysis section\n\n\nVariables and variable creation\n\nThis includes a description of analyses for Table 1 and what statistics were used to summarize the variables\n\nMore on creation of Table 1, not discussing the results of Table 1\n\nIncludes (not required)\n\nIndicators for gender identity or race\nCreating BMI\nCategorizing a continuous variable (even if performed in model selection)\nUsing scoring for an ordered categorical variable (that is not your explanatory variable)\n\n1-2 sentences per variable\n\nModel building: we performed purposeful selection\n\n3-5 sentences\nIncludes\n\nDescribe purposeful selection: combining existing literature, clinical significance, and analysis\nHow did you build the model? Describe the process\nDid you consider confounders and effect modifiers?\n\n\nModel diagnostics\n\n2-5 sentences\nIncludes\n\nProcess of investigating model diagnostics\nBy the time you build the model, LINE assumptions should be met\nIf assumptions were not met, what process did you use to fix it?\n\n\n\n\n\n\n2.4 Results\n\nLength: ~3 paragraphs\nPurpose: Relay the results from our sample’s analysis typically focusing on the numbers and interpretations\nSome important results to discuss (also could be sections)\n\nSample data set statistics (Table 1)\n\n3-5 sentences\nInclude a brief description of the sample’s characteristics\nTable 1 should be referenced and appear here!\n\nFinal model\n\n1-2 sentences\nDescribe final model (or models if comparing a few)\n\nWhat variables were included in your final model?\nWhat interactions with your explanatory variable did you include?\n\n\nInterpret the model coefficients in the context of the research question\n\n1-2 paragraphs\nInterpreting the explanatory variable’s relationship with IAT score is the most important thing to report!!\n\nWhen doing this, make sure you account for ALL interactions: If your explanatory variable has multiple interactions and you are trying to interpret one, then what does that mean about the other variables involved in the other interactions? If this is confusing, please make an appointment with me!!\n\n\nResults of model diagnostics if there is anything worth noting\n\nTables & figures\n\nThe following are required tables or figures\n\nTable 1 summarizing participant characteristics both overall and stratified by your primary independent variable\nTable or figure with regression results\n\nCan be a forest plot\nIf you have A LOT of coefficient estimates, the forest plot may not work well!\n\n\n1-3 figures that you think are helpful in understanding the results, for example\n\nDAG explaining connection between variables (if you did this)\nTable or figure to compare model fit statistics (if you did this)\nTable or figure for unadjusted relationship between outcome and explanatory variables\n\n\n\n\n\n2.5 Discussion\n\nLength: 2-3 paragraphs\nPurpose: Discuss the results and give them context outside of the sample and its analysis\nSome important things to include\n\nInclude a paragraph on the limitations of the results\n\nYou don’t need to hit all the limitations, but think about the big ones (generalizability? independence of samples? large sample size vs. clinical significance? the way we handled variables?)\n\nAfter limitations, discuss the positive parts of the results\n\nWhat can we do with these results? What impact can it have?\n\nAny overarching trends that are worth noting? (Giebel et al. 2024)\n\nShould contain some references\n\n\n\n2.6 Conclusion\n\nLength: 1 short paragraph (more like ~3 sentences)\nPurpose: Describe the main conclusions to a non-technical audience\n\n\n\n2.7 References\n\nInclude your references here!\nYou introduction should have references, especially when discussing the social science behind the analysis\nYou must reference the IAT data source!!"
  },
  {
    "objectID": "homework/HW5.html#a",
    "href": "homework/HW5.html#a",
    "title": "Homework 5",
    "section": "a)",
    "text": "a)\nCreate a figure with pairwise scatter plots of the variables. Note that depression index (DEP) is the outcome measure (or response variable, Y).\n\nDo you see any signs of the linearity assumption not being met? If so, for which variables?\nDo you see any strong correlations between independent variables that could potentially cause collinearity problems? Confirm your observations by calculating pairwise correlations among the predictors."
  },
  {
    "objectID": "homework/HW5.html#b",
    "href": "homework/HW5.html#b",
    "title": "Homework 5",
    "section": "b)",
    "text": "b)\nCheck if the DEP data are normally distributed. If needed, what would be an appropriate transformation for this variable?"
  },
  {
    "objectID": "homework/HW5.html#c",
    "href": "homework/HW5.html#c",
    "title": "Homework 5",
    "section": "c)",
    "text": "c)\nTest whether the association between DEP and WP is significant using alpha = .05."
  },
  {
    "objectID": "homework/HW5.html#d",
    "href": "homework/HW5.html#d",
    "title": "Homework 5",
    "section": "d)",
    "text": "d)\nSuppose researchers would like to use a log transformation for the MC variable, based on what has been done in other studies. Do you agree with this choice? Why or why not? Whether or not you agree, test whether the association between DEP and log(MC) is significant, using alpha = .05, and use log(MC) instead of MC for the remainder of the assignment."
  },
  {
    "objectID": "homework/HW5.html#e",
    "href": "homework/HW5.html#e",
    "title": "Homework 5",
    "section": "e)",
    "text": "e)\nTest whether SEX is an effect modifier that changes the association between DEP and WP. Use alpha = .10."
  },
  {
    "objectID": "homework/HW5.html#f",
    "href": "homework/HW5.html#f",
    "title": "Homework 5",
    "section": "f)",
    "text": "f)\nTest whether SEX is an effect modifier that changes the association between DEP and log(MC). Use alpha = .10."
  },
  {
    "objectID": "homework/HW5.html#g",
    "href": "homework/HW5.html#g",
    "title": "Homework 5",
    "section": "g)",
    "text": "g)\nFrom the results obtained in parts (e) and (f), should we further check whether SEX is a confounder? Why or why not? If yes, determine whether SEX is a confounder for the associations between DEP and WP and between DEP and log(MC)"
  },
  {
    "objectID": "homework/HW5.html#h",
    "href": "homework/HW5.html#h",
    "title": "Homework 5",
    "section": "h)",
    "text": "h)\nDetermine whether AGE is a confounder for the associations between DEP and WP and between DEP and log(MC)."
  },
  {
    "objectID": "homework/HW5.html#i",
    "href": "homework/HW5.html#i",
    "title": "Homework 5",
    "section": "i)",
    "text": "i)\nWhat is your final association model based on the results from the previous questions?"
  },
  {
    "objectID": "homework/HW5.html#j",
    "href": "homework/HW5.html#j",
    "title": "Homework 5",
    "section": "j)",
    "text": "j)\nPerform model diagnostics for your final association model. Use the steps outlined below.\n\nj1)\nDetermine whether the independence assumption has been met.\n\n\nj2)\nDetermine whether the linearity assumption has been met.\n\n\nj3)\nDetermine whether the homoscedasticity assumption has been met.\n\n\nj4)\nDetermine whether the normality assumption has been met.\n\n\nj5)\nDetermine whether there any outliers (e.g., high leverage, lack of fit) or influential points (e.g., dffits, Cook’s distance, dfbetas)?\n\n\nj6)\nIs there evidence of collinearity in the model? If there is collinearity, make changes to your model to reduce the collinearity and justify your method."
  },
  {
    "objectID": "homework/HW5.html#k",
    "href": "homework/HW5.html#k",
    "title": "Homework 5",
    "section": "k)",
    "text": "k)\nUsing the final association model obtained from part (j5), interpret the (adjusted) association between DEP and MC."
  },
  {
    "objectID": "homework/HW5.html#l",
    "href": "homework/HW5.html#l",
    "title": "Homework 5",
    "section": "l)",
    "text": "l)\nWhat is the R-squared value for your final association model? Explain it in the context of study."
  },
  {
    "objectID": "homework/HW5.html#m",
    "href": "homework/HW5.html#m",
    "title": "Homework 5",
    "section": "m)",
    "text": "m)\nFor your final association model, run a hypothesis test and report the 95% CI for the slope of the WP variable. Interpret the results (both test and CI)."
  },
  {
    "objectID": "homework/HW5.html#a-1",
    "href": "homework/HW5.html#a-1",
    "title": "Homework 5",
    "section": "a)",
    "text": "a)\nRun the four automatic selection procedures discussed in class (forward selection, backward elimination, forward stepwise, and backward stepwise) using all the independent variables and the interactions listed above. What are the parsimonious models from the different procedures? How are they similar or different? If the results are different, provide some reasons as to why they are different.\n\nForward Selection\n\n\nBackwards Selection\n\n\nForward Stepwise\n\n\nBackwards Stepwise\n\n\nComparison of models"
  },
  {
    "objectID": "homework/HW5.html#b-1",
    "href": "homework/HW5.html#b-1",
    "title": "Homework 5",
    "section": "b)",
    "text": "b)\nRestricting to just the variables that appeared in any of the prediction models obtained in the previous part, use Mallow’s \\(C_p\\) and the models’ adjusted R-squared values to decide on a parsimonious final prediction model. Include an explanation on how you chose your final prediction model."
  },
  {
    "objectID": "homework/HW3.html#directions---important",
    "href": "homework/HW3.html#directions---important",
    "title": "Homework 3",
    "section": "Directions - important!!!",
    "text": "Directions - important!!!\n\nHypothesis tests\n\nFor every hypothesis test make sure to include the following:\n\nNull & alternative hypotheses\nCalculation of test statistic using the formula\nCalculate the p-value directly using its probability distribution\nRunning the test using R\nConclusion in the context of the research problem. This includes referring to variables by what they actually are and not \\(X_1\\), \\(X_2\\), etc.\n\n\n\nSee additional instructions/ clarifications in green."
  },
  {
    "objectID": "homework/HW3.html#tips",
    "href": "homework/HW3.html#tips",
    "title": "Homework 3",
    "section": "Tips",
    "text": "Tips\n\nYou will be running a lot of different tests below. I highly recommend coming up with a naming convention that will easily help you keep track of what variables are being included in which models.\n\nThe names model1, model2, etc. will not be helpful."
  },
  {
    "objectID": "homework/HW3.html#a",
    "href": "homework/HW3.html#a",
    "title": "Homework 3",
    "section": "(a)",
    "text": "(a)\n\nConduct the overall regression F test for the model where \\(Y\\) is regressed on \\(X_1, X_2\\), and \\(X_3\\). Use alpha = 0.05. Interpret your result."
  },
  {
    "objectID": "homework/HW3.html#b",
    "href": "homework/HW3.html#b",
    "title": "Homework 3",
    "section": "(b)",
    "text": "(b)\n\nProvide variables-added-in-order tests for the order \\(X_2, X_1\\), and \\(X_3\\).\n\nThis means that there are 3 tests: (1) test model with just \\(X_2\\), (2) test adding \\(X_1\\) to the model given that \\(X_2\\) is already in the model, and (3) test adding \\(X_3\\) to the model given that \\(X_2, X_1\\) are already in the model. See the subsections below to divide up the work.\n\nTest model with just \\(X_2\\)\n\n\nAdding \\(X_1\\) to the model given that \\(X_2\\) is already in the model\n\n\nAdding \\(X_3\\) to the model given that \\(X_2, X_1\\) are already in the model"
  },
  {
    "objectID": "homework/HW3.html#e",
    "href": "homework/HW3.html#e",
    "title": "Homework 3",
    "section": "(e)",
    "text": "(e)\n\nProvide variables-added-last tests for \\(X_1, X_2\\), and \\(X_3\\).\n\nThis means that there are 3 tests: (1) test adding \\(X_1\\) to the model given that \\(X_2, X_3\\) are already in the model, (2) test adding \\(X_2\\) to the model given that \\(X_1, X_3\\) are already in the model, and (3) test adding \\(X_3\\) to the model given that \\(X_1, X_2\\) are already in the model. See the subsections below to divide up the work.\n\nFor adding \\(X_2\\) and \\(X_3\\) last to the model, you do not need to calculate the test statistic using the formula or the p-value directly using its probability distribution. You can instead run the appropriate tests in R.\n\n\nAdding \\(X_1\\) to the model given that \\(X_2, X_3\\) are already in the model\n\n\nAdding \\(X_2\\) to the model given that \\(X_1, X_3\\) are already in the model\n\n\nAdding \\(X_3\\) to the model given that \\(X_1, X_2\\) are already in the model"
  },
  {
    "objectID": "homework/HW3.html#f",
    "href": "homework/HW3.html#f",
    "title": "Homework 3",
    "section": "(f)",
    "text": "(f)\n\nProvide the variables-added-last test for \\(X_4 = X_2 X_3\\) given that \\(X_2\\) and \\(X_3\\) are already in the model. Does \\(X_4\\) significantly improve the prediction of \\(Y\\) given that \\(X_2\\) and \\(X_3\\) are already in the model?"
  },
  {
    "objectID": "homework/HW3.html#a-1",
    "href": "homework/HW3.html#a-1",
    "title": "Homework 3",
    "section": "(a)",
    "text": "(a)\nCalculate the coefficient of determination \\(r^2_{Y|X1,X2,X3}\\) and interpret this value in the context of the problem. Do the calculation using the formula and then check your answer with R. In particular, where in the R output do we find this value?"
  },
  {
    "objectID": "homework/HW3.html#b-1",
    "href": "homework/HW3.html#b-1",
    "title": "Homework 3",
    "section": "(b)",
    "text": "(b)\nCalculate the partial coefficient of determination \\(r^2_{YX1|X2}\\) and interpret this value in the context of the problem. Do the calculation using the formula and then check your answer with R."
  },
  {
    "objectID": "homework/HW3.html#c",
    "href": "homework/HW3.html#c",
    "title": "Homework 3",
    "section": "(c)",
    "text": "(c)\nUse \\(r^2_{YX1|X2}\\) to calculate \\(r_{YX1|X2}\\) and interpret this value in the context of the problem. Check your answer with R."
  },
  {
    "objectID": "homework/HW3.html#d",
    "href": "homework/HW3.html#d",
    "title": "Homework 3",
    "section": "(d)",
    "text": "(d)\nExplain how the interpretations of \\(r^2_{YX1|X2}\\) and \\(r_{YX1|X2}\\). In particular, what information do each of these values tell us that the other does not?"
  },
  {
    "objectID": "homework/HW3.html#e-1",
    "href": "homework/HW3.html#e-1",
    "title": "Homework 3",
    "section": "(e)",
    "text": "(e)\nCalculate the partial coefficient of determination \\(r^2_{YX1|X2X3}\\) and interpret this value in the context of the problem. Do the calculation using the formula and then check your answer with R."
  },
  {
    "objectID": "homework/HW3.html#f-1",
    "href": "homework/HW3.html#f-1",
    "title": "Homework 3",
    "section": "(f)",
    "text": "(f)\nUse \\(r^2_{YX1|X2X3}\\) to calculate \\(r_{YX1|X2X3}\\) and interpret this value in the context of the problem. Check your answer with R."
  },
  {
    "objectID": "homework/HW3.html#g",
    "href": "homework/HW3.html#g",
    "title": "Homework 3",
    "section": "(g)",
    "text": "(g)\nUse your answers to parts (b, c, e, f), to discuss the change in the first-order partial correlation to the second-order partial correlation."
  },
  {
    "objectID": "homework/HW3.html#penguins-flipper-length-vs.-species",
    "href": "homework/HW3.html#penguins-flipper-length-vs.-species",
    "title": "Homework 3",
    "section": "Penguins: Flipper length vs. species",
    "text": "Penguins: Flipper length vs. species\nFor this problem we will be using the penguins dataset from the palmerpenguins R package.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data are at https://allisonhorst.github.io/palmerpenguins/.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\n(1) Outcome averages stratified by category levels\nCalculate the average flipper lengths stratified by each of the penguin species.\n\n\n(2) Visualize the “regression”\nMake a scatterplot of flipper lengths by species, and include diamond-shape points for the averages of the flipper lengths for each of the species.\n\n\n(3) Regression equations\nBefore running the regression in R, we are going to find the regression equation”manually.”\nWrite out the regression equation using LaTeX math markup (see class notes) that models the flipper length by penguin species. Do not yet insert values for the regression coefficients, i.e. us the generic coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1,\\) etc. Use Adelie as the reference level.\n\n\n(4) Interpret coefficients\nHow do we interpret each of the regression coefficients for this model? Write out a separate interpretation for each of the coefficients.\n\n\n(5) Regression coefficients “manually”\n“Manually” calculate the values for each of the coefficients, and update the regression model with the values inserted.\nYou must show your work for this. Do not run the linear model in this step to get the values.\n\n\n(6) Regression table with lm() function\nRun the linear regression of flipper lengths vs. species in R, and display the regression table output. Which species did R choose as the reference level, and how did you determine this?\n\n\n(7) Mean calculation using regression output\nCalculate the mean flipper length of penguins in the Chinstrap and Gentoo species using only the results from the regression table. You must show your work."
  },
  {
    "objectID": "homework/HW3.html#problem-9.7-c",
    "href": "homework/HW3.html#problem-9.7-c",
    "title": "Homework 3",
    "section": "Problem 9.7 (c)",
    "text": "Problem 9.7 (c)\nDo this problem after completing 9.7 (b) above\n\nProvide variables-added-in-order tests for the order \\(X_3, X_1\\), and \\(X_2\\).\n\nThis means that there are 3 tests: (1) test model with just \\(X_3\\), (2) test adding \\(X_1\\) to the model given that \\(X_3\\) is already in the model, and (3) test adding \\(X_2\\) to the model given that \\(X_3, X_1\\) are already in the model. See the subsections below to divide up the work.\n\nTest model with just \\(X_3\\)\n\n\nAdding \\(X_1\\) to the model given that \\(X_3\\) is already in the model\n\n\nAdding \\(X_2\\) to the model given that \\(X_3, X_1\\) are already in the model"
  },
  {
    "objectID": "homework/HW3.html#chapter-10-c-using-the-formula",
    "href": "homework/HW3.html#chapter-10-c-using-the-formula",
    "title": "Homework 3",
    "section": "Chapter 10 (c) using the formula",
    "text": "Chapter 10 (c) using the formula\nDo this after completing Chapter 10 (c) above.\nCalculate \\(r_{YX1|X2}\\) using the formula and check that your answer matches that of Chapter 10 (c) above."
  },
  {
    "objectID": "homework/HW3.html#regression-with-one-categorical-predictor-prequel-to-ch-11-12-change-the-reference-level-to-gentoo",
    "href": "homework/HW3.html#regression-with-one-categorical-predictor-prequel-to-ch-11-12-change-the-reference-level-to-gentoo",
    "title": "Homework 3",
    "section": "Regression with one categorical predictor (Prequel to Ch 11 & 12): Change the reference level to Gentoo",
    "text": "Regression with one categorical predictor (Prequel to Ch 11 & 12): Change the reference level to Gentoo\nAfter completing exercises (1) - (7) in the section Regression with one categorical predictor (Prequel to Ch 11 & 12), do the problems below.\n\n(8)\nWrite out the regression equation using LaTeX math markup (see class notes) that models the flipper length by penguin species. Do not yet insert values for the regression coefficients, i.e. us the generic coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1,\\) etc. Use Gentoo as the reference level.\n\n\n(9)\nHow do we interpret each of the regression coefficients for this model? Write out a separate interpretation for each of the coefficients.\n\n\n(10)\n“Manually” calculate the values for each of the coefficients, and update the regression model with the values inserted. You must show your work for this. Do not run the linear model in this step to get the values."
  },
  {
    "objectID": "homework/HW2.html#question-1-chapter-6",
    "href": "homework/HW2.html#question-1-chapter-6",
    "title": "Homework 2",
    "section": "Question 1 (chapter 6)",
    "text": "Question 1 (chapter 6)\nWe will continue to work with the study and dataset from Question 2 above.\n\nPart a\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in Question 2? Why or why not?\n\n\nPart b\nCalculate the coefficient of determination using linear regression summary output. Can we also calculate the coefficient of determination from the ANOVA in Question 2?\n\n\nPart c\nGive an interpretation of the coefficient of determination in the context of the study."
  },
  {
    "objectID": "homework/HW4.html#question-1",
    "href": "homework/HW4.html#question-1",
    "title": "Homework 4",
    "section": "Question 1",
    "text": "Question 1\nUse the data from Chapter 12 Problem 3 to answer the questions below.\n\na)\na. How many dummy variable(s) do you need to create for the categorical variable Diet (protein-rich vs. protein-poor)? Create the dummy variable(s) with the reference cell coding approach{0,1}.\n\n\nb)\nb. At a level of significance alpha = .05, test whether if Age is significantly associated with Height. Would this association be modified depending on diet group (e.g., rich-protein or poor-protein)? In other words, is Diet an effect-modifier that changes the association between Height and Age? Justify your answer (e.g., perform a hypothesis test at a level of alpha =.1).\nNote: recall that an effect modifier is an interaction.\n\n\nc)\nc. From the results obtained in part b, should we perform an assessment of a confounder for Diet? Justify your answer. Perform such an assessment if needed.\n\n\nd)\nd. Perform a regression analysis on the model obtained from the results obtained from parts a- c. Write down a general regression equation that is applicable to both groups—rich-protein vs.poor-protein. Write down regression lines for each specific groups—rich-protein or poor-protein."
  },
  {
    "objectID": "homework/HW4.html#question-2",
    "href": "homework/HW4.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\nUse the data from Chapter 9 Problem 5 to answer the questions below.\n\na)\na. Use \\(\\alpha= 0.05\\), test whether the (crude) association between Y and X1 could be established.\n\n\nb)\nb. Use\\(\\alpha= 0.1\\), test whether X3 is an effect modifier of the association between Y and X1.\nNote: To identify effect modifiers, we perform a hypothesis test of interaction term, e.g.,X1X3. That is: The full model includes X1, X3, X1X3. the reduced model includes X1 and X3\n\n\nc)\nc. From the result obtained in part b, do we need to perform an assessment of a confounder for X3? Justify your answer. Perform such an assessment if needed.\n\n\nd)\nd. Perform an assessment of a confounder for X2 which potentially changes the association between Y and X1.\n\n\ne)\ne. From the results in parts a-d, what is your final association model?"
  },
  {
    "objectID": "homework/HW4.html#a-2",
    "href": "homework/HW4.html#a-2",
    "title": "Homework 4",
    "section": "a)",
    "text": "a)\na. Obtain scatter plot: Y vs. X. Does linear trend support the relationship between Y and X?"
  },
  {
    "objectID": "homework/HW4.html#b-2",
    "href": "homework/HW4.html#b-2",
    "title": "Homework 4",
    "section": "b)",
    "text": "b)\nb. At the level alpha =.05, test whether the linear relationship could be established between Y and X."
  },
  {
    "objectID": "homework/HW4.html#c-2",
    "href": "homework/HW4.html#c-2",
    "title": "Homework 4",
    "section": "c)",
    "text": "c)\nc. At the level alpha =.05, test whether the quadratic term (\\(X^2\\)) should be included in the model to improve the prediction in Y, given the linear term (\\(X\\)) is already in the model."
  },
  {
    "objectID": "homework/HW4.html#d-2",
    "href": "homework/HW4.html#d-2",
    "title": "Homework 4",
    "section": "d)",
    "text": "d)\nd. From the result obtained from part c, should we test if the linear term (X) is necessary to be included in the model, given the quadratic term is already in the model? Explain your answer."
  },
  {
    "objectID": "homework/HW4.html#e-1",
    "href": "homework/HW4.html#e-1",
    "title": "Homework 4",
    "section": "e)",
    "text": "e)\ne. From the results you obtain from parts c-d, should we further examine whether cubic term (\\(X^3\\)) or fourth polynomial degree (i.e., \\(X^4\\)) to improve the prediction in Y? Explain your answer and report the result of such a test if needed."
  },
  {
    "objectID": "homework/HW4.html#f",
    "href": "homework/HW4.html#f",
    "title": "Homework 4",
    "section": "f)",
    "text": "f)\nf. From parts a – e, what is the final model that you have obtained? Interpret the R-square result from this model in the context of the study. Plot fitted value curve vs. X overlaid with scatter plot. Comments about the fitting model."
  },
  {
    "objectID": "weeks/week_01_sched.html#announcements",
    "href": "weeks/week_01_sched.html#announcements",
    "title": "Week 1",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 1/8\n\nWe came from two different sections of the same course\n\nWhile I am confident we all learned a lot and mostly the same material\nWe definitely learned it in different ways\nThree components of 511/512/513\n\nTheory\nApplication\nCoding\n\n\nThere is a workshop on Friday, 1/12\n\nData Equity Primer from We All Count\nIf you attend, I will give you an extra 3% on your project report.\nUnfortunately, I was in contact with the workshop, but they cannot offer free seats to the whole class.\nI am attending, and will try to see if I can share a recording afterwards!\n\nHere are a few resources if you’d like to practice R:\n\nhttps://rladiessydney.org/courses/ryouwithme/\nhttps://r-bootcamp.netlify.app/\n\nThis is step by step, and lets you practice your code in real time\nWe will use ggplot2, dplyr, and tidyr extensively in this class!\nI am happy to help with coding, but during our class time our focus will by statistics topics\n\nCoding help will mostly be done outside of class time\n\n\n\nWe will also be using an attachment within Rstudio called Quarto\n\nHere is a great tutorial on Quarto: https://quarto.org/docs/get-started/hello/rstudio.html\n\nMine is the Quarto queen!\nThere is some expectation of knowledge with the above packages (ggplot2, dplyr, and tidyr)\n\n\n\n\n\nWednesday 1/10\n\nAha! I finally found the fun source on connecting tests from 511 to linear models!!\n\nHere is the site\nAnd here is the cheat sheet: \n\nWebsite updates\n\nPlease see the muddiest points below\n\nI try to elaborate/answer questions from your exit tickets\n\nSchedule updated to include Holidays\nNew resources tab that I will try to update after every class!\n\nAsking about getting a different room - seems like we’re cramped\nLab grading\n\nEach lab will be graded using a rubric (on each lab page)\nYou will need to hit specific points to get full credit for the lab\nThis is not a “turn in as is” assignment. You need to turn it in on time OR ask me for an extension.\n\nIt is likely that I can give you a few more days to finish\n\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\n\nIf you need another extension, then you need to email me to ask"
  },
  {
    "objectID": "instructors.html#statistics-tutor-for-epidemiology-students",
    "href": "instructors.html#statistics-tutor-for-epidemiology-students",
    "title": "Instructors",
    "section": "Statistics Tutor for Epidemiology Students",
    "text": "Statistics Tutor for Epidemiology Students\n\nBecky Lanford\n\nEmail: lanford@ohsu.edu\nLink to Becky’s Calendly\n\nBecky can help with:\n\nStatistical coding support\nSupport with stats concepts you are learning in class\nData management and analysis plan scheming during your PE\n\nIntroduction from Becky:\n\nHello fellow MPH classmates! My name is Becky Lanford. I’m looking forward to helping support you in your coursework this quarter. A little about my background: I am currently in my final year in the MPH Epidemiology track and have completed most of my coursework including the biostatistics and epidemiology series (mostly working in R). I enrolled at the SPH as someone re-entering the workforce and quite new to statistical programming. Though I had previously completed a graduate degree (as a Physician Assistant/Associate), re-acclimating to graduate work and learning programming skills made for a steep learning curve my first academic year. I credit the collaborative learning environment at SPH - support of TA’s and classmates and availability of instructors - for helping me be successful. I hope I can help answer course-content questions, problem solve with you and find answers if I don’t have them myself. I know there are many challenges to being a graduate student and I am excited to help our public health student community grow stronger and more knowledgeable together."
  },
  {
    "objectID": "instructors.html#becky-lanford",
    "href": "instructors.html#becky-lanford",
    "title": "Instructors",
    "section": "Becky Lanford",
    "text": "Becky Lanford\nEmail: lanford@ohsu.edu\nHello fellow MPH classmates! My name is Becky Lanford. I’m looking forward to helping support you in your coursework this quarter. A little about my background: I am currently in my final year in the MPH Epidemiology track and have completed most of my coursework including the biostatistics and epidemiology series (mostly working in R). I enrolled at the SPH as someone re-entering the workforce and quite new to statistical programming. Though I had previously completed a graduate degree (as a Physician Assistant/Associate), re-acclimating to graduate work and learning programming skills made for a steep learning curve my first academic year. I credit the collaborative learning environment at SPH - support of TA’s and classmates and availability of instructors - for helping me be successful. I hope I can help answer course-content questions, problem solve with you and find answers if I don’t have them myself. I know there are many challenges to being a graduate student and I am excited to help our public health student community grow stronger and more knowledgeable together."
  },
  {
    "objectID": "labs/Lab_01.html#questions",
    "href": "labs/Lab_01.html#questions",
    "title": "Lab 1",
    "section": "Questions",
    "text": "Questions\n\nRead the article and collect other sources of information\n\n\nAnswer the following questions\n\n\nGet a sense of how you would like to analyze the data"
  },
  {
    "objectID": "labs/Lab_01.html#lab-activities",
    "href": "labs/Lab_01.html#lab-activities",
    "title": "Lab 1",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Reading and listening activities\n\n1.1 Article: Implicit and explicit anti-fat bias: The role of weight-related attitudes and beliefs\nThis article will serve as a reference point for our project. The article is meant to introduce social scientists’ approaches to research and analyses. However, the article is not meant to be a basis for which we perform our analysis.\n\n\n\n\n\n\nWarning\n\n\n\nThis article discusses anti-fat bias. It uses words that may be triggering to larger-bodied people.\n\n\nPlease read sections 1 - 2, through 2.2 (“Procedures and measures”). Answer the following questions:\n\nIn your own words, what is anti-fat bias?\nWhat were the three social theoretical models that the paper discusses? Which do you personally think is the biggest contributor to anti-fat bias and why?\nFrom the following measures in section 2.2, select two and discuss why the named measure may or may not accurately represent the italicized statement taken from the IAT questionnaire. Feel free to answer this question after taking the IAT yourself.\n\nSelf-perception of weight\nThin/fat group identity\nControllability of weight\nAwareness of societal standards\nInternalization of societal standards\n\nFor example, for Self-perception of weight, the italicized statement is the following statement outlined in red:\n\n\n\n1.2 Podcast: Anti-Fat Bias by Maintenance Phase\n\n\n\n\n\n\nWarning\n\n\n\nThis podcast shares the experience of one of its hosts that involves anti-fat bias. This may be triggering if you have experienced this type of bias.\n\n\nThis is an optional listening for this lab, but I highly encourage you listen at some point this quarter. This is a really good way to see how research can be integrated into conversation and experience.\nIf you decide to listen, feel free to share a quote that most impacted you.\n\n\n\n2. Familiarizing ourselves with the Implicit Association Test (IAT)\n\n2.1 Learn more about the test\nVisit the Project Implicit site, and read about the test. What is your initial reaction to the test? What questions about the test do you have? Do you have any questions about the test’s validity? The point here is not to attempt to discredit the test itself, but see what specific questions the test can help us answer and what is outside the scope of our analysis. For example, are there any potential issues with the fact that people are self-selected to take the test? Does that mean our sample is representative of our population? Is it an issue that someone can take the test more than once?\nThis exercise will serve as a good starting point for the discussion section of our project report. The more effort you put in here and now, the more prepared you will be for the report.\n\n\n2.2 Take the test\nYou will spend 15 minutes taking the IAT. You can go to the Project Implicit website, register, and select a specific test to take. Once registered, you can click “Take a Test,” read the Preliminary Information, and then click “I wish to proceed” at the bottom. Then you can click the button “Weight IAT” to take this particular test.\nI will not check that you have completed this test, but it will help you understand the data you are analyzing.\n\n\n\n3. Get a sense of how you would like to analyze the data\nFor our project, we will examine the association betwen the IAT score and one other variable. From the above article, and the introduced variables in section 2.2, which association are you most interested in analyzing? Please write this in the form of a research question.\nWe will have a chance to adjust our research question once we have explored the data in Lab 2.\n\n\n4. Compile above work into an introduction\nAt this point, you have done a lot of the work needed to write an introduction for your report. Write a brief description of anti-fat bias, IAT, your research question, and the context for the question. This description should be in complete sentences and written as a single paragraph.\nIn the next lab, we will work on a summary of the dataset (e.g. where are the data from, when were they collected, how many subjects, what are the variables, what are the exposure and outcomes variables of interest, etc.)."
  },
  {
    "objectID": "slides/01_Review.html#data-visualization-2",
    "href": "slides/01_Review.html#data-visualization-2",
    "title": "Review",
    "section": "Data visualization 2",
    "text": "Data visualization 2\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_density()\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/01_Review.html#histogram-using-ggplot2",
    "href": "slides/01_Review.html#histogram-using-ggplot2",
    "title": "Review",
    "section": "Histogram using ggplot2",
    "text": "Histogram using ggplot2\nWe can make a basic graph for a continuous variable:\n\n\ndata(\"dds.discr\")\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nggplot() +\n  geom_histogram(data = dds.discr, \n       aes(x = age))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nSome more information on histograms using ggplot2"
  },
  {
    "objectID": "slides/01_Review.html#histogram-using-ggplot2-1",
    "href": "slides/01_Review.html#histogram-using-ggplot2-1",
    "title": "Review",
    "section": "Histogram using ggplot2",
    "text": "Histogram using ggplot2\nWe can make a more formal, presentable graph:\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() +\n  theme(text = element_text(size=20)) +\n  labs(x = \"Age\", \n       y = \"Count\", \n       title = \"Distribution Age in Sample\")\n\n\nI would like you to turn in homework, labs, and project reports with graphs like these."
  },
  {
    "objectID": "slides/01_Review.html#other-basic-plots-from-ggplot2",
    "href": "slides/01_Review.html#other-basic-plots-from-ggplot2",
    "title": "Review",
    "section": "Other basic plots from ggplot2",
    "text": "Other basic plots from ggplot2\nWe can also make a density and boxplot for the continuous variable with ggplot2\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_density()\n\n\n\n\n\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_boxplot()"
  },
  {
    "objectID": "slides/01_Review.html#where-are-we-1",
    "href": "slides/01_Review.html#where-are-we-1",
    "title": "Review",
    "section": "Where are we?",
    "text": "Where are we?\n\n\nReview\n\n\n\nIntro to SLR: estimation and testing\nIntro to MLR: estimation and testing\nDiving into our predictors: categorical variables, interactions between variable\nKey ingredient: model evaluation, diagnostics, selection, and building"
  },
  {
    "objectID": "slides/01_Review.html#spruced-up-histogram-using-ggplot2",
    "href": "slides/01_Review.html#spruced-up-histogram-using-ggplot2",
    "title": "Review",
    "section": "Spruced up histogram using ggplot2",
    "text": "Spruced up histogram using ggplot2\nWe can make a more formal, presentable graph:\n\nggplot(data = dds.discr, \n       aes(x = age)) +\n  geom_histogram() +\n  theme(text = element_text(size=20)) +\n  labs(x = \"Age\", \n       y = \"Count\", \n       title = \"Distribution of Age in Sample\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nI would like you to turn in homework, labs, and project reports with graphs like these."
  },
  {
    "objectID": "slides/01_Review.html#descriptive-statistics-continuous-variables-r-code",
    "href": "slides/01_Review.html#descriptive-statistics-continuous-variables-r-code",
    "title": "Review",
    "section": "Descriptive Statistics: continuous variables (R code)",
    "text": "Descriptive Statistics: continuous variables (R code)\n\n\nMeasures of central tendency\n\nSample mean\n\nmean( sample )\n\nMedian\n\nmedian( sample )\n\n\n\nMeasures of variability (or dispersion)\n\nSample variance\n\nvar( sample )\n\nSample standard deviation\n\nsd( sample )\n\nIQR\n\nIQR( sample )"
  },
  {
    "objectID": "slides/01_Review.html#what-will-this-class-cover",
    "href": "slides/01_Review.html#what-will-this-class-cover",
    "title": "Review",
    "section": "What will this class cover?",
    "text": "What will this class cover?"
  },
  {
    "objectID": "slides/01_Review.html#what-will-we-learn-in-this-class",
    "href": "slides/01_Review.html#what-will-we-learn-in-this-class",
    "title": "Review",
    "section": "What will we learn in this class?",
    "text": "What will we learn in this class?\n\nWe will be building towards models that can handle many variables!\n \n\nRegression is the building block for modeling multivariable relationships\n\n \nIn Linear Models we will build, interpret, and evaluate linear regression models"
  },
  {
    "objectID": "slides/01_Review.html#now",
    "href": "slides/01_Review.html#now",
    "title": "Review",
    "section": "NOW!!",
    "text": "NOW!!\nPreviously, if the exposure was:\n\nCategorical: we could run a t-test or ANOVA\nContinuous\n\nMain sections:\n\nIntro to SLR: estimation and testing\nIntro to MLR: estimation and testing\nDiving into our predictors: categorical variables, interactions between variable\nKey ingredients: model evaluation, diagnostics, selection, and building"
  },
  {
    "objectID": "slides/01_Review.html#what-did-we-learn-in-511",
    "href": "slides/01_Review.html#what-did-we-learn-in-511",
    "title": "Review",
    "section": "",
    "text": "In 511, we talked about categorical and continuous outcomes (dependent variables)\n \nWe also talked about their relationship with 1-2 continuous or categorical exposure (independent variables or predictor)\n \nWe had many good ways to assess the relationship between an outcome and exposure:\n \n\n\n\n\n\n\n\n\n\n\nContinuous Outcome\nCategorical Outcome\n\n\nContinuous Exposure\nCorrelation, simple linear regression\n??\n\n\nCategorical Exposure\nt-tests, paired t-tests, 2 sample t-tests, ANOVA\nproportion t-test, Chi-squared goodness of fit test, Fisher’s Exact test, Chi-squared test of independence, etc."
  },
  {
    "objectID": "slides/01_Review.html#what-did-we-learn-in-511-1",
    "href": "slides/01_Review.html#what-did-we-learn-in-511-1",
    "title": "Review",
    "section": "What did we learn in 511?",
    "text": "What did we learn in 511?\n\nYou set up a really important foundation\n\nIncluding distributions, mathematical definitions, hypothesis testing, and more!\n\n \nTests and statistical approaches learned are incredibly helpful!\n \nWhile you had to learn a lot of different tests and approaches for each combination of categorical/continuous exposure with categorical/continuous outcome\n\nThose tests cannot handle more complicated data\n\n \nWhat happens when other variables influence the relationship between your exposure and outcome?\n\nDo we just ignore them?"
  },
  {
    "objectID": "slides/01_Review.html#process-of-regression-data-analysis",
    "href": "slides/01_Review.html#process-of-regression-data-analysis",
    "title": "Review",
    "section": "Process of regression data analysis",
    "text": "Process of regression data analysis"
  },
  {
    "objectID": "slides/01_Review.html#main-sections-of-the-course",
    "href": "slides/01_Review.html#main-sections-of-the-course",
    "title": "Review",
    "section": "Main sections of the course",
    "text": "Main sections of the course\n\nReview\nIntro to SLR: estimation and testing\n\nModel fitting\n\nIntro to MLR: estimation and testing\n\nModel fitting\n\nDiving into our predictors: categorical variables, interactions between variable\n\nModel fitting\n\nKey ingredients: model evaluation, diagnostics, selection, and building\n\nModel evaluation and Model selection\n\n\n\nlibrary(ggplot2)"
  },
  {
    "objectID": "slides/01_Review.html",
    "href": "slides/01_Review.html",
    "title": "Review",
    "section": "",
    "text": "In 511, we talked about categorical and continuous outcomes (dependent variables)\n \nWe also talked about their relationship with 1-2 continuous or categorical exposure (independent variables or predictor)\n \nWe had many good ways to assess the relationship between an outcome and exposure:\n \n\n\n\n\n\n\n\n\n\n\nContinuous Outcome\nCategorical Outcome\n\n\nContinuous Exposure\nCorrelation, simple linear regression\n??\n\n\nCategorical Exposure\nt-tests, paired t-tests, 2 sample t-tests, ANOVA\nproportion t-test, Chi-squared goodness of fit test, Fisher’s Exact test, Chi-squared test of independence, etc."
  },
  {
    "objectID": "slides/01_Review.html#main-sections-of-the-course-1",
    "href": "slides/01_Review.html#main-sections-of-the-course-1",
    "title": "Review",
    "section": "Main sections of the course",
    "text": "Main sections of the course\n\n\nReview\n\n\n\nIntro to SLR: estimation and testing\n\nModel fitting\n\nIntro to MLR: estimation and testing\n\nModel fitting\n\nDiving into our predictors: categorical variables, interactions between variable\n\nModel fitting\n\nKey ingredients: model evaluation, diagnostics, selection, and building\n\nModel evaluation and Model selection"
  },
  {
    "objectID": "slides/00_Intro.html#positionality",
    "href": "slides/00_Intro.html#positionality",
    "title": "Welcome to BSTA 512/612!",
    "section": "Positionality",
    "text": "Positionality"
  },
  {
    "objectID": "syllabus.html#assessment-breakdown",
    "href": "syllabus.html#assessment-breakdown",
    "title": "BSTA 512/612 Syllabus",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 512)\nPercentage of final grade (BSTA 612)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n33%\n28%\n\n\nQuizzes\nSummative\n1/29, 2/21, 3/11\n25%\n25%\n\n\nProject Labs\nFormative\nEvery 2-3 weeks\n25%\n25%\n\n\nProject Report\nSummative\n3/21\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\nMid-Quarter Feedback\nN/A\nTBD\n2%\n2%\n\n\n612 Readings\nFormative\nApprox. every other week\n0%\n5%\n\n\n\n\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn something in (whether it is incomplete, complete, correct, or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nAfter the due date, the TAs will give you feedback (on one or more complete problems) and post the solutions.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "slides/01_Review.html#confidence-interval-for-one-mean-1",
    "href": "slides/01_Review.html#confidence-interval-for-one-mean-1",
    "title": "Review",
    "section": "Confidence interval for one mean",
    "text": "Confidence interval for one mean\n\n\nThe confidence interval for population mean \\(\\mu\\):\n\\[\n\\bar{x} \\pm t^{*}\\dfrac{s}{\\sqrt{n}}\n\\]\n\nwhere \\(t^*\\) is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on \\(df=n-1\\)\n\n\n\nWe can use R to find the critical t-value, \\(t^*\\)\n\n\nFor example the critical value for the 95% CI with \\(n=10\\) subjects is…\n\nqt(0.975, df=9)\n\n[1] 2.262157\n\n\n\nRecall, that as the \\(df\\) increases, the t-distribution converges towards the Normal distribution\n\n\n\n\nWe can also use t.test in R to calculate the confidence interval if we have a dataset.\n\nt.test(dds.discr$age)\n\n\n    One Sample t-test\n\ndata:  dds.discr$age\nt = 39.053, df = 999, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.65434 23.94566\nsample estimates:\nmean of x \n     22.8"
  },
  {
    "objectID": "slides/01_Review.html#vjsv",
    "href": "slides/01_Review.html#vjsv",
    "title": "Review",
    "section": "vjsv",
    "text": "vjsv\n\n\n\nWe can also use t.test in R to calculate the confidence interval if we have a dataset.\n\nt.test(dds.discr$age)\n\n\n    One Sample t-test\n\ndata:  dds.discr$age\nt = 39.053, df = 999, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.65434 23.94566\nsample estimates:\nmean of x \n     22.8"
  },
  {
    "objectID": "slides/01_Review.html#heres-a-decent-source-for-other-r-code-for-tests-in-511",
    "href": "slides/01_Review.html#heres-a-decent-source-for-other-r-code-for-tests-in-511",
    "title": "Review",
    "section": "Here’s a decent source for other R code for tests in 511",
    "text": "Here’s a decent source for other R code for tests in 511\nWebsite from UCLA"
  },
  {
    "objectID": "slides/01_Review.html#critical-region-method",
    "href": "slides/01_Review.html#critical-region-method",
    "title": "Review",
    "section": "Critical region method",
    "text": "Critical region method\n\nThe test statistic is: \\(t_{\\bar{x}} = \\dfrac{\\bar{x}-\\mu_0}{\\dfrac{s}{\\sqrt{n}}}\\)\n\nLet’s say we have data for: \\(t_{\\bar{x}} = \\dfrac{98.25-98.6}{\\dfrac{0.73}{\\sqrt{130}}} = -5.45\\)"
  },
  {
    "objectID": "slides/01_Review.html#how-did-we-get-the-95-ci",
    "href": "slides/01_Review.html#how-did-we-get-the-95-ci",
    "title": "Review",
    "section": "How did we get the 95% CI?",
    "text": "How did we get the 95% CI?\n\nThe t.test function can help us answer this, and give us the needed information for both approaches.\n\n\nBodyTemps = read.csv(\"data/BodyTemperatures.csv\")\n\nt.test(x = BodyTemps$Temperature, \n       # alternative = \"two-sided\", \n       mu = 98.6)\n\n\n    One Sample t-test\n\ndata:  BodyTemps$Temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923"
  },
  {
    "objectID": "slides/01_Review.html#learning-objectives",
    "href": "slides/01_Review.html#learning-objectives",
    "title": "Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "slides/01_Review.html#example-one-sample-t-test",
    "href": "slides/01_Review.html#example-one-sample-t-test",
    "title": "Review",
    "section": "Example: one sample t-test",
    "text": "Example: one sample t-test\n\nBodyTemps = read.csv(\"data/BodyTemperatures.csv\")\n\nggplot(data = BodyTemps, \n       aes(x = Temperature)) +\n  geom_histogram() +\n  theme(text = element_text(size=20)) +\n  labs(x = \"Temperature\", y = \"Count\", \n       title = \"Distribution of Body Temperature in Sample\") +\n  geom_vline(aes(xintercept = mean(BodyTemps$Temperature, na.rm = T)), \n             color = \"red\", linewidth = 2)\n\nWarning: Use of `BodyTemps$Temperature` is discouraged.\nℹ Use `Temperature` instead.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides/01_Review.html#chi-squared-distribution-models-sampling-variance",
    "href": "slides/01_Review.html#chi-squared-distribution-models-sampling-variance",
    "title": "Review",
    "section": "Chi-squared distribution: models sampling variance",
    "text": "Chi-squared distribution: models sampling variance\n\n\n\nNotation: \\(X \\sim \\chi^2_{df}\\) OR \\(X \\sim \\chi^2_{\\nu}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(X\\) takes on only positive values\n\nIf \\(Z_i\\sim \\mbox{N}(0,1)\\), then \\(Z_i^2\\sim \\chi^2_1\\)\n\nA standard normal distribution squared is the Chi squared distribution with df of 1.\n\n\n\n\nUsed in hypothesis testing and CI’s for variance or standard deviation\n\nSample variance (and SD) is random and thus can be modeled by a probability distribution: Chi-sqaured\n\nChi-squared distribution used to model the ratio of the sample variance \\(s^2\\) to population variance \\(\\sigma^2\\):\n\n\\(\\dfrac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\)"
  },
  {
    "objectID": "slides/01_Review.html#students-t-distribution",
    "href": "slides/01_Review.html#students-t-distribution",
    "title": "Review",
    "section": "Student’s t Distribution",
    "text": "Student’s t Distribution\n\n\n\nNotation: \\(T \\sim t_{df}\\) OR \\(T \\sim t_{n-1}\\)\n\nDegrees of freedom (df): \\(df=n-1\\)\n\\(T = \\dfrac{\\bar{x} - \\mu_x}{\\dfrac{s}{\\sqrt{n}}}\\sim t_{n-1}\\)\n\nIn linear modeling, used for inference on individual regression parameters\n\nThink: our estimated coefficients (\\(\\hat{\\beta}\\))"
  },
  {
    "objectID": "slides/01_Review.html#f-distribution-1",
    "href": "slides/01_Review.html#f-distribution-1",
    "title": "Review",
    "section": "F-Distribution",
    "text": "F-Distribution\n\n\n\nModel ratio of sample variances\n\nRatio of variances is important for hypothesis testing of regression models\n\nIf \\(X_1^2\\sim \\chi^2_{df1}\\) and \\(X_2^2\\sim \\chi^2_{df2}\\), where \\(X_1^2\\perp X_2^2\\), then:\n\n\\[\\dfrac{X_1^2/df1}{X_2^2/df2} \\sim F_{df1,df2}\\] - only takes on positive values\n\nImportant relationship with \\(t\\) distribution: \\(T^2 \\sim F_{1,\\nu}\\)\n\nThe square of a t-distribution with \\(df=\\nu\\)\nis an F-distribution with numerator df (\\(df_1 = 1\\)) and denominator df (\\(df_2 = \\nu\\))\n\n\n\n\n\n\nIs there a relationship between our chi-squared and F-distribution?\n\n\nRecall, \\(\\dfrac{(n-1)s^2}{\\sigma^2}\\sim \\chi^2_{n-1}\\).\nThe F-distribution for a ratio of variances between two models is: \\(F = \\dfrac{s_1^2\\sigma^2_2}{s_2^2\\sigma^2_1} \\sim F_{n_1-1, n_2-1}\\)"
  },
  {
    "objectID": "slides/01_Review.html#r-code-for-probability-distributions",
    "href": "slides/01_Review.html#r-code-for-probability-distributions",
    "title": "Review",
    "section": "R code for probability distributions",
    "text": "R code for probability distributions\n\n\nHere is a site with the various probability distributions and their R code.\n\nIt also includes practice with R code to see what each function outputs"
  },
  {
    "objectID": "slides/01_Review.html#learning-objectives-1",
    "href": "slides/01_Review.html#learning-objectives-1",
    "title": "Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\nIdentify important descriptive statistics and visualize data from a continuous variable\n\n\n\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "slides/01_Review.html#learning-objectives-2",
    "href": "slides/01_Review.html#learning-objectives-2",
    "title": "Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\n\n\n\nIdentify important distributions that will be used in 512/612\n\n\n\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "slides/01_Review.html#learning-objectives-3",
    "href": "slides/01_Review.html#learning-objectives-3",
    "title": "Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\n\n\n\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\n\n\n\nUse our previous tools in 511 to conduct a hypothesis test\nDefine error rates and power"
  },
  {
    "objectID": "slides/01_Review.html#learning-objectives-4",
    "href": "slides/01_Review.html#learning-objectives-4",
    "title": "Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\n\n\n\nUse our previous tools in 511 to conduct a hypothesis test\n\n\n\nDefine error rates and power"
  },
  {
    "objectID": "slides/01_Review.html#learning-objectives-5",
    "href": "slides/01_Review.html#learning-objectives-5",
    "title": "Review",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify important descriptive statistics and visualize data from a continuous variable\nIdentify important distributions that will be used in 512/612\nUse our previous tools in 511 to estimate a parameter and construct a confidence interval\nUse our previous tools in 511 to conduct a hypothesis test\n\n\n\nDefine error rates and power"
  },
  {
    "objectID": "slides/02_Data_Management.html#hello",
    "href": "slides/02_Data_Management.html#hello",
    "title": "Data Management with tidyr",
    "section": "Hello",
    "text": "Hello\nclass: middle, inverse"
  },
  {
    "objectID": "slides/02_Data_Management.html#what-is-the-tidyverse-1",
    "href": "slides/02_Data_Management.html#what-is-the-tidyverse-1",
    "title": "Data Management with tidyr",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nggplot2 - data visualisation\ndplyr - data manipulation\ntidyr - tidy data\nreadr - read rectangular data\npurrr - functional programming\ntibble - modern data frames\nstringr - string manipulation\nforcats - factors\nand many more …"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidy-data",
    "href": "slides/02_Data_Management.html#tidy-data",
    "title": "Data Management with tidyr",
    "section": "Tidy data",
    "text": "Tidy data\n\n\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\n1\nSource: R for Data Science. Grolemund and Wickham."
  },
  {
    "objectID": "slides/02_Data_Management.html#tidy-data-tidyverse-references",
    "href": "slides/02_Data_Management.html#tidy-data-tidyverse-references",
    "title": "Data Management with the tidyverse",
    "section": "Tidy data + Tidyverse references",
    "text": "Tidy data + Tidyverse references\n\n\n\n\n\n\n\n\n\nWickham (2014). Tidy data. Journal of Statistical Software, 59(10), 1-23.\nWickham et al. (2019). Welcome to the Tidyverse. Journal of Open Source Software, 4(43), 1686."
  },
  {
    "objectID": "slides/02_Data_Management.html#pipe-operator-magrittr",
    "href": "slides/02_Data_Management.html#pipe-operator-magrittr",
    "title": "Data Management with the tidyverse",
    "section": "Pipe operator (magrittr)",
    "text": "Pipe operator (magrittr)\n\nThe pipe operator (%&gt;%) allows us to step through sequential functions in the same way we follow if-then statements or steps from instructions\n\n \n\nI want to find my keys, then start my car, then drive to work, then park my car.\n\n \n\n\nNested\n\npark(drive(start_car(find(\"keys\")), \n           to = \"work\"))\n\n\nPiped\n\nfind(\"keys\") %&gt;%\n  start_car() %&gt;%\n  drive(to = \"work\") %&gt;%\n  park()"
  },
  {
    "objectID": "slides/02_Data_Management.html#magrittr-vs-native-pipe",
    "href": "slides/02_Data_Management.html#magrittr-vs-native-pipe",
    "title": "Data Management with the tidyverse",
    "section": "magrittr vs native pipe",
    "text": "magrittr vs native pipe\n \nAs of R 4.1.0 there is now a native pipe operator in R (|&gt;) which is very similar to magrittr’s (%&gt;%).\n \nFor teaching purposes we would strongly recommend using magrittr for the foreseeable future.\n \n\n|&gt; only supports piping to the first argument (no support for .)\n \nFor most use cases, package dependencies are easier than R version dependencies"
  },
  {
    "objectID": "slides/02_Data_Management.html#recoding-a-binary-variable",
    "href": "slides/02_Data_Management.html#recoding-a-binary-variable",
    "title": "Data Management with the tidyverse",
    "section": "Recoding a binary variable",
    "text": "Recoding a binary variable\n \n\nLet’s say I want a variable transmission to show the category names that are assigned to numeric values in the code. I want 0 to be coded as automatic and 1 to be coded as manual.\n\n \n\n\nBase R:\n\nmtcars$transmission &lt;-\n  ifelse(\n    mtcars$am == 0,\n    \"automatic\",\n    \"manual\"\n  )\n\n\nTidyverse:\n\nmtcars &lt;- mtcars %&gt;%\n  mutate(\n    transmission = case_when(\n      am == 0 ~ \"automatic\",\n      am == 1 ~ \"manual\"\n    )\n  )\n\n \n\nmutate() creates new columns that are functions of existing variables"
  },
  {
    "objectID": "slides/02_Data_Management.html#recoding-a-multi-level-variable",
    "href": "slides/02_Data_Management.html#recoding-a-multi-level-variable",
    "title": "Data Management with the tidyverse",
    "section": "Recoding a multi-level variable",
    "text": "Recoding a multi-level variable\n \n\nLet’s say I want a variable gear to show the category names that are assigned to numeric values in the code. I want 3 to be coded as gear three, 4 to be coded as gear four, 5 to be coded as gear five.\n\n \n\n\nBase R:\n\nmtcars$gear_char &lt;-\n  ifelse(\n    mtcars$gear == 3,\n    \"three\",\n    ifelse(\n      mtcars$gear == 4,\n      \"four\",\n      \"five\"\n    )\n  )\n\n\nTidyverse:\n\nmtcars &lt;- mtcars %&gt;%\n  mutate(\n    gear_char = case_when(\n      gear == 3 ~ \"three\",\n      gear == 4 ~ \"four\",\n      gear == 5 ~ \"five\"\n    )\n  )"
  },
  {
    "objectID": "slides/02_Data_Management.html#visualising-multiple-variables",
    "href": "slides/02_Data_Management.html#visualising-multiple-variables",
    "title": "Data Management with tidyr",
    "section": "Visualising multiple variables",
    "text": "Visualising multiple variables\nTidyverse\n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)\n) +\n  geom_point()"
  },
  {
    "objectID": "slides/02_Data_Management.html#visualising-even-more-variables",
    "href": "slides/02_Data_Management.html#visualising-even-more-variables",
    "title": "Data Management with tidyr",
    "section": "Visualising even more variables",
    "text": "Visualising even more variables\nTidyverse\n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)\n) +\n  geom_point() +\n  facet_wrap(~ cyl)\n\n\n\n\nBase R\n\nmtcars$trans_color &lt;- ifelse(mtcars$transmission == \"automatic\", \"green\", \"blue\")\nmtcars_cyl4 = mtcars[mtcars$cyl == 4, ]\nmtcars_cyl6 = mtcars[mtcars$cyl == 6, ]\nmtcars_cyl8 = mtcars[mtcars$cyl == 8, ]\npar(mfrow = c(1, 3), mar = c(2.5, 2.5, 2, 0), mgp = c(1.5, 0.5, 0))\nplot(mpg ~ disp, data = mtcars_cyl4, col = trans_color, main = \"Cyl 4\")\nplot(mpg ~ disp, data = mtcars_cyl6, col = trans_color, main = \"Cyl 6\")\nplot(mpg ~ disp, data = mtcars_cyl8, col = trans_color, main = \"Cyl 8\")\nlegend(\"topright\", legend = c(\"automatic\", \"manual\"), pch = 1, col = c(\"green\", \"blue\"))\n\n\n\n\n]"
  },
  {
    "objectID": "slides/02_Data_Management.html#benefits-of-starting-with-the-tidyverse",
    "href": "slides/02_Data_Management.html#benefits-of-starting-with-the-tidyverse",
    "title": "Data Management with the tidyverse",
    "section": "Benefits of starting with the tidyverse",
    "text": "Benefits of starting with the tidyverse\n\nMore (human) readable syntax\nMore consistent syntax\nEase of multivariable visualizations\nData tidying/rectangling without advanced programming\nGrowth opportunities:\n\ndplyy \\(\\to\\) SQL / Spark / etc\npurrr \\(\\to\\) functional programming\nmodeling \\(\\to\\) tidymodels"
  },
  {
    "objectID": "slides/02_Data_Management.html#ggplot2-in-tidyverse",
    "href": "slides/02_Data_Management.html#ggplot2-in-tidyverse",
    "title": "Data Management with the tidyverse",
    "section": "ggplot2 in tidyverse",
    "text": "ggplot2 in tidyverse\n\n\n\n\n\n\n\n\n\nWe talked about this in our review notes\n\nI want to revisit it: always helps to have more examples!\nThis example is closer to the multivariable work we’ll do in this class!\n\n\n \n\nggplot2 is tidyverse’s data visualization package\n\n \n\nThe gg in “ggplot2” stands for Grammar of Graphics\n\n \n\nIt is inspired by the book Grammar of Graphics by Leland Wilkinson"
  },
  {
    "objectID": "slides/02_Data_Management.html#why-start-with-ggplot2",
    "href": "slides/02_Data_Management.html#why-start-with-ggplot2",
    "title": "Data Management with tidyr",
    "section": "Why start with ggplot2?",
    "text": "Why start with ggplot2?\n–\n\nStudents come in with intuition for being able to interpret data visualizations without needing much instructions.\n\n\nFocus the majority of class time initially on syntax and leave interpretations to students.\nLater on the scale tips – spend more class time on concepts and results interpretations and less on syntax.\n\n–\n\nIt can be easier for students to detect mistakes in visualizations compared to those in wrangling or modeling."
  },
  {
    "objectID": "slides/02_Data_Management.html#what-next",
    "href": "slides/02_Data_Management.html#what-next",
    "title": "Data Management with tidyr",
    "section": "What next?",
    "text": "What next?\nIt depends on the course and subject matter, but generally data munging with dplyr is a good next step.\nSome general guidance, - Start with a small subset of verbs (e.g. select(), filter(), mutate())\n\nAim to quickly get to group_by() and summarize() as this is where the action is.\nConnecting munging back to data visualization tends to be more motivating than generating numerical summaries.\nData cleaning provides opportunities to introduce additional packages (e.g. stringr, forcats)\n\nclass: middle, inverse"
  },
  {
    "objectID": "slides/02_Data_Management.html#instructional-staff-employment-trends",
    "href": "slides/02_Data_Management.html#instructional-staff-employment-trends",
    "title": "Data Management with the tidyverse",
    "section": "Instructional staff employment trends",
    "text": "Instructional staff employment trends\nThe American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below."
  },
  {
    "objectID": "slides/02_Data_Management.html#data",
    "href": "slides/02_Data_Management.html#data",
    "title": "Data Management with the tidyverse",
    "section": "Data",
    "text": "Data\nEach row in this dataset represents a faculty type, and the columns are the years for which we have data. The values are percentage of hires of that type of faculty for each year.\n   \n\n(staff &lt;- read_csv(\"data/instructional-staff.csv\"))\n\n# A tibble: 5 × 12\n  faculty_type    `1975` `1989` `1993` `1995` `1999` `2001` `2003` `2005` `2007`\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Full-Time Tenu…   29     27.6   25     24.8   21.8   20.3   19.3   17.8   17.2\n2 Full-Time Tenu…   16.1   11.4   10.2    9.6    8.9    9.2    8.8    8.2    8  \n3 Full-Time Non-…   10.3   14.1   13.6   13.6   15.2   15.5   15     14.8   14.9\n4 Part-Time Facu…   24     30.4   33.1   33.2   35.5   36     37     39.3   40.5\n5 Graduate Stude…   20.5   16.5   18.1   18.8   18.7   19     20     19.9   19.5\n# ℹ 2 more variables: `2009` &lt;dbl&gt;, `2011` &lt;dbl&gt;"
  },
  {
    "objectID": "slides/02_Data_Management.html#recreate-the-visualization",
    "href": "slides/02_Data_Management.html#recreate-the-visualization",
    "title": "Data Management with the tidyverse",
    "section": "Recreate the visualization",
    "text": "Recreate the visualization\n \n\nIn order to recreate this visualization we need to first reshape the data:\n\none variable for faculty type\none variable for year\n\n\n \n\nConvert the data from the wide format to long format\n\npivot_longer()"
  },
  {
    "objectID": "slides/02_Data_Management.html#pivot_-functions",
    "href": "slides/02_Data_Management.html#pivot_-functions",
    "title": "Data Management with the tidyverse",
    "section": "pivot_*() functions",
    "text": "pivot_*() functions"
  },
  {
    "objectID": "slides/02_Data_Management.html#pivot-staff-data",
    "href": "slides/02_Data_Management.html#pivot-staff-data",
    "title": "Data Management with the tidyverse",
    "section": "Pivot staff data",
    "text": "Pivot staff data\n\n(staff_long &lt;- staff %&gt;%\n  pivot_longer(\n    cols = -faculty_type,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(\n    percentage = as.numeric(percentage)\n  )\n)\n\n# A tibble: 55 × 3\n   faculty_type              year  percentage\n   &lt;chr&gt;                     &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty 1975        29  \n 2 Full-Time Tenured Faculty 1989        27.6\n 3 Full-Time Tenured Faculty 1993        25  \n 4 Full-Time Tenured Faculty 1995        24.8\n 5 Full-Time Tenured Faculty 1999        21.8\n 6 Full-Time Tenured Faculty 2001        20.3\n 7 Full-Time Tenured Faculty 2003        19.3\n 8 Full-Time Tenured Faculty 2005        17.8\n 9 Full-Time Tenured Faculty 2007        17.2\n10 Full-Time Tenured Faculty 2009        16.8\n# ℹ 45 more rows\n\n\n]"
  },
  {
    "objectID": "slides/02_Data_Management.html#meh",
    "href": "slides/02_Data_Management.html#meh",
    "title": "Data Management with the tidyverse",
    "section": "Meh",
    "text": "Meh\n\nggplot(staff_long, aes(x = percentage, y = year, fill = faculty_type)) +\n  geom_col(position = \"dodge\")"
  },
  {
    "objectID": "slides/02_Data_Management.html#some-improvement",
    "href": "slides/02_Data_Management.html#some-improvement",
    "title": "Data Management with the tidyverse",
    "section": "Some improvement…",
    "text": "Some improvement…\n\nggplot(staff_long, aes(x = percentage, y = year, fill = faculty_type)) +\n  geom_col()"
  },
  {
    "objectID": "slides/02_Data_Management.html#more-improvement",
    "href": "slides/02_Data_Management.html#more-improvement",
    "title": "Data Management with the tidyverse",
    "section": "More improvement",
    "text": "More improvement\n\n\n\nstaff_long %&gt;%\n  mutate( \n    part_time = if_else(faculty_type == \"Part-Time Faculty\",\n                        \"Part-Time Faculty\", \"Other Faculty\"),\n    year = as.numeric(year)) %&gt;% \n  ggplot(\n    aes(x = year, y = percentage/100, group = faculty_type, color = part_time)) +\n  geom_line() +\n  scale_color_manual(values = c(\"gray\", \"red\")) + \n  scale_y_continuous(labels = label_percent(accuracy = 1)) + \n  theme_minimal() +\n  labs(\n    title = \"Instructional staff employment trends\",\n    x = \"Year\", y = \"Percentage\", color = NULL) +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "slides/02_Data_Management.html#data-manipulation-with-purrr-or-not",
    "href": "slides/02_Data_Management.html#data-manipulation-with-purrr-or-not",
    "title": "Data Management with tidyr",
    "section": "Data manipulation with purrr (or not?)",
    "text": "Data manipulation with purrr (or not?)\n\npurrr is a package for functional programming with the tidyverse\nIf you picked up the tidyverse &gt;2 years ago, purrr was commonly used for data science tasks that involve iteration\nIn 2021, it’s possible to do many of these data science tasks with dplyr and tidyr, these approaches are often more approachable to new learners\n\n–\n.discussion[ How familiar are you with the purrr package? Have you taught purrr in your data science courses?]"
  },
  {
    "objectID": "slides/02_Data_Management.html#ex-1.-flattening-json-files",
    "href": "slides/02_Data_Management.html#ex-1.-flattening-json-files",
    "title": "Data Management with tidyr",
    "section": "Ex 1. Flattening JSON files",
    "text": "Ex 1. Flattening JSON files\nWe have data on lego sales and some information on the buyers in JSON format. We want to covert it into a tidy data frame.\n.tiny[\n\nsales &lt;- read_rds(\"data/lego_sales.rds\")\njsonlite::toJSON(sales[1], pretty = TRUE)\n\n[\n  {\n    \"gender\": [\"Female\"],\n    \"first_name\": [\"Kimberly\"],\n    \"last_name\": [\"Beckstead\"],\n    \"age\": [24],\n    \"phone_number\": [\"216-555-2549\"],\n    \"purchases\": [\n      {\n        \"SetID\": [24701],\n        \"Number\": [\"76062\"],\n        \"Theme\": [\"DC Comics Super Heroes\"],\n        \"Subtheme\": [\"Mighty Micros\"],\n        \"Year\": [2016],\n        \"Name\": [\"Robin vs. Bane\"],\n        \"Pieces\": [77],\n        \"USPrice\": [9.99],\n        \"ImageURL\": [\"http://images.brickset.com/sets/images/76062-1.jpg\"],\n        \"Quantity\": [1]\n      }\n    ]\n  }\n] \n\n\n]"
  },
  {
    "objectID": "slides/02_Data_Management.html#purrr-solution",
    "href": "slides/02_Data_Management.html#purrr-solution",
    "title": "Data Management with tidyr",
    "section": "purrr solution",
    "text": "purrr solution\n\nsales %&gt;%\n  purrr::map_dfr(\n    function(l) {\n      purchases &lt;- purrr::map_dfr(l$purchases, ~.)\n      l$purchases &lt;- NULL\n      \n      bind_cols(as_tibble(l), purchases)\n    }\n  )\n\n]"
  },
  {
    "objectID": "slides/02_Data_Management.html#purr-solution",
    "href": "slides/02_Data_Management.html#purr-solution",
    "title": "Data Management with tidyr",
    "section": "purr solution",
    "text": "purr solution\n\n\n# A tibble: 620 × 15\n   gender first_name last_name      age phone_number SetID Number Theme Subtheme\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   \n 1 Female Kimberly   Beckstead       24 216-555-2549 24701 76062  DC C… \"Mighty…\n 2 Male   Neel       Garvin          35 819-555-3189 25626 70595  Ninj… \"Rise o…\n 3 Male   Neel       Garvin          35 819-555-3189 24665 21031  Arch… \"\"      \n 4 Female Chelsea    Bouchard        41 &lt;NA&gt;         24695 31048  Crea… \"\"      \n 5 Female Chelsea    Bouchard        41 &lt;NA&gt;         25626 70595  Ninj… \"Rise o…\n 6 Female Chelsea    Bouchard        41 &lt;NA&gt;         24721 10831  Duplo \"\"      \n 7 Female Bryanna    Welsh           19 &lt;NA&gt;         24797 75138  Star… \"Episod…\n 8 Female Bryanna    Welsh           19 &lt;NA&gt;         24701 76062  DC C… \"Mighty…\n 9 Male   Caleb      Garcia-Wide…    37 907-555-9236 24730 41115  Frie… \"\"      \n10 Male   Caleb      Garcia-Wide…    37 907-555-9236 25611 21127  Mine… \"Minifi…\n# ℹ 610 more rows\n# ℹ 6 more variables: Year &lt;int&gt;, Name &lt;chr&gt;, Pieces &lt;int&gt;, USPrice &lt;dbl&gt;,\n#   ImageURL &lt;chr&gt;, Quantity &lt;dbl&gt;\n\n\n]"
  },
  {
    "objectID": "slides/02_Data_Management.html#a-tidyr-solution",
    "href": "slides/02_Data_Management.html#a-tidyr-solution",
    "title": "Data Management with tidyr",
    "section": "A tidyr solution",
    "text": "A tidyr solution\n\ntibble(sales = sales) %&gt;%\n  unnest_wider(sales) %&gt;%\n  unnest_longer(purchases) %&gt;%\n  unnest_wider(purchases)\n\n# A tibble: 620 × 15\n   gender first_name last_name      age phone_number SetID Number Theme Subtheme\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   \n 1 Female Kimberly   Beckstead       24 216-555-2549 24701 76062  DC C… \"Mighty…\n 2 Male   Neel       Garvin          35 819-555-3189 25626 70595  Ninj… \"Rise o…\n 3 Male   Neel       Garvin          35 819-555-3189 24665 21031  Arch… \"\"      \n 4 Female Chelsea    Bouchard        41 &lt;NA&gt;         24695 31048  Crea… \"\"      \n 5 Female Chelsea    Bouchard        41 &lt;NA&gt;         25626 70595  Ninj… \"Rise o…\n 6 Female Chelsea    Bouchard        41 &lt;NA&gt;         24721 10831  Duplo \"\"      \n 7 Female Bryanna    Welsh           19 &lt;NA&gt;         24797 75138  Star… \"Episod…\n 8 Female Bryanna    Welsh           19 &lt;NA&gt;         24701 76062  DC C… \"Mighty…\n 9 Male   Caleb      Garcia-Wide…    37 907-555-9236 24730 41115  Frie… \"\"      \n10 Male   Caleb      Garcia-Wide…    37 907-555-9236 25611 21127  Mine… \"Minifi…\n# ℹ 610 more rows\n# ℹ 6 more variables: Year &lt;int&gt;, Name &lt;chr&gt;, Pieces &lt;int&gt;, USPrice &lt;dbl&gt;,\n#   ImageURL &lt;chr&gt;, Quantity &lt;dbl&gt;\n\n\n]"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyr-solution-step-1",
    "href": "slides/02_Data_Management.html#tidyr-solution-step-1",
    "title": "Data Management with tidyr",
    "section": "tidyr solution (Step 1)",
    "text": "tidyr solution (Step 1)\n\ntibble(sales = sales)\n\n# A tibble: 250 × 1\n   sales           \n   &lt;list&gt;          \n 1 &lt;named list [6]&gt;\n 2 &lt;named list [6]&gt;\n 3 &lt;named list [5]&gt;\n 4 &lt;named list [5]&gt;\n 5 &lt;named list [6]&gt;\n 6 &lt;named list [6]&gt;\n 7 &lt;named list [6]&gt;\n 8 &lt;named list [6]&gt;\n 9 &lt;named list [6]&gt;\n10 &lt;named list [6]&gt;\n# ℹ 240 more rows\n\n\n]"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyr-solution-step-2",
    "href": "slides/02_Data_Management.html#tidyr-solution-step-2",
    "title": "Data Management with tidyr",
    "section": "tidyr solution (Step 2)",
    "text": "tidyr solution (Step 2)\n\ntibble(sales = sales) %&gt;%\n  unnest_wider(sales)\n\n# A tibble: 250 × 6\n   gender first_name last_name        age phone_number purchases \n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;        &lt;list&gt;    \n 1 Female Kimberly   Beckstead         24 216-555-2549 &lt;list [1]&gt;\n 2 Male   Neel       Garvin            35 819-555-3189 &lt;list [2]&gt;\n 3 Female Chelsea    Bouchard          41 &lt;NA&gt;         &lt;list [3]&gt;\n 4 Female Bryanna    Welsh             19 &lt;NA&gt;         &lt;list [2]&gt;\n 5 Male   Caleb      Garcia-Wideman    37 907-555-9236 &lt;list [2]&gt;\n 6 Male   Chase      Fortenberry       19 205-555-3704 &lt;list [2]&gt;\n 7 Male   Kevin      Cruz              20 947-555-7946 &lt;list [1]&gt;\n 8 Male   Connor     Brown             36 516-555-4310 &lt;list [3]&gt;\n 9 Female Toni       Borison           40 284-555-4560 &lt;list [2]&gt;\n10 Male   Daniel     Hurst             44 251-555-0845 &lt;list [2]&gt;\n# ℹ 240 more rows"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyr-solution-step-3",
    "href": "slides/02_Data_Management.html#tidyr-solution-step-3",
    "title": "Data Management with tidyr",
    "section": "tidyr solution (Step 3)",
    "text": "tidyr solution (Step 3)\n\ntibble(sales = sales) %&gt;%\n  unnest_wider(sales) %&gt;%\n  unnest_longer(purchases)\n\n# A tibble: 620 × 6\n   gender first_name last_name        age phone_number purchases        \n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;        &lt;list&gt;           \n 1 Female Kimberly   Beckstead         24 216-555-2549 &lt;named list [10]&gt;\n 2 Male   Neel       Garvin            35 819-555-3189 &lt;named list [10]&gt;\n 3 Male   Neel       Garvin            35 819-555-3189 &lt;named list [10]&gt;\n 4 Female Chelsea    Bouchard          41 &lt;NA&gt;         &lt;named list [10]&gt;\n 5 Female Chelsea    Bouchard          41 &lt;NA&gt;         &lt;named list [10]&gt;\n 6 Female Chelsea    Bouchard          41 &lt;NA&gt;         &lt;named list [10]&gt;\n 7 Female Bryanna    Welsh             19 &lt;NA&gt;         &lt;named list [10]&gt;\n 8 Female Bryanna    Welsh             19 &lt;NA&gt;         &lt;named list [10]&gt;\n 9 Male   Caleb      Garcia-Wideman    37 907-555-9236 &lt;named list [10]&gt;\n10 Male   Caleb      Garcia-Wideman    37 907-555-9236 &lt;named list [10]&gt;\n# ℹ 610 more rows"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyr-solution-step-4",
    "href": "slides/02_Data_Management.html#tidyr-solution-step-4",
    "title": "Data Management with tidyr",
    "section": "tidyr solution (Step 4)",
    "text": "tidyr solution (Step 4)\n\ntibble(sales = sales) %&gt;%\n  unnest_wider(sales) %&gt;%\n  unnest_longer(purchases) %&gt;%\n  unnest_wider(purchases)\n\n# A tibble: 620 × 15\n   gender first_name last_name      age phone_number SetID Number Theme Subtheme\n   &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   \n 1 Female Kimberly   Beckstead       24 216-555-2549 24701 76062  DC C… \"Mighty…\n 2 Male   Neel       Garvin          35 819-555-3189 25626 70595  Ninj… \"Rise o…\n 3 Male   Neel       Garvin          35 819-555-3189 24665 21031  Arch… \"\"      \n 4 Female Chelsea    Bouchard        41 &lt;NA&gt;         24695 31048  Crea… \"\"      \n 5 Female Chelsea    Bouchard        41 &lt;NA&gt;         25626 70595  Ninj… \"Rise o…\n 6 Female Chelsea    Bouchard        41 &lt;NA&gt;         24721 10831  Duplo \"\"      \n 7 Female Bryanna    Welsh           19 &lt;NA&gt;         24797 75138  Star… \"Episod…\n 8 Female Bryanna    Welsh           19 &lt;NA&gt;         24701 76062  DC C… \"Mighty…\n 9 Male   Caleb      Garcia-Wide…    37 907-555-9236 24730 41115  Frie… \"\"      \n10 Male   Caleb      Garcia-Wide…    37 907-555-9236 25611 21127  Mine… \"Minifi…\n# ℹ 610 more rows\n# ℹ 6 more variables: Year &lt;int&gt;, Name &lt;chr&gt;, Pieces &lt;int&gt;, USPrice &lt;dbl&gt;,\n#   ImageURL &lt;chr&gt;, Quantity &lt;dbl&gt;"
  },
  {
    "objectID": "slides/02_Data_Management.html#dplyr-improvements",
    "href": "slides/02_Data_Management.html#dplyr-improvements",
    "title": "Data Management with the tidyverse",
    "section": "dplyr improvements",
    "text": "dplyr improvements\nAnother common use case for purrr has been working across rows and/or columns of a data frames.\n \nMuch of this functionality is now available directly in dplyr via the across() and rowwise() functions. Additional details and examples are available in the vignettes:\n\ncolumn-wise operations vignette\nrow-wise operations vignette\n\n \nand the dplyr 1.0.0 release blog posts:\n\nworking across columns\nworking within rows"
  },
  {
    "objectID": "slides/02_Data_Management.html#recommended-reading",
    "href": "slides/02_Data_Management.html#recommended-reading",
    "title": "Data Management with tidyr",
    "section": "Recommended reading",
    "text": "Recommended reading\n\nKeep up to date with the tidyverse blog for packages you teach\nFour part blog series: Teaching the Tidyverse from 2020\n\nPart 1: Getting started\nPart 2: Data visualisation\nPart 3: Data wrangling and tidying\nPart 4: When to purrr?"
  },
  {
    "objectID": "slides/02_Data_Management.html#the-larger-tidy-ecosystem",
    "href": "slides/02_Data_Management.html#the-larger-tidy-ecosystem",
    "title": "Data Management with the tidyverse",
    "section": "The larger tidy ecosystem",
    "text": "The larger tidy ecosystem\nJust to name a few…\n\njanitor\nkableExtra\npatchwork\ngghighlight\ntidybayes"
  },
  {
    "objectID": "slides/02_Data_Management.html#credit-to-mine-çetinkaya-rundel",
    "href": "slides/02_Data_Management.html#credit-to-mine-çetinkaya-rundel",
    "title": "Data Management with the tidyverse",
    "section": "Credit to Mine Çetinkaya-Rundel",
    "text": "Credit to Mine Çetinkaya-Rundel\n\nThese notes were built from Mine’s notes\n\nMost pages and code were left as she made them\nI changed a few things to match our class\n\nPlease see her Github repository for the original notes\n\n\n\nData Management"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidy-data1",
    "href": "slides/02_Data_Management.html#tidy-data1",
    "title": "Data Management with the tidyverse",
    "section": "Tidy data1",
    "text": "Tidy data1\n\n\n\n\n\n\nEach variable must have its own column.\nEach observation must have its own row.\nEach value must have its own cell.\n\nSource: R for Data Science. Grolemund and Wickham."
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyverse-visualising-multiple-variables",
    "href": "slides/02_Data_Management.html#tidyverse-visualising-multiple-variables",
    "title": "Data Management with tidyr",
    "section": "Tidyverse: Visualising multiple variables",
    "text": "Tidyverse: Visualising multiple variables\n \n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)\n) +\n  geom_point()"
  },
  {
    "objectID": "slides/02_Data_Management.html#poll-everywhere-question",
    "href": "slides/02_Data_Management.html#poll-everywhere-question",
    "title": "Data Management with tidyr",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question\nWhat would you change on the following plot? click areas that you would make changes."
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyverse-visualising-even-more-variables",
    "href": "slides/02_Data_Management.html#tidyverse-visualising-even-more-variables",
    "title": "Data Management with tidyr",
    "section": "Tidyverse: Visualising even more variables",
    "text": "Tidyverse: Visualising even more variables\n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)\n) +\n  geom_point() +\n  facet_wrap(~ cyl)"
  },
  {
    "objectID": "slides/02_Data_Management.html#base-r-visualising-even-more-variables",
    "href": "slides/02_Data_Management.html#base-r-visualising-even-more-variables",
    "title": "Data Management with tidyr",
    "section": "Base R: Visualising even more variables",
    "text": "Base R: Visualising even more variables\n\nmtcars$trans_color &lt;- ifelse(mtcars$transmission == \"automatic\", \"green\", \"blue\")\nmtcars_cyl4 = mtcars[mtcars$cyl == 4, ]\nmtcars_cyl6 = mtcars[mtcars$cyl == 6, ]\nmtcars_cyl8 = mtcars[mtcars$cyl == 8, ]\npar(mfrow = c(1, 3), mar = c(2.5, 2.5, 2, 0), mgp = c(1.5, 0.5, 0))\nplot(mpg ~ disp, data = mtcars_cyl4, col = trans_color, main = \"Cyl 4\")\nplot(mpg ~ disp, data = mtcars_cyl6, col = trans_color, main = \"Cyl 6\")\nplot(mpg ~ disp, data = mtcars_cyl8, col = trans_color, main = \"Cyl 8\")\nlegend(\"topright\", legend = c(\"automatic\", \"manual\"), pch = 1, col = c(\"green\", \"blue\"))"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyverse-visualizing-multiple-variables",
    "href": "slides/02_Data_Management.html#tidyverse-visualizing-multiple-variables",
    "title": "Data Management with the tidyverse",
    "section": "Tidyverse: Visualizing multiple variables",
    "text": "Tidyverse: Visualizing multiple variables\n \n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)) +\n  geom_point()"
  },
  {
    "objectID": "slides/02_Data_Management.html#poll-everywhere-question-1",
    "href": "slides/02_Data_Management.html#poll-everywhere-question-1",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/02_Data_Management.html#tidyverse-visualizing-even-more-variables",
    "href": "slides/02_Data_Management.html#tidyverse-visualizing-even-more-variables",
    "title": "Data Management with the tidyverse",
    "section": "Tidyverse: Visualizing even more variables",
    "text": "Tidyverse: Visualizing even more variables\n\nggplot(\n  mtcars,\n  aes(x = disp, y = mpg, color = transmission)) +\n  geom_point() +\n  facet_wrap(~ cyl)"
  },
  {
    "objectID": "slides/02_Data_Management.html#base-r-visualizing-even-more-variables",
    "href": "slides/02_Data_Management.html#base-r-visualizing-even-more-variables",
    "title": "Data Management with the tidyverse",
    "section": "Base R: Visualizing even more variables",
    "text": "Base R: Visualizing even more variables\n\nmtcars$trans_color &lt;- ifelse(mtcars$transmission == \"automatic\", \"green\", \"blue\")\nmtcars_cyl4 = mtcars[mtcars$cyl == 4, ]\nmtcars_cyl6 = mtcars[mtcars$cyl == 6, ]\nmtcars_cyl8 = mtcars[mtcars$cyl == 8, ]\npar(mfrow = c(1, 3), mar = c(2.5, 2.5, 2, 0), mgp = c(1.5, 0.5, 0))\nplot(mpg ~ disp, data = mtcars_cyl4, col = trans_color, main = \"Cyl 4\")\nplot(mpg ~ disp, data = mtcars_cyl6, col = trans_color, main = \"Cyl 6\")\nplot(mpg ~ disp, data = mtcars_cyl8, col = trans_color, main = \"Cyl 8\")\nlegend(\"topright\", legend = c(\"automatic\", \"manual\"), pch = 1, col = c(\"green\", \"blue\"))"
  },
  {
    "objectID": "slides/02_Data_Management.html#ggplot2-in-tidyverse-1",
    "href": "slides/02_Data_Management.html#ggplot2-in-tidyverse-1",
    "title": "Data Management with tidyr",
    "section": "ggplot2 \\(\\in\\) tidyverse",
    "text": "ggplot2 \\(\\in\\) tidyverse\n\n\nggplot2 is tidyverse’s data visualization package - The gg in “ggplot2” stands for Grammar of Graphics - It is inspired by the book Grammar of Graphics by Leland Wilkinson]"
  },
  {
    "objectID": "slides/02_Data_Management.html#introduction-to-some-important-reshaping-functions",
    "href": "slides/02_Data_Management.html#introduction-to-some-important-reshaping-functions",
    "title": "Data Management with the tidyverse",
    "section": "Introduction to some important reshaping functions",
    "text": "Introduction to some important reshaping functions\n\npivot_longer() and pivot_wider()\nmutate()\nacross()\nfilter()\nselect()\nrename()\n\nSummarizing data\n\ntabyl() from janitor package to make frequency tables of categorical variables\ntbl_summary()\nsummarize() to get summary statistics of variables\ngroup_by() to group data by categorical variables before finding summaries"
  },
  {
    "objectID": "slides/02_Data_Management.html#poll-everywhere-question-2",
    "href": "slides/02_Data_Management.html#poll-everywhere-question-2",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/02_Data_Management.html#footnotes",
    "href": "slides/02_Data_Management.html#footnotes",
    "title": "Data Management with tidyr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: R for Data Science. Grolemund and Wickham.↩︎"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of the major resources that have been mentioned in class:\n\nExtra materials for the class\n\nTable for summary of hypothesis tests\nCoefficient interpretations\nLaTeX in .qmd formatting by Ariel\n\n\n\nCourse sites\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nMeike’s 511 Website\n\nReview\n\n\nJessica’s R programming class site\n\nData Management\n\n\n\n\n\n\n\n\n\n\nR coding\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nggplot histograms\n\nReview\n\n\nProbability distributions and their R code\n\nReview\n\n\nVarious hypothesis tests and their R code\n\nReview\n\n\nJessica’s R programming class site\n\nData Management\n\n\n\n\n\n\n\n\n\n\nAcademic Help\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nStudent Academic Support Services\n\nIntro"
  },
  {
    "objectID": "resources.html#course-sites",
    "href": "resources.html#course-sites",
    "title": "Resources",
    "section": "Course sites:",
    "text": "Course sites:\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nMeike’s 511 Website\n\nReview\n\n\nJessica’s R programming class site\n\nData Management"
  },
  {
    "objectID": "resources.html#r-coding",
    "href": "resources.html#r-coding",
    "title": "Resources",
    "section": "R coding:",
    "text": "R coding:\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nggplot histograms\n\nReview\n\n\nProbability distributions and their R code\n\nReview\n\n\nVarious hypothesis tests and their R code\n\nReview\n\n\nJessica’s R programming class site\n\nData Management"
  },
  {
    "objectID": "resources.html#academic-help",
    "href": "resources.html#academic-help",
    "title": "Resources",
    "section": "Academic Help:",
    "text": "Academic Help:\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nStudent Academic Support Services\n\nIntro"
  },
  {
    "objectID": "slides/02_Data_Management.html#section",
    "href": "slides/02_Data_Management.html#section",
    "title": "Data Management with the tidyverse",
    "section": "",
    "text": "Artwork by @allison_horst"
  },
  {
    "objectID": "slides/02_Data_Management.html#what-is-the-tidyverse",
    "href": "slides/02_Data_Management.html#what-is-the-tidyverse",
    "title": "Data Management with the tidyverse",
    "section": "What is the tidyverse?",
    "text": "What is the tidyverse?\nThe tidyverse is a collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\n\n\nggplot2 - data visualisation\ndplyr - data manipulation\ntidyr - tidy data\nreadr - read rectangular data\npurrr - functional programming\ntibble - modern data frames\nstringr - string manipulation\nforcats - factors\nand many more …"
  },
  {
    "objectID": "slides/02_Data_Management.html#dplyr-resources",
    "href": "slides/02_Data_Management.html#dplyr-resources",
    "title": "Data Management with the tidyverse",
    "section": "dplyr resources",
    "text": "dplyr resources\n\nMore dpylr functions to reference!\n\nAdditional details and examples are available in the vignettes:\n\ncolumn-wise operations vignette\nrow-wise operations vignette\n\n \nand the dplyr 1.0.0 release blog posts:\n\nworking across columns\nworking within rows"
  },
  {
    "objectID": "slides/02_Data_Management.html#pivot-staff-data-and-mutate-percentage",
    "href": "slides/02_Data_Management.html#pivot-staff-data-and-mutate-percentage",
    "title": "Data Management with the tidyverse",
    "section": "Pivot staff data and mutate percentage",
    "text": "Pivot staff data and mutate percentage\n\n(staff_long &lt;- staff %&gt;%\n  pivot_longer(\n    cols = -faculty_type,    # columns to pivot\n    names_to = \"year\",       # name of new column for variable names\n    values_to = \"percentage\" # name of new column for values\n  ) %&gt;%\n  mutate(percentage = as.numeric(percentage))\n)\n\n# A tibble: 55 × 3\n   faculty_type              year  percentage\n   &lt;chr&gt;                     &lt;chr&gt;      &lt;dbl&gt;\n 1 Full-Time Tenured Faculty 1975        29  \n 2 Full-Time Tenured Faculty 1989        27.6\n 3 Full-Time Tenured Faculty 1993        25  \n 4 Full-Time Tenured Faculty 1995        24.8\n 5 Full-Time Tenured Faculty 1999        21.8\n 6 Full-Time Tenured Faculty 2001        20.3\n 7 Full-Time Tenured Faculty 2003        19.3\n 8 Full-Time Tenured Faculty 2005        17.8\n 9 Full-Time Tenured Faculty 2007        17.2\n10 Full-Time Tenured Faculty 2009        16.8\n# ℹ 45 more rows"
  },
  {
    "objectID": "slides/02_Data_Management.html#across",
    "href": "slides/02_Data_Management.html#across",
    "title": "Data Management with the tidyverse",
    "section": "across()",
    "text": "across()"
  },
  {
    "objectID": "slides/02_Data_Management.html#filter",
    "href": "slides/02_Data_Management.html#filter",
    "title": "Data Management with the tidyverse",
    "section": "filter()",
    "text": "filter()"
  },
  {
    "objectID": "slides/02_Data_Management.html#select",
    "href": "slides/02_Data_Management.html#select",
    "title": "Data Management with the tidyverse",
    "section": "select()",
    "text": "select()\nselect()"
  },
  {
    "objectID": "slides/02_Data_Management.html#rename",
    "href": "slides/02_Data_Management.html#rename",
    "title": "Data Management with the tidyverse",
    "section": "Rename",
    "text": "Rename\n\nrename()"
  },
  {
    "objectID": "slides/02_Data_Management.html#important-functions-for-data-management",
    "href": "slides/02_Data_Management.html#important-functions-for-data-management",
    "title": "Data Management with the tidyverse",
    "section": "Important functions for data management",
    "text": "Important functions for data management\n \nData manipulation\n\npivot_longer() and pivot_wider()\nrename()\nmutate()\nfilter()\nselect()\n\nSummarizing data\n\ntbl_summary()\ngroup_by()\nsummarize()\nacross()"
  },
  {
    "objectID": "slides/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-birefly-used-last-class",
    "href": "slides/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-birefly-used-last-class",
    "title": "Data Management with the tidyverse",
    "section": "Let’s look back at the dds.discr dataset that I birefly used last class",
    "text": "Let’s look back at the dds.discr dataset that I birefly used last class\n   \n\nWe will load the data (This is a special case! dds.discr is a built-in R dataset)\n\n\ndata(\"dds.discr\")\n\n\nNow, let’s take a glimpse at the dataset:\n\n\nglimpse(dds.discr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides/02_Data_Management.html#important-functions-for-data-management-1",
    "href": "slides/02_Data_Management.html#important-functions-for-data-management-1",
    "title": "Data Management with the tidyverse",
    "section": "Important functions for data management",
    "text": "Important functions for data management\nData manipulation\n\npivot_longer() and pivot_wider()\nrename()\nmutate()\nfilter()\nselect()\n\nSummarizing data\n\ntabyl() from janitor package to make frequency tables of categorical variables\ntbl_summary()\nsummarize() to get summary statistics of variables\ngroup_by() to group data by categorical variables before finding summaries\nacross()"
  },
  {
    "objectID": "slides/02_Data_Management.html#rename-one-of-the-first-things-i-usually-do",
    "href": "slides/02_Data_Management.html#rename-one-of-the-first-things-i-usually-do",
    "title": "Data Management with the tidyverse",
    "section": "rename(): one of the first things I usually do",
    "text": "rename(): one of the first things I usually do\n\nI notice that two variables have values that don’t necessarily match the variable name\n\nFemale and male are not genders\n“White not Hispanic” combines race and ethnicity into one category\n\n\n\nI want to rename gender to SAB (sex assigned at birth) and rename ethnicity to R_E (race and ethnicity)\n\n \n\ndds.discr1 = dds.discr %&gt;% \n  rename(SAB = gender, \n         R_E = ethnicity)\n\nglimpse(dds.discr1)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have",
    "href": "slides/02_Data_Management.html#mutate-constructing-new-variables-from-what-you-have",
    "title": "Data Management with the tidyverse",
    "section": "mutate(): constructing new variables from what you have",
    "text": "mutate(): constructing new variables from what you have\n\nWe’ve seen a couple examples for mutate() so far (mostly because its used so often!)\nWe haven’t seen an example where we make a new variable from two variables\n\n\nI want to make a variable that is the ratio of expenditures over age\n\n \n\ndds.discr2 = dds.discr1 %&gt;%\n  mutate(exp_to_age = expenditures/age)\n\nglimpse(dds.discr2)\n\nRows: 1,000\nColumns: 7\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…"
  },
  {
    "objectID": "slides/02_Data_Management.html#poll-everywhere-question-3",
    "href": "slides/02_Data_Management.html#poll-everywhere-question-3",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/02_Data_Management.html#tabyl",
    "href": "slides/02_Data_Management.html#tabyl",
    "title": "Data Management with the tidyverse",
    "section": "tabyl():",
    "text": "tabyl():\n\n\n\nlibrary(janitor)\n\ndds.discr2 %&gt;% \n  tabyl(R_E)  \n\n                R_E   n percent\n    American Indian   4   0.004\n              Asian 129   0.129\n              Black  59   0.059\n           Hispanic 376   0.376\n         Multi Race  26   0.026\n    Native Hawaiian   3   0.003\n              Other   2   0.002\n White not Hispanic 401   0.401\n\n\n\n\ndds.discr2 %&gt;% \n  tabyl(R_E) %&gt;%\n  adorn_totals(\"row\") %&gt;% \n  adorn_pct_formatting(digits=2)   \n\n                R_E    n percent\n    American Indian    4   0.40%\n              Asian  129  12.90%\n              Black   59   5.90%\n           Hispanic  376  37.60%\n         Multi Race   26   2.60%\n    Native Hawaiian    3   0.30%\n              Other    2   0.20%\n White not Hispanic  401  40.10%\n              Total 1000 100.00%"
  },
  {
    "objectID": "slides/02_Data_Management.html#tbl_summary",
    "href": "slides/02_Data_Management.html#tbl_summary",
    "title": "Data Management with the tidyverse",
    "section": "tbl_summary()",
    "text": "tbl_summary()"
  },
  {
    "objectID": "slides/02_Data_Management.html#summarize",
    "href": "slides/02_Data_Management.html#summarize",
    "title": "Data Management with the tidyverse",
    "section": "summarize()",
    "text": "summarize()"
  },
  {
    "objectID": "slides/02_Data_Management.html#group_by",
    "href": "slides/02_Data_Management.html#group_by",
    "title": "Data Management with the tidyverse",
    "section": "group_by()",
    "text": "group_by()"
  },
  {
    "objectID": "slides/02_Data_Management.html#r-programming-class-at-ohsu",
    "href": "slides/02_Data_Management.html#r-programming-class-at-ohsu",
    "title": "Data Management with the tidyverse",
    "section": "R programming class at OHSU!",
    "text": "R programming class at OHSU!\nYou can check out Dr. Jessica Minnier’s R class page if you want more notes, videos, etc."
  },
  {
    "objectID": "slides/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-briefly-used-last-class",
    "href": "slides/02_Data_Management.html#lets-look-back-at-the-dds.discr-dataset-that-i-briefly-used-last-class",
    "title": "Data Management with the tidyverse",
    "section": "Let’s look back at the dds.discr dataset that I briefly used last class",
    "text": "Let’s look back at the dds.discr dataset that I briefly used last class\n   \n\nWe will load the data (This is a special case! dds.discr is a built-in R dataset)\n\n\ndata(\"dds.discr\")\n\n\nNow, let’s take a glimpse at the dataset:\n\n\nglimpse(dds.discr)\n\nRows: 1,000\nColumns: 6\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ gender       &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ ethnicity    &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…"
  },
  {
    "objectID": "slides/02_Data_Management.html#tbl_summary-table-summary",
    "href": "slides/02_Data_Management.html#tbl_summary-table-summary",
    "title": "Data Management with the tidyverse",
    "section": "tbl_summary() : table summary",
    "text": "tbl_summary() : table summary\n\nWhat if I want one of those fancy summary tables that are at the top of most research articles? (lovingly called “Table 1”)\n\n\n\n\nlibrary(gtsummary)\ntbl_summary(dds.discr2)\n\n\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    id\n55,385 (31,809, 76,135)\n    age.cohort\n\n        0-5\n82 (8.2%)\n        6-12\n175 (18%)\n        13-17\n212 (21%)\n        18-21\n199 (20%)\n        22-50\n226 (23%)\n        51+\n106 (11%)\n    age\n18 (12, 26)\n    SAB\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    expenditures\n7,026 (2,899, 37,713)\n    R_E\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n    exp_to_age\n462 (274, 938)\n  \n  \n  \n    \n      1 Median (IQR); n (%)"
  },
  {
    "objectID": "slides/02_Data_Management.html#tbl_summary-table-summary-12",
    "href": "slides/02_Data_Management.html#tbl_summary-table-summary-12",
    "title": "Data Management with the tidyverse",
    "section": "tbl_summary() : table summary (1/2)",
    "text": "tbl_summary() : table summary (1/2)\n\nWhat if I want one of those fancy summary tables that are at the top of most research articles? (lovingly called “Table 1”)\n\n\n\n\nlibrary(gtsummary)\ntbl_summary(dds.discr2)\n\n\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    id\n55,385 (31,809, 76,135)\n    age.cohort\n\n        0-5\n82 (8.2%)\n        6-12\n175 (18%)\n        13-17\n212 (21%)\n        18-21\n199 (20%)\n        22-50\n226 (23%)\n        51+\n106 (11%)\n    age\n18 (12, 26)\n    SAB\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    expenditures\n7,026 (2,899, 37,713)\n    R_E\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n    exp_to_age\n462 (274, 938)\n  \n  \n  \n    \n      1 Median (IQR); n (%)"
  },
  {
    "objectID": "slides/02_Data_Management.html#tbl_summary-table-summary-22",
    "href": "slides/02_Data_Management.html#tbl_summary-table-summary-22",
    "title": "Data Management with the tidyverse",
    "section": "tbl_summary() : table summary (2/2)",
    "text": "tbl_summary() : table summary (2/2)\n\nLet’s make this more presentable\n\n \n\n\n\ndds.discr2 %&gt;%\n  select(-id, -age.cohort, -exp_to_age) %&gt;%\n  tbl_summary(label = c(age ~ \"Age\", \n                        R_E ~ \"Race/Ethnicity\", \n                        SAB ~ \"Sex Assigned at Birth\", \n                        expenditures ~ \"Expenditures\") ,\n              statistic = list(all_continuous() ~ \"{mean} ({sd})\"))\n\n\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 1,0001\n    \n  \n  \n    Age\n23 (18)\n    Sex Assigned at Birth\n\n        Female\n503 (50%)\n        Male\n497 (50%)\n    Expenditures\n18,066 (19,543)\n    Race/Ethnicity\n\n        American Indian\n4 (0.4%)\n        Asian\n129 (13%)\n        Black\n59 (5.9%)\n        Hispanic\n376 (38%)\n        Multi Race\n26 (2.6%)\n        Native Hawaiian\n3 (0.3%)\n        Other\n2 (0.2%)\n        White not Hispanic\n401 (40%)\n  \n  \n  \n    \n      1 Mean (SD); n (%)"
  },
  {
    "objectID": "slides/02_Data_Management.html#filter-keep-rows-that-match-a-condition",
    "href": "slides/02_Data_Management.html#filter-keep-rows-that-match-a-condition",
    "title": "Data Management with the tidyverse",
    "section": "filter(): keep rows that match a condition",
    "text": "filter(): keep rows that match a condition\n\nWhat if I want to subset the data frame? (keep certain rows of observations)\n\n\nI want to look at the data for people who between 50 and 60 years old\n\n \n\ndds.discr3 = dds.discr2 %&gt;%\n  filter(age &gt;= 50 & age &lt;= 60)\n\nglimpse(dds.discr3)\n\nRows: 23\nColumns: 7\n$ id           &lt;int&gt; 15970, 19412, 29506, 31658, 36123, 39287, 39672, 43455, 4…\n$ age.cohort   &lt;fct&gt; 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51+, 51…\n$ age          &lt;int&gt; 51, 60, 56, 60, 59, 59, 54, 57, 52, 57, 55, 52, 59, 54, 5…\n$ SAB          &lt;fct&gt; Female, Female, Female, Female, Male, Female, Female, Mal…\n$ expenditures &lt;int&gt; 54267, 57702, 48215, 46873, 42739, 44734, 52833, 48363, 5…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, White not Hispani…\n$ exp_to_age   &lt;dbl&gt; 1064.0588, 961.7000, 860.9821, 781.2167, 724.3898, 758.20…"
  },
  {
    "objectID": "slides/02_Data_Management.html#select-keep-or-drop-columns-using-their-names-and-types",
    "href": "slides/02_Data_Management.html#select-keep-or-drop-columns-using-their-names-and-types",
    "title": "Data Management with the tidyverse",
    "section": "select(): keep or drop columns using their names and types",
    "text": "select(): keep or drop columns using their names and types\n\nWhat if I want to remove or keep certain variables?\n\n\nI want to only have age and expenditure in my data frame\n\n \n\ndds.discr4 = dds.discr2 %&gt;%\n  select(age, expenditures)\n\nglimpse(dds.discr4)\n\nRows: 1,000\nColumns: 2\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…"
  },
  {
    "objectID": "slides/02_Data_Management.html#summarize-summarize-your-data-or-grouped-data-into-one-row",
    "href": "slides/02_Data_Management.html#summarize-summarize-your-data-or-grouped-data-into-one-row",
    "title": "Data Management with the tidyverse",
    "section": "summarize(): summarize your data or grouped data into one row",
    "text": "summarize(): summarize your data or grouped data into one row\n\nWhat if I want to calculate specific descriptive statistics for my variables?\nThis function is often best used with group_by()\nIf only presenting the summaries, functions like tbl_summary() is better\nsummarize() creates a new data frame, which means you can plot and manipulate the summarized data\n\n \n\n\nOver whole sample:\n\ndds.discr2 %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 1 × 3\n     ave     SD   med\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18066. 19543.  7026\n\n\n\nGrouped by sex assigned at birth:\n\ndds.discr2 %&gt;% \n  group_by(SAB) %&gt;% \n  summarize(\n    ave = mean(expenditures),\n    SD = sd(expenditures),\n    med = median(expenditures))\n\n# A tibble: 2 × 4\n  SAB       ave     SD   med\n  &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1 Female 18130. 20020.  6400\n2 Male   18001. 19068.  7219"
  },
  {
    "objectID": "slides/02_Data_Management.html#group_by-group-by-one-or-more-variables",
    "href": "slides/02_Data_Management.html#group_by-group-by-one-or-more-variables",
    "title": "Data Management with the tidyverse",
    "section": "group_by(): group by one or more variables",
    "text": "group_by(): group by one or more variables\n\nWhat if I want to quickly look at group differences?\nIt will not change how the data look, but changes the actions of following functions\n\n\nI want to group my data by sex assigned at birth.\n\n \n\ndds.discr5 = dds.discr2 %&gt;%\n  group_by(SAB)\nglimpse(dds.discr5)\n\nRows: 1,000\nColumns: 7\nGroups: SAB [2]\n$ id           &lt;int&gt; 10210, 10409, 10486, 10538, 10568, 10690, 10711, 10778, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;int&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;int&gt; 2113, 41924, 1454, 6400, 4412, 4566, 3915, 3873, 5021, 28…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…\n\n\n\nLet’s see how the groups change something like the summarize() function in the next slide"
  },
  {
    "objectID": "slides/02_Data_Management.html#across-apply-a-function-across-multiple-columns",
    "href": "slides/02_Data_Management.html#across-apply-a-function-across-multiple-columns",
    "title": "Data Management with the tidyverse",
    "section": "across(): apply a function across multiple columns",
    "text": "across(): apply a function across multiple columns\n\nLike group_by(), this function is often paired with another transformation function\n\n\nI want all my integer values to have two significant figures.\n\n \n\ndds.discr6 = dds.discr2 %&gt;%\n  mutate(across(where(is.integer), signif, digits = 2))\n\nglimpse(dds.discr6)\n\nRows: 1,000\nColumns: 7\n$ id           &lt;dbl&gt; 10000, 10000, 10000, 11000, 11000, 11000, 11000, 11000, 1…\n$ age.cohort   &lt;fct&gt; 13-17, 22-50, 0-5, 18-21, 13-17, 13-17, 13-17, 13-17, 13-…\n$ age          &lt;dbl&gt; 17, 37, 3, 19, 13, 15, 13, 17, 14, 13, 13, 14, 15, 17, 20…\n$ SAB          &lt;fct&gt; Female, Male, Male, Female, Male, Female, Female, Male, F…\n$ expenditures &lt;dbl&gt; 2100, 42000, 1500, 6400, 4400, 4600, 3900, 3900, 5000, 29…\n$ R_E          &lt;fct&gt; White not Hispanic, White not Hispanic, Hispanic, Hispani…\n$ exp_to_age   &lt;dbl&gt; 124.2941, 1133.0811, 484.6667, 336.8421, 339.3846, 304.40…"
  },
  {
    "objectID": "slides/02_Data_Management.html#recoding-a-binary-variable-with-pipe-operator",
    "href": "slides/02_Data_Management.html#recoding-a-binary-variable-with-pipe-operator",
    "title": "Data Management with the tidyverse",
    "section": "Recoding a binary variable with pipe operator",
    "text": "Recoding a binary variable with pipe operator\n \n\nLet’s say I want a variable transmission to show the category names that are assigned to numeric values in the code. I want 0 to be coded as automatic and 1 to be coded as manual.\n\n \n\n\nBase R:\n\nmtcars$transmission &lt;-\n  ifelse(\n    mtcars$am == 0,\n    \"automatic\",\n    \"manual\"\n  )\n\n\nTidyverse:\n\nmtcars &lt;- mtcars %&gt;%\n  mutate(\n    transmission = case_when(\n      am == 0 ~ \"automatic\",\n      am == 1 ~ \"manual\"\n    )\n  )\n\n \n\nmutate() creates new columns that are functions of existing variables"
  },
  {
    "objectID": "slides/02_Data_Management.html#example-for-pivot_longer-instructional-staff-employment-trends",
    "href": "slides/02_Data_Management.html#example-for-pivot_longer-instructional-staff-employment-trends",
    "title": "Data Management with the tidyverse",
    "section": "Example for pivot_longer(): Instructional staff employment trends",
    "text": "Example for pivot_longer(): Instructional staff employment trends\nThe American Association of University Professors (AAUP) is a nonprofit membership association of faculty and other academic professionals. This report by the AAUP shows trends in instructional staff employees between 1975 and 2011, and contains an image very similar to the one given below."
  },
  {
    "objectID": "slides/02_Data_Management.html#poll-everywhere-question-2-1",
    "href": "slides/02_Data_Management.html#poll-everywhere-question-2-1",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/02_Data_Management.html#poll-everywhere-question-3-1",
    "href": "slides/02_Data_Management.html#poll-everywhere-question-3-1",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/02_Data_Management.html#a-meh-plot-over-the-years",
    "href": "slides/02_Data_Management.html#a-meh-plot-over-the-years",
    "title": "Data Management with the tidyverse",
    "section": "A “meh” plot over the years",
    "text": "A “meh” plot over the years\n\nggplot(staff_long, aes(x = percentage, y = year, fill = faculty_type)) +\n  geom_col()"
  },
  {
    "objectID": "slides/02_Data_Management.html#all-that-just-to-show-one-helpful-function",
    "href": "slides/02_Data_Management.html#all-that-just-to-show-one-helpful-function",
    "title": "Data Management with the tidyverse",
    "section": "All that just to show one helpful function",
    "text": "All that just to show one helpful function\nNow we can move onto the other functions mentioned:\n \nData manipulation\n\npivot_longer() and pivot_wider()\nrename()\nmutate()\nfilter()\nselect()\n\nSummarizing data\n\ntbl_summary()\ngroup_by()\nsummarize()\nacross()"
  },
  {
    "objectID": "slides/02_Data_Management.html#poll-everywhere-question-4",
    "href": "slides/02_Data_Management.html#poll-everywhere-question-4",
    "title": "Data Management with the tidyverse",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "homework/HW1.html#question-1",
    "href": "homework/HW1.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\nVisit this site on dplyr. https://dplyr.tidyverse.org/reference/index.html#groups\nFor one of the functions that we have not discussed in class, please use it on the dds.discr dataset.\n\nQuestion 1\nPlease use R code to determine the following answers. (adapted from problem 3.3)\n\n\n\n\n\n\nType ?pnorm in the console to get some information on a potentially helpful function.\n\n\n\n\nPart a\nFrom a normal distribution with mean 4 and standard deviation 6, what is \\(P(X&gt;2)\\)?\n\n\nPart b\nFrom a normal distribution with mean 4 and standard deviation 6, for what value (in place of ??) would \\(P(X&gt;??) = 0.1\\)?\n\n\n\nQuestion 2\nSuppose that the height (\\(H\\)) of assigned-male-at-birth (AMAB) patients registered at a clinic has the normal distribution with mean 70 inches and variance 4. (adapted from problem 3.11)\n\nPart a\nFor a random sample of patients of size \\(n = 25\\), the expression \\(P(\\bar{H} &lt; 65)\\), in which \\(\\bar{H}\\) denotes the sample mean height, is equivalent to saying \\(P(Z &lt; ?)\\)\n\n\n\n\n\n\n\\(Z\\) is a standard normal random variable.\n\n\n\n\n\nPart b\nUsing the pnorm function, show that the probability expressions in Part a are equal.\n\n\nPart c\nFind an interval \\((a, b)\\) such that \\(P(a&lt; \\bar{H} &lt;b) = 0.80\\) for the same random sample in Part a.\n\n\n\nQuestion 3\nTest the null hypothesis that the true population average height is the same for two independent groups from one hospital versus the alternative hypothesis that these two population averages are different, using the following data:\n\nGroup 1: [69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73]\nGroup 2: [67.54, 68.51, 71.84, 70.59, 71.52, 71.50]\n\nYou may assume that the populations from which the data come are each normally distributed, with equal population variances. What conclusion should be drawn, with \\(\\alpha = 0.05\\)?\n\n\n\n\n\n\nPlease attempt this problem using R. Take a look at the information for the t.test function. You will need to set x, y, alternative, and var.equal=T. You can use the below groups coded in R.\n\n\n\n\ngrp1 = c(69.25, 72.80, 68.73, 72.01, 70.36, 71.49, 72.73)\ngrp2 = c(67.54, 68.51, 71.84, 70.59, 71.52, 71.50)\n\n\n\nQuestion 4\nThe choice of an alternative hypothesis (\\(H_A\\) or \\(H_1\\)) should depend primarily on (choose all that apply)\n\nthe data obtained from the study.\nwhat the investigator is interested in determining.\nthe critical region.\nthe significance level.\nthe power of the test.\n\n\n\nQuestion 5\nThe accompanying table gives the dry weights (Y ) of 11 chick embryos ranging in age from 6 to 16 days (X ). Also given in the table are the values of the common logarithms of the weights (Z).\n\nLoad the dataset using the readxl package.\n\nThis readxl package was installed as a part of the tidyverse, however it does not get loaded when you load the tidyverse package and thus you need to do that separately.\nUse the command read_excel(), as shown below\n\n\n\nlibrary(readxl)\n# you might need to update the location of the data file\n# you can choose whatever name you like for the tibble when loading it into R's workspace \nch05q01 &lt;- read_excel(\"./data/CH05Q01.xls\")\n\n\n\nPart a\nCreate the scatterplots in R using ggplot the above dataset. Observe the following two scatter diagrams. Describe the relationships between age (X) and dry weight (Y) and between age and log10 dry weight (Z).\n\n\nPart b\nState the simple linear regression models for these two regressions: Y regressed on X and Z regressed on X.\n\n\n\n\n\n\nThis is asking for the regression models BEFORE you find the values of the coefficients.\n\n\n\n\n\nPart c\nDetermine the least-squares estimates of each of the regression lines in part (b).\n\n\n\n\n\n\nNow get the regression coefficients using R and plug them into the regression models from (b). You can get the coefficients from the R output - you don’t have to use the formulas.\n\n\n\n\n\nPart d\nCreate a line on your plots. Which of the two regression lines has the better fit? Based on your answers to parts (a)–(c), is it more appropriate to run a linear regression of Y on X or of Z on X? Explain.\n\n\nPart e\nFor the regression that you chose as being more appropriate in part (d), find 95% confidence intervals for the true slope and intercept. Interpret each interval with regard to the null hypothesis that the true value is 0.\n\n\n\n\n\n\nYou can get the CI’s from the R output - you don’t have to use the formulas.\n\n\n\n\n\nPart f\nFor the regression that you chose as being more appropriate in part (d), add 95% confidence and prediction bands. Using your sketch, find and interpret an approximate 95% confidence interval for the mean response of an 8-day-old chick."
  },
  {
    "objectID": "slides/01_Review.html#outcomes-of-our-hypothesis-test",
    "href": "slides/01_Review.html#outcomes-of-our-hypothesis-test",
    "title": "Review",
    "section": "Outcomes of our hypothesis test",
    "text": "Outcomes of our hypothesis test"
  },
  {
    "objectID": "slides/01_Review.html#prabilities-of-outcomes",
    "href": "slides/01_Review.html#prabilities-of-outcomes",
    "title": "Review",
    "section": "Prabilities of outcomes",
    "text": "Prabilities of outcomes\n\nType 1 error is \\(\\alpha\\)\n\nThe probability that we falsly reject the null hypothesis (but the null is true!!)\n\nPower is \\(1-\\beta\\)\n\nThe probability of correctly rejecting the null hypothesis"
  },
  {
    "objectID": "slides/01_Review.html#what-i-think-is-the-most-intuitive-way-to-look-at-it",
    "href": "slides/01_Review.html#what-i-think-is-the-most-intuitive-way-to-look-at-it",
    "title": "Review",
    "section": "What I think is the most intuitive way to look at it",
    "text": "What I think is the most intuitive way to look at it"
  },
  {
    "objectID": "weeks/week_02_sched.html#announcements",
    "href": "weeks/week_02_sched.html#announcements",
    "title": "Week 2",
    "section": "Announcements",
    "text": "Announcements\n\nWednesday 1/17\n\nOur physical classroom space will be changing…\n\nIt’s a little confusing - our time will be split between three classrooms in the RLSB\n2 are right next to each other\nTo start, our classes for next week are in:\n\non Monday, 1/22: RLSB 3A003B\non Wednesday, 1/22: RLSB 3A003A\n\n\nHW 1 IS NOT DUE THIS WEEK!!! This is my mistake!!\n\nHomework 1 is due 1/25!!\nThe finalized HW1 is finally up! Thank you for your patience!\n\nMuddiest points for Week 1 are added\nOffice hours starting this week!\n\nFirst one is today at 4:30 with Antara\n\nIf you are in 612, the reading assignments are posted\nWanted to clear something up about attendance\n\nIf you miss the exit tickets for less than or equal to 5 classes, your grade will not be impacted\nIf you miss more than 5 exit tickets, then your attendance grade will be affected\n\nAny questions on the lab? (10 minutes)"
  },
  {
    "objectID": "slides/03_SLR.html#topics",
    "href": "slides/03_SLR.html#topics",
    "title": "Simple Linear Regression (SLR)",
    "section": "Topics",
    "text": "Topics\nTopics for SLR:\nassumptions\nparameter estimation and interpretation\nproperties of LSE\nestimation of variance?\nTopics\n\nInference for slope and intercept\n\nCI’s and hypothesis tests\n\nInference for mean value of y at specific values of x\n\nConfidence bands of best-fit line\n\nPrediction intervals for individual predictions\n\nPrediction bands\n\nF-test for the slope\nPearson correlation coefficient r\nCoefficient of determination R2\nTesting the correlation coefficient !\nCI for the correlation coefficient !"
  },
  {
    "objectID": "slides/03_SLR.html#lets-start-with-an-example",
    "href": "slides/03_SLR.html#lets-start-with-an-example",
    "title": "Simple Linear Regression (SLR)",
    "section": "Let’s start with an example",
    "text": "Let’s start with an example\n\n\n\n\n\n\n\n\n \nAverage life expectancy vs. female literacy rate\n \n\nEach point on the plot is for a different country\n\n \n\n\\(X\\) = country’s adult female literacy rate\n\n \n\n\\(Y\\) = country’s average life expectancy (years)\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#questions-we-can-ask-with-this-model",
    "href": "slides/03_SLR.html#questions-we-can-ask-with-this-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Questions we can ask with this model",
    "text": "Questions we can ask with this model\n\n\n\n\n\n\n\n\n\nHow do we…\n\ncalculate slope & intercept?\ninterpret slope & intercept?\ndo inference for slope & intercept?\n\nCI, p-value\n\ndo prediction with regression line?\n\nCI for prediction?\n\n\nDoes the model fit the data well?\n\nShould we be using a line to model the data?\n\nShould we add additional variables to the model?\n\nmultiple/multivariable regression\n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#dependent-vs.-independent-variables",
    "href": "slides/03_SLR.html#dependent-vs.-independent-variables",
    "title": "Simple Linear Regression (SLR)",
    "section": "Dependent vs. Independent Variables",
    "text": "Dependent vs. Independent Variables"
  },
  {
    "objectID": "slides/03_SLR.html#association-vs.-prediction",
    "href": "slides/03_SLR.html#association-vs.-prediction",
    "title": "Simple Linear Regression (SLR)",
    "section": "Association vs. prediction",
    "text": "Association vs. prediction\n\n\n\n\nAssociation\n\n\n\nWhat is the association between countries’ life expectancy and female literacy rate?\nUse the slope of the line or correlation coefficient\n\n\n\n\n\n\nPrediction\n\n\n\nWhat is the expected average life expectancy for a country with a specified female literacy rate?    \n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#study-design",
    "href": "slides/03_SLR.html#study-design",
    "title": "Simple Linear Regression (SLR)",
    "section": "Study Design",
    "text": "Study Design\n\nExperiment\n\nObservational units are randomly assigned to important predictor levels\n\nRandom assignment controls for confounding variables (age, gender, race, etc.)\n“gold standard” for determining causality\nObservational unit is often at the participant-level\n\n\nQuasi-experiment\n\nParticipants are assigned to intervention levels without randomization\nNot common study design\n\nObservational\n\nNo randomization or assignment of intervention conditions\nIn general cannot infer causality\n\nHowever, there are casual inference methods…"
  },
  {
    "objectID": "slides/03_SLR.html#simple-linear-regression-model",
    "href": "slides/03_SLR.html#simple-linear-regression-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X\\) is our independent variable\n\nAka predictor, regressor, exposure variable\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/03_SLR.html#linear-models",
    "href": "slides/03_SLR.html#linear-models",
    "title": "Simple Linear Regression (SLR)",
    "section": "“Linear” Models",
    "text": "“Linear” Models"
  },
  {
    "objectID": "slides/03_SLR.html#model-components",
    "href": "slides/03_SLR.html#model-components",
    "title": "Simple Linear Regression (SLR)",
    "section": "Model Components",
    "text": "Model Components"
  },
  {
    "objectID": "slides/03_SLR.html#interpretations",
    "href": "slides/03_SLR.html#interpretations",
    "title": "Simple Linear Regression (SLR)",
    "section": "Interpretations",
    "text": "Interpretations"
  },
  {
    "objectID": "slides/03_SLR.html#parameter-estimation-best-fit-line",
    "href": "slides/03_SLR.html#parameter-estimation-best-fit-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "Parameter estimation: best fit line",
    "text": "Parameter estimation: best fit line"
  },
  {
    "objectID": "slides/03_SLR.html#least-squares-model-assumptions",
    "href": "slides/03_SLR.html#least-squares-model-assumptions",
    "title": "Simple Linear Regression (SLR)",
    "section": "Least squares model assumptions",
    "text": "Least squares model assumptions"
  },
  {
    "objectID": "slides/03_SLR.html#estimate-of-variance",
    "href": "slides/03_SLR.html#estimate-of-variance",
    "title": "Simple Linear Regression (SLR)",
    "section": "Estimate of variance??",
    "text": "Estimate of variance??"
  },
  {
    "objectID": "slides/03_SLR.html",
    "href": "slides/03_SLR.html",
    "title": "Simple Linear Regression (SLR)",
    "section": "",
    "text": "Identify the aims of your research and see how they align with the intended purpose of simple linear regression\nIdentify the simple linear regression model and define statistics language for key notation\nIllustrate how ordinary least squares (OLS) finds the best model parameter estimates\nSolve the optimal coefficient estimates for simple linear regression using OLS\nApply OLS in R for simple linear regression of real data\n\n\n\nTopics for SLR:\nassumptions\nparameter estimation and interpretation\nproperties of LSE\nestimation of variance?\nTopics\n\nInference for slope and intercept\n\nCI’s and hypothesis tests\n\nInference for mean value of y at specific values of x\n\nConfidence bands of best-fit line\n\nPrediction intervals for individual predictions\n\nPrediction bands\n\nF-test for the slope\nPearson correlation coefficient r\nCoefficient of determination R2\nTesting the correlation coefficient !\nCI for the correlation coefficient !\n\n\n\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\nAverage life expectancy vs. female literacy rate \\(\\beta\\)\n\nEach point on the plot is for a different country\n\\(X\\) = country’s adult female literacy rate\n\\(Y\\) = country’s average life expectancy (years)\nData are from Gapminder (2011)\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]\n\n\n\n\n\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "slides/Day16_bsta511.html#where-are-we",
    "href": "slides/Day16_bsta511.html#where-are-we",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/Day16_bsta511.html#goals-for-today-sections-6.3-6.4",
    "href": "slides/Day16_bsta511.html#goals-for-today-sections-6.3-6.4",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Goals for today (Sections 6.3-6.4)",
    "text": "Goals for today (Sections 6.3-6.4)\nSimple Linear Regression Part 2\n\nReview of\n\nbest-fit line (aka regression line or least-squares line)\nresiduals\npopulation model\n\nLINE conditions and how to assess them\n\nNew diagnostic tools:\n\nNormal QQ plots of residuals\nResidual plots\n\n\nCoefficient of determination (\\(R^2\\))\nRegression inference\n\nInference for population slope \\(\\beta_1\\)\n\nCI & hypothesis test\n\nCI for mean response \\(\\mu_{Y|x^*}\\)\nPrediction interval for predicting individual observations\n\n\nConfidence bands vs. predictions bands"
  },
  {
    "objectID": "slides/Day16_bsta511.html#life-expectancy-vs.-female-adult-literacy-rate",
    "href": "slides/Day16_bsta511.html#life-expectancy-vs.-female-adult-literacy-rate",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Life expectancy vs. female adult literacy rate",
    "text": "Life expectancy vs. female adult literacy rate\nhttps://www.gapminder.org/tools/#$model$markers$bubble$encoding$x$data$concept=literacy_rate_adult_female_percent_of_females_ages_15_above&source=sg&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2011;;;;;&chart-type=bubbles&url=v1"
  },
  {
    "objectID": "slides/Day16_bsta511.html#dataset-description",
    "href": "slides/Day16_bsta511.html#dataset-description",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData file: lifeexp_femlit_water_2011.csv\nData were downloaded from https://www.gapminder.org/data/\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same. Source: https://www.gapminder.org/data/documentation/gd004/\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life. Source: http://data.uis.unesco.org/\nAt least basic water source (%) = the percentage of people using at least basic water services. This indicator encompasses both people using basic water services as well as those using safely managed water services. Basic drinking water services is defined as drinking water from an improved source, provided collection time is not more than 30 minutes for a round trip. Improved water sources include piped water, boreholes or tubewells, protect dug wells, protected springs, and packaged or delivered water."
  },
  {
    "objectID": "slides/Day16_bsta511.html#get-to-know-the-data",
    "href": "slides/Day16_bsta511.html#get-to-know-the-data",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Get to know the data",
    "text": "Get to know the data\nLoad data\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\nRows: 194 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): country, water_2011_quart\ndbl (3): life_expectancy_years_2011, female_literacy_rate_2011, water_basic_...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nGlimpse of the data\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\nNote the missing values for our variables of interest\n\ngapm_original %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…   187  47.5  82.9   72.7  64.3  76.9  12.6  9.04  70.7  8.44 0.617\n2 female_lit…    80  13    99.8   91.6  71.0  98.0  27.0 11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;"
  },
  {
    "objectID": "slides/Day16_bsta511.html#remove-missing-values",
    "href": "slides/Day16_bsta511.html#remove-missing-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Remove missing values",
    "text": "Remove missing values\nRemove rows with missing data for life expectancy and female literacy rate\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\nNo missing values now for our variables of interest\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "slides/Day16_bsta511.html#regression-line-best-fit-line",
    "href": "slides/Day16_bsta511.html#regression-line-best-fit-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\n\n\\(\\hat{y}\\) is the predicted outcome for a specific value of \\(x\\).\n\\(b_0\\) is the intercept\n\\(b_1\\) is the slope of the line, i.e., the increase in \\(\\hat{y}\\) for every increase of one (unit increase) in \\(x\\).\n\nslope = rise over run\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nIntercept\n\nThe expected outcome for the \\(y\\)-variable when the \\(x\\)-variable is 0.\n\nSlope\n\nFor every increase of 1 unit in the \\(x\\)-variable, there is an expected increase of, on average, \\(b_1\\) units in the \\(y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase."
  },
  {
    "objectID": "slides/Day16_bsta511.html#regression-in-r-lm-summary-tidy",
    "href": "slides/Day16_bsta511.html#regression-in-r-lm-summary-tidy",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Regression in R: lm(), summary(), & tidy()",
    "text": "Regression in R: lm(), summary(), & tidy()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nRegression equation for our model:\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]"
  },
  {
    "objectID": "slides/Day16_bsta511.html#residuals",
    "href": "slides/Day16_bsta511.html#residuals",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residuals",
    "text": "Residuals\n\n\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/Day16_bsta511.html#the-population-regresison-model",
    "href": "slides/Day16_bsta511.html#the-population-regresison-model",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "The (population) regresison model",
    "text": "The (population) regresison model\n\n\n\nThe (population) regression model is denoted by\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable:\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\nvariance \\(\\sigma^2\\) is constant\n\n\n\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions\n\n\n\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates for \\(\\beta_0\\) and \\(\\beta_1\\) based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#what-are-the-line-conditions",
    "href": "slides/Day16_bsta511.html#what-are-the-line-conditions",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides/Day16_bsta511.html#l-linearity-of-relationship-between-variables",
    "href": "slides/Day16_bsta511.html#l-linearity-of-relationship-between-variables",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "slides/Day16_bsta511.html#i-independence-of-the-residuals-y-values",
    "href": "slides/Day16_bsta511.html#i-independence-of-the-residuals-y-values",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#n-normality-of-the-residuals-1",
    "href": "slides/Day16_bsta511.html#n-normality-of-the-residuals-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides/Day16_bsta511.html#extract-models-residuals-in-r",
    "href": "slides/Day16_bsta511.html#extract-models-residuals-in-r",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 8\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/Day16_bsta511.html#check-normality-with-usual-distribution-plots",
    "href": "slides/Day16_bsta511.html#check-normality-with-usual-distribution-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides/Day16_bsta511.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides/Day16_bsta511.html#normal-qq-plots-qq-quantile-quantile",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-15",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-15",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-25",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-25",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-35",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-35",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-45",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-45",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-55",
    "href": "slides/Day16_bsta511.html#examples-of-normal-qq-plots-55",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#qq-plot-of-residuals-of-model1",
    "href": "slides/Day16_bsta511.html#qq-plot-of-residuals-of-model1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/Day16_bsta511.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides/Day16_bsta511.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides/Day16_bsta511.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides/Day16_bsta511.html#randomly-generated-normal-qq-plots-n100",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides/Day16_bsta511.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/Day16_bsta511.html#back-to-our-example",
    "href": "slides/Day16_bsta511.html#back-to-our-example",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/Day16_bsta511.html#residual-plot",
    "href": "slides/Day16_bsta511.html#residual-plot",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n[3] \".fitted\"                    \".resid\"                    \n[5] \".hat\"                       \".sigma\"                    \n[7] \".cooksd\"                    \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides/Day16_bsta511.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides/Day16_bsta511.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "slides/Day16_bsta511.html#r2-coefficient-of-determination-12",
    "href": "slides/Day16_bsta511.html#r2-coefficient-of-determination-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides/Day16_bsta511.html#r2-coefficient-of-determination-22",
    "href": "slides/Day16_bsta511.html#r2-coefficient-of-determination-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides/Day16_bsta511.html#r2-in-summary-r-output",
    "href": "slides/Day16_bsta511.html#r2-in-summary-r-output",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "slides/Day16_bsta511.html#inference-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#inference-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() # conf.int = TRUE part is new! \n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?\nHow do we calculate the standard error, statistic, p-value, and CI?\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe can also test & calculate CI for the population intercept\nThis will be covered in BSTA 512"
  },
  {
    "objectID": "slides/Day16_bsta511.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "href": "slides/Day16_bsta511.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "slides/Day16_bsta511.html#ci-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for population slope \\(\\beta_1\\)",
    "text": "CI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\textrm{Point Estimate} \\pm t^*\\cdot SE_{\\textrm{Point Estimate}}\\]\nFor the CI of the coefficient \\(b_1\\) this translates to\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\] where \\(t^*\\) is the critical value from a \\(t\\)-distribution with \\(df = n -2\\).\n\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295"
  },
  {
    "objectID": "slides/Day16_bsta511.html#standard-error-of-fitted-slope-b_1",
    "href": "slides/Day16_bsta511.html#standard-error-of-fitted-slope-b_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Standard error of fitted slope \\(b_1\\)",
    "text": "Standard error of fitted slope \\(b_1\\)\n\n\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\n\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744"
  },
  {
    "objectID": "slides/Day16_bsta511.html#calculate-ci-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#calculate-ci-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Calculate CI for population slope \\(\\beta_1\\)",
    "text": "Calculate CI for population slope \\(\\beta_1\\)\n\n\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nSave regression output for the row with the slope’s information:\n\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\n\nCompare CI bounds below with the ones in the regression table above.\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "slides/Day16_bsta511.html#hypothesis-test-for-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#hypothesis-test-for-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Hypothesis test for population slope \\(\\beta_1\\)\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "slides/Day16_bsta511.html#p-value-for-testing-population-slope-beta_1",
    "href": "slides/Day16_bsta511.html#p-value-for-testing-population-slope-beta_1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "\\(p\\)-value for testing population slope \\(\\beta_1\\)",
    "text": "\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()  # compare p-value calculated above to p-value in table\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-with-regression-line",
    "href": "slides/Day16_bsta511.html#prediction-with-regression-line",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nWhat is the predicted life expectancy for a country with female literacy rate 60%?\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\n\nHow do we interpret the predicted value?\nHow variable is it?"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-with-regression-line-1",
    "href": "slides/Day16_bsta511.html#prediction-with-regression-line-1",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\)."
  },
  {
    "objectID": "slides/Day16_bsta511.html#ci-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "CI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-ci-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#example-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(x^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides/Day16_bsta511.html#confidence-bands-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nFor what values of x are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\") \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/Day16_bsta511.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "slides/Day16_bsta511.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nFor what values of \\(x^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-interval-for-predicting-individual-observations",
    "href": "slides/Day16_bsta511.html#prediction-interval-for-predicting-individual-observations",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|x^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!"
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-prediction-interval",
    "href": "slides/Day16_bsta511.html#example-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Prediction interval",
    "text": "Example: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|x^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 6.217898\n\n\n\n\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] 12.37888\n\n\n\n\n\n\nY60 - MOE_Ypred\n\n[1] 52.48072\n\n\n\n\n\n\nY60 + MOE_Ypred\n\n[1] 77.23849"
  },
  {
    "objectID": "slides/Day16_bsta511.html#example-using-r-for-prediction-interval",
    "href": "slides/Day16_bsta511.html#example-using-r-for-prediction-interval",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Example: Using R for prediction interval",
    "text": "Example: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 60.21570 47.63758 72.79382\n2 64.85961 52.48072 77.23849\n3 69.50351 57.19879 81.80823\n\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-12",
    "href": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-12",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…"
  },
  {
    "objectID": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-22",
    "href": "slides/Day16_bsta511.html#prediction-bands-vs.-confidence-bands-22",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Prediction bands vs. confidence bands (2/2)",
    "text": "Prediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \"life_expectancy_years_2011\" \"female_literacy_rate_2011\" \n [3] \".fitted\"                    \".lower\"                    \n [5] \".upper\"                     \".resid\"                    \n [7] \".hat\"                       \".sigma\"                    \n [9] \".cooksd\"                    \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\") \n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/Day16_bsta511.html#corrrelation-doesnt-imply-causation",
    "href": "slides/Day16_bsta511.html#corrrelation-doesnt-imply-causation",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Corrrelation doesn’t imply causation*!",
    "text": "Corrrelation doesn’t imply causation*!\n\nThis might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn’t warrant it (such as an observational study).\nBeware of spurious correlations: http://www.tylervigen.com/spurious-correlations\n\n\n\n*Caveat: there is a whole field of statistics/epidemiology on causal inference. https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"
  },
  {
    "objectID": "slides/Day16_bsta511.html#whats-next",
    "href": "slides/Day16_bsta511.html#whats-next",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "What’s next?",
    "text": "What’s next?"
  },
  {
    "objectID": "slides/03_SLR.html#where-are-we",
    "href": "slides/03_SLR.html#where-are-we",
    "title": "Day 16: Simple Linear Regression Part 2 (Sections 6.3-6.4)",
    "section": "Where are we?",
    "text": "Where are we?"
  },
  {
    "objectID": "slides/03_SLR.html#goals-for-today-sections-6.3-6.4",
    "href": "slides/03_SLR.html#goals-for-today-sections-6.3-6.4",
    "title": "Simple Linear Regression (SLR)",
    "section": "Goals for today (Sections 6.3-6.4)",
    "text": "Goals for today (Sections 6.3-6.4)\nSimple Linear Regression Part 2\n\nReview of\n\nbest-fit line (aka regression line or least-squares line)\nresiduals\npopulation model\n\nLINE conditions and how to assess them\n\nNew diagnostic tools:\n\nNormal QQ plots of residuals\nResidual plots\n\n\nCoefficient of determination (\\(R^2\\))\nRegression inference\n\nInference for population slope \\(\\beta_1\\)\n\nCI & hypothesis test\n\nCI for mean response \\(\\mu_{Y|x^*}\\)\nPrediction interval for predicting individual observations\n\n\nConfidence bands vs. predictions bands"
  },
  {
    "objectID": "slides/03_SLR.html#life-expectancy-vs.-female-adult-literacy-rate",
    "href": "slides/03_SLR.html#life-expectancy-vs.-female-adult-literacy-rate",
    "title": "Simple Linear Regression (SLR)",
    "section": "Life expectancy vs. female adult literacy rate",
    "text": "Life expectancy vs. female adult literacy rate\nhttps://www.gapminder.org/tools/#$model$markers$bubble$encoding$x$data$concept=literacy_rate_adult_female_percent_of_females_ages_15_above&source=sg&space@=country&=time;;&scale$domain:null&zoomed:null&type:null;;&frame$value=2011;;;;;&chart-type=bubbles&url=v1"
  },
  {
    "objectID": "slides/03_SLR.html#dataset-description",
    "href": "slides/03_SLR.html#dataset-description",
    "title": "Simple Linear Regression (SLR)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData files\n\nCleaned: lifeexp_femlit_2011.csv\nNeeds cleaning: lifeexp_femlit_water_2011.csv\n\nData were downloaded from Gapminder\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life."
  },
  {
    "objectID": "slides/03_SLR.html#get-to-know-the-data",
    "href": "slides/03_SLR.html#get-to-know-the-data",
    "title": "Simple Linear Regression (SLR)",
    "section": "Get to know the data",
    "text": "Get to know the data\n\nLoad data\n\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\n\nGlimpse of the data\n\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\n\nNote the missing values for our variables of interest\n\n\ngapm_original %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…   187  47.5  82.9   72.7  64.3  76.9  12.6  9.04  70.7  8.44 0.617\n2 female_lit…    80  13    99.8   91.6  71.0  98.0  27.0 11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;"
  },
  {
    "objectID": "slides/03_SLR.html#remove-missing-values",
    "href": "slides/03_SLR.html#remove-missing-values",
    "title": "Simple Linear Regression (SLR)",
    "section": "Remove missing values",
    "text": "Remove missing values\n\nRemove rows with missing data for life expectancy and female literacy rate\n\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\n\nNo missing values now for our variables of interest\n\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "slides/03_SLR.html#regression-line-best-fit-line",
    "href": "slides/03_SLR.html#regression-line-best-fit-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\]\n\n\\(\\widehat{Y}\\) is the predicted outcome for a specific value of \\(X\\)\n\\(\\widehat{\\beta}_0\\) is the intercept of the best-fit line\n\\(\\widehat{\\beta}_1\\) is the slope of the best-fit line, i.e., the increase in \\(\\widehat{Y}\\) for every increase of one (unit increase) in \\(X\\)\n\nslope = rise over run"
  },
  {
    "objectID": "slides/03_SLR.html#regression-in-r-lm-summary-tidy",
    "href": "slides/03_SLR.html#regression-in-r-lm-summary-tidy",
    "title": "Simple Linear Regression (SLR)",
    "section": "Regression in R: lm(), summary(), & tidy()",
    "text": "Regression in R: lm(), summary(), & tidy()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nRegression equation for our model:\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]"
  },
  {
    "objectID": "slides/03_SLR.html#residuals",
    "href": "slides/03_SLR.html#residuals",
    "title": "Simple Linear Regression (SLR)",
    "section": "Residuals",
    "text": "Residuals\n\n\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values\n\n\n\n\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\n[1] 2942.635"
  },
  {
    "objectID": "slides/03_SLR.html#the-population-regresison-model",
    "href": "slides/03_SLR.html#the-population-regresison-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "The (population) regresison model",
    "text": "The (population) regresison model\n\n\n\nThe (population) regression model is denoted by\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable:\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\nvariance \\(\\sigma^2\\) is constant\n\n\n\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions\n\n\n\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates for \\(\\beta_0\\) and \\(\\beta_1\\) based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides/03_SLR.html#what-are-the-line-conditions",
    "href": "slides/03_SLR.html#what-are-the-line-conditions",
    "title": "Simple Linear Regression (SLR)",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides/03_SLR.html#l-linearity-of-relationship-between-variables",
    "href": "slides/03_SLR.html#l-linearity-of-relationship-between-variables",
    "title": "Simple Linear Regression (SLR)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)"
  },
  {
    "objectID": "slides/03_SLR.html#i-independence-of-the-residuals-y-values",
    "href": "slides/03_SLR.html#i-independence-of-the-residuals-y-values",
    "title": "Simple Linear Regression (SLR)",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/03_SLR.html#n-normality-of-the-residuals-1",
    "href": "slides/03_SLR.html#n-normality-of-the-residuals-1",
    "title": "Simple Linear Regression (SLR)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides/03_SLR.html#extract-models-residuals-in-r",
    "href": "slides/03_SLR.html#extract-models-residuals-in-r",
    "title": "Simple Linear Regression (SLR)",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/03_SLR.html#check-normality-with-usual-distribution-plots",
    "href": "slides/03_SLR.html#check-normality-with-usual-distribution-plots",
    "title": "Simple Linear Regression (SLR)",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/03_SLR.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides/03_SLR.html#normal-qq-plots-qq-quantile-quantile",
    "title": "Simple Linear Regression (SLR)",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "slides/03_SLR.html#examples-of-normal-qq-plots-15",
    "href": "slides/03_SLR.html#examples-of-normal-qq-plots-15",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides/03_SLR.html#examples-of-normal-qq-plots-25",
    "href": "slides/03_SLR.html#examples-of-normal-qq-plots-25",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides/03_SLR.html#examples-of-normal-qq-plots-35",
    "href": "slides/03_SLR.html#examples-of-normal-qq-plots-35",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides/03_SLR.html#examples-of-normal-qq-plots-45",
    "href": "slides/03_SLR.html#examples-of-normal-qq-plots-45",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides/03_SLR.html#examples-of-normal-qq-plots-55",
    "href": "slides/03_SLR.html#examples-of-normal-qq-plots-55",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides/03_SLR.html#qq-plot-of-residuals-of-model1",
    "href": "slides/03_SLR.html#qq-plot-of-residuals-of-model1",
    "title": "Simple Linear Regression (SLR)",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/03_SLR.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides/03_SLR.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "Simple Linear Regression (SLR)",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides/03_SLR.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides/03_SLR.html#randomly-generated-normal-qq-plots-n100",
    "title": "Simple Linear Regression (SLR)",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/03_SLR.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides/03_SLR.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/03_SLR.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides/03_SLR.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/03_SLR.html#back-to-our-example",
    "href": "slides/03_SLR.html#back-to-our-example",
    "title": "Simple Linear Regression (SLR)",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/03_SLR.html#residual-plot",
    "href": "slides/03_SLR.html#residual-plot",
    "title": "Simple Linear Regression (SLR)",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \".rownames\"                  \"life_expectancy_years_2011\"\n[3] \"female_literacy_rate_2011\"  \".fitted\"                   \n[5] \".resid\"                     \".hat\"                      \n[7] \".sigma\"                     \".cooksd\"                   \n[9] \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides/03_SLR.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides/03_SLR.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "Simple Linear Regression (SLR)",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "slides/03_SLR.html#r2-coefficient-of-determination-12",
    "href": "slides/03_SLR.html#r2-coefficient-of-determination-12",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides/03_SLR.html#r2-coefficient-of-determination-22",
    "href": "slides/03_SLR.html#r2-coefficient-of-determination-22",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides/03_SLR.html#r2-in-summary-r-output",
    "href": "slides/03_SLR.html#r2-in-summary-r-output",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "slides/03_SLR.html#inference-for-population-slope-beta_1",
    "href": "slides/03_SLR.html#inference-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() # conf.int = TRUE part is new! \n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?\nHow do we calculate the standard error, statistic, p-value, and CI?\n\n\n\n\n\n\n\nNote\n\n\n\nWe can also test & calculate CI for the population intercept\nThis will be covered in BSTA 512"
  },
  {
    "objectID": "slides/03_SLR.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "href": "slides/03_SLR.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "title": "Simple Linear Regression (SLR)",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "slides/03_SLR.html#ci-for-population-slope-beta_1",
    "href": "slides/03_SLR.html#ci-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "CI for population slope \\(\\beta_1\\)",
    "text": "CI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\textrm{Point Estimate} \\pm t^*\\cdot SE_{\\textrm{Point Estimate}}\\]\nFor the CI of the coefficient \\(b_1\\) this translates to\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\] where \\(t^*\\) is the critical value from a \\(t\\)-distribution with \\(df = n -2\\).\n\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295"
  },
  {
    "objectID": "slides/03_SLR.html#standard-error-of-fitted-slope-b_1",
    "href": "slides/03_SLR.html#standard-error-of-fitted-slope-b_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Standard error of fitted slope \\(b_1\\)",
    "text": "Standard error of fitted slope \\(b_1\\)\n\n\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\n\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] NA"
  },
  {
    "objectID": "slides/03_SLR.html#calculate-ci-for-population-slope-beta_1",
    "href": "slides/03_SLR.html#calculate-ci-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Calculate CI for population slope \\(\\beta_1\\)",
    "text": "Calculate CI for population slope \\(\\beta_1\\)\n\n\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nSave regression output for the row with the slope’s information:\n\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\n\nCompare CI bounds below with the ones in the regression table above.\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "slides/03_SLR.html#hypothesis-test-for-population-slope-beta_1",
    "href": "slides/03_SLR.html#hypothesis-test-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Hypothesis test for population slope \\(\\beta_1\\)\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "slides/03_SLR.html#p-value-for-testing-population-slope-beta_1",
    "href": "slides/03_SLR.html#p-value-for-testing-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(p\\)-value for testing population slope \\(\\beta_1\\)",
    "text": "\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()  # compare p-value calculated above to p-value in table\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "slides/03_SLR.html#prediction-with-regression-line",
    "href": "slides/03_SLR.html#prediction-with-regression-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nWhat is the predicted life expectancy for a country with female literacy rate 60%?\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\n\nHow do we interpret the predicted value?\nHow variable is it?"
  },
  {
    "objectID": "slides/03_SLR.html#prediction-with-regression-line-1",
    "href": "slides/03_SLR.html#prediction-with-regression-line-1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\)."
  },
  {
    "objectID": "slides/03_SLR.html#ci-for-mean-response-mu_yx",
    "href": "slides/03_SLR.html#ci-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "CI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "slides/03_SLR.html#example-ci-for-mean-response-mu_yx",
    "href": "slides/03_SLR.html#example-ci-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] NA\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] NA\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] NA\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] NA"
  },
  {
    "objectID": "slides/03_SLR.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "href": "slides/03_SLR.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(x^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides/03_SLR.html#confidence-bands-for-mean-response-mu_yx",
    "href": "slides/03_SLR.html#confidence-bands-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nFor what values of x are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\")"
  },
  {
    "objectID": "slides/03_SLR.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "slides/03_SLR.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nFor what values of \\(x^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#prediction-interval-for-predicting-individual-observations",
    "href": "slides/03_SLR.html#prediction-interval-for-predicting-individual-observations",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|x^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!"
  },
  {
    "objectID": "slides/03_SLR.html#example-prediction-interval",
    "href": "slides/03_SLR.html#example-prediction-interval",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: Prediction interval",
    "text": "Example: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|x^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n\n\n\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] NA\n\n\n\n\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] NA\n\n\n\n\n\n\nY60 - MOE_Ypred\n\n[1] NA\n\n\n\n\n\n\nY60 + MOE_Ypred\n\n[1] NA"
  },
  {
    "objectID": "slides/03_SLR.html#example-using-r-for-prediction-interval",
    "href": "slides/03_SLR.html#example-using-r-for-prediction-interval",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: Using R for prediction interval",
    "text": "Example: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 60.21570 47.63758 72.79382\n2 64.85961 52.48072 77.23849\n3 69.50351 57.19879 81.80823\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "slides/03_SLR.html#prediction-bands-vs.-confidence-bands-12",
    "href": "slides/03_SLR.html#prediction-bands-vs.-confidence-bands-12",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \".rownames\"                  \"life_expectancy_years_2011\"\n [3] \"female_literacy_rate_2011\"  \".fitted\"                   \n [5] \".lower\"                     \".upper\"                    \n [7] \".resid\"                     \".hat\"                      \n [9] \".sigma\"                     \".cooksd\"                   \n[11] \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…"
  },
  {
    "objectID": "slides/03_SLR.html#prediction-bands-vs.-confidence-bands-22",
    "href": "slides/03_SLR.html#prediction-bands-vs.-confidence-bands-22",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction bands vs. confidence bands (2/2)",
    "text": "Prediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \".rownames\"                  \"life_expectancy_years_2011\"\n [3] \"female_literacy_rate_2011\"  \".fitted\"                   \n [5] \".lower\"                     \".upper\"                    \n [7] \".resid\"                     \".hat\"                      \n [9] \".sigma\"                     \".cooksd\"                   \n[11] \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\")"
  },
  {
    "objectID": "slides/03_SLR.html#corrrelation-doesnt-imply-causation",
    "href": "slides/03_SLR.html#corrrelation-doesnt-imply-causation",
    "title": "Simple Linear Regression (SLR)",
    "section": "Corrrelation doesn’t imply causation*!",
    "text": "Corrrelation doesn’t imply causation*!\n\nThis might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn’t warrant it (such as an observational study).\nBeware of spurious correlations: http://www.tylervigen.com/spurious-correlations\n\n\n\n*Caveat: there is a whole field of statistics/epidemiology on causal inference. https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"
  },
  {
    "objectID": "slides/03_SLR.html#whats-next",
    "href": "slides/03_SLR.html#whats-next",
    "title": "Simple Linear Regression (SLR)",
    "section": "What’s next?",
    "text": "What’s next?\n \n\n\n\nSLR 1"
  },
  {
    "objectID": "slides/03_SLR.html#slide-with-my-regression-analysis-process",
    "href": "slides/03_SLR.html#slide-with-my-regression-analysis-process",
    "title": "Simple Linear Regression (SLR)",
    "section": "Slide with my regression analysis process!!!",
    "text": "Slide with my regression analysis process!!!\n   \n \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nPrediction vs interpretation\nComparing models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nParameter estimation\nInterpret model parameters\nHypothesis tests for coefficients\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of full and reduced models\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity"
  },
  {
    "objectID": "slides/03_SLR.html#reference-how-did-i-code-that",
    "href": "slides/03_SLR.html#reference-how-did-i-code-that",
    "title": "Simple Linear Regression (SLR)",
    "section": "Reference: How did I code that?",
    "text": "Reference: How did I code that?\n\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))"
  },
  {
    "objectID": "slides/03_SLR.html#three-types-of-study-design-there-are-more",
    "href": "slides/03_SLR.html#three-types-of-study-design-there-are-more",
    "title": "Simple Linear Regression (SLR)",
    "section": "Three types of study design (there are more)",
    "text": "Three types of study design (there are more)\n\n\n\n\nExperiment\n\n\n\nObservational units are randomly assigned to important predictor levels\n\nRandom assignment controls for confounding variables (age, gender, race, etc.)\n“gold standard” for determining causality\nObservational unit is often at the participant-level\n\n\n\n\n\n\n\nQuasi-experiment\n\n\n\nParticipants are assigned to intervention levels without randomization\nNot common study design\n\n\n\n\n\n\nObservational\n\n\n\nNo randomization or assignment of intervention conditions\nIn general cannot infer causality\n\nHowever, there are casual inference methods…"
  },
  {
    "objectID": "slides/03_SLR.html#fnjfns",
    "href": "slides/03_SLR.html#fnjfns",
    "title": "Simple Linear Regression (SLR)",
    "section": "fnjfns",
    "text": "fnjfns\n\nExperiment\n\nObservational units are randomly assigned to important predictor levels\n\nRandom assignment controls for confounding variables (age, gender, race, etc.)\n“gold standard” for determining causality\nObservational unit is often at the participant-level\n\n\nQuasi-experiment\n\nParticipants are assigned to intervention levels without randomization\nNot common study design\n\nObservational\n\nNo randomization or assignment of intervention conditions\nIn general cannot infer causality\n\nHowever, there are casual inference methods…"
  },
  {
    "objectID": "homework/HW1.html#question-7",
    "href": "homework/HW1.html#question-7",
    "title": "Homework 1",
    "section": "Question 7",
    "text": "Question 7\nGo to the R Documentation of the lm() function. Please answer the following questions about the function arguments and output.\n\nPart a\nIf you wanted to perform weighted least squares in R, what argument would you need to change?\n\n\nPart b\nWhat is the default method in lm()? Can it be used to solve OLS? (Hint: check out the Wiki page for the method)\n\n\nPart c\nIn linear regression, a singular fit means that the estimated standard deviation of your residuals is very close to zero. In the lm() function, will you get an error if you have a singular fit?"
  },
  {
    "objectID": "slides/03_SLR.html#learning-objectives",
    "href": "slides/03_SLR.html#learning-objectives",
    "title": "Simple Linear Regression (SLR)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIdentify the aims of your research and see how they align with the intended purpose of simple linear regression\nIdentify the simple linear regression model and define statistics language for key notation\nIllustrate how ordinary least squares (OLS) finds the best model parameter estimates\nSolve the optimal coefficient estimates for simple linear regression using OLS\nApply OLS in R for simple linear regression of real data"
  },
  {
    "objectID": "slides/03_SLR.html#make-it-clear-on-same-slide-that",
    "href": "slides/03_SLR.html#make-it-clear-on-same-slide-that",
    "title": "Simple Linear Regression (SLR)",
    "section": "Make it clear on same slide that",
    "text": "Make it clear on same slide that"
  },
  {
    "objectID": "slides/03_SLR.html#how-do-we-get-the-best-fit-line",
    "href": "slides/03_SLR.html#how-do-we-get-the-best-fit-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "How do we get the best fit line?",
    "text": "How do we get the best fit line?\nexample of us trying to eyeball it?"
  },
  {
    "objectID": "slides/03_SLR.html#translate-eyeballing-to-mathematical-form",
    "href": "slides/03_SLR.html#translate-eyeballing-to-mathematical-form",
    "title": "Simple Linear Regression (SLR)",
    "section": "Translate eyeballing to mathematical form??",
    "text": "Translate eyeballing to mathematical form??\n\nWe want to minimize the difference between the observed \\(Y\\) value and the estimated expected response given the predictor ( \\(\\widehat{E}[Y|X]\\) )\nWe use ordinary least squares (OLS) to do this\nIdea behind this: reduce the total error between the fitted line and the obeerved point (error between is called residuals)\n\nVague use of total error: more precisely, we want to reduce the sum of squared errors\nWe need to mathematically define this!"
  },
  {
    "objectID": "slides/03_SLR.html#the-population-regression-model",
    "href": "slides/03_SLR.html#the-population-regression-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "The (population) regression model",
    "text": "The (population) regression model\n\n\n\nThe (population) regression model is denoted by\n\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\epsilon\\]\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable:\n\n\\(\\epsilon \\sim N(0, \\sigma^2)\\)\nvariance \\(\\sigma^2\\) is constant\n\n\n\n\n\n\n\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions\n\n\n\n\nThe line is the average (expected) value of \\(Y\\) given a value of \\(x\\): \\(E(Y|x)\\).\nThe point estimates for \\(\\beta_0\\) and \\(\\beta_1\\) based on a sample are denoted by \\(b_0, b_1, s_{residuals}^2\\)\n\nNote: also common notation is \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\sigma}^2\\)"
  },
  {
    "objectID": "slides/03_SLR.html#lets-revisit-the-regression-analysis-process",
    "href": "slides/03_SLR.html#lets-revisit-the-regression-analysis-process",
    "title": "Simple Linear Regression (SLR)",
    "section": "Let’s revisit the regression analysis process",
    "text": "Let’s revisit the regression analysis process\n   \n \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nPrediction vs interpretation\nComparing models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nParameter estimation\nInterpret model parameters\nHypothesis tests for coefficients\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of full and reduced models\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity"
  },
  {
    "objectID": "slides/03_SLR.html#how-do-we-interpret-the-coefficients",
    "href": "slides/03_SLR.html#how-do-we-interpret-the-coefficients",
    "title": "Simple Linear Regression (SLR)",
    "section": "How do we interpret the coefficients?",
    "text": "How do we interpret the coefficients?\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]\n\n\nIntercept\n\nThe expected outcome for the \\(Y\\)-variable when the \\(X\\)-variable is 0\nExample: The expected/average life expectancy is 50.9 years for a country with 0% female literacy.\n\nSlope\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected increase of, on average, \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase.\nExample: For every 1 percent increase in the female literacy rate, the expected/average life expectancy increases, on average, 0.232 years."
  },
  {
    "objectID": "slides/03_SLR.html#more",
    "href": "slides/03_SLR.html#more",
    "title": "Simple Linear Regression (SLR)",
    "section": "MOre??",
    "text": "MOre??\n\nOur goal in regression is to estimate \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\)\n\nThe point estimates based on a sample are denoted by \\(\\widehat{\\beta}_0\\), \\(\\widehat{\\beta}_1\\), and \\(\\widehat{\\sigma^2}\\)"
  },
  {
    "objectID": "slides/03_SLR.html#simple-linear-regression-model-another-way-to-identify-the-components",
    "href": "slides/03_SLR.html#simple-linear-regression-model-another-way-to-identify-the-components",
    "title": "Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model (another way to identify the components)",
    "text": "Simple Linear Regression Model (another way to identify the components)\nThe (population) regression line is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term\n\n\n\n\n\\(Y\\): response, outcome, dependent variable\n\\(\\beta_0\\):\n\\(\\beta_1\\):\n\\(X\\):\n\\(\\epsilon\\):"
  },
  {
    "objectID": "slides/03_SLR.html#simple-linear-regression-model-another-way-to-view-components",
    "href": "slides/03_SLR.html#simple-linear-regression-model-another-way-to-view-components",
    "title": "Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model (another way to view components)",
    "text": "Simple Linear Regression Model (another way to view components)\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term"
  },
  {
    "objectID": "slides/03_SLR.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "href": "slides/03_SLR.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "title": "Simple Linear Regression (SLR)",
    "section": "If the population parameters are unobservable, how did we get the line for life expectancy?",
    "text": "If the population parameters are unobservable, how did we get the line for life expectancy?\n\n\n \n\nNote: the population model is the true, underlying model that we are trying to estimate using our sample data\n\nOur goal in simple linear regression is to estimate \\(\\beta_0\\) and \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/03_SLR.html#okay-so-how-do-we-estimate-the-regression-line",
    "href": "slides/03_SLR.html#okay-so-how-do-we-estimate-the-regression-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "Okay, so how do we estimate the regression line?",
    "text": "Okay, so how do we estimate the regression line?\n \nAt this point, we are going to move over to an R shiny app that I made.\n \nLet’s see if we can eyeball the best-fit line!"
  },
  {
    "objectID": "slides/03_SLR_01/03_SLR_shiny_app.html",
    "href": "slides/03_SLR_01/03_SLR_shiny_app.html",
    "title": "Untitled",
    "section": "",
    "text": "Loading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks plotly::filter(), stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nLife Expectancy Simple Linear Regression\nLet’s try to find a model fit with the lowest score!\nClick two distinct points of the plot to draw a line. Try to minimize the score (at the top of the plot) using different lines. To create a new line just click on the plot more than twice. In this activity, the number we get is our “score.” In statistics, this score is actually called the sum of squared errors (SSE). Were you able to get the best-fit line presented in class?"
  },
  {
    "objectID": "slides/03_SLR.html#simple-linear-regression-model-1",
    "href": "slides/03_SLR.html#simple-linear-regression-model-1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\n\nPopulation regression model\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term\n\n\n\n\nEstimated regression line\n\n\\[\\widehat{Y} =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\\]\n\n \nComponents\n\n\n\n\n\n\n\n\\(\\widehat{Y}\\)\nestimated expected response given predictor \\(X\\)\n\n\n\\(\\widehat{\\beta}_0\\)\nestimated intercept\n\n\n\\(\\widehat{\\beta}_1\\)\nestimated slope\n\n\n\\(X\\)\npredictor, covariate, independent variable"
  },
  {
    "objectID": "slides/03_SLR.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "href": "slides/03_SLR.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "We get it, Nicky! How do we estimate the regression line?",
    "text": "We get it, Nicky! How do we estimate the regression line?\nFirst let’s take a break!!"
  },
  {
    "objectID": "slides/03_SLR.html#it-all-starts-with-a-residual",
    "href": "slides/03_SLR.html#it-all-starts-with-a-residual",
    "title": "Simple Linear Regression (SLR)",
    "section": "It all starts with a residual…",
    "text": "It all starts with a residual…\n\n\n\nRecall, one characteristic of our population model was that the residuals, \\(\\epsilon\\), were Normally distributed: \\(\\epsilon \\sim N(0, \\sigma^2)\\)\nIn our population regression model, we had: \\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\nWe can also take the average (expected) value of the population model\nWe take the expected value of both sides and get:\n\n\\[\\begin{aligned}\n        E[Y] & = E[\\beta_0 + \\beta_1X + \\epsilon] \\\\\n        E[Y] & = E[\\beta_0] + E[\\beta_1X] + E[\\epsilon] \\\\\n        E[Y] & = \\beta_0 + \\beta_1X + E[\\epsilon] \\\\\n        E[Y|X] & = \\beta_0 + \\beta_1X \\\\\n\\end{aligned}\\]\n\nWe call \\(E[Y|X]\\) the expected value of \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/03_SLR.html#so-now-we-have-two-representations-of-our-population-model",
    "href": "slides/03_SLR.html#so-now-we-have-two-representations-of-our-population-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "So now we have two representations of our population model",
    "text": "So now we have two representations of our population model\n\n\n\n\nWith observed \\(Y\\) values and residuals:\n\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\n\n\nWith the population expected value of \\(Y\\) given \\(X\\):\n\n\n\\[E[Y|X] = \\beta_0 + \\beta_1X\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our residuals:\n\\[\\begin{aligned}\nY & = (\\beta_0 + \\beta_1X) + \\epsilon \\\\\nY & = E[Y|X] + \\epsilon \\\\\nY - E[Y|X] & = \\epsilon \\\\\n\\epsilon & = Y - E[Y|X]\n\\end{aligned}\\]\nAnd so we have our true, population model, residuals!\n\nThis is an important fact! For the population model, the residuals: \\(\\epsilon = Y - E[Y|X]\\)"
  },
  {
    "objectID": "slides/03_SLR.html#back-to-our-estimated-model",
    "href": "slides/03_SLR.html#back-to-our-estimated-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Back to our estimated model",
    "text": "Back to our estimated model\nWe have the same two representations of our estimated/fitted model:\n\n\n\n\nWith observed values:\n\n\n\\[Y =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\epsilon}\\]\n\n\n\n\n\nWith the estimated expected value of \\(Y\\) given \\(X\\):\n\n\n\\[\\begin{aligned}\n\\widehat{E}[Y|X] & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{E[Y|X]} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{Y} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\end{aligned}\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our estimated residuals:\n\\[\\begin{aligned}\nY & = (\\widehat{\\beta}_0 + \\widehat{\\beta}_1X) + \\widehat\\epsilon \\\\\nY & = \\widehat{Y} + \\widehat\\epsilon \\\\\n\\widehat\\epsilon & = Y - \\widehat{Y}\n\\end{aligned}\\]\n\nThis is an important fact! For the estimated/fitted model, the residuals: \\(\\widehat\\epsilon = Y - \\widehat{Y}\\)"
  },
  {
    "objectID": "slides/03_SLR.html#coefficient-estimates",
    "href": "slides/03_SLR.html#coefficient-estimates",
    "title": "Simple Linear Regression (SLR)",
    "section": "Coefficient estimates",
    "text": "Coefficient estimates"
  },
  {
    "objectID": "slides/03_SLR.html#section",
    "href": "slides/03_SLR.html#section",
    "title": "Simple Linear Regression (SLR)",
    "section": "",
    "text": "Get to: \\(\\widehat{\\epsilon} = Y - \\widehat{E}[Y|X] = Y - \\widehat{Y}\\)"
  },
  {
    "objectID": "slides/03_SLR.html#so-what-do-we-do-with-the-residuals",
    "href": "slides/03_SLR.html#so-what-do-we-do-with-the-residuals",
    "title": "Simple Linear Regression (SLR)",
    "section": "So what do we do with the residuals?",
    "text": "So what do we do with the residuals?\n\nWe want to minimize the residuals\n\nAka minimize the difference between the observed \\(Y\\) value and the estimated expected response given the predictor ( \\(\\widehat{E}[Y|X]\\) )\n\nWe can use ordinary least squares (OLS) to do this in linear regression!\nIdea behind this: reduce the total error between the fitted line and the observed point (error between is called residuals)\n\nVague use of total error: more precisely, we want to reduce the sum of squared errors\nThink back to my R Shiny app!\nWe need to mathematically define this!\n\n\n \n \n\nNote: there are other ways to estimate the best-fit line!!\n\nExample: Maximum likelihood estimation"
  },
  {
    "objectID": "slides/04_SLR_02.html#topics",
    "href": "slides/04_SLR_02.html#topics",
    "title": "Simple Linear Regression (SLR)",
    "section": "Topics",
    "text": "Topics\nTopics for SLR:\nassumptions\nparameter estimation and interpretation\nproperties of LSE\nestimation of variance?\nTopics\n\nInference for slope and intercept\n\nCI’s and hypothesis tests\n\nInference for mean value of y at specific values of x\n\nConfidence bands of best-fit line\n\nPrediction intervals for individual predictions\n\nPrediction bands\n\nF-test for the slope\nPearson correlation coefficient r\nCoefficient of determination R2\nTesting the correlation coefficient !\nCI for the correlation coefficient !"
  },
  {
    "objectID": "slides/04_SLR_02.html#what-are-the-line-conditions",
    "href": "slides/04_SLR_02.html#what-are-the-line-conditions",
    "title": "Simple Linear Regression (SLR)",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides/04_SLR_02.html#l-linearity-of-relationship-between-variables",
    "href": "slides/04_SLR_02.html#l-linearity-of-relationship-between-variables",
    "title": "Simple Linear Regression (SLR)",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)"
  },
  {
    "objectID": "slides/04_SLR_02.html#i-independence-of-the-residuals-y-values",
    "href": "slides/04_SLR_02.html#i-independence-of-the-residuals-y-values",
    "title": "Simple Linear Regression (SLR)",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/04_SLR_02.html#n-normality-of-the-residuals-1",
    "href": "slides/04_SLR_02.html#n-normality-of-the-residuals-1",
    "title": "Simple Linear Regression (SLR)",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides/04_SLR_02.html#extract-models-residuals-in-r",
    "href": "slides/04_SLR_02.html#extract-models-residuals-in-r",
    "title": "Simple Linear Regression (SLR)",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/04_SLR_02.html#check-normality-with-usual-distribution-plots",
    "href": "slides/04_SLR_02.html#check-normality-with-usual-distribution-plots",
    "title": "Simple Linear Regression (SLR)",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/04_SLR_02.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides/04_SLR_02.html#normal-qq-plots-qq-quantile-quantile",
    "title": "Simple Linear Regression (SLR)",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "slides/04_SLR_02.html#examples-of-normal-qq-plots-15",
    "href": "slides/04_SLR_02.html#examples-of-normal-qq-plots-15",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides/04_SLR_02.html#examples-of-normal-qq-plots-25",
    "href": "slides/04_SLR_02.html#examples-of-normal-qq-plots-25",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides/04_SLR_02.html#examples-of-normal-qq-plots-35",
    "href": "slides/04_SLR_02.html#examples-of-normal-qq-plots-35",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides/04_SLR_02.html#examples-of-normal-qq-plots-45",
    "href": "slides/04_SLR_02.html#examples-of-normal-qq-plots-45",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides/04_SLR_02.html#examples-of-normal-qq-plots-55",
    "href": "slides/04_SLR_02.html#examples-of-normal-qq-plots-55",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides/04_SLR_02.html#qq-plot-of-residuals-of-model1",
    "href": "slides/04_SLR_02.html#qq-plot-of-residuals-of-model1",
    "title": "Simple Linear Regression (SLR)",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/04_SLR_02.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides/04_SLR_02.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "Simple Linear Regression (SLR)",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides/04_SLR_02.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides/04_SLR_02.html#randomly-generated-normal-qq-plots-n100",
    "title": "Simple Linear Regression (SLR)",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/04_SLR_02.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides/04_SLR_02.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/04_SLR_02.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides/04_SLR_02.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "Simple Linear Regression (SLR)",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/04_SLR_02.html#back-to-our-example",
    "href": "slides/04_SLR_02.html#back-to-our-example",
    "title": "Simple Linear Regression (SLR)",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/04_SLR_02.html#residual-plot",
    "href": "slides/04_SLR_02.html#residual-plot",
    "title": "Simple Linear Regression (SLR)",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \".rownames\"                  \"life_expectancy_years_2011\"\n[3] \"female_literacy_rate_2011\"  \".fitted\"                   \n[5] \".resid\"                     \".hat\"                      \n[7] \".sigma\"                     \".cooksd\"                   \n[9] \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides/04_SLR_02.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides/04_SLR_02.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "Simple Linear Regression (SLR)",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "slides/04_SLR_02.html#r2-coefficient-of-determination-12",
    "href": "slides/04_SLR_02.html#r2-coefficient-of-determination-12",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides/04_SLR_02.html#r2-coefficient-of-determination-22",
    "href": "slides/04_SLR_02.html#r2-coefficient-of-determination-22",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides/04_SLR_02.html#r2-in-summary-r-output",
    "href": "slides/04_SLR_02.html#r2-in-summary-r-output",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "slides/04_SLR_02.html#inference-for-population-slope-beta_1",
    "href": "slides/04_SLR_02.html#inference-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() # conf.int = TRUE part is new! \n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?\nHow do we calculate the standard error, statistic, p-value, and CI?\n\n\n\n\n\n\n\nNote\n\n\n\nWe can also test & calculate CI for the population intercept\nThis will be covered in BSTA 512"
  },
  {
    "objectID": "slides/04_SLR_02.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "href": "slides/04_SLR_02.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "title": "Simple Linear Regression (SLR)",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "slides/04_SLR_02.html#ci-for-population-slope-beta_1",
    "href": "slides/04_SLR_02.html#ci-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "CI for population slope \\(\\beta_1\\)",
    "text": "CI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\textrm{Point Estimate} \\pm t^*\\cdot SE_{\\textrm{Point Estimate}}\\]\nFor the CI of the coefficient \\(b_1\\) this translates to\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\] where \\(t^*\\) is the critical value from a \\(t\\)-distribution with \\(df = n -2\\).\n\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295"
  },
  {
    "objectID": "slides/04_SLR_02.html#standard-error-of-fitted-slope-b_1",
    "href": "slides/04_SLR_02.html#standard-error-of-fitted-slope-b_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Standard error of fitted slope \\(b_1\\)",
    "text": "Standard error of fitted slope \\(b_1\\)\n\n\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\n\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] NA"
  },
  {
    "objectID": "slides/04_SLR_02.html#calculate-ci-for-population-slope-beta_1",
    "href": "slides/04_SLR_02.html#calculate-ci-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Calculate CI for population slope \\(\\beta_1\\)",
    "text": "Calculate CI for population slope \\(\\beta_1\\)\n\n\n\\[b_1 \\pm t^*\\cdot SE_{b_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nSave regression output for the row with the slope’s information:\n\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\n\nCompare CI bounds below with the ones in the regression table above.\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "slides/04_SLR_02.html#hypothesis-test-for-population-slope-beta_1",
    "href": "slides/04_SLR_02.html#hypothesis-test-for-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Hypothesis test for population slope \\(\\beta_1\\)\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "slides/04_SLR_02.html#p-value-for-testing-population-slope-beta_1",
    "href": "slides/04_SLR_02.html#p-value-for-testing-population-slope-beta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "\\(p\\)-value for testing population slope \\(\\beta_1\\)",
    "text": "\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()  # compare p-value calculated above to p-value in table\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "slides/04_SLR_02.html#prediction-with-regression-line",
    "href": "slides/04_SLR_02.html#prediction-with-regression-line",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\nWhat is the predicted life expectancy for a country with female literacy rate 60%?\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\n\nHow do we interpret the predicted value?\nHow variable is it?"
  },
  {
    "objectID": "slides/04_SLR_02.html#prediction-with-regression-line-1",
    "href": "slides/04_SLR_02.html#prediction-with-regression-line-1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\)."
  },
  {
    "objectID": "slides/04_SLR_02.html#ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_02.html#ci-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "CI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "slides/04_SLR_02.html#example-ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_02.html#example-ci-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] NA\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] NA\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] NA\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] NA"
  },
  {
    "objectID": "slides/04_SLR_02.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_02.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Example: Using R for CI for mean response \\(\\mu_{Y|x^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(x^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides/04_SLR_02.html#confidence-bands-for-mean-response-mu_yx",
    "href": "slides/04_SLR_02.html#confidence-bands-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nFor what values of x are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\")"
  },
  {
    "objectID": "slides/04_SLR_02.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "slides/04_SLR_02.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "Simple Linear Regression (SLR)",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|x^*}\\)\n\nFor what values of \\(x^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\\\\n\\widehat{E[Y|x^*]} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/04_SLR_02.html#prediction-interval-for-predicting-individual-observations",
    "href": "slides/04_SLR_02.html#prediction-interval-for-predicting-individual-observations",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|x^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!"
  },
  {
    "objectID": "slides/04_SLR_02.html#example-prediction-interval",
    "href": "slides/04_SLR_02.html#example-prediction-interval",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: Prediction interval",
    "text": "Example: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|x^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n\n\n\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] NA\n\n\n\n\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] NA\n\n\n\n\n\n\nY60 - MOE_Ypred\n\n[1] NA\n\n\n\n\n\n\nY60 + MOE_Ypred\n\n[1] NA"
  },
  {
    "objectID": "slides/04_SLR_02.html#example-using-r-for-prediction-interval",
    "href": "slides/04_SLR_02.html#example-using-r-for-prediction-interval",
    "title": "Simple Linear Regression (SLR)",
    "section": "Example: Using R for prediction interval",
    "text": "Example: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 60.21570 47.63758 72.79382\n2 64.85961 52.48072 77.23849\n3 69.50351 57.19879 81.80823\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "slides/04_SLR_02.html#prediction-bands-vs.-confidence-bands-12",
    "href": "slides/04_SLR_02.html#prediction-bands-vs.-confidence-bands-12",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \".rownames\"                  \"life_expectancy_years_2011\"\n [3] \"female_literacy_rate_2011\"  \".fitted\"                   \n [5] \".lower\"                     \".upper\"                    \n [7] \".resid\"                     \".hat\"                      \n [9] \".sigma\"                     \".cooksd\"                   \n[11] \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…"
  },
  {
    "objectID": "slides/04_SLR_02.html#prediction-bands-vs.-confidence-bands-22",
    "href": "slides/04_SLR_02.html#prediction-bands-vs.-confidence-bands-22",
    "title": "Simple Linear Regression (SLR)",
    "section": "Prediction bands vs. confidence bands (2/2)",
    "text": "Prediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \".rownames\"                  \"life_expectancy_years_2011\"\n [3] \"female_literacy_rate_2011\"  \".fitted\"                   \n [5] \".lower\"                     \".upper\"                    \n [7] \".resid\"                     \".hat\"                      \n [9] \".sigma\"                     \".cooksd\"                   \n[11] \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\")"
  },
  {
    "objectID": "slides/04_SLR_02.html#corrrelation-doesnt-imply-causation",
    "href": "slides/04_SLR_02.html#corrrelation-doesnt-imply-causation",
    "title": "Simple Linear Regression (SLR)",
    "section": "Corrrelation doesn’t imply causation*!",
    "text": "Corrrelation doesn’t imply causation*!\n\nThis might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn’t warrant it (such as an observational study).\nBeware of spurious correlations: http://www.tylervigen.com/spurious-correlations\n\n\n\n*Caveat: there is a whole field of statistics/epidemiology on causal inference. https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"
  },
  {
    "objectID": "slides/04_SLR_02.html#whats-next",
    "href": "slides/04_SLR_02.html#whats-next",
    "title": "Simple Linear Regression (SLR)",
    "section": "What’s next?",
    "text": "What’s next?\n \n\n\n\nSLR 1"
  },
  {
    "objectID": "slides/03_SLR.html#residuals-in-the-estimatedfitted-model",
    "href": "slides/03_SLR.html#residuals-in-the-estimatedfitted-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Residuals in the estimated/fitted model",
    "text": "Residuals in the estimated/fitted model\n\n\n\nObserved values \\(y_i\\)\n\nthe values in the dataset\n\nFitted values \\(\\widehat{y}_i\\)\n\nthe values that fall on the best-fit line for a specific \\(x_i\\)\n\nResiduals \\(e_i = y_i - \\widehat{y}_i\\)\n\nthe differences between the observed and fitted values"
  },
  {
    "objectID": "slides/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model",
    "href": "slides/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each individual \\(i\\): \\(Y_i\\)\n\nValue in the dataset for individual \\(i\\)\n\nFitted value for each individual \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)"
  },
  {
    "objectID": "slides/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "href": "slides/03_SLR.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each individual \\(i\\): \\(Y_i\\)\n\nValue in the dataset for individual \\(i\\)\n\nFitted value for each individual \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)\n\n\n\n\nResidual for each individual: \\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\nDifference between the observed and fitted value"
  },
  {
    "objectID": "slides/03_SLR.html#what-is-ordinary-least-squares",
    "href": "slides/03_SLR.html#what-is-ordinary-least-squares",
    "title": "Simple Linear Regression (SLR)",
    "section": "What is ordinary least squares?",
    "text": "What is ordinary least squares?\n\n\n\nSum of Squared Errors (SSE)\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\end{aligned}\\]\n\n\n\nThings we used\n\n\n\n\\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1X_i\\)\n\n\n\n\n\n\nThen we want to find the estimated coefficient values that minimize the SSE!"
  },
  {
    "objectID": "slides/03_SLR.html#steps-to-estimate-coefficients-using-ols",
    "href": "slides/03_SLR.html#steps-to-estimate-coefficients-using-ols",
    "title": "Simple Linear Regression (SLR)",
    "section": "Steps to estimate coefficients using OLS",
    "text": "Steps to estimate coefficients using OLS\n\nSet up SSE (previous slide)\nMinimize SSE with respect to coefficient estimates\n\nNeed to solve a system of equations\n\nCompute derivative of SSE wrt \\(\\widehat\\beta_0\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\nCompute derivative of SSE wrt \\(\\widehat\\beta_1\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\nSubstitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)"
  },
  {
    "objectID": "slides/03_SLR.html#minimize-sse-with-respect-to-coefficients",
    "href": "slides/03_SLR.html#minimize-sse-with-respect-to-coefficients",
    "title": "Simple Linear Regression (SLR)",
    "section": "2. Minimize SSE with respect to coefficients",
    "text": "2. Minimize SSE with respect to coefficients\n\nWant to minimize with respect to (wrt) the potential coefficient estimates ( \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\))\nTake derivative of SSE wrt \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\) and set equal to zero to find minimum SSE\n\n\\[\n\\dfrac{\\partial SSE}{\\partial \\widehat\\beta_0} = 0 \\text{ and } \\dfrac{\\partial SSE}{\\partial \\widehat\\beta_1} = 0\n\\]\n\nSolve the above system of equations in steps 3-6"
  },
  {
    "objectID": "slides/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "href": "slides/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "title": "Simple Linear Regression (SLR)",
    "section": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)",
    "text": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0}& =\\frac{\\partial\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)^2}{\\partial{\\widehat{\\beta}}_0}=\n\\sum_{i=1}^{n}\\frac{{\\partial\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}^2}{\\partial{\\widehat{\\beta}}_0} \\\\\n& =\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\\left(-1\\right)}=\\sum_{i=1}^{n}{-2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & = -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "slides/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "href": "slides/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "title": "Simple Linear Regression (SLR)",
    "section": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)",
    "text": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & =0 \\\\ -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\\n\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\ \\sum_{i=1}^{n}Y_i-n{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\sum_{i=1}^{n}X_i & =0 \\\\\n\\frac{1}{n}\\sum_{i=1}^{n}Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\frac{1}{n}\\sum_{i=1}^{n}X_i & =0 \\\\\n\\overline{Y}-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\overline{X} & =0 \\\\\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)"
  },
  {
    "objectID": "slides/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "href": "slides/03_SLR.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "title": "Simple Linear Regression (SLR)",
    "section": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)",
    "text": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1}& =\\frac{\\partial\\sum_{i=1}^{n}{(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1}=\\sum_{i=1}^{n}\\frac{{\\partial(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1} \\\\\n&=\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)(-X_i)}=\\sum_{i=1}^{n}{-2X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\ &=-2\\sum_{i=1}^{n}{X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "slides/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "href": "slides/03_SLR.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "title": "Simple Linear Regression (SLR)",
    "section": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)",
    "text": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1} & =0 \\\\ \\sum_{i=1}^{n}\\left({X_iY}_i-{\\widehat{\\beta}}_0X_i-{\\widehat{\\beta}}_1{X_i}^2\\right)&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i{\\widehat{\\beta}}_0}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1}&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\left(\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\right)}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\overline{Y}}+\\sum_{i=1}^{n}{{\\widehat{\\beta}}_1X_i\\overline{X}}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+\\sum_{i=1}^{n}{({\\widehat{\\beta}}_1X_i\\overline{X}}-{X_i}^2{\\widehat{\\beta}}_1) &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+{\\widehat{\\beta}}_1\\sum_{i=1}^{n}{X_i(\\overline{X}}-X_i) &=0 \\\\\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\({\\widehat{\\beta}}_0=\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\)\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)\n\n\n\n         \n \n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "href": "slides/03_SLR.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "title": "Simple Linear Regression (SLR)",
    "section": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)",
    "text": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)\nFinal coefficient estimates for SLR\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_1\\)\n\n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]\n\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_0\\)\n\n\n\\[\\begin{aligned}\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X} \\\\\n{\\widehat{\\beta}}_0 & = \\overline{Y} - \\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})} \\overline{X} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#do-i-need-to-do-all-that-work-every-time",
    "href": "slides/03_SLR.html#do-i-need-to-do-all-that-work-every-time",
    "title": "Simple Linear Regression (SLR)",
    "section": "Do I need to do all that work every time??",
    "text": "Do I need to do all that work every time??"
  },
  {
    "objectID": "slides/03_SLR.html#regression-in-r-lm-summary",
    "href": "slides/03_SLR.html#regression-in-r-lm-summary",
    "title": "Simple Linear Regression (SLR)",
    "section": "Regression in R: lm() + summary()",
    "text": "Regression in R: lm() + summary()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10"
  },
  {
    "objectID": "slides/03_SLR.html#regression-in-r-tidy",
    "href": "slides/03_SLR.html#regression-in-r-tidy",
    "title": "Simple Linear Regression (SLR)",
    "section": "Regression in R: tidy()",
    "text": "Regression in R: tidy()\n \n\ntidy(model1) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 45)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n \n\nRegression equation for our model (which we saw a looong time ago):\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#regression-in-r-lm-summary-output",
    "href": "slides/03_SLR.html#regression-in-r-lm-summary-output",
    "title": "Simple Linear Regression (SLR)",
    "section": "Regression in R: lm(), summary()",
    "text": "Regression in R: lm(), summary()\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10"
  },
  {
    "objectID": "slides/03_SLR.html#next-time",
    "href": "slides/03_SLR.html#next-time",
    "title": "Simple Linear Regression (SLR)",
    "section": "Next time",
    "text": "Next time\n\nInference of our estimated coefficients\nInference of estimated expected \\(Y\\) given \\(X\\)\nPrediction\nHypothesis testing!\n\n\n\nSLR 1"
  },
  {
    "objectID": "slides/03_SLR.html#get-to-know-the-data-12",
    "href": "slides/03_SLR.html#get-to-know-the-data-12",
    "title": "Simple Linear Regression (SLR)",
    "section": "Get to know the data (1/2)",
    "text": "Get to know the data (1/2)\n\nLoad data\n\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\n\nGlimpse of the data\n\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\n\nNote the missing values for our variables of interest"
  },
  {
    "objectID": "slides/03_SLR.html#get-to-know-the-data-22",
    "href": "slides/03_SLR.html#get-to-know-the-data-22",
    "title": "Simple Linear Regression (SLR)",
    "section": "Get to know the data (2/2)",
    "text": "Get to know the data (2/2)\n\nGet a sense of the summary statistics\n\n\ngapm_original %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  summary()\n\n life_expectancy_years_2011 female_literacy_rate_2011\n Min.   :47.50              Min.   :13.00            \n 1st Qu.:64.30              1st Qu.:70.97            \n Median :72.70              Median :91.60            \n Mean   :70.66              Mean   :81.65            \n 3rd Qu.:76.90              3rd Qu.:98.03            \n Max.   :82.90              Max.   :99.80            \n NA's   :7                  NA's   :114"
  },
  {
    "objectID": "slides/03_SLR.html#poll-everywhere-question-1",
    "href": "slides/03_SLR.html#poll-everywhere-question-1",
    "title": "Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/03_SLR.html#remove-missing-values-12",
    "href": "slides/03_SLR.html#remove-missing-values-12",
    "title": "Simple Linear Regression (SLR)",
    "section": "Remove missing values (1/2)",
    "text": "Remove missing values (1/2)\n\nRemove rows with missing data for life expectancy and female literacy rate\n\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\n\nNo missing values now for our variables of interest"
  },
  {
    "objectID": "slides/03_SLR.html#remove-missing-values-22",
    "href": "slides/03_SLR.html#remove-missing-values-22",
    "title": "Simple Linear Regression (SLR)",
    "section": "Remove missing values (2/2)",
    "text": "Remove missing values (2/2)\n\nAnd no more missing values when we look only at our two variables of interest\n\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\nNote\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "slides/03_SLR.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "href": "slides/03_SLR.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "title": "Simple Linear Regression (SLR)",
    "section": "Questions we can ask with a simple linear regression model",
    "text": "Questions we can ask with a simple linear regression model\n\n\n\n\n\n\n\n\n\nHow do we…\n\ncalculate slope & intercept?\ninterpret slope & intercept?\ndo inference for slope & intercept?\n\nCI, p-value\n\ndo prediction with regression line?\n\nCI for prediction?\n\n\nDoes the model fit the data well?\n\nShould we be using a line to model the data?\n\nShould we add additional variables to the model?\n\nmultiple/multivariable regression\n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/03_SLR.html#setting-up-for-ordinary-least-squares",
    "href": "slides/03_SLR.html#setting-up-for-ordinary-least-squares",
    "title": "Simple Linear Regression (SLR)",
    "section": "Setting up for ordinary least squares",
    "text": "Setting up for ordinary least squares\n\n\n\nSum of Squared Errors (SSE)\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1X_i\\)\n\n\n\n\n\n\nThen we want to find the estimated coefficient values that minimize the SSE!"
  },
  {
    "objectID": "slides/03_SLR.html#regression-in-r-lm",
    "href": "slides/03_SLR.html#regression-in-r-lm",
    "title": "Simple Linear Regression (SLR)",
    "section": "Regression in R: lm()",
    "text": "Regression in R: lm()\n\nLet’s discuss the syntax of this function\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)"
  },
  {
    "objectID": "slides/03_SLR.html#regression-in-r-lm-tidy",
    "href": "slides/03_SLR.html#regression-in-r-lm-tidy",
    "title": "Simple Linear Regression (SLR)",
    "section": "Regression in R: lm() + tidy()",
    "text": "Regression in R: lm() + tidy()\n \n\ntidy(model1) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 45)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n \n\nRegression equation for our model (which we saw a looong time ago):\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "homework/HW1.html#question-8",
    "href": "homework/HW1.html#question-8",
    "title": "Homework 1",
    "section": "Question 8",
    "text": "Question 8\nQuick True/False question: Is ordinary least squares the only way to find the best fit line for linear regression?"
  },
  {
    "objectID": "slides/03_SLR.html#poll-everywhere-question-2",
    "href": "slides/03_SLR.html#poll-everywhere-question-2",
    "title": "Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/03_SLR.html#poll-everywhere-question-3",
    "href": "slides/03_SLR.html#poll-everywhere-question-3",
    "title": "Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/03_SLR.html#poll-everywhere-question-4",
    "href": "slides/03_SLR.html#poll-everywhere-question-4",
    "title": "Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "slides/03_SLR.html#poll-everywhere-question-5",
    "href": "slides/03_SLR.html#poll-everywhere-question-5",
    "title": "Simple Linear Regression (SLR)",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "labs/Lab_02.html",
    "href": "labs/Lab_02.html",
    "title": "Lab 2",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nPlease delete everything that I wrote below except the headers and the task bubbles from the file you turn in! And include your answers and code. This just helps keep the lab clean when I’m grading it."
  },
  {
    "objectID": "labs/Lab_02.html#directions",
    "href": "labs/Lab_02.html#directions",
    "title": "Lab 2",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\nThe rest of this lab’s instructions are embedded into the lab activities.\n\nPurpose\nThe main purpose of this lab is to introduce our dataset, codebook, and variables. We will continue to think about the context of our research question, but our main focus is to become familiar with the data.\n\n\nGrading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\nRubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "labs/Lab_02.html#lab-activities",
    "href": "labs/Lab_02.html#lab-activities",
    "title": "Lab 2",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Access and download the data\nThis serves as good practice for accessing data that is online or needs to be downloaded from a collaborator.\nData can be accessed here. Under “Weight IAT 2004-2022” there are several drop down menus:\n\nI opened the first “Datasets & Codebooks,” then selected “OSF Storage (United States).” Once selected, the “Download as zip” option pops up in the top right part of the Files section.\n\nWe will be working with the Weight_IAT.public.2021.csv dataset. Please locate the zip file called Weight IAT.public.2021-CSV.zip . T0 download, you need to click the row of the zip file, but you can’t click the name of the zip file. If a link opens, then you clicked the name. If the row is highlighted blue and clickable “Download” and “View” buttons appear on the top right, then you selected it correctly! (See below image for what it should look like.)\n\nThen click the “Download” button to download! Note that the name does not have an underscore between “Weight” and “IAT.” I like to have my datasets named without spaces, so I will replace the space with an underscore.\nFor the codebook, perform the same process for the file named: Weight_IAT_public_2021_codebook.xlsx\nYou will need to unzip the actual data.\nMove the data to a folder that you can easily access as you work from this document. I like to have a folder named data to house my data.\n\n\n\n\n\n\nTask Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n2. Load data and needed packages\nFirst, load the packages that you will need in the remainder of this lab. You can add to this as you need to. At the top of your R code chunk, you can add the following option to repress the messages from the loading packages:\n\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\nUsing R, load the data (csv file) into this document. Note that this is a csv file that we can load with basic R packages. Name your dataset something that feels intuitive to you and will distinguish it from other datasets that you work with.\nLoading the csv file every time you render will take a long time. One way to speed this up is by saving the data as an rda file (R data file). Change the following R code to save the rda file. You will also need to remove the #| eval: false at the top of the code chunk once you have corrected the code. If you are confused on the syntax, don’t forget that you can use ?save for more information.\n\nsave(&lt;whatever you called the read csv file&gt;, file = \"Where you would like to save the file with its name\")\n\nCheck that you have an rda file where you saved it. Now use load() with the file path to load the rda data here.\n\nload(file = \"Where you would like to save the file with its name\")\n\nAt this point, if you think you loaded the file correctly, add #| eval: false to the code chunk where you loaded the csv file and back to the chunk where you saved the rda file.\nTake a glimpse at the data to make sure you loaded it correctly.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n3. Data wrangling\nAs you go through this process, it is important that you look at the codebook for more information on each variable.\n\n3.1 What’s our target population?\nAs many of you mentioned in Lab 1, individuals taking the IAT test are not necessarily representative of the world population. I want you to articulate the target population that you think our analysis can give information about. To what population can we generalize our analysis results? We can get very specific with this population, but try to restrict your population to 3-5 characteristics.\nAfter you articulate the population, I want to add one more restriction to our population: US residency. The sample includes individuals residing in many different countries. Since we are discussing attitudes and beliefs that is inherently connected to society and culture, I think it is important that we restrict our analysis and discussion to a country that we have some social experience in. Thus, let’s restrict our data to the US only by filtering the variable countryres to category 1 (corresponding to the US).\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\nWe are going to restrict our analysis to the single outcome, IAT score, which is named D_biep.Thin_Good_all. You can rename this variable.\nWe will also restrict our analysis to the following 9 potential variables so our work is a little more manageable.\n\n\n\n\n\n\nTask\n\n\n\nFrom the following 8 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n(Make sure you chose the variable that is part of your research question!)\n\nExplicit anti-fat bias (att7)\nSelf-perception of weight (iam_001)\nFat group identity (identfat_001 )\nThin group identity (identthen_001 )\nControllability of weight of others (controlother_001)\nControllability of weight of yourself (controlyou_001)\nAwareness of societal standards (mostpref_001 )\nInternalization of societal standards (important_001)\n\nWe will start our data exploration with the following 4 demographic variables:\n\nAge (we need to construct)\nRace (raceomb_002 or raceombmulti)\nEthnicity (ethnicityomb)\nSex assigned at birth (birthSex)\n\nPlease pick 2 additional variables to include in your analysis:\n\nEducation (edu)\nGender (genderIdentity)\nSelf-reported BMI (through self-reported height and weight)\nPolitical identity\nReligion\n\nI have chosen these variables for a mixture of reasons. For example, I have left out variables about residence and occupation because those variables have hundreds of categories that would be overwhelming in linear regression. For the 4 required demographic variables, I chose age because I really want us to get practice with a continuous variable. I chose race and ethnicity because of the intertwined history of racism and anti-fat bias in Western countries (including the U.S. where most participants reside).\n\n\n\n\n\n\nA note of the available variables on race\n\n\n\nThe dataset has two separate race variables. One has mutually exclusive categories (raceomb_002) and the other allows participants to make multiple selections (raceombmulti). The former (raceomb_002) allows one participant to identify with only one race category.\nImportant lesson from We All Count about using a multiple selection race question. We can try out all these options!\n\n\nFinally, I chose sex assigned at birth because adults in 2021 in the US were likely raised in a society where your sex assigned at birth impacted the gender stereotypes that you were raised in, which could impact exposure to diet culture. This in addition to the many medical conditions associated with one’s sex assigned at birth that may affect weight. The reason why I am leaving gender as an optional variable is because the question on gender allows participants to chose multiple options. The binary sex assigned at birth will make our analysis a little easier from a statistics stand point. Unfortunately, we need to balance achievable learning objectives and the most appropriate variable. Since I have required race as a variable and has a multi-level option, I do not want to overload our analysis with another multi-level variable. Sex assigned at birth will not create more work for you (that is outside of the course objectives) while capturing medical conditions and some of the societal impact of diet culture. This is certainly a limitation in our analysis that we should address in our discussion. I do encourage you to look into gender if the binary sex assigned at birth does not feel right for you. I am happy to help!\n\n\n\n\n\n\nA word on self-reported BMI\n\n\n\nThis variable is rooted in racism and anti-fat bias. The American Medical Association made a few press releases on policies using BMI as a measure, with alternative measures (frankly, just other measures of fatness to use as a diagnostic tool instead of checking true indicators of health). However, I can think of a couple examples where BMI might help us understand some context in this research, so I have left it as an option. Although still self-reported, it might be interesting to see how BMI (which is the closest measurement available in this dataset to an “objective” measure of fatness) is related to individuals’ attitudes and beliefs. I am not saying there is anything to the relationship, but it might be worth checking out if you are interested.\nI will also say, in this dataset, there are MANY issues constructing the variable for BMI from height and weight. If you do not feel strongly about including it, I would suggest you avoid the variable self-reported BMI. It is not worth bringing in a racist and anti-fat variable into the dataset if you do not have a specific use for it. If you do plan to use it, please come to me for help as early as possible!\n\n\nIf you would like to investigate a variable outside the list, please let me know by emailing or chatting with me.\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset. Your new dataset should have 10 columns for the 10 variables.\n\n\n\n\n3.3 Manipulating variables that are coded as numeric variables\nMany variables in this dataset are coded as numeric values, but have specific categories linking up to the numbers. Using mutate() and cases() similar to our Data Management lesson, please create a new categorical variable with the specified categories from the codebook. Make sure that you create a variable with a new name! Since some of these variables are ordered categories, we will investigate if it’s appropriate to use the numeric or categorical version of the variable.\n\n\n\n\n\n\nExample of how I would create new variable for self-perception of weight (iam_001):\n\n\n\nBy looking at the codebook, I see that respondents answer the following question: “Currently, I am:”\n\n“Very underweight”\n“Moderately underweight”\n“Slightly underweight”\n“Neither underweight nor underweight”\n“Slightly overweight”\n“Moderately overweight”\n“Very overweight”\n\nIf I look at the data as is, I see that the variable is numeric.\n\niat_2021 %&gt;%\n  dplyr::select(iam_001) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001\n\n        1\n2,023 (0.6%)\n        2\n7,902 (2.4%)\n        3\n24,399 (7.3%)\n        4\n148,081 (44%)\n        5\n88,566 (27%)\n        6\n43,090 (13%)\n        7\n18,978 (5.7%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nAgain, I want to create a varaible with the answers instead of numbers, so I will change transform the variable to include the text:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(iam_001_f = case_match(iam_001,\n                             7 ~ \"Very overweight\",\n                             6 ~ \"Moderately overweight\",\n                             5 ~ \"Slightly overweight\",\n                             4 ~ \"Neither underweight nor underweight\",\n                             3 ~ \"Slightly underweight\",\n                             2 ~ \"Moderately underweight\",\n                             1 ~ \"Very underweight\",\n                             .default = NA # to add NA if unknown\n                             ) %&gt;% factor())\niat_2021 %&gt;%\n  dplyr::select(iam_001_f) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001_f\n\n        Moderately overweight\n43,090 (13%)\n        Moderately underweight\n7,902 (2.4%)\n        Neither underweight nor underweight\n148,081 (44%)\n        Slightly overweight\n88,566 (27%)\n        Slightly underweight\n24,399 (7.3%)\n        Very overweight\n18,978 (5.7%)\n        Very underweight\n2,023 (0.6%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\nggplot(data=iat_2021) +\n  geom_boxplot(aes(x = iam_001_f, y = IAT_score))\n\nWarning: Removed 129215 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nI have called the new variable iam_001_f to indicate that the variable is not in factor form. You can also call it something like iam_001_cat to indicate the categorical form.\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n3.4 Creating age from birth date and test date\nThis dataset does not have an available “age” variable. However, we have enough information to determine each individual’s age from the test date and their self-reported birth date. We can use the lubridate package to configure the age. First, we need to use make_date() to construct the birth date and test date. Below, I have implemented make_date() to make the birth date.\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\niat_2021 = iat_2021 %&gt;%\n  mutate(birthdate = make_date(month = birthmonth, year = birthyear), \n         testdate = make_date(month = month, year = year))\n\nOnce the two dates are created, we can use further use lubridate to calculate the age in years. This code is a little complicated, so here is an example of how I have created age:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(age = interval(start = birthdate, end = testdate) %&gt;%\n          as.period() %&gt;% year()) %&gt;%\n  select(-birthmonth, -birthyear, -year, -month, \n         -testdate, -birthdate)\n\nNote that the name of my dataset is iat_2021 and I feed it into mutate(). Within mutate(), I assigned age to the interval between the name of my birth date (birthdate) and the name of my test date (testdate). I need to convert the interval to a period of time (as.period()), then to a measurement of years (year()).\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n3.5 If you chose BMI, create the variable\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\n\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\n\n\nI will post an extra resource outlining some of my work on the BMI variable.\n\n\n3.6 Make a new dataset with only complete cases\nHandling missing data is outside the scope of our class. There are many techniques to handling missing data, but we will use complete case analysis. This means we will only use observations that have information for every variable we chose. The function drop_na() will give you the complete cases. You can feed your dataset into the function and assign it as a new dataframe.\nFor example:\n\nnew_df = old_df %&gt;% drop_na()\n\n\n\n\n4. Some exploratory data analysis\n\n4.1 Peek at your outcome\nThis serves as a check to make sure we are all looking at the correct outcome: IAT score.\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n4.2 Univariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n4.3 Bivariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n5. Revisit your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. Please make sure it is only one question, one sentence long. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn research, we typically do NOT change our research question after looking at the data! Researchers typically form their questions from other research and their expertise. We may not have expertise in this field and we have not been studying implicit bias, so I want to be a little more flexible with our analysis."
  },
  {
    "objectID": "labs/Lab_02_work.html",
    "href": "labs/Lab_02_work.html",
    "title": "Lab 2 Work",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nPlease delete everything"
  },
  {
    "objectID": "labs/Lab_02_work.html#directions",
    "href": "labs/Lab_02_work.html#directions",
    "title": "Lab 2 Work",
    "section": "Directions",
    "text": "Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\nThe rest of this lab’s instructions are embedded into the lab activities.\n\nPurpose\nThe main purpose of this lab is to introduce our dataset, codebook, and variables. We will continue to think about the context of our research question, but our main focus is to become familiar with the data.\n\n\nGrading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\nRubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "labs/Lab_02_work.html#lab-activities",
    "href": "labs/Lab_02_work.html#lab-activities",
    "title": "Lab 2 Work",
    "section": "Lab activities",
    "text": "Lab activities\n\n1. Access and download the data\nThis serves as good practice for accessing data that is online or needs to be downloaded from a collaborator.\nData can be accessed here. Under “Weight IAT 2004-2022” there are several drop down menus:\n\nI opened the first “Datasets & Codebooks,” then selected “OSF Storage (United States).” Once selected, the “Download as zip” option pops up in the top right part of the Files section.\n\nWe will be working with the Weight_IAT.public.2021.csv dataset. Please locate the zip file called Weight IAT.public.2021-CSV.zip . T0 download, you need to click the row of the zip file, but you can’t click the name of the zip file. If a link opens, then you clicked the name. If the row is highlighted blue and clickable “Download” and “View” buttons appear on the top right, then you selected it correctly! (See below image for what it should look like.)\n\nThen click the “Download” button to download! Note that the name does not have an underscore between “Weight” and “IAT.” I like to have my datasets named without spaces, so I will replace the space with an underscore.\nFor the codebook, perform the same process for the file named: Weight_IAT_public_2021_codebook.xlsx\nYou will need to unzip the actual data.\nMove the data to a folder that you can easily access as you work from this document. I like to have a folder named data to house my data.\n\n\n\n\n\n\nTask Summary\n\n\n\nDownload the 2021 data and codebook from the archives and store in accessible folder.\n\n\n\n\n2. Load data and needed packages\nFirst, load the packages that you will need in the remainder of this lab. You can add to this as you need to. At the top of your R code chunk, you can add the following option to repress the messages from the loading packages:\n\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\nUsing R, load the data (csv file) into this document. Note that this is a csv file that we can load with basic R packages. Name your dataset something that feels intuitive to you and will distinguish it from other datasets that you work with.\n\n\n[1] \"/Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/W2024_BSTA_512_612/W2024_BSTA_512\"\n\n\nLoading the csv file every time you render will take a long time. One way to speed this up is by saving the data as an rda file (R data file). Change the following R code to save the rda file. You will also need to remove the #| eval: false at the top of the code chunk once you have corrected the code. If you are confused on the syntax, don’t forget that you can use ?save for more information.\n\nsave(&lt;whatever you called the read csv file&gt;, file = \"Where you would like to save the file with its name\")\n\nCheck that you have an rda file where you saved it. Now use load() with the file path to load the rda data here.\n\nload(file = \"Where you would like to save the file with its name\")\n\nAt this point, if you think you loaded the file correctly, add #| eval: false to the code chunk where you loaded the csv file and back to the chunk where you saved the rda file.\nTake a glimpse at the data to make sure you loaded it correctly.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n\n\nTask Summary\n\n\n\nRead csv, save as rda, load rda, glimpse at data.\nHow many rows and columns are in the dataset? Do you think we will need all these variables for our analysis?\n\n\n\n\n3. Data wrangling\nAs you go through this process, it is important that you look at the codebook for more information on each variable.\n\n3.1 What’s our target population?\nAs many of you mentioned in Lab 1, individuals taking the IAT test are not necessarily representative of the world population. I want you to articulate the target population that you think our analysis can give information about. To what population can we generalize our analysis results? We can get very specific with this population, but try to restrict your population to 3-5 characteristics.\nAfter you articulate the population, I want to add one more restriction to our population: US residency. The sample includes individuals residing in many different countries. Since we are discussing attitudes and beliefs that is inherently connected to society and culture, I think it is important that we restrict our analysis and discussion to a country that we have some social experience in. Thus, let’s restrict our data to the US only by filtering the variable countryres to category 1 (corresponding to the US).\n\n\n\n\n\n\nTask\n\n\n\nDescribe our target population. Keep your description to 3-5 characteristics, not including our restriction on the US population.\n\n\n\n\n3.2 Restrict your analysis to 1 outcome and 9 possible covariates/predictors\nWe are going to restrict our analysis to the single outcome, IAT score, which is named D_biep.Thin_Good_all. You can rename this variable.\nWe will also restrict our analysis to the following 9 potential variables so our work is a little more manageable.\n\n\n\n\n\n\nTask\n\n\n\nFrom the following 7 attitudes and beliefs, please select 3 that you think will be the most important variables related to your research question. In 1-2 lines, briefly explain why you chose each variable. This can be informal and bulleted.\n\n\n(Make sure you chose the variable that is part of your research question!)\n\nSelf-perception of weight (iam_001)\nFat group identity (identfat_001 )\nThin group identity (identthen_001 )\nControllability of weight of others (controlother_001)\nControllability of weight of yourself (controlyou_001)\nAwareness of societal standards (mostpref_001 )\nInternalization of societal standards (important_001)\n\nWe will start our data exploration with the following 4 demographic variables:\n\nAge (we need to construct)\nRace (raceomb_002 or raceombmulti)\nEthnicity\nSex assigned at birth (birthSex)\n\nPlease pick 2 additional variables to include in your analysis:\n\nEducation (edu)\nGender (genderIdentity)\nSelf-reported BMI (through self-reported height and weight)\nPolitical identity\nReligion\n\nI have chosen these variables for a mixture of reasons. For example, I have left out variables about residence and occupation because those variables have hundreds of categories that would be overwhelming in linear regression. For the 4 required demographic variables, I chose age because I really want us to get practice with a continuous variable. I chose race and ethnicity because of the intertwined history of racism and anti-fat bias in Western countries (including the U.S. where most participants reside).\n\n\n\n\n\n\nA note of the available variables on race\n\n\n\nThe dataset has two separate race variables. One has mutually exclusive categories (raceomb_002) and the other allows participants to make multiple selections (raceombmulti). The former (raceomb_002) allows one participant to identify with only one race category.\nImportant lesson from We All Count about using a multiple selection race question. We can try out all these options!\n\n\nFinally, I chose sex assigned at birth because adults in 2021 in the US were likely raised in a society where your sex assigned at birth impacted the gender stereotypes that you were raised in, which could impact exposure to diet culture. This in addition to the many medical conditions associated with one’s sex assigned at birth that may affect weight. The reason why I am leaving gender as an optional variable is because the question on gender allows participants to chose multiple options. The binary sex assigned at birth will make our analysis a little easier from a statistics stand point. Unfortunately, we need to balance achievable learning objectives and the most appropriate variable. Since I have required race as a variable and has a multi-level option, I do not want to overload our analysis with another multi-level variable. Sex assigned at birth will not create more work for you (that is outside of the course objectives) while capturing medical conditions and some of the societal impact of diet culture. This is certainly a limitation in our analysis that we should address in our discussion. I do encourage you to look into gender if the binary sex assigned at birth does not feel right for you. I am happy to help!\n\n\n\n\n\n\nA word on self-reported BMI\n\n\n\nThis variable is rooted in racism and anti-fat bias. The American Medical Association made a few press releases on policies using BMI as a measure, with alternative measures (frankly, just other measures of fatness to use as a diagnostic tool instead of checking true indicators of health). However, I can think of a couple examples where BMI might help us understand some context in this research, so I have left it as an option. Although still self-reported, it might be interesting to see how BMI (which is the closest measurement available in this dataset to an “objective” measure of fatness) is related to individuals’ attitudes and beliefs. I am not saying there is anything to the relationship, but it might be worth checking out if you are interested.\nI will also say, in this dataset, there are MANY issues constructing the variable for BMI from height and weight. If you do not feel strongly about including it, I would suggest you avoid the variable self-reported BMI. It is not worth bringing in a racist and anti-fat variable into the dataset if you do not have a specific use for it. If you do plan to use it, please come to me for help as early as possible!\n\n\nIf you would like to investigate a variable outside the list, please let me know by emailing or chatting with me.\n\n\n\n\n\n\nTask\n\n\n\nUsing R, select your identified variables from your dataset. Your new dataset should have 10 columns for the 10 variables.\n\n\n\n\n [1] \" \"             \"[2]\"           \"[1]\"           \"[3]\"          \n [5] \"[1,3]\"         \"[5]\"           \"[1,5,6]\"       \"[2,5]\"        \n [9] \"[1,2]\"         \"[1,5]\"         \"[4]\"           \"[6]\"          \n[13] \"[1,2,3,4,5,6]\" \"[5,6]\"         \"[1,6]\"         \"[3,5]\"        \n[17] \"[4,5]\"         \"[2,6]\"         \"[2,4]\"         \"[2,5,6]\"      \n[21] \"[1,4]\"         \"[2,3]\"         \"[3,4,5,6]\"     \"[1,2,3,4]\"    \n[25] \"[1,3,5,6]\"     \"[1,3,5]\"       \"[2,3,4]\"       \"[3,5,6]\"      \n[29] \"[2,4,6]\"       \"[1,2,5]\"       \"[3,6]\"         \"[1,2,6]\"      \n[33] \"[1,4,6]\"       \"[2,3,5]\"       \"[1,3,4,6]\"     \"[1,2,3,4,5]\"  \n[37] \"[4,6]\"         \"[2,4,5]\"       \"[1,2,4]\"       \"[1,3,6]\"      \n[41] \"[1,2,3,4,6]\"   \"[3,4,5]\"       \"[1,2,3]\"       \"[1,3,4,5,6]\"  \n[45] \"[4,5,6]\"       \"[1,2,3,5]\"     \"[1,4,5]\"       \"[3,4]\"        \n[49] \"[2,3,4,5]\"     \"[1,4,5,6]\"     \"[1,2,5,6]\"     \"[1,3,4]\"      \n\n\n\n\n3.3 Manipulating variables that are coded as numeric variables\nMany variables in this dataset are coded as numeric values, but have specific categories linking up to the numbers. Using mutate() and cases() similar to our Data Management lesson, please create a new categorical variable with the specified categories from the codebook. Make sure that you create a variable with a new name! Since some of these variables are ordered categories, we will investigate if it’s appropriate to use the numeric or categorical version of the variable.\n\n\n\n\n\n\nExample of how I would create new variable for self-perception of weight (iam_001):\n\n\n\nBy looking at the codebook, I see that respondents answer the following question: “Currently, I am:”\n\n“Very underweight”\n“Moderately underweight”\n“Slightly underweight”\n“Neither underweight nor underweight”\n“Slightly overweight”\n“Moderately overweight”\n“Very overweight”\n\nIf I look at the data as is, I see that the variable is numeric.\n\niat_2021 %&gt;%\n  dplyr::select(iam_001) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    iam_001\n\n        1\n2,023 (0.6%)\n        2\n7,902 (2.4%)\n        3\n24,399 (7.3%)\n        4\n148,081 (44%)\n        5\n88,566 (27%)\n        6\n43,090 (13%)\n        7\n18,978 (5.7%)\n        Unknown\n132,847\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nAgain, I want to create a varaible with the answers instead of numbers, so I will change transform the variable to include the text:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(iam_001_f = case_match(iam_001,\n                             7 ~ \"Very overweight\",\n                             6 ~ \"Moderately overweight\",\n                             5 ~ \"Slightly overweight\",\n                             4 ~ \"Neither underweight nor underweight\",\n                             3 ~ \"Slightly underweight\",\n                             2 ~ \"Moderately underweight\",\n                             1 ~ \"Very underweight\",\n                             .default = NA # to add NA if unknown\n                             ))\n\nI have called the new variable iam_001_f to indicate that the variable is not in factor form. You can also call it something like iam_001_cat to indicate the categorical form.\n\n\n\n\n\n\n\n\nTask\n\n\n\nIdentify and list the variables that are coded numerically and correspond to categories. Create a new variable for the categorical/factor version of the variable. It is up to you to check that your code ran properly!! If you are using multi-choice categorical variables (might include race, gender), then do not convert the variable yet!\n\n\n\n\n3.4 Creating age from birth date and test date\nThis dataset does not have an available “age” variable. However, we have enough information to determine each individual’s age from the test date and their self-reported birth date. We can use the lubridate package to configure the age. First, we need to use make_date() to construct the birth date and test date. Below, I have implemented make_date() to make the birth date.\n\n\n\n\n\n\nTask\n\n\n\nFrom the codebook, find the variables that we can use to construct the test date. Then use make_date() to create the test date.\n\n\n\niat_2021 = iat_2021 %&gt;%\n  mutate(birthdate = make_date(month = birthmonth, year = birthyear), \n         testdate = make_date(month = month, year = year))\n\nOnce the two dates are created, we can use further use lubridate to calculate the age in years. This code is a little complicated, so here is an example of how I have created age:\n\niat_2021 = iat_2021 %&gt;%\n  mutate(age = interval(start = birthdate, end = testdate) %&gt;%\n          as.period() %&gt;% year()) %&gt;%\n  select(-birthmonth, -birthyear, -year, -month, \n         -testdate, -birthdate)\n\nNote that the name of my dataset is iat_2021 and I feed it into mutate(). Within mutate(), I assigned age to the interval between the name of my birth date (birthdate) and the name of my test date (testdate). I need to convert the interval to a period of time (as.period()), then to a measurement of years (year()).\n\n\n\n\n\n\nTask\n\n\n\nFollowing the above example, create an age variable that measures the years between individuals’ birth and test date. Then remove the variables used to make age.\n\n\n\n\n3.5 If you chose BMI, create the variable\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\n\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\n\n\nI will post an extra resource outlining some of my work on the BMI variable.\n\n\n3.6 Make a new dataset with only complete cases\nHandling missing data is outside the scope of our class. There are many techniques to handling missing data, but we will use complete case analysis. This means we will only use observations that have information for every variable we chose. The function drop_na() will give you the complete cases. You can feed your dataset into the function and assign it as a new dataframe.\nFor example:\n\nnew_df = old_df %&gt;% drop_na()\n\niat_2021 = iat_2021 %&gt;%\n    mutate(iam_001_f = case_match(iam_001,\n                                  7 ~ \"Very overweight\",\n                                  6 ~ \"Moderately overweight\",\n                                  5 ~ \"Slightly overweight\",\n                                  4 ~ \"Neither underweight nor underweight\",\n                                  3 ~ \"Slightly underweight\",\n                                  2 ~ \"Moderately underweight\",\n                                  1 ~ \"Very underweight\",\n                                  .default = NA) %&gt;% \n             factor(levels = c(\"Very underweight\", \n                               \"Moderately underweight\", \n                               \"Slightly underweight\", \n                               \"Neither underweight nor underweight\", \n                               \"Slightly overweight\", \n                               \"Moderately overweight\", \n                               \"Very overweight\")))\n\n\n\n\n4. Some exploratory data analysis\n\n4.1 Peek at your outcome\nThis serves as a check to make sure we are all looking at the correct outcome: IAT score.\n\n\n\n\n\n\nTask\n\n\n\nPlease plot a histogram of the IAT scores. What do you notice about the outcome?\n\n\n\n\n4.2 Univariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nUsing ggplot or tables, visualize your variables. Get a sense of each variable’s distribution. Do you notice anything out of the ordinary?\n\n\n\n\n4.3 Bivariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\nTake a look at the scatterplot, violin, or box plot of IAT score and your variable of interest. Use R and ggplot to make this plot. If your variable of interest is categorical, then make sure to use a violin or boxplot.\n\n\n\n\n\n5. Revisit your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate the research question that you proposed in Lab 1. What are your thoughts on the research question now that we looked at the data? Feel free to change it now that we’ve looked at the data. If you change your question, make sure 4.2 reflects the new research question.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn research, we typically do NOT change our research question after looking at the data! Researchers typically form their questions from other research and their expertise. We may not have expertise in this field and we have not been studying implicit bias, so I want to be a little more flexible with our analysis."
  },
  {
    "objectID": "weeks/week_03_sched.html",
    "href": "weeks/week_03_sched.html",
    "title": "Week 3",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #213c96; background-color: #fff; padding: 10px; }\n.description{ font-size: 20px; color: #fff; background-color: #213c96; padding: 10px; } ```"
  },
  {
    "objectID": "weeks/week_03_sched.html#announcements",
    "href": "weeks/week_03_sched.html#announcements",
    "title": "Week 3",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 1/22\n\nClarification on the homework\n\nYou can always turn in homework late for credit\nThe only way to secure feedback from the TAs is by turning it in on time (unless you and I have discussed something else)\nNot the same for the lab!\n\nHW 1: changes some wording in Question 6, Part f and g (less work to do! Yay!)\n\n\n\nWednesday 1/24\n\nHW 1: You do not need to do Question 6f!!\nWhile grading Lab 1\n\nI noticed some really good answers that might have sources attached to them\nIt was not required that you cite resources in your lab, but it will be required in the project report\nJust wanted to let you know before some of your found sources leave your mind\n\nThey might by the end of the quarter\n\n\nStudent Leadership Council has set this year’s OHSU-PSU SPH National Public Health Week Conference for Thursday, April 4th, 2024.\n\nPresent your research in a low pressure situation!\nYou can even present on your project from this class!\n\nI can help you polish it up!"
  },
  {
    "objectID": "weeks/week_04_sched.html",
    "href": "weeks/week_04_sched.html",
    "title": "Week 4",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #213c96; background-color: #fff; padding: 10px; }\n.description{ font-size: 20px; color: #fff; background-color: #213c96; padding: 10px; } ```"
  },
  {
    "objectID": "weeks/week_04_sched.html#announcements",
    "href": "weeks/week_04_sched.html#announcements",
    "title": "Week 4",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 1/29\nQuiz - no announcements\n\n\nWednesday 1/31\n\nVote in Exit ticket: Emailing me questions privately vs. posting on Slack\n\nI am proposing a new class policy to anonymously post any emailed questions on Slack\nI know it can be scary to pose a question to the whole class\nEven questions about logistics can be answered by anyone and endorsed by me\n\nWill help get answers faster\n\nSometimes I see your email, but don’t have time carved out for my emails until later in the day or the next day\n\nLab 1 IS ALMOST GRADED!!\nLab 2 IS POSTED!!"
  },
  {
    "objectID": "weeks/week_05_sched.html#announcements",
    "href": "weeks/week_05_sched.html#announcements",
    "title": "Week 5",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 2/5\n\nNew extra resources!!\n\nAdding a page for coefficient interpretations under resources\nAriel made a page on LaTeX formatting!\n\n\n\n\nWednesday 2/7\n\nAdded BMI help page!\nOffice hours tomorrow at 11:30 if you need help on Lab 2!!\nDon’t forget that you can ask for a “no questions asked” extension\nI feel like I’m forgetting something…"
  },
  {
    "objectID": "weeks/week_06_sched.html#announcements",
    "href": "weeks/week_06_sched.html#announcements",
    "title": "Week 6",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 2/12\n\nLab 2: please resubmit with self-contained: true\n\nThe top of your file should have the following line:\n\nQuiz 2 next week\n\nLesson 4 (SLR: Inference, starting at mean response) to Lesson 9\nHW 4 will not be on it!\n\n\nWednesday 2/14\n\nMidterm feedback!!\n\nPlease complete by next Friday 2/23\nCompletely anonymous!\nA question at the end will take you to another survey to record your name"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#topics",
    "href": "slides/04_SLR_Inf_Pred.html#topics",
    "title": "SLR: Inference and Prediction",
    "section": "Topics",
    "text": "Topics\nDay 3: slide 13-end\n\nInference for slope and intercept\n\nCI’s and hypothesis tests\n\nInference for mean value of y at specific values of x\n\nConfidence bands of best-fit line\n\nInference for the estimated variance???\nPrediction intervals for individual predictions\n\nPrediction bands"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#inference-for-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#inference-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 45) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295\n  \n  \n  \n\n\n\n# conf.int = TRUE part is new! \n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "href": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-ci-and-hypothesis-test",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#ci-for-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#ci-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "CI for population slope \\(\\beta_1\\)",
    "text": "CI for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\widehat{\\beta}_1 \\pm t_{n-2}^* \\cdot SE_{\\widehat{\\beta}_1}\\] - Recall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level:\n\n(tstar = qt(0.975, df = 80-2))\n\n[1] 1.990847"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-b_1",
    "href": "slides/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-b_1",
    "title": "SLR: Inference and Prediction",
    "section": "Standard error of fitted slope \\(b_1\\)",
    "text": "Standard error of fitted slope \\(b_1\\)\n\n\n\\[\\text{SE}_{b_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{b_1}\\) is the variability of the statistic \\(b_1\\)\n\n\n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\n\n\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] NA"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Calculate CI for population slope \\(\\beta_1\\)",
    "text": "Calculate CI for population slope \\(\\beta_1\\)\n\n\n\\[\\widehat{\\beta}_1  \\pm t^*\\cdot SE_{b_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\nOption 1: Calculate using each value  \n\n\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\n\nUse formula to calculate each bound\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619\n\n\n\n\n\nOption 2: Use the regression table\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#hypothesis-test-for-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#hypothesis-test-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Hypothesis test for population slope \\(\\beta_1\\)\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\nState the null hypothesis\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nSpecify the significance level\nSpecify the test statistic and its distribution under the null\n\nThe test statistic for \\(b_1\\) is\n\\[t = \\frac{ b_1 - \\beta_1}{ \\text{SE}_{b_1}} = \\frac{ b_1}{ \\text{SE}_{b_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true. The test statistic, \\(t\\) follows a Student’s t-distribution.\n\nCompute the value of the test statistic\nCalculate the p-value\nWrite conclusion for hypothesis test\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#p-value-for-testing-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#p-value-for-testing-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "\\(p\\)-value for testing population slope \\(\\beta_1\\)",
    "text": "\\(p\\)-value for testing population slope \\(\\beta_1\\)\n\nAs usual, the \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true.\nTo calculate the \\(p\\)-value, we need to know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true.\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nRecall that this is a 2-sided test:\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\nCompare the \\(p\\)-value to the one from the regression table below\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#prediction-with-regression-line",
    "href": "slides/04_SLR_Inf_Pred.html#prediction-with-regression-line",
    "title": "SLR: Inference and Prediction",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\n\nWhen we take the expected value, at a given value \\(X^*\\), the average expected response at \\(X^*\\) is:\n\n\\[\\widehat{E}[Y|X^*] = \\widehat\\beta_0 + \\widehat\\beta_1 X^*\\]\n\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses have variability, and we can calculate a CI for it, for every value of \\(X^*\\)."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#prediction-with-regression-line-1",
    "href": "slides/04_SLR_Inf_Pred.html#prediction-with-regression-line-1",
    "title": "SLR: Inference and Prediction",
    "section": "Prediction with regression line",
    "text": "Prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\n\nWhen we take the expected value, at a given value \\(x^*\\), we have that the predicted response is the average expected response at \\(x^*\\):\n\n\\[\\widehat{E[Y|x^*]} = b_0 + b_1 x^*\\]\n\n\n\n\n\n\n\n\n\nThese are the points on the regression line.\nThe mean responses has variability, and we can calculate a CI for it, for every value of \\(x^*\\)."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#ci-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "CI for mean response \\(\\mu_{Y|x^*}\\)",
    "text": "CI for mean response \\(\\mu_{Y|x^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#example-ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#example-ci-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Example: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#example-using-r-for-ci-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Example: Using R for CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example: Using R for CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(X^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        40\n2                        60\n3                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 60.21570 57.26905 63.16236\n2 64.85961 62.93335 66.78586\n3 69.50351 68.13244 70.87457\n\n\n\nInterpretation\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#confidence-bands-for-mean-response-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#confidence-bands-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Confidence bands for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Confidence bands for mean response \\(\\mu_{Y|X^*}\\)\n\nOften we plot the CI for many values of X, creating confidence bands\nThe confidence bands are what ggplot creates when we set se = TRUE within geom_smooth\nThink about it: for what values of X are the confidence bands (intervals) narrowest?\n\n\nggplot(gapm,\n       aes(x=female_literacy_rate_2011, \n           y=life_expectancy_years_2011)) +\n  geom_point()+\n  geom_smooth(method = lm, se=TRUE)+\n  ggtitle(\"Life expectancy vs. female literacy rate\")"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#width-of-confidence-bands-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Width of confidence bands for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Width of confidence bands for mean response \\(\\mu_{Y|X^*}\\)\n\nFor what values of \\(X^*\\) are the confidence bands (intervals) narrowest? widest?\n\n\\[\\begin{align}\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\\\\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\n\\end{align}\\]\n\n\n\nSLR 2"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#prediction-interval-for-predicting-individual-observations",
    "href": "slides/04_SLR_Inf_Pred.html#prediction-interval-for-predicting-individual-observations",
    "title": "SLR: Inference and Prediction",
    "section": "Prediction interval for predicting individual observations",
    "text": "Prediction interval for predicting individual observations\n\nWe do not call this interval a CI since \\(Y\\) is a random variable instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y|X^*} \\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#example-prediction-interval",
    "href": "slides/04_SLR_Inf_Pred.html#example-prediction-interval",
    "title": "SLR: Inference and Prediction",
    "section": "Example: Prediction interval",
    "text": "Example: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\begin{align}\n\\widehat{Y|X^*} &\\pm t_{n-2}^* \\cdot s_{residuals} \\sqrt{1 + \\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.48072 &, 77.23849)\n\\end{align}\\]\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011))\n\n[1] NA\n\n\n\n\n\n(SE_Ypred &lt;- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] NA\n\n\n\n\n\n(MOE_Ypred &lt;- SE_Ypred*tstar)\n\n[1] NA\n\n\n\n\n\n\nY60 - MOE_Ypred\n\n[1] NA\n\n\n\n\n\n\nY60 + MOE_Ypred\n\n[1] NA"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#example-using-r-for-prediction-interval",
    "href": "slides/04_SLR_Inf_Pred.html#example-using-r-for-prediction-interval",
    "title": "SLR: Inference and Prediction",
    "section": "Example: Using R for prediction interval",
    "text": "Example: Using R for prediction interval\nFind the 95% prediction intervals for the life expectancy when the female literacy rate is 40, 60, and 80.\n\nnewdata  # previously defined for CI's\n\n  female_literacy_rate_2011\n1                        60\n2                        80\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")  # prediction instead of \"confidence\"\n\n       fit      lwr      upr\n1 64.85961 52.48072 77.23849\n2 69.50351 57.19879 81.80823\n\n\n\nInterpretation\nWe are 95% confident that a new selected country with a 60% female literacy rate will have a life expectancy between 52.5 and 77.2 years."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#prediction-bands-vs.-confidence-bands-12",
    "href": "slides/04_SLR_Inf_Pred.html#prediction-bands-vs.-confidence-bands-12",
    "title": "SLR: Inference and Prediction",
    "section": "Prediction bands vs. confidence bands (1/2)",
    "text": "Prediction bands vs. confidence bands (1/2)\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nFirst create a data frame with the original data points (both x and y values), their respective predicted values, andtheir respective prediction intervals\nCan do this with augment() from the broom package.\n\n\nmodel1_pred_bands &lt;- augment(model1, interval = \"prediction\")\n\n# take a look at new object:\nnames(model1_pred_bands) \n\n [1] \".rownames\"                  \"life_expectancy_years_2011\"\n [3] \"female_literacy_rate_2011\"  \".fitted\"                   \n [5] \".lower\"                     \".upper\"                    \n [7] \".resid\"                     \".hat\"                      \n [9] \".sigma\"                     \".cooksd\"                   \n[11] \".std.resid\"                \n\n# glimpse of select variables of interest:\nmodel1_pred_bands %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011, \n         .fitted:.upper) %&gt;% \n  glimpse()\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .lower                     &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ .upper                     &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#prediction-bands-vs.-confidence-bands-22",
    "href": "slides/04_SLR_Inf_Pred.html#prediction-bands-vs.-confidence-bands-22",
    "title": "SLR: Inference and Prediction",
    "section": "Prediction bands vs. confidence bands (2/2)",
    "text": "Prediction bands vs. confidence bands (2/2)\n\nnames(model1_pred_bands) \n\n [1] \".rownames\"                  \"life_expectancy_years_2011\"\n [3] \"female_literacy_rate_2011\"  \".fitted\"                   \n [5] \".lower\"                     \".upper\"                    \n [7] \".resid\"                     \".hat\"                      \n [9] \".sigma\"                     \".cooksd\"                   \n[11] \".std.resid\"                \n\n\n\nggplot(model1_pred_bands, \n       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands\n              alpha = 0.2, fill = \"red\") +\n  geom_smooth(method=lm) +  # confidence bands\n  labs(title = \"SLR with Confidence & Prediction Bands\")"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#corrrelation-doesnt-imply-causation",
    "href": "slides/04_SLR_Inf_Pred.html#corrrelation-doesnt-imply-causation",
    "title": "SLR: Inference and Prediction",
    "section": "Corrrelation doesn’t imply causation*!",
    "text": "Corrrelation doesn’t imply causation*!\n\nThis might seem obvious, but make sure to not write your analysis results in a way that implies causation if the study design doesn’t warrant it (such as an observational study).\nBeware of spurious correlations: http://www.tylervigen.com/spurious-correlations\n\n\n\n*Caveat: there is a whole field of statistics/epidemiology on causal inference. https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#whats-next",
    "href": "slides/04_SLR_Inf_Pred.html#whats-next",
    "title": "SLR: Inference and Prediction",
    "section": "What’s next?",
    "text": "What’s next?\n \n\n\n\nSLR 1"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#widehatsigma2-needed-ingredient-for-inference",
    "href": "slides/04_SLR_Inf_Pred.html#widehatsigma2-needed-ingredient-for-inference",
    "title": "SLR: Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): Needed ingredient for inference",
    "text": "\\(\\widehat\\sigma^2\\): Needed ingredient for inference\n\nRecall our population model residuals are distributed by \\(\\epsilon \\sim N(0, \\sigma^2)\\)\n\nAnd our estimated residuals are \\(\\widehat\\epsilon \\sim N(0, \\widehat\\sigma^2)\\)\n\nHence, the variance of the errors (residuals) is estimated by \\(\\widehat{\\sigma}^2\\)\n\n\n\\[\\widehat{\\sigma}^2 = S_{y|x}^2= \\frac{1}{n-2}\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 =\\frac{1}{n-2}SSE = MSE\\]"
  },
  {
    "objectID": "slides/out_try.html",
    "href": "slides/out_try.html",
    "title": "SLR: Inference and Prediction",
    "section": "",
    "text": "gapm &lt;- read_csv(\"data/lifeexp_femlit_2011.csv\")\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n             data = gapm)\n\n# some output, but not complete\nmodel1\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nCoefficients:\n              (Intercept)  female_literacy_rate_2011  \n                  50.9279                     0.2322  \n\n# better output\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n# Regression table:\n# library(broom)  # for tidy() command\n# library(gt)  # for gt() command\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "slides/05_SLR_Eval.html",
    "href": "slides/05_SLR_Eval.html",
    "title": "SLR: More inference + Evaluation",
    "section": "",
    "text": "Identify different sources of variation in an Analysis of Variance (ANOVA) table\nUsing the F-test, determine if there is enough evidence that population slope \\(\\beta_1\\) is not 0\nCalculate and interpret the coefficient of determination\nDescribe the model assumptions made in linear regression using ordinary least squares\n\n\n\n\n\nLesson 1 of SLR:\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 2 of SLR:\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#topics",
    "href": "slides/05_SLR_Eval.html#topics",
    "title": "SLR: Inference Revisited",
    "section": "Topics",
    "text": "Topics"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#what-are-the-line-conditions",
    "href": "slides/05_SLR_Eval.html#what-are-the-line-conditions",
    "title": "SLR: Model Evaluation",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#l-linearity-of-relationship-between-variables",
    "href": "slides/05_SLR_Eval.html#l-linearity-of-relationship-between-variables",
    "title": "SLR: Model Evaluation",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#i-independence-of-the-residuals-y-values",
    "href": "slides/05_SLR_Eval.html#i-independence-of-the-residuals-y-values",
    "title": "SLR: Model Evaluation",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#n-normality-of-the-residuals-1",
    "href": "slides/05_SLR_Eval.html#n-normality-of-the-residuals-1",
    "title": "SLR: Model Evaluation",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#extract-models-residuals-in-r",
    "href": "slides/05_SLR_Eval.html#extract-models-residuals-in-r",
    "title": "SLR: Model Evaluation",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#check-normality-with-usual-distribution-plots",
    "href": "slides/05_SLR_Eval.html#check-normality-with-usual-distribution-plots",
    "title": "SLR: Model Evaluation",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides/05_SLR_Eval.html#normal-qq-plots-qq-quantile-quantile",
    "title": "SLR: Model Evaluation",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-15",
    "href": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-15",
    "title": "SLR: Model Evaluation",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-25",
    "href": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-25",
    "title": "SLR: Model Evaluation",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-35",
    "href": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-35",
    "title": "SLR: Model Evaluation",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-45",
    "href": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-45",
    "title": "SLR: Model Evaluation",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-55",
    "href": "slides/05_SLR_Eval.html#examples-of-normal-qq-plots-55",
    "title": "SLR: Model Evaluation",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#qq-plot-of-residuals-of-model1",
    "href": "slides/05_SLR_Eval.html#qq-plot-of-residuals-of-model1",
    "title": "SLR: Model Evaluation",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides/05_SLR_Eval.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "SLR: Model Evaluation",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides/05_SLR_Eval.html#randomly-generated-normal-qq-plots-n100",
    "title": "SLR: Model Evaluation",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides/05_SLR_Eval.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "SLR: Model Evaluation",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides/05_SLR_Eval.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "SLR: Model Evaluation",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#back-to-our-example",
    "href": "slides/05_SLR_Eval.html#back-to-our-example",
    "title": "SLR: Model Evaluation",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#residual-plot",
    "href": "slides/05_SLR_Eval.html#residual-plot",
    "title": "SLR: Model Evaluation",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \".rownames\"                  \"life_expectancy_years_2011\"\n[3] \"female_literacy_rate_2011\"  \".fitted\"                   \n[5] \".resid\"                     \".hat\"                      \n[7] \".sigma\"                     \".cooksd\"                   \n[9] \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "SLR: Model Evaluation",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#r2-coefficient-of-determination-12",
    "href": "slides/05_SLR_Eval.html#r2-coefficient-of-determination-12",
    "title": "SLR: Model Evaluation",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\overline{y})^2}\n{\\sum_{i=1}^n(y_i-\\overline{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#r2-coefficient-of-determination-22",
    "href": "slides/05_SLR_Eval.html#r2-coefficient-of-determination-22",
    "title": "SLR: Model Evaluation",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#r2-in-summary-r-output",
    "href": "slides/05_SLR_Eval.html#r2-in-summary-r-output",
    "title": "SLR: Model Evaluation",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366\n\n\n\n\nSLR 3"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html",
    "href": "slides/06_SLR_Diagnostics.html",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "We have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#topics",
    "href": "slides/06_SLR_Diagnostics.html#topics",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Topics",
    "text": "Topics\n\nLINE assumptions\nchecking assumptions\nresidual analysis\noutlier detection"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#what-are-the-line-conditions",
    "href": "slides/06_SLR_Diagnostics.html#what-are-the-line-conditions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#l-linearity-of-relationship-between-variables",
    "href": "slides/06_SLR_Diagnostics.html#l-linearity-of-relationship-between-variables",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\n\n\n\n\n\nIs the association between the variables linear?\n\n\n\n\nDiagnostic tool: Scatterplot of \\(X\\) vs. \\(Y\\)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#i-independence-of-the-residuals-y-values",
    "href": "slides/06_SLR_Diagnostics.html#i-independence-of-the-residuals-y-values",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\n\n \n\nDiagnostic tool: reviewing the study design and not by inspecting the data"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-normality-of-the-residuals-1",
    "href": "slides/06_SLR_Diagnostics.html#n-normality-of-the-residuals-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#extract-models-residuals-in-r",
    "href": "slides/06_SLR_Diagnostics.html#extract-models-residuals-in-r",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#check-normality-with-usual-distribution-plots",
    "href": "slides/06_SLR_Diagnostics.html#check-normality-with-usual-distribution-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides/06_SLR_Diagnostics.html#normal-qq-plots-qq-quantile-quantile",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-15",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-15",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-25",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-25",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-35",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-35",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-45",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-45",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-55",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots-55",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#qq-plot-of-residuals-of-model1",
    "href": "slides/06_SLR_Diagnostics.html#qq-plot-of-residuals-of-model1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides/06_SLR_Diagnostics.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides/06_SLR_Diagnostics.html#randomly-generated-normal-qq-plots-n100",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#back-to-our-example",
    "href": "slides/06_SLR_Diagnostics.html#back-to-our-example",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Back to our example",
    "text": "Back to our example\n\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with \\(n = 80\\)\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#residual-plot",
    "href": "slides/06_SLR_Diagnostics.html#residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\n\n\nnames(aug1)\n\n[1] \".rownames\"                  \"life_expectancy_years_2011\"\n[3] \"female_literacy_rate_2011\"  \".fitted\"                   \n[5] \".resid\"                     \".hat\"                      \n[7] \".sigma\"                     \".cooksd\"                   \n[9] \".std.resid\"                \n\n\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides/06_SLR_Diagnostics.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#r2-coefficient-of-determination-12",
    "href": "slides/06_SLR_Diagnostics.html#r2-coefficient-of-determination-12",
    "title": "SLR: Model Diagnostics",
    "section": "\\(R^2\\) = Coefficient of determination (1/2)",
    "text": "\\(R^2\\) = Coefficient of determination (1/2)\n\nRecall that the correlation coefficient \\(r\\) measures the strength of the linear relationship between two numerical variables\n\\(R^2\\) is usually used to measure the strength of a linear fit\n\nFor a simple linear regression model (one numerical predictor), \\(R^2\\) is just the square of the correlation coefficient\n\nIn general, \\(R^2\\) is the proportion of the variability of the dependent variable that is explained by the independent variable(s)\n\n\\[R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n= \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}\\] \\[R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}\\] where \\(\\frac{s_{\\textrm{residuals}}^2}{s_y^2}\\) is the proportion of “unexplained” variability in the \\(y\\) values,\nand thus \\(R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}\\) is the proportion of “explained” variability in the \\(y\\) values"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#r2-coefficient-of-determination-22",
    "href": "slides/06_SLR_Diagnostics.html#r2-coefficient-of-determination-22",
    "title": "SLR: Model Diagnostics",
    "section": "\\(R^2\\) = Coefficient of determination (2/2)",
    "text": "\\(R^2\\) = Coefficient of determination (2/2)\n\nRecall, \\(-1&lt;r&lt;1\\)\nThus, \\(0&lt;R^2&lt;1\\)\nIn practice, we want “high” \\(R^2\\) values, i.e. \\(R^2\\) as close to 1 as possible.\n\nCalculating \\(R^2\\) in R using glance() from the broom package:\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(model1)$r.squared\n\n[1] 0.4109366\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nA model can have a high \\(R^2\\) value when there is a curved pattern.\nAlways first check whether a linear model is reasonable or not."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#r2-in-summary-r-output",
    "href": "slides/06_SLR_Diagnostics.html#r2-in-summary-r-output",
    "title": "SLR: Model Diagnostics",
    "section": "\\(R^2\\) in summary() R output",
    "text": "\\(R^2\\) in summary() R output\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n\nCompare to the square of the correlation coefficient \\(r\\):\n\nr &lt;- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#vfknmbv",
    "href": "slides/04_SLR_Inf_Pred.html#vfknmbv",
    "title": "SLR: Inference and Prediction",
    "section": "vfknmbv",
    "text": "vfknmbv\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\n# Regression table:\n# library(broom)  # for tidy() command\n# library(gt)  # for gt() command\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "homework/HW3.html#section-1",
    "href": "homework/HW3.html#section-1",
    "title": "Homework 3",
    "section": "(3)",
    "text": "(3)\nAssess the normality of the model’s (ordinary) residuals by creating a histogram, density plot, and boxplot of the residuals to visually inspect the distribution of the residuals, and describe any deviations from normality."
  },
  {
    "objectID": "homework/HW3.html#section-2",
    "href": "homework/HW3.html#section-2",
    "title": "Homework 3",
    "section": "(4)",
    "text": "(4)\nAssess the normality of the model’s (ordinary) residuals by creating a normal probability plot of the residuals. Compare the normality probability plot to 8 such plots simulated from normal data, and discuss why or why not the residuals could have come from a normal distribution."
  },
  {
    "objectID": "homework/HW3.html#section-3",
    "href": "homework/HW3.html#section-3",
    "title": "Homework 3",
    "section": "(5)",
    "text": "(5)\nTest the normality of the model’s (ordinary) residuals and comment on whether the test’s conclusion is consistent with your visual inspection or not. Make sure to include the hypotheses and a conclusion to the test based on the p-value."
  },
  {
    "objectID": "homework/HW3.html#section-4",
    "href": "homework/HW3.html#section-4",
    "title": "Homework 3",
    "section": "(6)",
    "text": "(6)\nCreate a residual plot using ggplot and the standardized residuals and discuss what this shows us in terms of whether the model assumptions have been met or not."
  },
  {
    "objectID": "homework/HW3.html#section-5",
    "href": "homework/HW3.html#section-5",
    "title": "Homework 3",
    "section": "(7)",
    "text": "(7)\nDetermine whether there are any observations with high leverage. If there are observations with high leverage, identify their coordinates and describe how they relate to the other observations. Why would these points have high leverage compared to the other observations? Do you think removing the points would change the linear model much? (you do not need to remove the points and rerun the model, just comment on whether you think they are influential)"
  },
  {
    "objectID": "homework/HW3.html#section-6",
    "href": "homework/HW3.html#section-6",
    "title": "Homework 3",
    "section": "(8)",
    "text": "(8)\nDetermine whether there are any observations with high Cook’s distance. If there are observations with high Cook’s distance, identify their coordinates and describe how they relate to the other observations. Why would these points have high Cook’s distance compared to the other observations? Do you think removing the points would change the linear model much? (you do not need to remove the points and rerun the model, just comment on whether you think they are influential)"
  },
  {
    "objectID": "homework/HW3.html#section-7",
    "href": "homework/HW3.html#section-7",
    "title": "Homework 3",
    "section": "(9)",
    "text": "(9)\nCreate histograms and density plots of the dependent and independent variables and describe their distribution shapes."
  },
  {
    "objectID": "homework/HW3.html#section-8",
    "href": "homework/HW3.html#section-8",
    "title": "Homework 3",
    "section": "(10)",
    "text": "(10)\nUse Tukey’s ladder of transformations to choose two possible transformations for the dependent variable. Explain why you chose them. Note: questions below will ask about model fit with the transformations. For now, just explain why you chose the ones that you did."
  },
  {
    "objectID": "homework/HW3.html#section-9",
    "href": "homework/HW3.html#section-9",
    "title": "Homework 3",
    "section": "(11)",
    "text": "(11)\nUse Tukey’s ladder of transformations to choose two possible transformations for the independent variable. Explain why you chose them. Note: questions below will ask about model fit with the transformations. For now, just explain why you chose the ones that you did."
  },
  {
    "objectID": "homework/HW3.html#section-10",
    "href": "homework/HW3.html#section-10",
    "title": "Homework 3",
    "section": "(12)",
    "text": "(12)\nAdd the 4 transformations you chose above (2 for the dependent variable and 2 for the independent variable) to the dataset."
  },
  {
    "objectID": "homework/HW3.html#section-11",
    "href": "homework/HW3.html#section-11",
    "title": "Homework 3",
    "section": "(13)",
    "text": "(13)\nCreate scatterplots using the transformed variables and discuss whether any of the transformations improve the model fit and why (or why not). Include plots with just the x or y variables transformed, and at least one plot with both the x and y variables transformed."
  },
  {
    "objectID": "homework/HW3.html#section-12",
    "href": "homework/HW3.html#section-12",
    "title": "Homework 3",
    "section": "(14)",
    "text": "(14)\nRun the various transformed models and save the output to use for the diagnostic questions below."
  },
  {
    "objectID": "homework/HW3.html#section-13",
    "href": "homework/HW3.html#section-13",
    "title": "Homework 3",
    "section": "(15)",
    "text": "(15)\nCompare the normal QQ plots of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW3.html#section-14",
    "href": "homework/HW3.html#section-14",
    "title": "Homework 3",
    "section": "(16)",
    "text": "(16)\nCompare the residual plots of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW3.html#section-15",
    "href": "homework/HW3.html#section-15",
    "title": "Homework 3",
    "section": "(17)",
    "text": "(17)\nCompare the leverage & Cook’s distance of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW3.html#section-16",
    "href": "homework/HW3.html#section-16",
    "title": "Homework 3",
    "section": "(18)",
    "text": "(18)\nCompare the \\(R^2\\) values and F-test p-values of the different models and discuss whether any of the transformations improve the model fit and why (or why not)."
  },
  {
    "objectID": "homework/HW3.html#section-17",
    "href": "homework/HW3.html#section-17",
    "title": "Homework 3",
    "section": "(19)",
    "text": "(19)\nWhich of the models would you recommend using for analyses? Discuss why you chose the model and why you did not choose the other models."
  },
  {
    "objectID": "homework/HW3.html#section-18",
    "href": "homework/HW3.html#section-18",
    "title": "Homework 3",
    "section": "(19)",
    "text": "(19)\nWhich of the models would you recommend using for analyses? Discuss why you chose the model and why you did not choose the other models."
  },
  {
    "objectID": "homework/HW3.html#section-19",
    "href": "homework/HW3.html#section-19",
    "title": "Homework 3",
    "section": "(19)",
    "text": "(19)\nWhich of the models would you recommend using for analyses? Discuss why you chose the model and why you did not choose the other models."
  },
  {
    "objectID": "homework/HW2.html#question-1",
    "href": "homework/HW2.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nThis homework assignment is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. It is SIMILAR but does not exactly match the data in the article. It contains information on complete cases (i.e. excludes participants who had missing data on one or more variables of interest) who suffered a stroke. The two variables we are interested in are:\n\nCovariate: Fatalism (larger values indicate that the individual feels less control of their life)\n\nScores range from 8 to 40\n\nOutcome: Depression (larger values imply increased depression)\n\nScores range from 0 to 27\n\n\nFor our homework purposes we will assume they are continuous.\n\nfatal_dep = read_sas(here(\"./homework/data/completedata.sas7bdat\"))\n\n\nPart a\nPlot the data, with title and axis labels, for Depression (y-axis) vs. Fatalism (x-axis). Comment on what you see.\n\n\nPart b\nFit a linear regression model to estimate the association between the predictor Fatalism and the outcome Depression.\nInterpret the slope and intercept. Does the intercept make sense?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for fatalism and depression are scores.\n\n\n\n\nPart c\nIn your dataset, make a new variable FatalismC, equal to Fatalism centered at its median (C is for centered).\n\\[\n\\text{FatalismC} = \\text{Fatalism} - \\text{median of Fatalism}\n\\]\nThis is one way of centering a variable, and can be used when the intercept estimate does not make sense. (Hint: the mutate() function will work well here!)\nPlot the data, with title and axis labels, for Depression (y-axis) vs. FatalismC (x-axis).\n\n\nPart d\nRe-run the regression from Part b using this new variable for FatalismC. Interpret the new slope and intercept. Which of the following are the same as Part b: intercept, slope?\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include the confidence interval. Whenever asked to interpret coefficients, you must include confidence intervals. Also, the “units” for the centered fatalism is still the score.\n\n\n\n\nPart e\nFrom the above interpretations, what would be the equivalent conclusion from a hypothesis test for the association between Depression and Fatalism?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the whole process for the hypothesis test. You only need to state whether it is rejected or not and site the confidence interval as evidence."
  },
  {
    "objectID": "homework/HW2.html#chapter-7",
    "href": "homework/HW2.html#chapter-7",
    "title": "Homework 2",
    "section": "Chapter 7",
    "text": "Chapter 7\nUse the data from Chapter 5 Question 9 to answer the following questions. Use the log-transformed values as given in the dataset.\nNote: the question numbers below do not refer to questions from the textbook. Complete the problems below instead of the ones in the book.\n\n(1)\nUse R to create the ANOVA table for the regression described in the exercise.\n\n\n(2)\nUsing the values in the Df and Sum Sq columns, show how the remaining values in the table are calculated.\n\n\n(3)\nCalculate the SSY and its degrees of freedom. Use these values to calculate the standard deviation of the outcome variable. \n\n\n(4)\nWhat are the hypotheses being tested in the ANOVA table? Make sure to include a description of the parameter being tested in the context of the research question.\n\n\n(5)\nUsing just the values in the ANOVA table, find the value of the t-distribution test statistic for testing the slope that one would find in the regression output of the linear model. What are the degrees of freedom for that t-distribution?\n\n\n(6)\nShow the regression output for the linear model. Using just the regression output, calculate the SSR and SSE values in the ANOVA table."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#lets-revisit-the-regression-analysis-process",
    "href": "slides/04_SLR_Inf_Pred.html#lets-revisit-the-regression-analysis-process",
    "title": "SLR: Inference and Prediction",
    "section": "Let’s revisit the regression analysis process",
    "text": "Let’s revisit the regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html",
    "href": "slides/04_SLR_Inf_Pred.html",
    "title": "SLR: Inference and Prediction",
    "section": "",
    "text": "Estimate the variance of the residuals\nUsing a hypothesis test, determine if there is enough evidence that population slope \\(\\beta_1\\) is not 0 (applies to \\(\\beta_0\\) as well)\nCalculate and report the estimate and confidence interval for the population slope \\(\\beta_1\\) (applies to \\(\\beta_0\\) as well)\nCalculate and report the estimate and confidence interval for the expected/mean response given \\(X\\)\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\nWe fit Gapminder data with female literacy rate as our independent variable and life expectancy as our dependent variable\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~\n               female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1) %&gt;% gt() %&gt;% \n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 108 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 108 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n \nThe (population) regression model is denoted by:\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me",
    "href": "slides/04_SLR_Inf_Pred.html#widehatsigma2-i-hope-r-can-calculate-that-for-me",
    "title": "SLR: Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me…",
    "text": "\\(\\widehat\\sigma^2\\): I hope R can calculate that for me…\n\nThe standard deviation \\(\\widehat{\\sigma}\\) is given in the R output as the Residual standard error\n\n\\(4^{th}\\) line from the bottom in the summary() output of the model:\n\n\n\n(m1_sum = summary(model1))\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nm1_sum$sigma\n\n[1] 6.142157\n\n# number of observations (pairs of data) used to run the model\nnobs(model1) \n\n[1] 80"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#widehatsigma2-to-sse",
    "href": "slides/04_SLR_Inf_Pred.html#widehatsigma2-to-sse",
    "title": "SLR: Inference and Prediction",
    "section": "\\(\\widehat\\sigma^2\\) to SSE",
    "text": "\\(\\widehat\\sigma^2\\) to SSE\n\nRecall how we minimized the SSE to find our line of best fit\nSSE and \\(\\widehat\\sigma^2\\) are closely related:\n\n\\[\\begin{aligned}\n\\widehat{\\sigma}^2 & = \\frac{1}{n-2}SSE\\\\\n6.142^2 & = \\frac{1}{80-2}SSE\\\\\nSSE & = 78 \\cdot 6.142^2 = 2942.48\n\\end{aligned}\\]\n\n2942.48 is the smallest sums of squares of all possible regression lines through the data"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#steps-in-hypothesis-testing",
    "href": "slides/04_SLR_Inf_Pred.html#steps-in-hypothesis-testing",
    "title": "SLR: Inference and Prediction",
    "section": "Steps in hypothesis testing",
    "text": "Steps in hypothesis testing"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "General steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "General steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution.\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[t^* = \\frac{ \\widehat\\beta_1 - \\beta_1}{ \\text{SE}_{\\widehat\\beta_1}} = \\frac{ \\widehat\\beta_1}{ \\text{SE}_{\\widehat\\beta_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(t &gt; t*)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(t &gt; t*)\\))."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "href": "slides/04_SLR_Inf_Pred.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "title": "SLR: Inference and Prediction",
    "section": "Let’s remind ourselves of the model that we fit last lesson",
    "text": "Let’s remind ourselves of the model that we fit last lesson\n\nWe fit Gapminder data with female literacy rate as our independent variable and life expectancy as our dependent variable\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~\n               female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1) %&gt;% gt() %&gt;% \n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#inference-for-population-slope-beta_1-1",
    "href": "slides/04_SLR_Inf_Pred.html#inference-for-population-slope-beta_1-1",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for population slope \\(\\beta_1\\)",
    "text": "Inference for population slope \\(\\beta_1\\)\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\n# Get regression table:\ntidy(model1, conf.int = TRUE) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 45) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295\n  \n  \n  \n\n\n\n# conf.int = TRUE part is new! \n\n\\[\\begin{align}\n\\widehat{y} =& b_0 + b_1 \\cdot x\\\\\n\\widehat{\\text{life expectancy}} =& 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{align}\\]\n\nWhat are \\(H_0\\) and \\(H_A\\)?"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-ci-and-hypothesis-test-1",
    "href": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-ci-and-hypothesis-test-1",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for the population slope: CI and hypothesis test",
    "text": "Inference for the population slope: CI and hypothesis test\n\n\nPopulation model\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma)\\)\n\\(\\sigma\\) is the variability (SD) of the residuals\n\nSample best-fit (least-squares) line:\n\\[\\widehat{y} = b_0 + b_1 \\cdot x \\]\nNote: Some sources use \\(\\widehat{\\beta}\\) instead of \\(b\\).\n\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)\n\n\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#fitted-line-is-derived-from-the-population-slr-model",
    "href": "slides/04_SLR_Inf_Pred.html#fitted-line-is-derived-from-the-population-slr-model",
    "title": "SLR: Inference and Prediction",
    "section": "Fitted line is derived from the population SLR model",
    "text": "Fitted line is derived from the population SLR model\n \nThe (population) regression model is denoted by:\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#do-we-trust-our-estimate-widehatbeta_1",
    "href": "slides/04_SLR_Inf_Pred.html#do-we-trust-our-estimate-widehatbeta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Do we trust our estimate \\(\\widehat\\beta_1\\)?",
    "text": "Do we trust our estimate \\(\\widehat\\beta_1\\)?\n\nSo far, we have shown that we think the estimate is 0.232  \n\\(\\widehat\\beta_1\\) uses our sample data to estimate the population parameter \\(\\beta_1\\)  \nInference helps us figure out mathematically how much we trust our best-fit line  \nAre we certain that the relationship between \\(X\\) and \\(Y\\) that we estimated reflects the true, underlying relationship?"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-1",
    "href": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-1",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci",
    "href": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for the population slope: hypothesis test and CI",
    "text": "Inference for the population slope: hypothesis test and CI\n\n\n\n\nPopulation model\n\n\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\\(\\sigma^2\\) is the variance of the residuals\n\n\n\n\nSample best-fit (least-squares) line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\nNote: Some sources use \\(b\\) instead of \\(\\widehat{\\beta}\\)\n\n\n\n\n\n \nWe have two options for inference:\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis. Often, we are curious if the slope is 0 or not:\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#making-the-needed-calculations-for-the-test",
    "href": "slides/04_SLR_Inf_Pred.html#making-the-needed-calculations-for-the-test",
    "title": "SLR: Inference and Prediction",
    "section": "Making the needed calculations for the test",
    "text": "Making the needed calculations for the test\n\nCompute the value of the test statistic\nCalculate the p-value\nWrite conclusion for hypothesis test\n\n\ntidy(model1, conf.int = TRUE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n45.6314348\n56.2243615\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n0.1695284\n0.2948619\n  \n  \n  \n\n\n\n\nCalculate the test statistic using the values in the regression table:\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\nCompare this test statistic value to the one from the regression table above"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#some-important-notes",
    "href": "slides/04_SLR_Inf_Pred.html#some-important-notes",
    "title": "SLR: Inference and Prediction",
    "section": "Some important notes",
    "text": "Some important notes\n\nToday we are discussing the hypothesis test for a single coefficient\n\n \n\nThe test statistic for a single coefficient follows a Student’s t-distribution\n \n\nIt can also follow an F-distribution, but we will discuss this more with multiple linear regression and multi-level categorical covariates\n\n\n \n\nSingle coefficient testing can be done on any coefficient, but it is most useful for continuous covariates or binary covariates\n \n\nThis is because testing the single coefficient will still tell us something about the overall relationship between the covariate and the outcome\n\n \n\nWe will talk more about this with multiple linear regression and multi-level categorical covariates"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-12",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-12",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/2)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/2)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis. Often, we are curious if the slope is 0 or not:\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-22",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-22",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/2)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/2)\n\n\n\nCompute the value of the test statistic\n\n\n\n \n\nOption 1: Get the test statistic value (\\(t^*\\)) from R\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\nOption 2: Calculate the test statistic using the values in the regression table:\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n(TestStat &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#ags",
    "href": "slides/04_SLR_Inf_Pred.html#ags",
    "title": "SLR: Inference and Prediction",
    "section": "ags",
    "text": "ags\n\nCalculate the p-value\nWrite conclusion for hypothesis test"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-22-1",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-22-1",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/2)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/2)\n\n\n\nCalculate the p-value\n\n\n\n\nThe \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true\nWe know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\n\nOption 2: Use the regression table output\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-22-2",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-22-2",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/2)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/2)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#reporting-the-coefficient",
    "href": "slides/04_SLR_Inf_Pred.html#reporting-the-coefficient",
    "title": "SLR: Inference and Prediction",
    "section": "Reporting the coefficient",
    "text": "Reporting the coefficient"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#note-on-hypothesis-testing-using-r",
    "href": "slides/04_SLR_Inf_Pred.html#note-on-hypothesis-testing-using-r",
    "title": "SLR: Inference and Prediction",
    "section": "Note on hypothesis testing using R",
    "text": "Note on hypothesis testing using R\n\nWe can basically skip Step 5 if we are using the “Option 2” route\n\n \n\nIn our assignments: if you use Option 2, Step 5 is optional\n\nUnless I specifically ask for the test statistic!!"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-14",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-14",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (1/4)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nOption 1: Calculate the test statistic using the values in the regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n(TestStat_b1 &lt;- model1_b1$estimate / model1_b1$std.error)\n\n[1] 7.376557\n\n\n\nOption 2: Get the test statistic value (\\(t^*\\)) from R\n\n\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nThe \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true\nWe know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat_b1, df=80-2, lower.tail=F))\n\n[1] 1.501286e-10\n\n\n\nOption 2: Use the regression table output\n\n\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-14",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-14",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (1/4)",
    "text": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (1/4)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the intercept is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_0 = 0\\\\\n\\text{vs. } H_A&: \\beta_0 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThis is the same as the slope. The test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-24",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-24",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n \n\nOption 1: Get the test statistic value (\\(t^*\\)) from R\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\nOption 2: Calculate the test statistic using the values in the regression table:\n\n\n# model1_b0 is regression table restricted to the intercept\nmodel1_b0 &lt;-tidy(model1) %&gt;% filter(term == \"(Intercept)\")\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n  \n  \n  \n\n\n\n(TestStat_b0 &lt;- model1_b0$estimate / model1_b0$std.error)\n\n[1] 19.1429"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-34",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-34",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nThe \\(p\\)-value is the probability of obtaining a test statistic just as extreme or more extreme than the observed test statistic assuming the null hypothesis \\(H_0\\) is true\nWe know the probability distribution of the test statistic (the null distribution) assuming \\(H_0\\) is true\nStatistical theory tells us that the test statistic \\(t\\) can be modeled by a \\(t\\)-distribution with \\(df = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat_b0, df=80-2, lower.tail=F))\n\n[1] 3.325312e-31\n\n\n\nOption 2: Use the regression table output\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-44",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-example-hypothesis-test-for-population-intercept-beta_0-44",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population intercept \\(\\beta_0\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the intercept is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that the intercept for the association between average female life expectancy and female literacy rates is different from 0 (p-value &lt; 0.0001).\n   \n\nNote: if we fail to reject \\(H_0\\), then we could decide to remove the intercept from the model to force the regression line to go through the origin (0,0) if it makes sense to do so for the application."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci-1",
    "href": "slides/04_SLR_Inf_Pred.html#inference-for-the-population-slope-hypothesis-test-and-ci-1",
    "title": "SLR: Inference and Prediction",
    "section": "Inference for the population slope: hypothesis test and CI",
    "text": "Inference for the population slope: hypothesis test and CI\n\n\n\n\nPopulation model\n\n\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\\(\\sigma^2\\) is the variance of the residuals\n\n\n\n\nSample best-fit (least-squares) line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\nNote: Some sources use \\(b\\) instead of \\(\\widehat{\\beta}\\)\n\n\n\n\n\n \nWe have two options for inference:\n\nConduct the hypothesis test\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote: R reports p-values for 2-sided tests\n\nConstruct a 95% confidence interval for the population slope \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#se",
    "href": "slides/04_SLR_Inf_Pred.html#se",
    "title": "SLR: Inference and Prediction",
    "section": "SE?",
    "text": "SE?\nHow is \\(\\text{SE}_{b_1}\\) calculated? See next slide.\n\n\ntidy(model1, conf.int = TRUE)\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)             50.9      2.66       19.1  3.33e-31   45.6      56.2  \n2 female_literacy_rate…    0.232    0.0315      7.38 1.50e-10    0.170     0.295"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#confidence-interval-for-population-slope-beta_1",
    "href": "slides/04_SLR_Inf_Pred.html#confidence-interval-for-population-slope-beta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Confidence interval for population slope \\(\\beta_1\\)",
    "text": "Confidence interval for population slope \\(\\beta_1\\)\nRecall the general CI formula:\n\\[\\widehat{\\beta}_1 \\pm t_{\\alpha, n-2}^* \\cdot SE_{\\widehat{\\beta}_1}\\]\nTo construct the confidence interval, we need to:\n\nSet our \\(\\alpha\\)-level\nFind \\(\\widehat\\beta_1\\)\nCalculate the \\(t_{n-2}^*\\)\nCalculate \\(SE_{\\widehat{\\beta}_1}\\)"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1",
    "href": "slides/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1",
    "title": "SLR: Inference and Prediction",
    "section": "Standard error of fitted slope \\(\\widehat\\beta_1\\)",
    "text": "Standard error of fitted slope \\(\\widehat\\beta_1\\)\n\n   \n\n\n\\[\\text{SE}_{\\widehat\\beta_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{\\widehat\\beta_1}\\) is the variability of the statistic \\(\\widehat\\beta_1\\)\n\n\n   \n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1-1",
    "href": "slides/04_SLR_Inf_Pred.html#standard-error-of-fitted-slope-widehatbeta_1-1",
    "title": "SLR: Inference and Prediction",
    "section": "Standard error of fitted slope \\(\\widehat\\beta_1\\)",
    "text": "Standard error of fitted slope \\(\\widehat\\beta_1\\)\n\n\n\n\\[\\text{SE}_{\\widehat\\beta_1} = \\frac{s_{\\textrm{residuals}}}{s_x\\sqrt{n-1}}\\]\n\n\\(\\text{SE}_{\\widehat\\beta_1}\\) is the variability of the statistic \\(\\widehat\\beta_1\\)\n\n\n   \n\n\n\n\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\n\n\n\n\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\n\n\n\\(n\\) is the sample size, or the number of (complete) pairs of points"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12",
    "href": "slides/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12",
    "title": "SLR: Inference and Prediction",
    "section": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)",
    "text": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)\n\n\nOption 1: Calculate using the formula\n\n\nglance(model1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n# standard deviation of the residuals (Residual standard error in summary() output)\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n# standard deviation of x's\n(s_x &lt;- sd(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 21.95371\n\n# number of pairs of complete observations\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(se_b1 &lt;- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output\n\n[1] 0.03147744"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12-1",
    "href": "slides/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-12-1",
    "title": "SLR: Inference and Prediction",
    "section": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)",
    "text": "Calculating standard error for \\(\\widehat\\beta_1\\) (1/2)\n\n\nOption 2: Use regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-22",
    "href": "slides/04_SLR_Inf_Pred.html#calculating-standard-error-for-widehatbeta_1-22",
    "title": "SLR: Inference and Prediction",
    "section": "Calculating standard error for \\(\\widehat\\beta_1\\) (2/2)",
    "text": "Calculating standard error for \\(\\widehat\\beta_1\\) (2/2)\n\n\nOption 2: Use regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b1 &lt;-tidy(model1) %&gt;% filter(term == \"female_literacy_rate_2011\")\nmodel1_b1 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 45) %&gt;% fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n0.2322\n0.0315\n7.3766\n0.0000"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-14",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-14",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (1/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (1/4)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the intercept is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_0 = 0\\\\\n\\text{vs. } H_A&: \\beta_0 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThis is the same as the slope. The test statistic is \\(t\\), and follows a Student’s t-distribution."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-24",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-24",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (2/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nOption 1: Calculate the test statistic using the values in the regression table\n\n\n# recall model1_b1 is regression table restricted to b1 row\nmodel1_b0 &lt;-tidy(model1) %&gt;% filter(term == \"(Intercept)\")\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n  \n  \n  \n\n\n\n(TestStat_b0 &lt;- model1_b0$estimate / model1_b0$std.error)\n\n[1] 19.1429\n\n\n\nOption 2: Get the test statistic value (\\(t^*\\)) from R\n\n\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-34",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-34",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (3/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n \n\nOption 1: Use pt() and our calculated test statistic\n\n\n(pv = 2*pt(TestStat_b0, df=80-2, lower.tail=F))\n\n[1] 3.325312e-31\n\n\n \n\nOption 2: Use the regression table output\n\n\nmodel1_b0 %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9279\n2.660407\n19.1429\n3.325312e-31"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-44",
    "href": "slides/04_SLR_Inf_Pred.html#life-expectancy-ex-hypothesis-test-for-population-intercept-beta_0-44",
    "title": "SLR: Inference and Prediction",
    "section": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (4/4)",
    "text": "Life expectancy ex: hypothesis test for population intercept \\(\\beta_0\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the intercept is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that the intercept for the association between average female life expectancy and female literacy rates is different from 0 (p-value &lt; 0.0001).\n   \n\nNote: if we fail to reject \\(H_0\\), then we could decide to remove the intercept from the model to force the regression line to go through the origin (0,0) if it makes sense to do so for the application."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-12",
    "href": "slides/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-12",
    "title": "SLR: Inference and Prediction",
    "section": "Calculate CI for population slope \\(\\beta_1\\) (1/2)",
    "text": "Calculate CI for population slope \\(\\beta_1\\) (1/2)\n\n\n\\[\\widehat{\\beta}_1  \\pm t^*\\cdot SE_{\\beta_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\nOption 1: Calculate using each value\n\nSave values needed for CI:\n\nb1 &lt;- model1_b1$estimate\nSE_b1 &lt;- model1_b1$std.error\n\n\nnobs(model1) # sample size n\n\n[1] 80\n\n(tstar &lt;- qt(.975, df = 80-2))\n\n[1] 1.990847\n\n\nUse formula to calculate each bound\n\n(CI_LB &lt;- b1 - tstar*SE_b1)\n\n[1] 0.1695284\n\n(CI_UB &lt;- b1 + tstar*SE_b1)\n\n[1] 0.2948619"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-22",
    "href": "slides/04_SLR_Inf_Pred.html#calculate-ci-for-population-slope-beta_1-22",
    "title": "SLR: Inference and Prediction",
    "section": "Calculate CI for population slope \\(\\beta_1\\) (2/2)",
    "text": "Calculate CI for population slope \\(\\beta_1\\) (2/2)\n\n\n\\[\\widehat{\\beta}_1  \\pm t^*\\cdot SE_{\\beta_1}\\]\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\).\n\n\n\nOption 2: Use the regression table\n\n\ntidy(model1, conf.int = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate",
    "href": "slides/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate",
    "title": "SLR: Inference and Prediction",
    "section": "Reporting the coefficient estimate",
    "text": "Reporting the coefficient estimate\n\nWhen we report our results to someone else, we don’t usually show them our full hypothesis test\n\nIn an informal setting, someone may want to see it\n\nTypically, we report the estimate with the confidence interval\n\nFrom the confidence interval, your audience can also deduce the results of a hypothesis test\n\nOnce we found our CI, we often just write the interpretation of the coefficient estimate:\n\n\n\nGeneral statement for population slope inference\n\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected average increase of \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\nIn our example: For every 1% increase in female literacy rate, the average life expectancy is expected to increase, on average, 0.232 years (95% CI: 0.170, 0.295)."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question",
    "href": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#finding-a-mean-response-given-a-value-of-our-independent-variable",
    "href": "slides/04_SLR_Inf_Pred.html#finding-a-mean-response-given-a-value-of-our-independent-variable",
    "title": "SLR: Inference and Prediction",
    "section": "Finding a mean response given a value of our independent variable",
    "text": "Finding a mean response given a value of our independent variable\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot \\textrm{female literacy rate} \\]\n\nWhat is the expected/predicted life expectancy for a country with female literacy rate 60%?\n\n\\[\\widehat{\\textrm{life expectancy}} = 50.9 + 0.232 \\cdot 60 = 64.82\\]\n\n(y_60 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n\n\nHow do we interpret the expected value?\n\nWe sometimes call this “predicted” value, since we can technically use a literacy rate that is not in our sample\n\nHow variable is it?"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#mean-responseprediction-with-regression-line",
    "href": "slides/04_SLR_Inf_Pred.html#mean-responseprediction-with-regression-line",
    "title": "SLR: Inference and Prediction",
    "section": "Mean response/prediction with regression line",
    "text": "Mean response/prediction with regression line\n\n\nRecall the population model:\nline + random “noise”\n\\[Y = \\beta_0 + \\beta_1 \\cdot X + \\varepsilon\\] with \\(\\varepsilon \\sim N(0,\\sigma^2)\\)\n\n\nWhen we take the expected value, at a given value \\(X^*\\), the average expected response at \\(X^*\\) is:\n\n\\[\\widehat{E}[Y|X^*] = \\widehat\\beta_0 + \\widehat\\beta_1 X^*\\]\n\n\n\n\n\n\n\n\n\nThese are the points on the regression line\nThe mean responses have variability, and we can calculate a CI for it, for every value of \\(X^*\\)"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "CI for population mean response ( \\(E[Y|X^*]\\) ) \\(\\mu_{Y|X^*}\\)",
    "text": "CI for population mean response ( \\(E[Y|X^*]\\) ) \\(\\mu_{Y|X^*}\\)\n\\[\\widehat{E[Y|x^*]} \\pm t_{n-2}^* \\cdot SE_{\\widehat{E[Y|x^*]}}\\]\n\n\\(SE_{\\widehat{E[Y|x^*]}}\\) is calculated using\n\n\\[SE_{\\widehat{E[Y|x^*]}} = s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\n\\(\\widehat{E[Y|x^*]}\\) is the predicted value at the specified point \\(x^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\bar{x}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_x\\) is the sample sd of the explanatory variable \\(x\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-or-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#ci-for-population-mean-response-eyx-or-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "CI for population mean response (\\(E[Y|X^*]\\) or \\(\\mu_{Y|X^*})\\)",
    "text": "CI for population mean response (\\(E[Y|X^*]\\) or \\(\\mu_{Y|X^*})\\)\n\\[\\widehat{E}[Y|X^*] \\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\]\n\\[SE_{\\widehat{E}[Y|X^*]} = s_{\\text{residuals}} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\overline{X})^2}{(n-1)s_X^2}}\\]\n\n\\(\\widehat{E}[Y|X^*]\\) is the predicted value at the specified point \\(X^*\\) of the explanatory variable\n\\(s_{\\textrm{residuals}}^2\\) is the sd of the residuals\n\\(n\\) is the sample size, or the number of (complete) pairs of points\n\\(\\overline{X}\\) is the sample mean of the explanatory variable \\(x\\)\n\\(s_X\\) is the sample sd of the explanatory variable \\(X\\)\n\n\n\nRecall that \\(t_{n-2}^*\\) is calculated using qt() and depends on the confidence level (\\(1-\\alpha\\))"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI for the mean life expectancy when the female literacy rate is 60.\n\n\\[\\begin{align}\n\\widehat{E}[Y|X^*] &\\pm t_{n-2}^* \\cdot SE_{\\widehat{E}[Y|X^*]}\\\\\n64.8596 &\\pm 1.990847 \\cdot s_{residuals} \\sqrt{\\frac{1}{n} + \\frac{(X^* - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 6.142157 \\sqrt{\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n64.8596 &\\pm 1.990847 \\cdot 0.9675541\\\\\n64.8596 &\\pm 1.926252\\\\\n(62.93335 &, 66.78586)\n\\end{align}\\]\n\n\n\n\n\n(Y60 &lt;- 50.9278981 + 0.2321951 * 60)\n\n[1] 64.8596\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\n(s_resid &lt;- glance(model1)$sigma)\n\n[1] 6.142157\n\n\n\n\n(n &lt;- nobs(model1))\n\n[1] 80\n\n(mx &lt;- mean(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 81.65375\n\n(s_x &lt;- sd(gapm$female_literacy_rate_2011, na.rm=T))\n\n[1] 21.95371\n\n\n\n\n\n(SE_Yx &lt;- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))\n\n[1] 0.9675541\n\n\n\n\n\n(MOE_Yx &lt;- SE_Yx*tstar)\n\n[1] 1.926252\n\n\n\n\n\n\nY60 - MOE_Yx\n\n[1] 62.93335\n\n\n\n\n\n\nY60 + MOE_Yx\n\n[1] 66.78586"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx-1",
    "href": "slides/04_SLR_Inf_Pred.html#example-option-1-ci-for-mean-response-mu_yx-1",
    "title": "SLR: Inference and Prediction",
    "section": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example Option 1: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 40, 60, and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(X^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        60\n2                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 64.85961 62.93335 66.78586\n2 69.50351 68.13244 70.87457\n\n\n\n\n\nInterpretation\n\n\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#example-option-2-ci-for-mean-response-mu_yx",
    "href": "slides/04_SLR_Inf_Pred.html#example-option-2-ci-for-mean-response-mu_yx",
    "title": "SLR: Inference and Prediction",
    "section": "Example Option 2: CI for mean response \\(\\mu_{Y|X^*}\\)",
    "text": "Example Option 2: CI for mean response \\(\\mu_{Y|X^*}\\)\nFind the 95% CI’s for the mean life expectancy when the female literacy rate is 60 and 80.\n\nUse the base R predict() function\nRequires specification of a newdata “value”\n\nThe newdata value is \\(X^*\\)\nThis has to be in the format of a data frame though\nwith column name identical to the predictor variable in the model\n\n\n\nnewdata &lt;- data.frame(female_literacy_rate_2011 = c(60, 80)) \nnewdata\n\n  female_literacy_rate_2011\n1                        60\n2                        80\n\n\n\n\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 64.85961 62.93335 66.78586\n2 69.50351 68.13244 70.87457\n\n\n\n\n\nInterpretation\n\n\nWe are 95% confident that the average life expectancy for a country with a 60% female literacy rate will be between 62.9 and 66.8 years."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#for-reference-quick-ci-for-beta_0",
    "href": "slides/04_SLR_Inf_Pred.html#for-reference-quick-ci-for-beta_0",
    "title": "SLR: Inference and Prediction",
    "section": "For reference: quick CI for \\(\\beta_0\\)",
    "text": "For reference: quick CI for \\(\\beta_0\\)\n\nCalculate CI for population intercept \\(\\beta_0\\): \\(\\widehat{\\beta}_0 \\pm t^*\\cdot SE_{\\beta_0}\\)\n\nwhere \\(t^*\\) is the \\(t\\)-distribution critical value with \\(df = n -2\\)\n\nUse the regression table\n\n\ntidy(model1, conf.int = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n45.631\n56.224\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n0.170\n0.295\n  \n  \n  \n\n\n\n\n\n\nGeneral statement for population intercept inference\n\n\nThe expected outcome for the \\(Y\\)-variable is (\\(\\widehat\\beta_0\\)) when the \\(X\\)-variable is 0 (95% CI: LB, UB).\n\n\n\nFor example: The expected/average life expectancy is 50.9 years when the female literacy rate is 0 (95% CI: 45.63, 56.22)."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate-of-the-population-slope",
    "href": "slides/04_SLR_Inf_Pred.html#reporting-the-coefficient-estimate-of-the-population-slope",
    "title": "SLR: Inference and Prediction",
    "section": "Reporting the coefficient estimate of the population slope",
    "text": "Reporting the coefficient estimate of the population slope\n\nWhen we report our results to someone else, we don’t usually show them our full hypothesis test\n\nIn an informal setting, someone may want to see it\n\nTypically, we report the estimate with the confidence interval\n\nFrom the confidence interval, your audience can also deduce the results of a hypothesis test\n\nOnce we found our CI, we often just write the interpretation of the coefficient estimate:\n\n\n\nGeneral statement for population slope inference\n\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected average increase of \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\nIn our example: For every 1% increase in female literacy rate, the average life expectancy is expected to increase, on average, 0.232 years (95% CI: 0.170, 0.295)."
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-1-1",
    "href": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-1-1",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-2",
    "href": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-2",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-3",
    "href": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-3",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-4",
    "href": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-4",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-5",
    "href": "slides/04_SLR_Inf_Pred.html#poll-everywhere-question-5",
    "title": "SLR: Inference and Prediction",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#explained-vs.-unexplained-variation",
    "href": "slides/05_SLR_Eval.html#explained-vs.-unexplained-variation",
    "title": "SLR: More inference + Evaluation",
    "section": "Explained vs. Unexplained Variation",
    "text": "Explained vs. Unexplained Variation\n\n\n\n\n\n\n\n\n\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\\\\n\\text{Total unexplained variation} & = \\text{Residual variation after regression} + \\text{Variation explained by regression}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#what-is-the-f-statistic-testing",
    "href": "slides/05_SLR_Eval.html#what-is-the-f-statistic-testing",
    "title": "SLR: More inference + Evaluation",
    "section": "What is the F statistic testing?",
    "text": "What is the F statistic testing?\n\\[F = \\frac{MSR}{MSE}\\]\n\nIt can be shown that\n\n\\[E(MSE)=\\sigma^2\\ \\text{and}\\ E(MSR) = \\sigma^2 + \\beta_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2\\]\n\nRecall that \\(\\sigma^2\\) is the variance of the residuals\nThus if\n\n\\(\\beta_1 = 0\\), then \\(F \\approx \\frac{\\hat{\\sigma}^2}{\\hat{\\sigma}^2} = 1\\)\n\\(\\beta_1 \\neq 0\\), then \\(F \\approx \\frac{\\hat{\\sigma}^2 + \\hat{\\beta}_1^2\\sum_{i=1}^n (X_i- \\overline{X})^2}{\\hat{\\sigma}^2} &gt; 1\\)\n\nSo the \\(F\\) statistic can also be used to test \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#f-test-for-the-slope",
    "href": "slides/05_SLR_Eval.html#f-test-for-the-slope",
    "title": "SLR: Model Evaluation",
    "section": "F-test for the slope",
    "text": "F-test for the slope\nHypotheses\n\\[H_0: \\beta_1 = 0\\\\\nH_A: \\beta_1 \\neq 0\\]\nTest statistic and probability distribution\n\\[F = \\frac{MSR}{MSE} \\sim F_{1, n-2}\\]\n\nThe F statistic has an \\(F\\)-distribution with numerator \\(df = 1\\) and denominator \\(df = n-2\\).\n\nDecision\nCritical value approach:\nLet \\(F_{1, n-2, 1-\\alpha}\\) be the critical value for the \\(F_{1, n-2}\\) distribution with area \\(\\alpha\\) to the right.\n\nIf \\(F &gt; F_{1, n-2, 1-\\alpha}\\), then reject \\(H_0\\)\nIf \\(F \\leq F_{1, n-2, 1-\\alpha}\\), then fail to reject \\(H_0\\)\n\n\\(p\\)-value approach:\n\nIf \\(p\\)-value \\(&lt; \\alpha\\), then reject \\(H_0\\)\nIf \\(p\\)-value \\(\\geq \\alpha\\), then fail to reject \\(H_0\\)\n\nConclusion\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that there exists a linear relationship between \\(X\\) and \\(Y\\) (\\(\\beta_1 \\neq 0\\))."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#f-test-vs.-t-test-for-the-slope",
    "href": "slides/05_SLR_Eval.html#f-test-vs.-t-test-for-the-slope",
    "title": "SLR: Model Evaluation",
    "section": "F-test vs. t-test for the slope",
    "text": "F-test vs. t-test for the slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#example-f-test-for-the-slope",
    "href": "slides/05_SLR_Eval.html#example-f-test-for-the-slope",
    "title": "SLR: Model Evaluation",
    "section": "Example: F-test for the slope",
    "text": "Example: F-test for the slope\nHypotheses\n\\[H_0: \\beta_1 = 0\\\\\nH_A: \\beta_1 \\neq 0\\]\nTest statistic and probability distribution\n\\[F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73} = 54.414 \\sim F_{1, 78}\\]\nDecision\nCritical value approach:\n\n# critical value for F 1,78 with alpha = 0.05:\nqf(.95, df1 = 1, df2 = 78)\n\n[1] 3.963472\n\n\nSince \\(F = 54.414 &gt; 3.9634721\\), we reject \\(H_0\\).\n\\(p\\)-value approach:\n\n# p-value is ALWAYS the right tail for F-test\npf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)\n\n[1] 1.501104e-10\n\n\nSince \\(p\\text{-value} = 1.5011043\\times 10^{-10} &lt; 0.05\\), we reject \\(H_0\\).\nConclusion\nThere is sufficient evidence that there exists a linear relationship between countries’ female literacy rate and average life expectancy (p &lt; 0.001)."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#r-code",
    "href": "slides/05_SLR_Eval.html#r-code",
    "title": "SLR: Model Evaluation",
    "section": "R code",
    "text": "R code\nANOVA table & regression output\n\n# Load the data - update code if the csv file is not in the same location on your computer\ngapm &lt;- read_csv(\"data/lifeexp_femlit_2011.csv\")\n\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n             data = gapm)\n\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: life_expectancy_years_2011\n                          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfemale_literacy_rate_2011  1 2052.8 2052.81  54.414 1.501e-10 ***\nResiduals                 78 2942.6   37.73                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(model1) %&gt;% tidy()\n\n# A tibble: 2 × 6\n  term                         df sumsq meansq statistic   p.value\n  &lt;chr&gt;                     &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 female_literacy_rate_2011     1 2053. 2053.       54.4  1.50e-10\n2 Residuals                    78 2943.   37.7      NA   NA       \n\nanova(model1) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n#tidy(anova(model1))\n\n# Regression table:\n# library(broom)  # for tidy() command\n# library(gt)  # for gt() command\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\nFigure with colored residuals\n\n# code from https://drsimonj.svbtle.com/visualising-residuals\n\n# model1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n#                  data = gapm)\nregression_points &lt;- augment(model1)\n\nggplot(regression_points, \n       aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"lightgrey\") +\n  geom_segment(aes(\n    xend = female_literacy_rate_2011, \n    yend = .fitted), \n    alpha = .2) +\n  # &gt; Color adjustments made here...\n  geom_point(aes(color = .resid), size = 4) +  # Color mapped here\n  scale_color_gradient2(low = \"blue\", mid = \"white\", high = \"red\") +  # Colors to use here\n  guides(color = \"none\") +\n  geom_point(aes(y = .fitted), shape = 1, size = 3) +\n  labs(x = \"Female literacy rate\", \n       y = \"Average life expectancy\",\n       title = \"Regression line with residuals\") +\n  geom_hline(yintercept = mean(model1$model$life_expectancy_years_2011),\n             color = \"grey\") +\n  theme_bw() + \n  theme(text = element_text(size = 14))"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#correlation-coefficient",
    "href": "slides/05_SLR_Eval.html#correlation-coefficient",
    "title": "SLR: More inference + Evaluation",
    "section": "Correlation coefficient",
    "text": "Correlation coefficient\n\n\nThe (Pearson) correlation coefficient \\(r\\) of variables \\(X\\) and \\(Y\\) can be computed using the formula:\n\\[\\begin{aligned}\nr  & = \\frac{\\sum_{i=1}^n (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\Big(\\sum_{i=1}^n (X_i - \\overline{X})^2 \\sum_{i=1}^n (Y_i - \\overline{Y})^2\\Big)^{1/2}} \\\\\n& = \\frac{SSXY}{\\sqrt{SSX \\cdot SSY}}\n\\end{aligned}\\]\nwe have the relationship\n\\[\\widehat{\\beta}_1 = r\\frac{SSY}{SSX},\\ \\ \\text{or},\\ \\  r = \\widehat{\\beta}_1\\frac{SSX}{SSY}\\]"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#coefficient-of-determination-r2",
    "href": "slides/05_SLR_Eval.html#coefficient-of-determination-r2",
    "title": "SLR: More inference + Evaluation",
    "section": "Coefficient of determination: \\(R^2\\)",
    "text": "Coefficient of determination: \\(R^2\\)\nIt can be shown that the square of the correlation coefficient \\(r\\) is equal to\n\\[R^2 = \\frac{SSR}{SSY} = \\frac{SSY - SSE}{SSY}\\]\n\n\\(R^2\\) is called the coefficient of determination.\nInterpretation: The proportion of variation in the \\(Y\\) values explained by the regression model\n\\(R^2\\) measures the strength of the linear relationship between \\(X\\) and \\(Y\\):\n\n\\(R^2 = \\pm 1\\): Perfect relationship\n\nHappens when \\(SSE = 0\\), i.e. no error, all points on the line\n\n\\(R^2 = 0\\): No relationship\n\nHappens when \\(SSY = SSE\\), i.e. using the line doesn’t not improve model fit over using \\(\\overline{Y}\\) to model the \\(Y\\) values."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#r-example",
    "href": "slides/05_SLR_Eval.html#r-example",
    "title": "SLR: Model Evaluation",
    "section": "R Example",
    "text": "R Example\n\n\n\n(r = cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\"))\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366\n\n(sum_m1 = summary(model1)) # for R^2 value\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nsum_m1$r.squared\n\n[1] 0.4109366\n\n\n\n   \n\n\nInterpretation\n\n\n41.1% of the variation in countries’ average life expectancy is explained by the linear model with female literacy rate as the independent variable."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#test-rho-0",
    "href": "slides/05_SLR_Eval.html#test-rho-0",
    "title": "SLR: Model Evaluation",
    "section": "Test \\(\\rho = 0\\)",
    "text": "Test \\(\\rho = 0\\)\n\\(\\rho\\) is the population parameter for the correlation coefficient \\(r\\)\nHypotheses\n\\[H_0: \\rho = 0 \\ \\ \\text{vs.} \\ \\ H_A: \\rho \\neq 0\\]\nTest statistic and probability distribution\n\\[T = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} \\sim t_{n-2}\\]\n\nThe t statistic has a Student’s \\(t\\)-distribution with \\(df = n-2\\).\n\nDecision\nCritical value approach:\nLet \\(t_{n-2, 1-\\alpha/2}\\) be the critical value for the \\(t_{n-2}\\) distribution with area \\(\\alpha/2\\) in each of the tails.\n\nIf \\(|T| &gt; t_{n-2, 1-\\alpha/2}\\), then reject \\(H_0\\)\nIf \\(|T| \\leq t_{n-2, 1-\\alpha/2}\\), then fail to reject \\(H_0\\)\n\n\\(p\\)-value approach:\n\nIf \\(p\\)-value \\(&lt; \\alpha\\), then reject \\(H_0\\)\nIf \\(p\\)-value \\(\\geq \\alpha\\), then fail to reject \\(H_0\\)\n\nConclusion\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that there exists a linear correlation between \\(X\\) and \\(Y\\) (\\(\\rho \\neq 0\\)).\nExample: Test \\(\\rho = 0\\)\nHypotheses\n\\[H_0: \\rho = 0 \\ \\ \\text{vs.} \\ \\ H_A: \\rho \\neq 0\\]\nTest statistic and probability distribution\n\nr # calculated previously\n\n[1] 0.6410434\n\n(teststat &lt;- r*sqrt(78)/sqrt(1-r^2))\n\n[1] 7.376557\n\n\n\\[T = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} =\n\\frac{0.6410434\\sqrt{80-2}}{\\sqrt{1-0.6410434^2}} = 7.3765572 \\sim t_{78}\\]\n\n# critical value for t78 with alpha = 0.05:\nqt(.975, df = 78)\n\n[1] 1.990847\n\n\nSince \\(t = 7.376557 &gt; 1.9908471\\), we reject \\(H_0\\).\n\\(p\\)-value approach:\n\n2*pt(7.376557, df = 78, lower.tail = FALSE)\n\n[1] 1.501287e-10\n\n\nSince \\(p\\text{-value} = 1.5012873\\times 10^{-10} &lt; 0.05\\), we reject \\(H_0\\).\n\ncor.test(x = gapm$life_expectancy_years_2011, \n         y = gapm$female_literacy_rate_2011)\n\n\n    Pearson's product-moment correlation\n\ndata:  gapm$life_expectancy_years_2011 and gapm$female_literacy_rate_2011\nt = 7.3766, df = 78, p-value = 1.501e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.4903981 0.7544916\nsample estimates:\n      cor \n0.6410434 \n\ncor.test(x = gapm$life_expectancy_years_2011, \n         y = gapm$female_literacy_rate_2011) %&gt;% tidy()\n\n# A tibble: 1 × 8\n  estimate statistic  p.value parameter conf.low conf.high method    alternative\n     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;      \n1    0.641      7.38 1.50e-10        78    0.490     0.754 Pearson'… two.sided  \n\ncor.test(x = gapm$life_expectancy_years_2011, \n         y = gapm$female_literacy_rate_2011) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      estimate\n      statistic\n      p.value\n      parameter\n      conf.low\n      conf.high\n      method\n      alternative\n    \n  \n  \n    0.6410434\n7.376557\n1.501286e-10\n78\n0.4903981\n0.7544916\nPearson's product-moment correlation\ntwo.sided"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#ci-for-rho",
    "href": "slides/05_SLR_Eval.html#ci-for-rho",
    "title": "SLR: Model Evaluation",
    "section": "CI for \\(\\rho\\)",
    "text": "CI for \\(\\rho\\)\n\nr\n\n[1] 0.6410434\n\n(tstar &lt;- qt(.975, 78))\n\n[1] 1.990847\n\n(SErho &lt;- sqrt((1-r^2)/sqrt(78)))\n\n[1] 0.2582601\n\nr + tstar*SErho\n\n[1] 1.1552\n\nr - tstar*SErho\n\n[1] 0.126887\n\n\n\\[r \\pm t^* \\cdot \\frac{\\sqrt{1-r^2}}{\\sqrt{n-2}}\\\\\n0.6410434 \\pm 1.9908471 \\cdot 0.2582601\\\\\n(0.126887, 1.1551998) \\\\\n(0.126887, 1)\\]"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#testing-rho-for-some-value-rho_0-neq-0",
    "href": "slides/05_SLR_Eval.html#testing-rho-for-some-value-rho_0-neq-0",
    "title": "SLR: Model Evaluation",
    "section": "Testing \\(\\rho\\) for some value \\(\\rho_0 \\neq 0\\)",
    "text": "Testing \\(\\rho\\) for some value \\(\\rho_0 \\neq 0\\)\nHypotheses\n\\[H_0: \\rho = \\rho_0 \\ \\ \\text{vs.} \\ \\ H_A: \\rho \\neq \\rho_0\\]\nTest statistic and probability distribution\n\nWhen \\(\\rho \\neq 0\\), the distribution of the test statistic we used before is skewed\nThus we apply Fisher’s Z transformation: \\(\\frac{1}{2} \\ln \\frac{1+r}{1-r}\\), with which the test statistic has an approximately standard normal distribution:\n\n\\[Z = \\frac{\\frac{1}{2} \\ln \\frac{1+r}{1-r} - \\frac{1}{2} \\ln \\frac{1+\\rho_0}{1-\\rho_0}}{\\frac{1}{\\sqrt{n-3}}} \\sim N(0,1)\\]\nDecision\nCritical value approach:\nLet \\(z_{1-\\alpha/2}\\) be the critical value for the \\(N(0,1)\\) distribution with area \\(\\alpha/2\\) in each of the tails.\n\nIf \\(|Z| &gt; z_{1-\\alpha/2}\\), then reject \\(H_0\\)\nIf \\(|Z| \\leq z_{1-\\alpha/2}\\), then fail to reject \\(H_0\\)\n\n\\(p\\)-value approach:\n\nIf \\(p\\)-value \\(&lt; \\alpha\\), then reject \\(H_0\\)\nIf \\(p\\)-value \\(\\geq \\alpha\\), then fail to reject \\(H_0\\)\n\nConclusion\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that the linear correlation between \\(X\\) and \\(Y\\) is significantly different from \\(\\rho_0\\).\nExample\nTest whether the correlation is different from 0.5.\nHypotheses\n\\[H_0: \\rho = 0.5 \\ \\ \\text{vs.} \\ \\ H_A: \\rho \\neq 0.5\\]\nTest statistic and probability distribution\n\nr # calculated previously\n\n[1] 0.6410434\n\nrho0 &lt;- 0.5\n(fisher_r &lt;- 0.5*log((1+r)/(1-r)))\n\n[1] 0.759943\n\n(fisher_rho0 &lt;- 0.5*log((1+rho0)/(1-rho0)))\n\n[1] 0.5493061\n\n(z_stat &lt;- (fisher_r - fisher_rho0) * sqrt(77))\n\n[1] 1.848331\n\n\n\\[Z = \\frac{\\frac{1}{2} \\ln \\frac{1+r}{1-r} - \\frac{1}{2} \\ln \\frac{1+\\rho_0}{1-\\rho_0}}{\\frac{1}{\\sqrt{n-3}}} \\\\\n= \\frac{\\frac{1}{2} \\ln \\frac{1+0.6410434}{1-0.6410434} - \\frac{1}{2} \\ln \\frac{1+0.5}{1-0.5}}{\\frac{1}{\\sqrt{80-3}}} \\\\\n= 1.8483309\\]\n\n# critical value for N(0,1) with alpha = 0.05:\nqnorm(.975)\n\n[1] 1.959964\n\n\nSince \\(Z = 1.848331 &lt; 1.959964\\), we fail to reject \\(H_0\\).\n\\(p\\)-value approach:\n\n2*pnorm(1.848331, lower.tail = FALSE)\n\n[1] 0.06455447\n\n\nSince \\(p\\text{-value} = 0.0645545 &gt; 0.05\\), we fail to reject \\(H_0\\).\nConclusion\nThere is insufficient evidence that the linear correlation between countries’ female literacy rate and average life expectancy is significantly different from 0.5 (p = 0.06)."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#ci-for-rho-when-rho-neq-0",
    "href": "slides/05_SLR_Eval.html#ci-for-rho-when-rho-neq-0",
    "title": "SLR: Model Evaluation",
    "section": "CI for \\(\\rho\\) when \\(\\rho \\neq 0\\)",
    "text": "CI for \\(\\rho\\) when \\(\\rho \\neq 0\\)\n\nCI bounds for the Fisher transformed correlations:\n\n\\[\\frac{1}{2} \\ln \\frac{1+r}{1-r} \\pm z^* \\cdot \\frac{1}{\\sqrt{n-3}}\\] * After finding CI bounds (\\(z\\)), back transform the values to \\(r\\):\n\\[r = \\frac{e^{2z}-1}{e^{2z}+1}\\]\nExample: CI for \\(\\rho\\) when \\(\\rho \\neq 0\\)\n\nfisher_r\n\n[1] 0.759943\n\n(zstar &lt;- qnorm(.975))\n\n[1] 1.959964\n\n(SE_fisher &lt;- 1/sqrt(77))\n\n[1] 0.1139606\n\n# Fisher transformed CI bounds\n(LB_fish &lt;- fisher_r - zstar * SE_fisher)\n\n[1] 0.5365844\n\n(UB_fish &lt;- fisher_r + zstar * SE_fisher)\n\n[1] 0.9833016\n\n# Back transform to r values\n(LB_r &lt;- (exp(2*LB_fish) - 1)/(exp(2*LB_fish)+1))\n\n[1] 0.4903981\n\n(UB_r &lt;- (exp(2*UB_fish) - 1)/(exp(2*UB_fish)+1))\n\n[1] 0.7544916\n\n\n\nCI bounds for the Fisher transformed correlations:\n\n\\[\\frac{1}{2} \\ln \\frac{1+r}{1-r} \\pm z^* \\cdot \\frac{1}{\\sqrt{n-3}}\\\\\n\\frac{1}{2} \\ln \\frac{1+0.6410434}{1-0.6410434} \\pm 1.959964 \\cdot \\frac{1}{\\sqrt{77}}\\\\\n0.759943 \\pm 1.959964 \\cdot 0.1139606\\\\\n(0.5365844, 0.9833016)\\] * After finding CI bounds (\\(z\\)), back transform the values to \\(r\\):\n\\[LB = \\frac{e^{2z}-1}{e^{2z}+1} = \\frac{e^{20.5365844}-1}{e^{20.5365844}+1} = 0.4903981\\\\\nUB = \\frac{e^{2z}-1}{e^{2z}+1} = \\frac{e^{20.9833016}-1}{e^{20.9833016}+1} = 0.7544916\\]\nWe are 95% confident that the linear correlation coefficient between countries’ female literacy rate and average life expectancy is within (0.49, 0.75)."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#ci-using-psych-package",
    "href": "slides/05_SLR_Eval.html#ci-using-psych-package",
    "title": "SLR: Model Evaluation",
    "section": "CI using psych package",
    "text": "CI using psych package\n\n# First install the psych package\n# install.packages(\"psych\")\nlibrary(psych)\n\n# Fisher transformed values\nfisherz(0.5)\n\n[1] 0.5493061\n\nfisherz(r)\n\n[1] 0.759943\n\n# compare with values from computations above\nfisher_rho0\n\n[1] 0.5493061\n\nfisher_r\n\n[1] 0.759943\n\n#----\n# CI with back transformed values\nr.con(rho = r, n = 80, p=.95, twotailed=TRUE)\n\n[1] 0.4903981 0.7544916\n\n# compare with values from computations above\nLB_r \n\n[1] 0.4903981\n\nUB_r \n\n[1] 0.7544916"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#so-far-in-our-regression-example",
    "href": "slides/05_SLR_Eval.html#so-far-in-our-regression-example",
    "title": "SLR: More inference + Evaluation",
    "section": "",
    "text": "Lesson 1 of SLR:\n\nFit regression line\nCalculate slope & intercept\nInterpret slope & intercept\n\nLesson 2 of SLR:\n\nEstimate variance of the residuals\nInference for slope & intercept: CI, p-value\nConfidence bands of regression line for mean value of Y|X\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#lets-revisit-the-regression-analysis-process",
    "href": "slides/05_SLR_Eval.html#lets-revisit-the-regression-analysis-process",
    "title": "SLR: More inference + Evaluation",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#quickly-go-over-our-estimates-from-last-class",
    "href": "slides/05_SLR_Eval.html#quickly-go-over-our-estimates-from-last-class",
    "title": "SLR: Inference Revisited",
    "section": "Quickly go over our estimates from last class???",
    "text": "Quickly go over our estimates from last class???\nNow we are making sure our whole model is trustworthy"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#more-of-the-equation",
    "href": "slides/05_SLR_Eval.html#more-of-the-equation",
    "title": "SLR: More inference + Evaluation",
    "section": "More of the equation",
    "text": "More of the equation\n\n\n\\[Y_i - \\overline{Y} = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_i\\) ).\n\n\\(Y_i - \\hat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_i\\) ).\n\n\\(\\hat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\hat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_i\\) )"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model",
    "href": "slides/05_SLR_Eval.html#how-is-this-actually-calculated-for-our-fitted-model",
    "title": "SLR: More inference + Evaluation",
    "section": "How is this actually calculated for our fitted model?",
    "text": "How is this actually calculated for our fitted model?\n\\[ \\begin{aligned}\nY_i - \\overline{Y} & = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\\\\n\\text{Total unexplained variation} & = \\text{Variation due to regression} + \\text{Residual variation after regression}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 & = \\sum_{i=1}^n (\\hat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2 \\\\\nSSY & = SSR + SSE\n\\end{aligned}\\] \\[\\text{Total Sum of Squares} = \\text{Sum of Squares due to Regression} + \\text{Sum of Squares due to Error (residuals)}\\]\n\n\n\n\nANOVA table:\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(1\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{1}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\n\n\nError\n\\(n-2\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#anova-table-in-r",
    "href": "slides/05_SLR_Eval.html#anova-table-in-r",
    "title": "SLR: Model Evaluation",
    "section": "ANOVA table in R",
    "text": "ANOVA table in R\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n             data = gapm)\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: life_expectancy_years_2011\n                          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfemale_literacy_rate_2011  1 2052.8 2052.81  54.414 1.501e-10 ***\nResiduals                 78 2942.6   37.73                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1.000\n2,052.812\n2,052.812\n54.414\n0.000\n    Residuals\n78.000\n2,942.635\n37.726\nNA\nNA"
  },
  {
    "objectID": "homework/HW2.html#question-2",
    "href": "homework/HW2.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nThis question and data are adapted from this textbook.\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels. The response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes. The results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nclot = read_excel(here(\"./homework/data/CH05Q09.xls\"))\nclot %&gt;% gt() %&gt;%\n  cols_label(RAT = md(\"**Rat**\"),\n             LOGCONC = md(\"**Log10 Concentration (Y)**\"),\n             LOGDOSE = md(\"**Log10 Dose (X)**\"))\n\n\n\n\n\n  \n    \n    \n      Rat\n      Log10 Concentration (Y)\n      Log10 Dose (X)\n    \n  \n  \n    1\n2.65\n0.18\n    2\n2.25\n0.33\n    3\n2.26\n0.42\n    4\n1.95\n0.54\n    5\n1.72\n0.65\n    6\n1.60\n0.75\n    7\n1.55\n0.83\n    8\n1.32\n0.92\n    9\n1.13\n1.01\n    10\n1.07\n1.04\n    11\n0.95\n1.09\n    12\n0.88\n1.15\n  \n  \n  \n\n\n\n\nUse the log-transformed values as given in the dataset.\nUse the following scatterplot to build your answers off of:\n\n\n\n\n\n\n\n\n\n\nPart a\nFit a linear regression model to the data and add the regression line to the plot.\n\n\nPart b\nUse R to create the ANOVA table for the regression described in the exercise.\n\n\nPart c\nUsing the F-test, determine whether there is an association between the log10 concentration and log10 dose.\n\n\n\n\n\n\nNote\n\n\n\nMake sure to include all needed steps for an F-test. Calculating the F test statistic (step 5) is not needed if you use the ANOVA table. Make sure your conclusion connects back the research context.\n\n\n\n\nPart d\nRewrite your hypothesis test in Part c to show the null and alternative models that we are testing. Did we reject the smaller (reduced) model?\n\n\n\n\n\n\nNote\n\n\n\nYou do not need to go through the hypothesis test process again. A quick statement on rejecting or not is okay.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you prefer to write out the models by hand, remember that you can take a picture of your work and insert it into this document. HW0 can be a good reference for how we’ve done this before."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "href": "slides/05_SLR_Eval.html#analysis-of-variance-anova-table-in-r",
    "title": "SLR: More inference + Evaluation",
    "section": "Analysis of Variance (ANOVA) table in R",
    "text": "Analysis of Variance (ANOVA) table in R\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n             data = gapm)\n\nanova(model1)\n\nAnalysis of Variance Table\n\nResponse: life_expectancy_years_2011\n                          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfemale_literacy_rate_2011  1 2052.8 2052.81  54.414 1.501e-10 ***\nResiduals                 78 2942.6   37.73                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1.000\n2,052.812\n2,052.812\n54.414\n0.000\n    Residuals\n78.000\n2,942.635\n37.726\nNA\nNA"
  },
  {
    "objectID": "slides/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1-t-test",
    "href": "slides/04_SLR_Inf_Pred.html#general-steps-for-hypothesis-test-for-population-slope-beta_1-t-test",
    "title": "SLR: Inference and Prediction",
    "section": "General steps for hypothesis test for population slope \\(\\beta_1\\) (t-test)",
    "text": "General steps for hypothesis test for population slope \\(\\beta_1\\) (t-test)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions (checked in our Model Evaluation step)\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(t\\), and follows a Student’s t-distribution.\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[t = \\frac{ \\widehat\\beta_1 - \\beta_1}{ \\text{SE}_{\\widehat\\beta_1}} = \\frac{ \\widehat\\beta_1}{ \\text{SE}_{\\widehat\\beta_1}}\\]\nwhen we assume \\(H_0: \\beta_1 = 0\\) is true.\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(2\\cdot P(T &gt; t)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(T &gt; t)\\))."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#getting-to-the-f-test",
    "href": "slides/05_SLR_Eval.html#getting-to-the-f-test",
    "title": "SLR: More inference + Evaluation",
    "section": "Getting to the F-test",
    "text": "Getting to the F-test\nThe F statistic in linear regression is essentially a proportion of the variance explained by the model vs. the variance not explained by the model\n\nStart with visual of explained vs. unexplained variation\nFigure out the mathematical representations of this variation\nLook at the ANOVA table to establish key values measuring our variance from our model\nBuild the F-test"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "href": "slides/05_SLR_Eval.html#f-test-vs.-t-test-for-the-population-slope",
    "title": "SLR: More inference + Evaluation",
    "section": "F-test vs. t-test for the population slope",
    "text": "F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#f-test-for-the-population-slope",
    "href": "slides/05_SLR_Eval.html#f-test-for-the-population-slope",
    "title": "SLR: Model Evaluation",
    "section": "F-test for the population slope",
    "text": "F-test for the population slope\nHypotheses\n\\[H_0: \\beta_1 = 0\\\\\nH_A: \\beta_1 \\neq 0\\]\nTest statistic and probability distribution\n\\[F = \\frac{MSR}{MSE} \\sim F_{1, n-2}\\]\n\nThe F statistic has an \\(F\\)-distribution with numerator \\(df = 1\\) and denominator \\(df = n-2\\).\n\nDecision\nCritical value approach:\nLet \\(F_{1, n-2, 1-\\alpha}\\) be the critical value for the \\(F_{1, n-2}\\) distribution with area \\(\\alpha\\) to the right.\n\nIf \\(F &gt; F_{1, n-2, 1-\\alpha}\\), then reject \\(H_0\\)\nIf \\(F \\leq F_{1, n-2, 1-\\alpha}\\), then fail to reject \\(H_0\\)\n\n\\(p\\)-value approach:\n\nIf \\(p\\)-value \\(&lt; \\alpha\\), then reject \\(H_0\\)\nIf \\(p\\)-value \\(\\geq \\alpha\\), then fail to reject \\(H_0\\)\n\nConclusion\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that there exists a linear relationship between \\(X\\) and \\(Y\\) (\\(\\beta_1 \\neq 0\\))."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "href": "slides/05_SLR_Eval.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "title": "SLR: More inference + Evaluation",
    "section": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "href": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\)\n\nSteps 1-4 are setting up our hypothesis test: not much change from the general steps\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nWe are testing if the slope is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2 = 80-2\\).\n\n\nnobs(model1)\n\n[1] 80"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "href": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-24",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (2/4)\n\n\n\nCompute the value of the test statistic\n\n\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\n\nOption 1: Calculate the test statistic using the values in the ANOVA table\n\n\\[F = \\frac{MSR}{MSE} = \\frac{2052.81}{37.73}=54.414\\]\n\nOption 2: Get the test statistic value (F) from the ANOVA table\n\n\nI tend to skip this step because I can do it all with step 6"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "href": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-34",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (3/4)\n\n\n\nCalculate the p-value\n\n\n\n\nAs per Step 4, test statistic \\(F\\) can be modeled by a \\(F\\)-distribution with \\(df1 = 1\\) and \\(df2 = n-2\\).\n\nWe had 80 countries’ data, so \\(n=80\\)\n\nOption 1: Use pf() and our calculated test statistic\n\n\n# p-value is ALWAYS the right tail for F-test\npf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)\n\n[1] 1.501104e-10\n\n\n\nOption 2: Use the ANOVA table\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "href": "slides/05_SLR_Eval.html#life-expectancy-example-hypothesis-test-for-population-slope-beta_1-44",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)",
    "text": "Life expectancy example: hypothesis test for population slope \\(\\beta_1\\) (4/4)\n\n\n\nWrite conclusion for the hypothesis test\n\n\n\nWe reject the null hypothesis that the slope is 0 at the \\(5\\%\\) significance level. There is sufficient evidence that there is significant association between female life expectancy and female literacy rates (p-value &lt; 0.0001)."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "href": "slides/05_SLR_Eval.html#did-you-notice-anything-about-the-p-value",
    "title": "SLR: More inference + Evaluation",
    "section": "Did you notice anything about the p-value?",
    "text": "Did you notice anything about the p-value?\nThe p-value of the t-test and F-test are the same!!\n\nFor the t-test:\n\n\ntidy(model1) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n\nFor the F-test:\n\n\nanova(model1) %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 40)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    female_literacy_rate_2011\n1\n2052.812\n2052.81234\n54.4136\n1.501286e-10\n    Residuals\n78\n2942.635\n37.72609\nNA\nNA\n  \n  \n  \n\n\n\n\nThis is true when we use the F-test for a single coefficient!"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#chapter-6-the-correlation-coefficient-and-straight-line-regression-analysis",
    "href": "slides/05_SLR_Eval.html#chapter-6-the-correlation-coefficient-and-straight-line-regression-analysis",
    "title": "SLR: Model Evaluation",
    "section": "Chapter 6: The Correlation Coefficient and Straight-line Regression Analysis",
    "text": "Chapter 6: The Correlation Coefficient and Straight-line Regression Analysis"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#correlation-coefficient-from-511",
    "href": "slides/05_SLR_Eval.html#correlation-coefficient-from-511",
    "title": "SLR: More inference + Evaluation",
    "section": "Correlation coefficient from 511",
    "text": "Correlation coefficient from 511\n\n\nCorrelation coefficient \\(r\\) can tell us about the strength of a relationship\n\nIf \\(r = -1\\), then there is a perfect negative linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 1\\), then there is a perfect positive linear relationship between \\(X\\) and \\(Y\\)\nIf \\(r = 0\\), then there is no linear relationship between \\(X\\) and \\(Y\\)\n\nNote: All other values of \\(r\\) tell us that the relationship between \\(X\\) and \\(Y\\) is not perfect. The closer \\(r\\) is to 0, the weaker the linear relationship."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#r2-coefficient-of-determination",
    "href": "slides/05_SLR_Eval.html#r2-coefficient-of-determination",
    "title": "SLR: Model Evaluation",
    "section": "\\(R^2\\) = Coefficient of determination",
    "text": "\\(R^2\\) = Coefficient of determination\nAnother way to assess model fit"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-and-coefficient-of-determination",
    "href": "slides/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-and-coefficient-of-determination",
    "title": "SLR: Model Evaluation",
    "section": "Life expectancy example: correlation coeffiicent and coefficient of determination",
    "text": "Life expectancy example: correlation coeffiicent and coefficient of determination\n\n\n\n(r = cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\"))\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366\n\n(sum_m1 = summary(model1)) # for R^2 value\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nsum_m1$r.squared\n\n[1] 0.4109366\n\n\n\n   \n\n\nInterpretation\n\n\n41.1% of the variation in countries’ average life expectancy is explained by the linear model with female literacy rate as the independent variable."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "href": "slides/05_SLR_Eval.html#life-expectancy-example-correlation-coeffiicent-r-and-coefficient-of-determination-r2",
    "title": "SLR: More inference + Evaluation",
    "section": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)",
    "text": "Life expectancy example: correlation coeffiicent \\(r\\) and coefficient of determination \\(R^2\\)\n\n\n\n(r = cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\"))\n\n[1] 0.6410434\n\nr^2\n\n[1] 0.4109366\n\n(sum_m1 = summary(model1)) # for R^2 value\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n\nsum_m1$r.squared\n\n[1] 0.4109366\n\n\n\n   \n\n\nInterpretation\n\n\n41.1% of the variation in countries’ average life expectancy is explained by the linear model with female literacy rate as the independent variable."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#what-does-r2-not-measure",
    "href": "slides/05_SLR_Eval.html#what-does-r2-not-measure",
    "title": "SLR: More inference + Evaluation",
    "section": "What does \\(R^2\\) not measure?",
    "text": "What does \\(R^2\\) not measure?\n\n\n\n\\(R^2\\) is not a measure of the magnitude of the slope of the regression line\n\nExample: can have \\(R^2 = 1\\) for many different slopes!!\n\n\\(R^2\\) is not a measure of the appropriateness of the straight-line model\n\nExample: figure"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "href": "slides/05_SLR_Eval.html#planting-a-seed-about-the-f-test",
    "title": "SLR: More inference + Evaluation",
    "section": "Planting a seed about the F-test",
    "text": "Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test!"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#poll-everywhere-question",
    "href": "slides/05_SLR_Eval.html#poll-everywhere-question",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#least-squares-method",
    "href": "slides/05_SLR_Eval.html#least-squares-method",
    "title": "SLR: Inference Revisited",
    "section": "Least-squares method",
    "text": "Least-squares method\n\nThe regression coefficients are calculated by\n\nminimizing the sums of the squares of the errors, or residuals, (SSE, or SSR)\n\nMathematically, we need to find the values \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) that minimize the equation:\n\n\\[SSE=\\sum_{i=1}^n (y_i - \\widehat{y}_i)^2 = \\sum_{i=1}^n (y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 x_i)^2\\]\n\nThe solutions to this problem are the formulas for \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) from a previous slide:\n\n\\[\\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n\\widehat{\\beta}_0 = \\bar{y} - \\widehat{\\beta}_1\\bar{x}\\]\n\nThis is an optimization problem that is solved using calculus\n\nIf you want to see the mathematical details, check out pages 222-223 (pdf pages 10-11) of https://www.stat.cmu.edu/~hseltman/309/Book/chapter9.pdf."
  },
  {
    "objectID": "slides/05_SLR_Eval.html#least-squares-model-assumptions-eline",
    "href": "slides/05_SLR_Eval.html#least-squares-model-assumptions-eline",
    "title": "SLR: More inference + Evaluation",
    "section": "Least-squares model assumptions: eLINE",
    "text": "Least-squares model assumptions: eLINE\nThese are the model assumptions made in ordinary least squares:\n \n\ne xistence: For any \\(X\\), there exists a distribution for \\(Y\\)\n\n \n\nL inearity of relationship between variables\n\n \n\nI ndependence of the \\(Y\\) values\n\n \n\nN ormality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n \n\nE quality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#l-linearity",
    "href": "slides/05_SLR_Eval.html#l-linearity",
    "title": "SLR: More inference + Evaluation",
    "section": "L: Linearity",
    "text": "L: Linearity\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#i-independence-of-observations",
    "href": "slides/05_SLR_Eval.html#i-independence-of-observations",
    "title": "SLR: More inference + Evaluation",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\n\n \n\nExamples of when they are not independent, include\n \n\nrepeated measures (such as baseline, 3 months, 6 months)\n\n \n\ndata from clusters, such as different hospitals or families\n\n\n \n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#n-normality",
    "href": "slides/05_SLR_Eval.html#n-normality",
    "title": "SLR: More inference + Evaluation",
    "section": "N: Normality",
    "text": "N: Normality\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check\n\nWe will discuss how to assess this in practice in Chapter 14 (Regression Diagnostics)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "href": "slides/05_SLR_Eval.html#e-equality-of-variance-of-the-residuals",
    "title": "SLR: More inference + Evaluation",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity\nWe will discuss how to assess this in practice in Chapter 14 (Regression Diagnostics)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#summary-of-eline-model-assumptions",
    "href": "slides/05_SLR_Eval.html#summary-of-eline-model-assumptions",
    "title": "SLR: More inference + Evaluation",
    "section": "Summary of eLINE model assumptions",
    "text": "Summary of eLINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#e-existence-of-ys-distribution",
    "href": "slides/05_SLR_Eval.html#e-existence-of-ys-distribution",
    "title": "SLR: More inference + Evaluation",
    "section": "e: Existence of Y’s distribution",
    "text": "e: Existence of Y’s distribution\n\nFor any fixed value of the variable \\(X\\), \\(Y\\) is a\n\nrandom variable with a certain probability distribution\nhaving finite\n\nmean and\nvariance\n\n\nThis leads to the normality assumption\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#poll-everywhere-question-1",
    "href": "slides/05_SLR_Eval.html#poll-everywhere-question-1",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#lob-e-existence-of-ys-distribution",
    "href": "slides/05_SLR_Eval.html#lob-e-existence-of-ys-distribution",
    "title": "SLR: Inference Revisited",
    "section": "::: lob e: Existence of Y’s distribution :::",
    "text": "::: lob e: Existence of Y’s distribution :::\n\nFor any fixed value of the variable \\(X\\), \\(Y\\) is a\n\nrandom variable with a certain probability distribution\nhaving finite\n\nmean and\nvariance\n\n\nThis leads to the normality assumption\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#anscombes-quartet",
    "href": "slides/05_SLR_Eval.html#anscombes-quartet",
    "title": "SLR: More inference + Evaluation",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#more-on-the-equation",
    "href": "slides/05_SLR_Eval.html#more-on-the-equation",
    "title": "SLR: More inference + Evaluation",
    "section": "More on the equation",
    "text": "More on the equation\n\n\n\\[Y_i - \\overline{Y} = (Y_i - \\hat{Y}_i) + (\\hat{Y}_i- \\overline{Y})\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_i\\) ).\n\n\\(Y_i - \\hat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_i\\) ).\n\n\\(\\hat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\hat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_i\\) )"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#poll-everywhere-question-2",
    "href": "slides/05_SLR_Eval.html#poll-everywhere-question-2",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/05_SLR_Eval.html#poll-everywhere-question-3",
    "href": "slides/05_SLR_Eval.html#poll-everywhere-question-3",
    "title": "SLR: More inference + Evaluation",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/Slack_Pred_bands.html",
    "href": "slides/Slack_Pred_bands.html",
    "title": "Prediction Bands",
    "section": "",
    "text": "# Note that the R chunk includes `message = FALSE` and `warning=FALSE` in the first line. \n# This is so that the messages & warnings that appear when loading the packages don't show up in the knitted html file.\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(knitr)\nlibrary(broom)\nlibrary(rstatix)\nlibrary(gt)\ngapm &lt;- read_csv(\"data/lifeexp_femlit_2011.csv\")\n\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Fit regression model:\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n             data = gapm)\nnewdata &lt;- data.frame(female_literacy_rate_2011 = 60) \nnewdata\n\n  female_literacy_rate_2011\n1                        60\n\npredict(model1, \n        newdata=newdata, \n        interval=\"confidence\")\n\n       fit      lwr      upr\n1 64.85961 62.93335 66.78586"
  },
  {
    "objectID": "slides/Slack_Pred_bands.html#prediction-interval-for-predicting-individual-values",
    "href": "slides/Slack_Pred_bands.html#prediction-interval-for-predicting-individual-values",
    "title": "Prediction Bands",
    "section": "Prediction interval for predicting individual values",
    "text": "Prediction interval for predicting individual values\n\nWe do not call this interval a CI since Y is a random variables instead of a parameter\nThe form is similar to a CI though:\n\n\\[\\widehat{Y}_{X_0} \\pm t_{n-2}^* \\cdot s_{Y|X} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{(n-1)s_x^2}}\\]\n\nNote that the only difference to the CI for a mean value of y is the additional 1+ under the square root.\n\nThus the width is wider!\n\n\n\nExample: Prediction interval\nFind the 95% prediction interval for the life expectancy when the female literacy rate is 60.\n\\[\\widehat{Y}_{X_0} \\pm t_{n-2}^* \\cdot s_{Y|X} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{(n-1)s_x^2}}\\\\\n64.82 \\pm 1.990847 \\cdot 6.142157 \\sqrt{1+\\frac{1}{80} + \\frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\\\\n(52.44112, 77.19888)\\]\n\n(Y_X0 &lt;- 50.9 + 0.232*60)\n\n[1] 64.82\n\n(tstar &lt;- qt(.975, df = 78))\n\n[1] 1.990847\n\nsd(gapm$female_literacy_rate_2011, na.rm = T)\n\n[1] 21.95371\n\nmean(gapm$female_literacy_rate_2011, na.rm = T)\n\n[1] 81.65375\n\n(pred_SE_Y_X0 &lt;- 6.142157 *sqrt(1 + 1/80 + (60 - 81.65375)^2/((80-1)*21.95371^2)))\n\n[1] 6.217898\n\n(pred_MOE_Y_X0 &lt;- pred_SE_Y_X0*tstar)\n\n[1] 12.37888\n\nY_X0 - pred_MOE_Y_X0\n\n[1] 52.44112\n\nY_X0 + pred_MOE_Y_X0\n\n[1] 77.19888\n\n\n\n\nUsing R\n\nnewdata\n\n  female_literacy_rate_2011\n1                        60\n\npredict(model1, \n        newdata=newdata, \n        interval=\"prediction\")\n\n       fit      lwr      upr\n1 64.85961 52.48072 77.23849\n\n\n\n\nPrediction bands\nCreate a scatterplot with the regression line, 95% confidence bands, and 95% prediction bands.\n\nnames(model1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n[13] \"model\"        \n\n# data used to create the regression model are stored in `model`\n# Useful since rows with missing values already removed\nhead(model1$model)  \n\n  life_expectancy_years_2011 female_literacy_rate_2011\n1                       56.7                      13.0\n2                       76.7                      95.7\n5                       60.9                      58.6\n6                       76.9                      99.4\n7                       76.0                      97.9\n8                       73.8                      99.5\n\npred &lt;- predict(model1, \n                interval=\"prediction\")\n\nWarning in predict.lm(model1, interval = \"prediction\"): predictions on current data refer to _future_ responses\n\nhead(pred)\n\n       fit      lwr      upr\n1 53.94643 40.91166 66.98121\n2 73.14897 60.81324 85.48470\n5 64.53453 52.14572 76.92334\n6 74.00809 61.65365 86.36253\n7 73.65980 61.31347 86.00613\n8 74.03131 61.67631 86.38632\n\n# cbind is column bind to merge datasets together\ndata_pred_bands &lt;- cbind(model1$model, pred)\nglimpse(data_pred_bands)\n\nRows: 80\nColumns: 5\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ fit                        &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ lwr                        &lt;dbl&gt; 40.91166, 60.81324, 52.14572, 61.65365, 61.…\n$ upr                        &lt;dbl&gt; 66.98121, 85.48470, 76.92334, 86.36253, 86.…\n\nggplot(data_pred_bands, \n       aes(x=female_literacy_rate_2011,\n           y=life_expectancy_years_2011)) +\n  geom_point() +\n  geom_smooth(method=lm) +\n  geom_line(aes(y=lwr), \n            color=\"violet\", \n            linetype=\"dashed\")+\n  geom_line(aes(y=upr), \n            color=\"violet\", \n            linetype=\"dashed\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#least-squares-model-assumptions-eline",
    "href": "slides/06_SLR_Diagnostics.html#least-squares-model-assumptions-eline",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Least-squares model assumptions: eLINE",
    "text": "Least-squares model assumptions: eLINE\nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#e-existence-of-ys-distribution",
    "href": "slides/06_SLR_Diagnostics.html#e-existence-of-ys-distribution",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "e: Existence of Y’s distribution",
    "text": "e: Existence of Y’s distribution\n\nFor any fixed value of the variable \\(X\\), \\(Y\\) is a\n\nrandom variable with a certain probability distribution\nhaving finite\n\nmean and\nvariance\n\n\nThis leads to the normality assumption\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#l-linearity",
    "href": "slides/06_SLR_Diagnostics.html#l-linearity",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "L: Linearity",
    "text": "L: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#i-independence-of-observations",
    "href": "slides/06_SLR_Diagnostics.html#i-independence-of-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#poll-everywhere-question",
    "href": "slides/06_SLR_Diagnostics.html#poll-everywhere-question",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-normality",
    "href": "slides/06_SLR_Diagnostics.html#n-normality",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normality",
    "text": "N: Normality\n\n\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#e-equality-of-variance-of-the-residuals",
    "href": "slides/06_SLR_Diagnostics.html#e-equality-of-variance-of-the-residuals",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#summary-of-eline-model-assumptions",
    "href": "slides/06_SLR_Diagnostics.html#summary-of-eline-model-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of eLINE model assumptions",
    "text": "Summary of eLINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#anscombes-quartet",
    "href": "slides/06_SLR_Diagnostics.html#anscombes-quartet",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet"
  },
  {
    "objectID": "homework/HW2.html#question-3",
    "href": "homework/HW2.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nWe will continue to work with the study and dataset from Question 2 above.\n\nPart a\nFind the correlation coefficient between the two variables. Is the value consistent with your description of the relationship in Question 2? Why or why not?\n\n\nPart b\nCalculate the coefficient of determination using linear regression summary output. Can we also calculate the coefficient of determination from the ANOVA in Question 2?\n\n\nPart c\nGive an interpretation of the coefficient of determination in the context of the study."
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#topics",
    "href": "slides/07_SLR_Diagnostics_02.html#topics",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Topics",
    "text": "Topics\n\nLINE assumptions\nchecking assumptions\nresidual analysis\noutlier detection"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#least-squares-model-assumptions-eline",
    "href": "slides/07_SLR_Diagnostics_02.html#least-squares-model-assumptions-eline",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Least-squares model assumptions: eLINE",
    "text": "Least-squares model assumptions: eLINE\nThese are the model assumptions made in ordinary least squares:\n \n\ne xistence: For any \\(X\\), there exists a distribution for \\(Y\\)\n\n \n\nL inearity of relationship between variables\n\n \n\nI ndependence of the \\(Y\\) values\n\n \n\nN ormality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n \n\nE quality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#e-existence-of-ys-distribution",
    "href": "slides/07_SLR_Diagnostics_02.html#e-existence-of-ys-distribution",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "e: Existence of Y’s distribution",
    "text": "e: Existence of Y’s distribution\n\nFor any fixed value of the variable \\(X\\), \\(Y\\) is a\n\nrandom variable with a certain probability distribution\nhaving finite\n\nmean and\nvariance\n\n\nThis leads to the normality assumption\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#l-linearity",
    "href": "slides/07_SLR_Diagnostics_02.html#l-linearity",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "L: Linearity",
    "text": "L: Linearity\n\nThe relationship between the variables is linear (a straight line):\n\nThe mean value of \\(Y\\) given \\(X\\), \\(\\mu_{y|x}\\) or \\(E[Y|X]\\), is a straight-line function of \\(X\\)\n\n\n\\[\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\]"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#i-independence-of-observations",
    "href": "slides/07_SLR_Diagnostics_02.html#i-independence-of-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "I: Independence of observations",
    "text": "I: Independence of observations\n\nThe \\(Y\\)-values are statistically independent of one another\n\n \n\nExamples of when they are not independent, include\n \n\nrepeated measures (such as baseline, 3 months, 6 months)\n\n \n\ndata from clusters, such as different hospitals or families\n\n\n \n\nThis condition is checked by reviewing the study design and not by inspecting the data\n\n \n\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question",
    "href": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question\nAbout the homework??"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#n-normality",
    "href": "slides/07_SLR_Diagnostics_02.html#n-normality",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normality",
    "text": "N: Normality\n\nFor any fixed value of \\(X\\), \\(Y\\) has normal distribution.\n\nNote: This is not about \\(Y\\) alone, but \\(Y|X\\)\n\nEquivalently, the measurement (random) errors \\(\\epsilon_i\\) ’s normally distributed\n\nThis is more often what we check\n\nWe will discuss how to assess this in practice in Chapter 14 (Regression Diagnostics)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#e-equality-of-variance-of-the-residuals",
    "href": "slides/07_SLR_Diagnostics_02.html#e-equality-of-variance-of-the-residuals",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nThe variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\nWe use just \\(\\sigma^2\\) to denote the common variance\n\nThis is also called homoscedasticity\nWe will discuss how to assess this in practice in Chapter 14 (Regression Diagnostics)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#summary-of-eline-model-assumptions",
    "href": "slides/07_SLR_Diagnostics_02.html#summary-of-eline-model-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of eLINE model assumptions",
    "text": "Summary of eLINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#anscombes-quartet",
    "href": "slides/07_SLR_Diagnostics_02.html#anscombes-quartet",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Anscombe’s Quartet",
    "text": "Anscombe’s Quartet"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#what-are-the-line-conditions",
    "href": "slides/07_SLR_Diagnostics_02.html#what-are-the-line-conditions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "What are the LINE conditions?",
    "text": "What are the LINE conditions?\nFor “good” model fit and to be able to make inferences and predictions based on our models, 4 conditions need to be satisfied.\nBriefly:\n\nL inearity of relationship between variables\nI ndependence of the Y values\nN ormality of the residuals\nE quality of variance of the residuals (homoscedasticity)\n\nMore in depth:\n\nL : there is a linear relationship between the mean response (Y) and the explanatory variable (X),\nI : the errors are independent—there’s no connection between how far any two points lie from the regression line,\nN : the responses are normally distributed at each level of X, and\nE : the variance or, equivalently, the standard deviation of the responses is equal for all levels of X."
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#l-linearity-of-relationship-between-variables",
    "href": "slides/07_SLR_Diagnostics_02.html#l-linearity-of-relationship-between-variables",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "L: Linearity of relationship between variables",
    "text": "L: Linearity of relationship between variables\nIs the association between the variables linear?\n\nDiagnostic tools:\n\nScatterplot\nResidual plot (see later section for E : Equality of variance of the residuals)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#i-independence-of-the-residuals-y-values",
    "href": "slides/07_SLR_Diagnostics_02.html#i-independence-of-the-residuals-y-values",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "I: Independence of the residuals (\\(Y\\) values)",
    "text": "I: Independence of the residuals (\\(Y\\) values)\n\nAre the data points independent of each other?\nExamples of when they are not independent, include\n\nrepeated measures (such as baseline, 3 months, 6 months)\ndata from clusters, such as different hospitals or families\n\nThis condition is checked by reviewing the study design and not by inspecting the data\nHow to analyze data using regression models when the \\(Y\\)-values are not independent is covered in BSTA 519 (Longitudinal data)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#n-normality-of-the-residuals-1",
    "href": "slides/07_SLR_Diagnostics_02.html#n-normality-of-the-residuals-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nThe responses Y are normally distributed at each level of x\n\n\n\nhttps://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#extract-models-residuals-in-r",
    "href": "slides/07_SLR_Diagnostics_02.html#extract-models-residuals-in-r",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Extract model’s residuals in R",
    "text": "Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#check-normality-with-usual-distribution-plots",
    "href": "slides/07_SLR_Diagnostics_02.html#check-normality-with-usual-distribution-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Check normality with “usual” distribution plots",
    "text": "Check normality with “usual” distribution plots\nNote that below I save each figure, and then combine them together in one row of output using grid.arrange() from the gridExtra package.\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#normal-qq-plots-qq-quantile-quantile",
    "href": "slides/07_SLR_Diagnostics_02.html#normal-qq-plots-qq-quantile-quantile",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normal QQ plots (QQ = quantile-quantile)",
    "text": "Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n\n\n\n\n\n\n\n\n\nData are approximately normal if points fall on a line.\n\nSee more info at https://data.library.virginia.edu/understanding-QQ-plots/"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-15",
    "href": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-15",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (1/5)",
    "text": "Examples of Normal QQ plots (1/5)\n\nData:\n\nBody measurements from 507 physically active individuals\nin their 20’s or early 30’s\nwithin normal weight range."
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-25",
    "href": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-25",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (2/5)",
    "text": "Examples of Normal QQ plots (2/5)\nSkewed right distribution"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-35",
    "href": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-35",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (3/5)",
    "text": "Examples of Normal QQ plots (3/5)\nLong tails in distribution"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-45",
    "href": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-45",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (4/5)",
    "text": "Examples of Normal QQ plots (4/5)\nBimodal distribution"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-55",
    "href": "slides/07_SLR_Diagnostics_02.html#examples-of-normal-qq-plots-55",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots (5/5)",
    "text": "Examples of Normal QQ plots (5/5)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#qq-plot-of-residuals-of-model1",
    "href": "slides/07_SLR_Diagnostics_02.html#qq-plot-of-residuals-of-model1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "QQ plot of residuals of model1",
    "text": "QQ plot of residuals of model1\n\n\n\n\n\n\n\n\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#compare-to-randomly-generated-normal-qq-plots",
    "href": "slides/07_SLR_Diagnostics_02.html#compare-to-randomly-generated-normal-qq-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Compare to randomly generated Normal QQ plots",
    "text": "Compare to randomly generated Normal QQ plots\nHow “good” we can expect a QQ plot to look depends on the sample size.\n\nThe QQ plots on the next slides are randomly generated\n\nusing random samples from actual standard normal distributions \\(N(0,1)\\).\n\nThus, all the points in the QQ plots should theoretically fall in a line\nHowever, there is sampling variability…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#randomly-generated-normal-qq-plots-n100",
    "href": "slides/07_SLR_Diagnostics_02.html#randomly-generated-normal-qq-plots-n100",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Randomly generated Normal QQ plots: n=100",
    "text": "Randomly generated Normal QQ plots: n=100\n\nNote that stat_qq_line() doesn’t work with randomly generated samples, and thus the code below manually creates the line that the points should be on (which is \\(y=x\\) in this case.)\n\n\n\n\n\nsamplesize &lt;- 100\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#examples-of-simulated-normal-qq-plots-n10",
    "href": "slides/07_SLR_Diagnostics_02.html#examples-of-simulated-normal-qq-plots-n10",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of simulated Normal QQ plots: n=10",
    "text": "Examples of simulated Normal QQ plots: n=10\nWith fewer data points,\n\nsimulated QQ plots are more likely to look “less normal”\neven though the data points were sampled from normal distributions.\n\n\n\n\n\nsamplesize &lt;- 10  # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#examples-of-simulated-normal-qq-plots-n1000",
    "href": "slides/07_SLR_Diagnostics_02.html#examples-of-simulated-normal-qq-plots-n1000",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of simulated Normal QQ plots: n=1,000",
    "text": "Examples of simulated Normal QQ plots: n=1,000\nWith more data points,\n\nsimulated QQ plots are more likely to look “more normal”\n\n\n\n\n\nsamplesize &lt;- 1000 # only change made to code!\n\nrand_qq1 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 &lt;- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\n\n\n\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#back-to-our-example",
    "href": "slides/07_SLR_Diagnostics_02.html#back-to-our-example",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Back to our example",
    "text": "Back to our example\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n\n\n# number of observations \n# in fitted model\nnobs(model1) \n\n[1] 80\n\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#residual-plot",
    "href": "slides/07_SLR_Diagnostics_02.html#residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residual plot",
    "text": "Residual plot\n\n\\(x\\) = explanatory variable from regression model,\nor the fitted values for a multiple regression\n\\(y\\) = residuals from regression model\n\n\nggplot(model1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "href": "slides/07_SLR_Diagnostics_02.html#e-equality-of-variance-of-the-residuals-homoscedasticity",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Equality of variance of the residuals (Homoscedasticity)",
    "text": "E: Equality of variance of the residuals (Homoscedasticity)\n\nThe variance or, equivalently, the standard deviation of the responses is equal for all values of x.\nThis is called homoskedasticity (top row)\nIf there is heteroskedasticity (bottom row), then the assumption is not met.\n\n\n\n\nSLR 4"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#scatterplot",
    "href": "slides/07_SLR_Diagnostics_02.html#scatterplot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\")"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#residuals",
    "href": "slides/07_SLR_Diagnostics_02.html#residuals",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residuals",
    "text": "Residuals\n\nUse augment() to get a tibble with the orginal data, as well as the residuals and some other important values.\n\n\n# augment is from the broom package\naug1 &lt;- augment(model1)\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#normality-with-usual-plots",
    "href": "slides/07_SLR_Diagnostics_02.html#normality-with-usual-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normality with “usual” plots",
    "text": "Normality with “usual” plots\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#normal-probability-plots-qq-plot",
    "href": "slides/07_SLR_Diagnostics_02.html#normal-probability-plots-qq-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normal probability plots (QQ-plot)",
    "text": "Normal probability plots (QQ-plot)\nQQ plot of residuals of model1\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#autoplot",
    "href": "slides/07_SLR_Diagnostics_02.html#autoplot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "autoplot()",
    "text": "autoplot()\n\nautoplot(model1) # in ggfortify package"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#leverage-h_i",
    "href": "slides/07_SLR_Diagnostics_02.html#leverage-h_i",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Leverage \\(h_i\\)",
    "text": "Leverage \\(h_i\\)\n\nValues of leverage are: \\(0 \\leq h_i \\leq 1\\)\nWe flag an observation if the leverage is “high”\n\nDifferent sources will define “high” differently\nSome textbooks use \\(h_i &gt; 4/n\\) where \\(n\\) = sample size\nSome people suggest \\(h_i &gt; 6/n\\)\nPennState site uses \\(h_i &gt; 3p/n\\) where \\(p\\) = number of regression coefficients\n\n\n\naug1 = aug1 %&gt;% relocate(.hat, .after = female_literacy_rate_2011)\naug1 %&gt;% arrange(desc(.hat))\n\n# A tibble: 80 × 10\n   .rownames country        life_expectancy_year…¹ female_literacy_rate…²   .hat\n   &lt;chr&gt;     &lt;chr&gt;                           &lt;dbl&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1         Afghanistan                      56.7                   13   0.136 \n 2 104       Mali                             60                     24.6 0.0980\n 3 34        Chad                             57                     25.4 0.0956\n 4 146       Sierra Leone                     55.7                   32.6 0.0757\n 5 62        Gambia                           66                     41.9 0.0540\n 6 70        Guinea-Bissau                    56.2                   42.1 0.0536\n 7 33        Central Afric…                   48                     44.2 0.0493\n 8 118       Nepal                            68.7                   46.7 0.0446\n 9 42        Cote d'Ivoire                    56.9                   47.6 0.0430\n10 169       Togo                             59.6                   48   0.0422\n# ℹ 70 more rows\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#cooks-distance",
    "href": "slides/07_SLR_Diagnostics_02.html#cooks-distance",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\nMeasures the overall influence of an observation\n\n \n\nAttempts to measure how much influence a single observation has over the fitted model\n\nMeasures how all fitted values change when the \\(ith\\) observation is removed from the model\nCombines leverage and outlier information"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#transform-dependent-variable",
    "href": "slides/07_SLR_Diagnostics_02.html#transform-dependent-variable",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transform dependent variable?",
    "text": "Transform dependent variable?\n\nggplot(gapm, aes(x = life_expectancy_years_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#transform-independent-variable",
    "href": "slides/07_SLR_Diagnostics_02.html#transform-independent-variable",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transform independent variable?",
    "text": "Transform independent variable?\n\nggplot(gapm, aes(x = female_literacy_rate_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#add-quadratic-and-cubic-transformations-to-dataset",
    "href": "slides/07_SLR_Diagnostics_02.html#add-quadratic-and-cubic-transformations-to-dataset",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Add quadratic and cubic transformations to dataset",
    "text": "Add quadratic and cubic transformations to dataset\n\nHelpful to make a new variable with the transformation in your dataset\n\n\ngapm &lt;- gapm %&gt;% \n  mutate(LE_2 = life_expectancy_years_2011^2,\n         LE_3 = life_expectancy_years_2011^3,\n         FLR_2 = female_literacy_rate_2011^2,\n         FLR_3 = female_literacy_rate_2011^3)\n\nglimpse(gapm)\n\nRows: 188\nColumns: 8\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"…\n$ LE_2                       &lt;dbl&gt; 3214.89, 5882.89, 5882.89, 6822.76, 3708.81…\n$ LE_3                       &lt;dbl&gt; 182284.3, 451217.7, 451217.7, 563560.0, 225…\n$ FLR_2                      &lt;dbl&gt; 169.00, 9158.49, NA, NA, 3433.96, 9880.36, …\n$ FLR_3                      &lt;dbl&gt; 2197.0, 876467.5, NA, NA, 201230.1, 982107.…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots",
    "href": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Compare Scatterplots",
    "text": "Compare Scatterplots"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#run-models-with-transformations",
    "href": "slides/07_SLR_Diagnostics_02.html#run-models-with-transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Run models with transformations",
    "text": "Run models with transformations\nExample: Model 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\naug2 &lt;- augment(model2)\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2401.27207\n352.069818\n6.820443\n1.726640e-09\n    female_literacy_rate_2011\n31.17351\n4.165624\n7.483514\n9.352191e-11\n  \n  \n  \n\n\n\n\nModel 3: \\(LE^3 \\sim FLR\\)\n\nmodel3 &lt;- lm(LE_3 ~ female_literacy_rate_2011,\n             data = gapm)\n\naug3 &lt;- augment(model3)\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n95453.189\n35631.6898\n2.678885\n9.005716e-03\n    female_literacy_rate_2011\n3166.481\n421.5875\n7.510853\n8.285324e-11\n  \n  \n  \n\n\n\n\nModel 4: \\(LE \\sim FLR + FLR^2\\)\n\nmodel4 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2,\n             data = gapm)\n\naug4 &lt;- augment(model4)\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n57.030875456\n6.282845592\n9.07723652\n8.512585e-14\n    female_literacy_rate_2011\n0.019348795\n0.201021963\n0.09625215\n9.235704e-01\n    FLR_2\n0.001578649\n0.001472592\n1.07202008\n2.870595e-01\n  \n  \n  \n\n\n\n\nModel 5: \\(LE \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel5 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\naug5 &lt;- augment(model5)\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n4.732796e+01\n1.117939e+01\n4.2335001\n6.373341e-05\n    female_literacy_rate_2011\n6.517986e-01\n6.354934e-01\n1.0256576\n3.083065e-01\n    FLR_2\n-9.952763e-03\n1.109080e-02\n-0.8973895\n3.723451e-01\n    FLR_3\n6.245016e-05\n5.953283e-05\n1.0490038\n2.975008e-01\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\naug6 &lt;- augment(model6)\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67691.7963283\n1.490569e+05\n0.4541338\n0.6510268\n    female_literacy_rate_2011\n8092.1325988\n8.473154e+03\n0.9550320\n0.3425895\n    FLR_2\n-128.5960879\n1.478757e+02\n-0.8696230\n0.3872447\n    FLR_3\n0.8404736\n7.937625e-01\n1.0588477\n0.2930229"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#normal-q-q-plots-comparison",
    "href": "slides/07_SLR_Diagnostics_02.html#normal-q-q-plots-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normal Q-Q plots comparison",
    "text": "Normal Q-Q plots comparison"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#residual-plots-comparison",
    "href": "slides/07_SLR_Diagnostics_02.html#residual-plots-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residual plots comparison",
    "text": "Residual plots comparison"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#leverage-cooks-distance-comparison",
    "href": "slides/07_SLR_Diagnostics_02.html#leverage-cooks-distance-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Leverage & Cook’s distance comparison",
    "text": "Leverage & Cook’s distance comparison"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#models-comparison",
    "href": "slides/07_SLR_Diagnostics_02.html#models-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Models comparison",
    "text": "Models comparison\n\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 &lt;- tbl_regression(model1)\n\ntbl_model2 &lt;- tbl_regression(model2)\n\ntbl_model3 &lt;- tbl_regression(model3)\n\ntbl_model4 &lt;- tbl_regression(model4)\n\ntbl_model5 &lt;- tbl_regression(model5)\n\ntbl_model6 &lt;- tbl_regression(model6)\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=LE\", \"Model 2: y=LE^2\", \"Model 3: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Model 1: y=LE\n      \n      \n        Model 2: y=LE^2\n      \n      \n        Model 3: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.23\n0.17, 0.29\n&lt;0.001\n31\n23, 39\n&lt;0.001\n3,166\n2,327, 4,006\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=LE\", \"Model 5: y=LE\", \"Model 6: y=LE^3\")\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Model 4: y=LE\n      \n      \n        Model 5: y=LE\n      \n      \n        Model 6: y=LE^3\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    female_literacy_rate_2011\n0.02\n-0.38, 0.42\n&gt;0.9\n0.65\n-0.61, 1.9\n0.3\n8,092\n-8,784, 24,968\n0.3\n    FLR_2\n0.00\n0.00, 0.00\n0.3\n-0.01\n-0.03, 0.01\n0.4\n-129\n-423, 166\n0.4\n    FLR_3\n\n\n\n0.00\n0.00, 0.00\n0.3\n0.84\n-0.74, 2.4\n0.3\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#other-fit-statistics-comparison",
    "href": "slides/07_SLR_Diagnostics_02.html#other-fit-statistics-comparison",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Other fit statistics comparison",
    "text": "Other fit statistics comparison\n\nglance(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4109366\n0.4033845\n6.142157\n54.4136\n1.501286e-10\n1\n-257.7164\n521.4329\n528.579\n2942.635\n78\n80\n  \n  \n  \n\n\n\nglance(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4179234\n0.4104609\n812.8336\n56.00298\n9.352191e-11\n1\n-648.5445\n1303.089\n1310.235\n51534476\n78\n80\n  \n  \n  \n\n\n\nglance(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4196986\n0.4122588\n82263.89\n56.41291\n8.285324e-11\n1\n-1017.917\n2041.835\n2048.981\n527853141587\n78\n80\n  \n  \n  \n\n\n\nglance(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4195991\n0.4045238\n6.13629\n27.83346\n8.008115e-10\n2\n-257.1239\n522.2477\n531.7758\n2899.362\n77\n80\n  \n  \n  \n\n\n\nglance(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4278828\n0.4052993\n6.132293\n18.94664\n2.844144e-09\n3\n-256.5488\n523.0977\n535.0078\n2857.981\n76\n80\n  \n  \n  \n\n\n\nglance(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.4414424\n0.419394\n81763.02\n20.02158\n1.160111e-09\n3\n-1016.39\n2042.78\n2054.69\n508074577758\n76\n80"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#untransform-the-variables",
    "href": "slides/07_SLR_Diagnostics_02.html#untransform-the-variables",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "“Untransform” the variables",
    "text": "“Untransform” the variables\n\nrats &lt;- rats %&gt;% \n  mutate(\n    DOSE = 10^LOGDOSE,\n    CONC = 10^LOGCONC\n    )\nglimpse(rats)\n\nRows: 12\nColumns: 5\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n$ DOSE    &lt;dbl&gt; 1.513561, 2.137962, 2.630268, 3.467369, 4.466836, 5.623413, 6.…\n$ CONC    &lt;dbl&gt; 446.683592, 177.827941, 181.970086, 89.125094, 52.480746, 39.8…\n\nxy_plot &lt;- ggplot(rats, aes(x = DOSE, y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Untransformed variables\")\n\ngrid.arrange(xy_plot, loglog_plot, ncol = 2)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#transform-dependent-variable-1",
    "href": "slides/07_SLR_Diagnostics_02.html#transform-dependent-variable-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transform dependent variable?",
    "text": "Transform dependent variable?\n\nggplot(rats, aes(x = CONC)) +\n  geom_histogram()\n\n\n\n\n\ngladder()\n\ngladder(rats$CONC)\n\n\n\n\n\n\nladder()\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\nladder(rats$CONC) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.3816940\n2.684203e-06\n    square\n0.4688066\n1.050091e-05\n    identity\n0.6865897\n6.277977e-04\n    sqrt\n0.8438759\n3.087109e-02\n    log\n0.9456675\n5.747562e-01\n    1/sqrt\n0.9344280\n4.294157e-01\n    inverse\n0.8654753\n5.728124e-02\n    1/square\n0.7368873\n1.956461e-03\n    1/cubic\n0.6489907\n2.837411e-04"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#transform-independent-variable-1",
    "href": "slides/07_SLR_Diagnostics_02.html#transform-independent-variable-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transform independent variable?",
    "text": "Transform independent variable?\n\nggplot(rats, aes(x = DOSE)) +\n  geom_histogram()\n\n\n\n\n\ngladder()\n\ngladder(rats$DOSE)\n\n\n\n\n\n\nladder()\n\nladder(rats$DOSE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.7919903\n7.607573e-03\n    square\n0.8687429\n6.299588e-02\n    identity\n0.9376639\n4.684145e-01\n    sqrt\n0.9536586\n6.909226e-01\n    log\n0.9442481\n5.549771e-01\n    1/sqrt\n0.9043429\n1.804541e-01\n    inverse\n0.8395191\n2.731726e-02\n    1/square\n0.6880945\n6.486810e-04\n    1/cubic\n0.5660067\n5.664946e-05"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#add-transformations-to-dataset",
    "href": "slides/07_SLR_Diagnostics_02.html#add-transformations-to-dataset",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Add transformations to dataset",
    "text": "Add transformations to dataset\n\nnames(rats)\n\n[1] \"RAT\"     \"LOGCONC\" \"LOGDOSE\" \"DOSE\"    \"CONC\"   \n\nrats &lt;- rats %&gt;% \n  mutate(\n    # LOGCONC = log10(CONC), # already in data\n    CONC_invsqrt = 1/sqrt(CONC),\n    # LOGDOSE = log10(DOSE), # already in data\n    DOSE_sqrd = DOSE^2\n  )\n\nglimpse(rats)\n\nRows: 12\nColumns: 7\n$ RAT          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC      &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.0…\n$ LOGDOSE      &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.0…\n$ DOSE         &lt;dbl&gt; 1.513561, 2.137962, 2.630268, 3.467369, 4.466836, 5.62341…\n$ CONC         &lt;dbl&gt; 446.683592, 177.827941, 181.970086, 89.125094, 52.480746,…\n$ CONC_invsqrt &lt;dbl&gt; 0.04731513, 0.07498942, 0.07413102, 0.10592537, 0.1380384…\n$ DOSE_sqrd    &lt;dbl&gt; 2.290868, 4.570882, 6.918310, 12.022644, 19.952623, 31.62…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots-1",
    "href": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Compare Scatterplots",
    "text": "Compare Scatterplots\n\nplot_m1 &lt;- ggplot(rats, aes(x = DOSE,\n                 y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod1: CONC ~ DOSE\")\n\nplot_m2 &lt;- ggplot(rats, aes(x = DOSE,\n                 y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod2: log10(CONC) ~ DOSE\")\n\nplot_m3 &lt;- ggplot(rats, aes(x = DOSE,\n                 y = CONC_invsqrt)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod3: 1/sqrt(CONC) ~ DOSE\")\n\nplot_m4 &lt;- ggplot(rats, aes(x = LOGDOSE,\n                 y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod4: CONC ~ log10(DOSE)\")\n\nplot_m5 &lt;- ggplot(rats, aes(x = DOSE_sqrd,\n                 y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod5: CONC ~ DOSE + DOSE^2\")\n\nplot_m6 &lt;- ggplot(rats, aes(x = LOGDOSE,\n                 y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod6: log10(CONC) ~ log10(DOSE)\")\n\ngrid.arrange(plot_m1, plot_m2, plot_m3, \n             plot_m4, plot_m5, plot_m6,\n             nrow = 2)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#run-models-with-transformations-1",
    "href": "slides/07_SLR_Diagnostics_02.html#run-models-with-transformations-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Run models with transformations",
    "text": "Run models with transformations\n\nModel 1: \\(CONC \\sim DOSE\\)\n\nmodel1 &lt;- lm(CONC ~ DOSE,\n             data = rats)\n\naug1 &lt;- augment(model1)\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n237.16138\n53.435813\n4.438248\n0.001257901\n    DOSE\n-21.32117\n6.679078\n-3.192232\n0.009617637\n  \n  \n  \n\n\n\n\n\nautoplot(model1)\n\n\n\n\n\nplot(model1, which = 5)\n\n\n\n\n\n\nModel 2: \\(log10(CONC) \\sim DOSE\\)\n\nmodel2 &lt;- lm(LOGCONC ~ DOSE,\n             data = rats)\n\naug2 &lt;- augment(model2)\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2.4997349\n0.09369594\n26.67922\n1.263560e-10\n    DOSE\n-0.1292264\n0.01171129\n-11.03434\n6.403923e-07\n  \n  \n  \n\n\n\n\n\nautoplot(model2)\n\n\n\n\n\nplot(model2, which = 5)\n\n\n\n\n\n\nModel 3: \\(1/sqrt(CONC) \\sim DOSE\\)\n\nmodel3 &lt;- lm(CONC_invsqrt ~ DOSE,\n             data = rats)\n\naug3 &lt;- augment(model3)\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n0.01453279\n0.0048459737\n2.998941\n1.336783e-02\n    DOSE\n0.02511651\n0.0006057106\n41.466185\n1.594470e-12\n  \n  \n  \n\n\n\n\n\nautoplot(model3)\n\n\n\n\n\nplot(model3, which = 5)\n\n\n\n\n\n\nModel 4: \\(CONC \\sim log10(DOSE)\\)\n\nmodel4 &lt;- lm(CONC ~ LOGDOSE,\n             data = rats)\n\naug4 &lt;- augment(model4)\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n344.8501\n52.57611\n6.559065\n6.399326e-05\n    LOGDOSE\n-342.5580\n65.45687\n-5.233339\n3.824494e-04\n  \n  \n  \n\n\n\n\n\nautoplot(model4)\n\n\n\n\n\nplot(model4, which = 5)\n\n\n\n\n\n\nModel 5: \\(CONC \\sim DOSE + DOSE^2\\)\n\nmodel5 &lt;- lm(CONC ~ DOSE + DOSE_sqrd,\n             data = rats)\n\naug5 &lt;- augment(model5)\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n415.325839\n71.27055\n5.827454\n0.0002507135\n    DOSE\n-89.294000\n23.10933\n-3.863980\n0.0038237524\n    DOSE_sqrd\n4.521273\n1.50119\n3.011792\n0.0146731693\n  \n  \n  \n\n\n\n\n\nautoplot(model5)\n\n\n\n\n\nplot(model5, which = 5)\n\n\n\n\n\n\nModel 6: \\(log10(CONC) \\sim log10(DOSE)\\)\n\nmodel6 &lt;- lm(LOGCONC ~ LOGDOSE,\n             data = rats)\n\naug6 &lt;- augment(model6)\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2.936205\n0.04230190\n69.41070\n9.390933e-15\n    LOGDOSE\n-1.785012\n0.05266556\n-33.89334\n1.182233e-11\n  \n  \n  \n\n\n\n\n\nautoplot(model6)\n\n\n\n\n\nplot(model6, which = 5)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#normal-q-q-plots-comparison-1",
    "href": "slides/07_SLR_Diagnostics_02.html#normal-q-q-plots-comparison-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normal Q-Q plots comparison",
    "text": "Normal Q-Q plots comparison\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 2)\nplot(model2, which = 2)\nplot(model3, which = 2)\nplot(model4, which = 2)\nplot(model5, which = 2)\nplot(model6, which = 2)\n\n\n\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#residual-plots-comparison-1",
    "href": "slides/07_SLR_Diagnostics_02.html#residual-plots-comparison-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residual plots comparison",
    "text": "Residual plots comparison\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 1)\nplot(model2, which = 1)\nplot(model3, which = 1)\nplot(model4, which = 1)\nplot(model5, which = 1)\nplot(model6, which = 1)\n\n\n\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#leverage-cooks-distance-comparison-1",
    "href": "slides/07_SLR_Diagnostics_02.html#leverage-cooks-distance-comparison-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Leverage & Cook’s distance comparison",
    "text": "Leverage & Cook’s distance comparison\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 5)\nplot(model2, which = 5)\nplot(model3, which = 5)\nplot(model4, which = 5)\nplot(model5, which = 5)\nplot(model6, which = 5)\n\n\n\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#models-comparison-1",
    "href": "slides/07_SLR_Diagnostics_02.html#models-comparison-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Models comparison",
    "text": "Models comparison\n\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 &lt;- tbl_regression(model1)\n# tbl_model1\n\ntbl_model2 &lt;- tbl_regression(model2)\n# tbl_model2\n\ntbl_model3 &lt;- tbl_regression(model3)\n# tbl_model3\n\ntbl_model4 &lt;- tbl_regression(model4)\n# tbl_model4\n\ntbl_model5 &lt;- tbl_regression(model5)\n# tbl_model5\n\ntbl_model6 &lt;- tbl_regression(model6)\n# tbl_model6\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=CONC\", \"Model 2: y=log10(CONC)\", \"Model 3: y=1/sqrt(CONC)\")\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Model 1: y=CONC\n      \n      \n        Model 2: y=log10(CONC)\n      \n      \n        Model 3: y=1/sqrt(CONC)\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    DOSE\n-21\n-36, -6.4\n0.010\n-0.13\n-0.16, -0.10\n&lt;0.001\n0.03\n0.02, 0.03\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=CONC\", \"Model 5: y=CONC\", \"Model 6: y=1/sqrt(CONC)\")\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Model 4: y=CONC\n      \n      \n        Model 5: y=CONC\n      \n      \n        Model 6: y=1/sqrt(CONC)\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    LOGDOSE\n-343\n-488, -197\n&lt;0.001\n\n\n\n-1.8\n-1.9, -1.7\n&lt;0.001\n    DOSE\n\n\n\n-89\n-142, -37\n0.004\n\n\n\n    DOSE_sqrd\n\n\n\n4.5\n1.1, 7.9\n0.015\n\n\n\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#other-fit-statistics-comparison-1",
    "href": "slides/07_SLR_Diagnostics_02.html#other-fit-statistics-comparison-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Other fit statistics comparison",
    "text": "Other fit statistics comparison\n\nglance(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.5047138\n0.4551852\n94.52814\n10.19035\n0.009617637\n1\n-70.5201\n147.0402\n148.4949\n89355.7\n10\n12\n  \n  \n  \n\n\n\nglance(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.9241025\n0.9165128\n0.1657485\n121.7567\n6.403923e-07\n1\n5.634075\n-5.26815\n-3.81343\n0.2747255\n10\n12\n  \n  \n  \n\n\n\nglance(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.9942178\n0.9936396\n0.008572545\n1719.444\n1.59447e-12\n1\n41.17696\n-76.35391\n-74.89919\n0.0007348852\n10\n12\n  \n  \n  \n\n\n\nglance(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.7325334\n0.7057867\n69.46529\n27.38784\n0.0003824494\n1\n-66.82326\n139.6465\n141.1012\n48254.26\n10\n12\n  \n  \n  \n\n\n\nglance(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.7533284\n0.6985125\n70.31878\n13.74288\n0.001838806\n2\n-66.33764\n140.6753\n142.6149\n44502.58\n9\n12\n  \n  \n  \n\n\n\nglance(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.9913701\n0.9905071\n0.05589067\n1148.759\n1.182233e-11\n1\n18.67896\n-31.35792\n-29.9032\n0.03123767\n10\n12"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#add-counties-names-to-augment-tibble-12",
    "href": "slides/07_SLR_Diagnostics_02.html#add-counties-names-to-augment-tibble-12",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Add counties’ names to augment tibble (1/2)",
    "text": "Add counties’ names to augment tibble (1/2)\n\nnames(gapm)\n\n[1] \"country\"                    \"life_expectancy_years_2011\"\n[3] \"female_literacy_rate_2011\" \n\nnames(aug1)\n\n[1] \".rownames\"                  \"life_expectancy_years_2011\"\n[3] \"female_literacy_rate_2011\"  \".fitted\"                   \n[5] \".resid\"                     \".hat\"                      \n[7] \".sigma\"                     \".cooksd\"                   \n[9] \".std.resid\"                \n\ngapm = gapm %&gt;% mutate(.rownames = 1:n() %&gt;% as.character())\nglimpse(gapm)\n\nRows: 188\nColumns: 4\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#add-counties-names-to-augment-tibble-22",
    "href": "slides/07_SLR_Diagnostics_02.html#add-counties-names-to-augment-tibble-22",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Add counties’ names to augment tibble (2/2)",
    "text": "Add counties’ names to augment tibble (2/2)\n\ndim(aug1)\n\n[1] 80  9\n\naug1 = left_join(aug1, gapm, \n                 by = c(\".rownames\",\n                        \"life_expectancy_years_2011\", \n                        \"female_literacy_rate_2011\"))\naug1 = aug1 %&gt;%\n  relocate(country, .after = .rownames)\nglimpse(aug1)\n\nRows: 80\nColumns: 10\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#countries-with-high-leverage-h_i-4n",
    "href": "slides/07_SLR_Diagnostics_02.html#countries-with-high-leverage-h_i-4n",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\n\nWe can look at the countries that have high leverage\n\n\naug1 %&gt;% \n  filter(.hat &gt; 4/80) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 6 × 10\n  .rownames country       life_expectancy_years_…¹ female_literacy_rate…²   .hat\n  &lt;chr&gt;     &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Afghanistan                       56.7                   13   0.136 \n2 104       Mali                              60                     24.6 0.0980\n3 34        Chad                              57                     25.4 0.0956\n4 146       Sierra Leone                      55.7                   32.6 0.0757\n5 62        Gambia                            66                     41.9 0.0540\n6 70        Guinea-Bissau                     56.2                   42.1 0.0536\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "href": "slides/07_SLR_Diagnostics_02.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "What does the model look like without the high leverage points?",
    "text": "What does the model look like without the high leverage points?\nSensitivity analysis removing countries with high leverage\n\nmodel1_lowlev &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                    data = aug1_lowlev)\ntidy(model1_lowlev) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n49.563\n3.888\n12.746\n0.000\n    female_literacy_rate_2011\n0.247\n0.044\n5.562\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#example-chp-5-problem-9",
    "href": "slides/07_SLR_Diagnostics_02.html#example-chp-5-problem-9",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Example: Chp 5 problem 9",
    "text": "Example: Chp 5 problem 9"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#example-life-expectancy",
    "href": "slides/07_SLR_Diagnostics_02.html#example-life-expectancy",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Example: life expectancy",
    "text": "Example: life expectancy"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html",
    "title": "Additional Example on Model Diagnostics",
    "section": "",
    "text": "In an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels.\nThe response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes.\nThe results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nNote: by “common logarithm” the authors mean a base 10 logarithm\n\n\n\nQuestion: why did they choose a log-log transformation?\n\n\nrats &lt;- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nRows: 12\nColumns: 3\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n\nloglog_plot &lt;- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#example-chapter-5-problem-9",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#example-chapter-5-problem-9",
    "title": "Additional Example on Model Diagnostics",
    "section": "",
    "text": "In an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels.\nThe response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes.\nThe results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nNote: by “common logarithm” the authors mean a base 10 logarithm\n\n\n\nQuestion: why did they choose a log-log transformation?\n\n\nrats &lt;- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nRows: 12\nColumns: 3\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n\nloglog_plot &lt;- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#untransform-the-variables",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#untransform-the-variables",
    "title": "Additional Example on Model Diagnostics",
    "section": "“Untransform” the variables",
    "text": "“Untransform” the variables\n\nrats &lt;- rats %&gt;% \n  mutate(\n    DOSE = 10^LOGDOSE,\n    CONC = 10^LOGCONC\n    )\nglimpse(rats)\n\nRows: 12\nColumns: 5\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n$ DOSE    &lt;dbl&gt; 1.513561, 2.137962, 2.630268, 3.467369, 4.466836, 5.623413, 6.…\n$ CONC    &lt;dbl&gt; 446.683592, 177.827941, 181.970086, 89.125094, 52.480746, 39.8…\n\nxy_plot &lt;- ggplot(rats, aes(x = DOSE, y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Untransformed variables\")\n\ngrid.arrange(xy_plot, loglog_plot, ncol = 2)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#transform-dependent-variable",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#transform-dependent-variable",
    "title": "Additional Example on Model Diagnostics",
    "section": "Transform dependent variable?",
    "text": "Transform dependent variable?\n\nggplot(rats, aes(x = CONC)) +\n  geom_histogram()\n\n\n\n\n\ngladder()\n\ngladder(rats$CONC)\n\n\n\n\n\n\nladder()\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\nladder(rats$CONC) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.3816940\n2.684203e-06\n    square\n0.4688066\n1.050091e-05\n    identity\n0.6865897\n6.277977e-04\n    sqrt\n0.8438759\n3.087109e-02\n    log\n0.9456675\n5.747562e-01\n    1/sqrt\n0.9344280\n4.294157e-01\n    inverse\n0.8654753\n5.728124e-02\n    1/square\n0.7368873\n1.956461e-03\n    1/cubic\n0.6489907\n2.837411e-04"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#transform-independent-variable",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#transform-independent-variable",
    "title": "Additional Example on Model Diagnostics",
    "section": "Transform independent variable?",
    "text": "Transform independent variable?\n\nggplot(rats, aes(x = DOSE)) +\n  geom_histogram()\n\n\n\n\n\ngladder()\n\ngladder(rats$DOSE)\n\n\n\n\n\n\nladder()\n\nladder(rats$DOSE) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.7919903\n7.607573e-03\n    square\n0.8687429\n6.299588e-02\n    identity\n0.9376639\n4.684145e-01\n    sqrt\n0.9536586\n6.909226e-01\n    log\n0.9442481\n5.549771e-01\n    1/sqrt\n0.9043429\n1.804541e-01\n    inverse\n0.8395191\n2.731726e-02\n    1/square\n0.6880945\n6.486810e-04\n    1/cubic\n0.5660067\n5.664946e-05"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#add-transformations-to-dataset",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#add-transformations-to-dataset",
    "title": "Additional Example on Model Diagnostics",
    "section": "Add transformations to dataset",
    "text": "Add transformations to dataset\n\nnames(rats)\n\n[1] \"RAT\"     \"LOGCONC\" \"LOGDOSE\" \"DOSE\"    \"CONC\"   \n\nrats &lt;- rats %&gt;% \n  mutate(\n    # LOGCONC = log10(CONC), # already in data\n    CONC_invsqrt = 1/sqrt(CONC),\n    # LOGDOSE = log10(DOSE), # already in data\n    DOSE_sqrd = DOSE^2\n  )\n\nglimpse(rats)\n\nRows: 12\nColumns: 7\n$ RAT          &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC      &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.0…\n$ LOGDOSE      &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.0…\n$ DOSE         &lt;dbl&gt; 1.513561, 2.137962, 2.630268, 3.467369, 4.466836, 5.62341…\n$ CONC         &lt;dbl&gt; 446.683592, 177.827941, 181.970086, 89.125094, 52.480746,…\n$ CONC_invsqrt &lt;dbl&gt; 0.04731513, 0.07498942, 0.07413102, 0.10592537, 0.1380384…\n$ DOSE_sqrd    &lt;dbl&gt; 2.290868, 4.570882, 6.918310, 12.022644, 19.952623, 31.62…"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#compare-scatterplots",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#compare-scatterplots",
    "title": "Additional Example on Model Diagnostics",
    "section": "Compare Scatterplots",
    "text": "Compare Scatterplots\n\nplot_m1 &lt;- ggplot(rats, aes(x = DOSE,\n                 y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod1: CONC ~ DOSE\")\n\nplot_m2 &lt;- ggplot(rats, aes(x = DOSE,\n                 y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod2: log10(CONC) ~ DOSE\")\n\nplot_m3 &lt;- ggplot(rats, aes(x = DOSE,\n                 y = CONC_invsqrt)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod3: 1/sqrt(CONC) ~ DOSE\")\n\nplot_m4 &lt;- ggplot(rats, aes(x = LOGDOSE,\n                 y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod4: CONC ~ log10(DOSE)\")\n\nplot_m5 &lt;- ggplot(rats, aes(x = DOSE_sqrd,\n                 y = CONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod5: CONC ~ DOSE + DOSE^2\")\n\nplot_m6 &lt;- ggplot(rats, aes(x = LOGDOSE,\n                 y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Mod6: log10(CONC) ~ log10(DOSE)\")\n\ngrid.arrange(plot_m1, plot_m2, plot_m3, \n             plot_m4, plot_m5, plot_m6,\n             nrow = 2)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#run-models-with-transformations",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#run-models-with-transformations",
    "title": "Additional Example on Model Diagnostics",
    "section": "Run models with transformations",
    "text": "Run models with transformations\n\nModel 1: \\(CONC \\sim DOSE\\)\n\nmodel1 &lt;- lm(CONC ~ DOSE,\n             data = rats)\n\naug1 &lt;- augment(model1)\ntidy(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n237.16138\n53.435813\n4.438248\n0.001257901\n    DOSE\n-21.32117\n6.679078\n-3.192232\n0.009617637\n  \n  \n  \n\n\n\n\n\nautoplot(model1)\n\n\n\n\n\nplot(model1, which = 5)\n\n\n\n\n\n\nModel 2: \\(log10(CONC) \\sim DOSE\\)\n\nmodel2 &lt;- lm(LOGCONC ~ DOSE,\n             data = rats)\n\naug2 &lt;- augment(model2)\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2.4997349\n0.09369594\n26.67922\n1.263560e-10\n    DOSE\n-0.1292264\n0.01171129\n-11.03434\n6.403923e-07\n  \n  \n  \n\n\n\n\n\nautoplot(model2)\n\n\n\n\n\nplot(model2, which = 5)\n\n\n\n\n\n\nModel 3: \\(1/sqrt(CONC) \\sim DOSE\\)\n\nmodel3 &lt;- lm(CONC_invsqrt ~ DOSE,\n             data = rats)\n\naug3 &lt;- augment(model3)\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n0.01453279\n0.0048459737\n2.998941\n1.336783e-02\n    DOSE\n0.02511651\n0.0006057106\n41.466185\n1.594470e-12\n  \n  \n  \n\n\n\n\n\nautoplot(model3)\n\n\n\n\n\nplot(model3, which = 5)\n\n\n\n\n\n\nModel 4: \\(CONC \\sim log10(DOSE)\\)\n\nmodel4 &lt;- lm(CONC ~ LOGDOSE,\n             data = rats)\n\naug4 &lt;- augment(model4)\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n344.8501\n52.57611\n6.559065\n6.399326e-05\n    LOGDOSE\n-342.5580\n65.45687\n-5.233339\n3.824494e-04\n  \n  \n  \n\n\n\n\n\nautoplot(model4)\n\n\n\n\n\nplot(model4, which = 5)\n\n\n\n\n\n\nModel 5: \\(CONC \\sim DOSE + DOSE^2\\)\n\nmodel5 &lt;- lm(CONC ~ DOSE + DOSE_sqrd,\n             data = rats)\n\naug5 &lt;- augment(model5)\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n415.325839\n71.27055\n5.827454\n0.0002507135\n    DOSE\n-89.294000\n23.10933\n-3.863980\n0.0038237524\n    DOSE_sqrd\n4.521273\n1.50119\n3.011792\n0.0146731693\n  \n  \n  \n\n\n\n\n\nautoplot(model5)\n\n\n\n\n\nplot(model5, which = 5)\n\n\n\n\n\n\nModel 6: \\(log10(CONC) \\sim log10(DOSE)\\)\n\nmodel6 &lt;- lm(LOGCONC ~ LOGDOSE,\n             data = rats)\n\naug6 &lt;- augment(model6)\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2.936205\n0.04230190\n69.41070\n9.390933e-15\n    LOGDOSE\n-1.785012\n0.05266556\n-33.89334\n1.182233e-11\n  \n  \n  \n\n\n\n\n\nautoplot(model6)\n\n\n\n\n\nplot(model6, which = 5)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#normal-q-q-plots-comparison",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#normal-q-q-plots-comparison",
    "title": "Additional Example on Model Diagnostics",
    "section": "Normal Q-Q plots comparison",
    "text": "Normal Q-Q plots comparison\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 2)\nplot(model2, which = 2)\nplot(model3, which = 2)\nplot(model4, which = 2)\nplot(model5, which = 2)\nplot(model6, which = 2)\n\n\n\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#residual-plots-comparison",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#residual-plots-comparison",
    "title": "Additional Example on Model Diagnostics",
    "section": "Residual plots comparison",
    "text": "Residual plots comparison\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 1)\nplot(model2, which = 1)\nplot(model3, which = 1)\nplot(model4, which = 1)\nplot(model5, which = 1)\nplot(model6, which = 1)\n\n\n\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#leverage-cooks-distance-comparison",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#leverage-cooks-distance-comparison",
    "title": "Additional Example on Model Diagnostics",
    "section": "Leverage & Cook’s distance comparison",
    "text": "Leverage & Cook’s distance comparison\n\n# par(mfrow=c(#row,#col)) is a base R command\n# It sets up the graphics window to show multiple plots in a grid\n# specify the number of rows and columns\npar(mfrow=c(2,3))  # 2 rows, 3 columns\nplot(model1, which = 5)\nplot(model2, which = 5)\nplot(model3, which = 5)\nplot(model4, which = 5)\nplot(model5, which = 5)\nplot(model6, which = 5)\n\n\n\npar(mfrow=c(1,1))  # set back to the standard 1 row x 1 column"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#models-comparison",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#models-comparison",
    "title": "Additional Example on Model Diagnostics",
    "section": "Models comparison",
    "text": "Models comparison\n\n# library(gtsummary) for tbl_regression() and tbl_merge()\n\ntbl_model1 &lt;- tbl_regression(model1)\n# tbl_model1\n\ntbl_model2 &lt;- tbl_regression(model2)\n# tbl_model2\n\ntbl_model3 &lt;- tbl_regression(model3)\n# tbl_model3\n\ntbl_model4 &lt;- tbl_regression(model4)\n# tbl_model4\n\ntbl_model5 &lt;- tbl_regression(model5)\n# tbl_model5\n\ntbl_model6 &lt;- tbl_regression(model6)\n# tbl_model6\n\n# Compare models 1-3\ntbl_merge(\n  tbls = list(tbl_model1, tbl_model2, tbl_model3),\n  tab_spanner = c(\"Model 1: y=CONC\", \"Model 2: y=log10(CONC)\", \"Model 3: y=1/sqrt(CONC)\")\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Model 1: y=CONC\n      \n      \n        Model 2: y=log10(CONC)\n      \n      \n        Model 3: y=1/sqrt(CONC)\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    DOSE\n-21\n-36, -6.4\n0.010\n-0.13\n-0.16, -0.10\n&lt;0.001\n0.03\n0.02, 0.03\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n# Compare models 4-6\ntbl_merge(\n  tbls = list(tbl_model4, tbl_model5, tbl_model6),\n  tab_spanner = c(\"Model 4: y=CONC\", \"Model 5: y=CONC\", \"Model 6: y=1/sqrt(CONC)\")\n  )\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      \n        Model 4: y=CONC\n      \n      \n        Model 5: y=CONC\n      \n      \n        Model 6: y=1/sqrt(CONC)\n      \n    \n    \n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    LOGDOSE\n-343\n-488, -197\n&lt;0.001\n\n\n\n-1.8\n-1.9, -1.7\n&lt;0.001\n    DOSE\n\n\n\n-89\n-142, -37\n0.004\n\n\n\n    DOSE_sqrd\n\n\n\n4.5\n1.1, 7.9\n0.015\n\n\n\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_ex_rats.html#other-fit-statistics-comparison",
    "href": "slides/07_SLR_Diagnostics_ex_rats.html#other-fit-statistics-comparison",
    "title": "Additional Example on Model Diagnostics",
    "section": "Other fit statistics comparison",
    "text": "Other fit statistics comparison\n\nglance(model1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.5047138\n0.4551852\n94.52814\n10.19035\n0.009617637\n1\n-70.5201\n147.0402\n148.4949\n89355.7\n10\n12\n  \n  \n  \n\n\n\nglance(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.9241025\n0.9165128\n0.1657485\n121.7567\n6.403923e-07\n1\n5.634075\n-5.26815\n-3.81343\n0.2747255\n10\n12\n  \n  \n  \n\n\n\nglance(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.9942178\n0.9936396\n0.008572545\n1719.444\n1.59447e-12\n1\n41.17696\n-76.35391\n-74.89919\n0.0007348852\n10\n12\n  \n  \n  \n\n\n\nglance(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.7325334\n0.7057867\n69.46529\n27.38784\n0.0003824494\n1\n-66.82326\n139.6465\n141.1012\n48254.26\n10\n12\n  \n  \n  \n\n\n\nglance(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.7533284\n0.6985125\n70.31878\n13.74288\n0.001838806\n2\n-66.33764\n140.6753\n142.6149\n44502.58\n9\n12\n  \n  \n  \n\n\n\nglance(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.9913701\n0.9905071\n0.05589067\n1148.759\n1.182233e-11\n1\n18.67896\n-31.35792\n-29.9032\n0.03123767\n10\n12"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#residuals",
    "href": "slides/06_SLR_Diagnostics.html#residuals",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residuals",
    "text": "Residuals\n\n\nWill be a big help for us again!!\n\nThe reiduals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#normality-with-usual-plots",
    "href": "slides/06_SLR_Diagnostics.html#normality-with-usual-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normality with “usual” plots",
    "text": "Normality with “usual” plots\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#normal-probability-plots-qq-plot",
    "href": "slides/06_SLR_Diagnostics.html#normal-probability-plots-qq-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Normal probability plots (QQ-plot)",
    "text": "Normal probability plots (QQ-plot)\nQQ plot of residuals of model1\n\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html",
    "href": "slides/07_SLR_Diagnostics_02.html",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "",
    "text": "Use visualizations and cut off points to flag potentially influential points using residuals, leverage, and Cook’s distance\nHandle influential points and assumption violations by checking data errors, reassessing the model, and making data transformations.\nImplement a model with data transformations and determine if it improves the model fit.\n\n\n\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function.\n\n\n\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#common-transformations",
    "href": "slides/07_SLR_Diagnostics_02.html#common-transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Common transformations",
    "text": "Common transformations\n\nTukey’s transformation (power) ladder\n\nUse R’s gladder() command from the describedata package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n\n\nHow to use the power ladder for the general distribution shape\n\nIf data are skewed left, we need to compress smaller values towards the rest of the data\n\nGo “up” ladder to transformations with power &gt; 1\n\nIf data are skewed right, we need to compress larger values towards the rest of the data\n\nGo “down” ladder to transformations with power &lt; 1\n\n\n\n\n\nHow to use the power ladder for heteroscedasticity\n\nIf higher \\(X\\) values have more spread\n\nCompress larger values towards the rest of the data\nGo “down” ladder to transformations with power &lt; 1\n\nIf lower \\(X\\) values have more spread\n\nCompress smaller values towards the rest of the data\nGo “up” ladder to transformations with power &gt; 1"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#gladder",
    "href": "slides/07_SLR_Diagnostics_02.html#gladder",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "gladder()",
    "text": "gladder()\n\ngladder(gapm$life_expectancy_years_2011)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#ladder",
    "href": "slides/07_SLR_Diagnostics_02.html#ladder",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder()",
    "text": "ladder()\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$life_expectancy_years_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.963\n0.000\n    square\n0.956\n0.000\n    identity\n0.944\n0.000\n    sqrt\n0.935\n0.000\n    log\n0.924\n0.000\n    1/sqrt\n0.911\n0.000\n    inverse\n0.896\n0.000\n    1/square\n0.860\n0.000\n    1/cubic\n0.815\n0.000"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#gladder-1",
    "href": "slides/07_SLR_Diagnostics_02.html#gladder-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "gladder()",
    "text": "gladder()\n\ngladder(gapm$female_literacy_rate_2011)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#ladder-1",
    "href": "slides/07_SLR_Diagnostics_02.html#ladder-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder()",
    "text": "ladder()\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$female_literacy_rate_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.850\n0.000\n    square\n0.830\n0.000\n    identity\n0.792\n0.000\n    sqrt\n0.755\n0.000\n    log\n0.693\n0.000\n    1/sqrt\n0.599\n0.000\n    inverse\n0.479\n0.000\n    1/square\n0.264\n0.000\n    1/cubic\n0.159\n0.000"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#tips",
    "href": "slides/07_SLR_Diagnostics_02.html#tips",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Tips",
    "text": "Tips\n\nRecall, assessing our LINE assumptions are not on \\(Y\\) alone!!\n\nWe can use gladder() to get a sense of what our transformations will do to the data, but we need to check with our residuals again!!\n\nTransformations usually work better if all values are positive (or negative)\nIf observation has a 0, then we cannot perform certain transformations\nLog function only defined for positive values\n\nWe might take the \\(log(X+1)\\) if \\(X\\) includes a 0 value\n\nWhen we make cubic or sqaure transformations, we MUST include the original \\(X\\)\n\nWe do not do this for \\(Y\\) though"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#example-chapter-5-problem-9",
    "href": "slides/07_SLR_Diagnostics_02.html#example-chapter-5-problem-9",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Example: Chapter 5 Problem 9",
    "text": "Example: Chapter 5 Problem 9\n\nIn an experiment designed to describe the dose–response curve for vitamin K, individual rats were depleted of their vitamin K reserves and then fed dried liver for 4 days at different dosage levels.\nThe response of each rat was measured as the concentration of a clotting agent needed to clot a sample of its blood in 3 minutes.\nThe results of the experiment on 12 rats are given in the following table; values are expressed in common logarithms for both dose and response.\n\nNote: by “common logarithm” the authors mean a base 10 logarithm\n\n\n\nQuestion: why did they choose a log-log transformation?\n\n\nrats &lt;- read_excel(\"data/CH05Q09.xls\")\nglimpse(rats)\n\nRows: 12\nColumns: 3\n$ RAT     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\n$ LOGCONC &lt;dbl&gt; 2.65, 2.25, 2.26, 1.95, 1.72, 1.60, 1.55, 1.32, 1.13, 1.07, 0.…\n$ LOGDOSE &lt;dbl&gt; 0.18, 0.33, 0.42, 0.54, 0.65, 0.75, 0.83, 0.92, 1.01, 1.04, 1.…\n\nloglog_plot &lt;- ggplot(rats, aes(x = LOGDOSE, y = LOGCONC)) +\n  geom_point() +\n  geom_smooth() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  labs(title = \"Transformed variables\")\nloglog_plot"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#reminder-of-our-regression-example",
    "href": "slides/06_SLR_Diagnostics.html#reminder-of-our-regression-example",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Reminder of our regression example",
    "text": "Reminder of our regression example\n\ngapm &lt;- read_csv(\"data/lifeexp_femlit_2011.csv\")\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\ndata = gapm)\n\ntbl_regression(model1, intercept = T, \n               label = list(female_literacy_rate_2011 ~ \"Female Literacy Rate\")) %&gt;% as_gt() %&gt;%\n  tab_options(table.font.size = 30)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n51\n46, 56\n&lt;0.001\n    Female Literacy Rate\n0.23\n0.17, 0.29\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#residuals-1",
    "href": "slides/06_SLR_Diagnostics.html#residuals-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Residuals",
    "text": "Residuals\n\nUse augment() to get a tibble with the orginal data, as well as the residuals and some other important values.\n\n\n# augment is from the broom package\naug1 &lt;- augment(model1)\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#how-do-we-determine-if-our-model-follows-the-eline-assumptions",
    "href": "slides/06_SLR_Diagnostics.html#how-do-we-determine-if-our-model-follows-the-eline-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "How do we determine if our model follows the eLINE assumptions?",
    "text": "How do we determine if our model follows the eLINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the errors are independent—there’s no connection between how far any two points lie from the regression line\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed - Usually measured through the residuals\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X - Usually measured through the residuals"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-normality-of-the-residuals",
    "href": "slides/06_SLR_Diagnostics.html#n-normality-of-the-residuals",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normality of the residuals",
    "text": "N: Normality of the residuals\n\nWe need to check if the errors/residuals (\\(\\epsilon_i\\)’s) are normally distributed\n\n \n\nDiagnostic tools:\n\nDistribution plots of residuals\nQQ plots of residuals\n\n\n \n\nExtra resource on how QQ plots are made"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-extract-models-residuals-in-r",
    "href": "slides/06_SLR_Diagnostics.html#n-extract-models-residuals-in-r",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Extract model’s residuals in R",
    "text": "N: Extract model’s residuals in R\n\nFirst extract the residuals’ values from the model output using the augment() function from the broom package.\nGet a tibble with the orginal data, as well as the residuals and some other important values.\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 &lt;- augment(model1) \n\nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-check-normality-with-usual-distribution-plots",
    "href": "slides/06_SLR_Diagnostics.html#n-check-normality-with-usual-distribution-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Check normality with “usual” distribution plots",
    "text": "N: Check normality with “usual” distribution plots\nNote that below I save each figure as an object, and then combine them together in one row of output using grid.arrange() from the gridExtra package\n\nhist1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_histogram()\n\ndensity1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_density()\n\nbox1 &lt;- ggplot(aug1, aes(x = .resid)) + geom_boxplot()\n\ngrid.arrange(hist1, density1, box1, nrow = 1)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots",
    "href": "slides/06_SLR_Diagnostics.html#examples-of-normal-qq-plots",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Examples of Normal QQ plots",
    "text": "Examples of Normal QQ plots"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "href": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n10-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n10-observations-1",
    "href": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n10-observations-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=10\\) observations)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "href": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n1000-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=1000\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "href": "slides/06_SLR_Diagnostics.html#n-examples-of-normal-qq-plots-from-n100-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)",
    "text": "N: Examples of Normal QQ plots (from \\(n=100\\) observations)\n\n\n\n\nNormal\n\nUniform\n\nT\n\nSkewed"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#lets-go-back-to-the-life-expectancy-example",
    "href": "slides/06_SLR_Diagnostics.html#lets-go-back-to-the-life-expectancy-example",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Let’s go back to the life expectancy example",
    "text": "Let’s go back to the life expectancy example\n\n\n\n2D plots of the distribution of residuals\n\n\n\n\n\n\n\n\n\n\n\n\nQQ Plot"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#gsa",
    "href": "slides/06_SLR_Diagnostics.html#gsa",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "gsa",
    "text": "gsa\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with \\(n = 80\\)\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#we-can-compare-the-qq-plots-model-vs.-theoretical",
    "href": "slides/06_SLR_Diagnostics.html#we-can-compare-the-qq-plots-model-vs.-theoretical",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "We can compare the QQ plots: model vs. theoretical",
    "text": "We can compare the QQ plots: model vs. theoretical\n\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with \\(n = 80\\)\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#e-equality-of-variance-of-the-residuals-1",
    "href": "slides/06_SLR_Diagnostics.html#e-equality-of-variance-of-the-residuals-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Equality of variance of the residuals",
    "text": "E: Equality of variance of the residuals\n\nHomoscedasticity: How do we determine if the variance across X values is constant?\nDiagnostic tool: residual plot"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "href": "slides/06_SLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of the assumptions and their diagnostic tool",
    "text": "Summary of the assumptions and their diagnostic tool\n\n\n\n\n\n\n\n\nAssumption\nWhat needs to hold?\nDiagnostic tool\n\n\n\n\nLinearity\n\\(\\text{}\\)\n\nRelationship between \\(X\\) and \\(Y\\) is linear\n\n\nScatterplot of \\(Y\\) vs. \\(X\\)\n\n\\(\\text{}\\)\n\n\nIndependence\n\\(\\text{}\\)\n\nObservations are independent from each other\n\n\nStudy design\n\n\\(\\text{}\\)\n\n\nNormality\n\\(\\text{}\\)\n\nResiduals (and thus \\(Y|X\\)) are normally distributed\n\n\nQQ plot of residuals\nDistribution of residuals\n\n\n\nEquality of variance\n\\(\\text{}\\)\n\nVariance of residuals (and thus \\(Y|X\\)) is same across \\(X\\) values (homoscedasticity)\n\n\nResidual plot\n\n\\(\\text{}\\)\n\n\n\n\n\nSLR 4"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-normal-qq-plots-qq-quantile-quantile",
    "href": "slides/06_SLR_Diagnostics.html#n-normal-qq-plots-qq-quantile-quantile",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Normal QQ plots (QQ = quantile-quantile)",
    "text": "N: Normal QQ plots (QQ = quantile-quantile)\n\nIt can be tricky to eyeball with a histogram or density plot whether the residuals are normal or not\nQQ plots are often used to help with this\n\n\n\n\nVertical axis: data quantiles\n\ndata points are sorted in order and\nassigned quantiles based on how many data points there are\n\nHorizontal axis: theoretical quantiles\n\nmean and standard deviation (SD) calculated from the data points\ntheoretical quantiles are calculated for each point, assuming the data are modeled by a normal distribution with the mean and SD of the data\n\n\n \n\nData are approximately normal if points fall on a line."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-lets-go-back-to-the-life-expectancy-example",
    "href": "slides/06_SLR_Diagnostics.html#n-lets-go-back-to-the-life-expectancy-example",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Let’s go back to the life expectancy example",
    "text": "N: Let’s go back to the life expectancy example\n\n\n\n2D plots of the distribution of residuals\n\n\n\n\n\n\n\n\n\n\n\n\nQQ Plot"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "href": "slides/06_SLR_Diagnostics.html#n-we-can-compare-the-qq-plots-model-vs.-theoretical",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: We can compare the QQ plots: model vs. theoretical",
    "text": "N: We can compare the QQ plots: model vs. theoretical\n\n\n\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\n\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n\n\n\n\n\n\n\n\n\n\nSimulated QQ plot of Normal Residuals with \\(n = 80\\)\n\n\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#e-residual-plot",
    "href": "slides/06_SLR_Diagnostics.html#e-residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Residual plot",
    "text": "E: Residual plot\n\n\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point(size = 2) +\n  geom_abline( intercept = 0, slope = 0,\n    size = 2, color = \"#FF8021\") +\n  labs(title = \"Residual plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30))"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "href": "slides/06_SLR_Diagnostics.html#lets-remind-ourselves-of-the-model-that-we-fit-last-lesson",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Let’s remind ourselves of the model that we fit last lesson",
    "text": "Let’s remind ourselves of the model that we fit last lesson\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "slides/06_SLR_Diagnostics.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Let’s remind ourselves of the model that we have been working with",
    "text": "Let’s remind ourselves of the model that we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "slides/06_SLR_Diagnostics.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Our residuals will help us a lot in our diagnostics!",
    "text": "Our residuals will help us a lot in our diagnostics!\n\n\n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#least-squares-model-assumptions-line",
    "href": "slides/06_SLR_Diagnostics.html#least-squares-model-assumptions-line",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Least-squares model assumptions: LINE",
    "text": "Least-squares model assumptions: LINE\n \nThese are the model assumptions made in ordinary least squares:\n \n\n[L] Linearity of relationship between variables\n\n\n[I] Independence of the \\(Y\\) values\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n[E] Equality of variance of the residuals (homoscedasticity)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#summary-of-line-model-assumptions",
    "href": "slides/06_SLR_Diagnostics.html#summary-of-line-model-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of LINE model assumptions",
    "text": "Summary of LINE model assumptions\n\n\\(Y\\) values are independent (check study design!)\n\n\n\n\n\n\nThe distribution of \\(Y\\) given \\(X\\) is\n\nnormal\nwith mean \\(\\mu_{y|x} = \\beta_0 + \\beta_1 \\cdot X\\)\nand common variance \\(\\sigma^2\\)\n\n\nThis means that the residuals are\n\nnormal\nwith mean = 0\nand common variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "href": "slides/06_SLR_Diagnostics.html#how-do-we-determine-if-our-model-follows-the-line-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "How do we determine if our model follows the LINE assumptions?",
    "text": "How do we determine if our model follows the LINE assumptions?\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#poll-everywhere-question-1",
    "href": "slides/06_SLR_Diagnostics.html#poll-everywhere-question-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#poll-everywhere-question-2",
    "href": "slides/06_SLR_Diagnostics.html#poll-everywhere-question-2",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#n-shapiro-wilk-test-of-normality",
    "href": "slides/06_SLR_Diagnostics.html#n-shapiro-wilk-test-of-normality",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "N: Shapiro-Wilk Test of Normality",
    "text": "N: Shapiro-Wilk Test of Normality\n\nGoodness-of-fit test for the normal distribution: Is there evidence that our residuals are from a normal distribution?\nHypothesis test:\n\n\\[\\begin{aligned}\nH_0 & : \\text{data are from a normally distributed population} \\\\\nH_1 & : \\text{data are NOT from a normally distributed population}\n\\end{aligned}\\]\n\n\n\nshapiro.test(aug1$.resid)\n\n\n    Shapiro-Wilk normality test\n\ndata:  aug1$.resid\nW = 0.90575, p-value = 2.148e-05\n\n\n\n\n\nConclusion\n\n\nReject the null. Data are not from a normal distribution."
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#e-creating-a-residual-plot",
    "href": "slides/06_SLR_Diagnostics.html#e-creating-a-residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "E: Creating a residual plot",
    "text": "E: Creating a residual plot\n\n\n\n\\(x\\) = explanatory variable from regression model\n\n(or the fitted values for a multiple regression)\n\n\\(y\\) = residuals from regression model\n\n\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point(size = 2) +\n  geom_abline( intercept = 0, slope = 0,\n    size = 2, color = \"#FF8021\") +\n  labs(title = \"Residual plot\") +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 30))"
  },
  {
    "objectID": "slides/06_SLR_Diagnostics.html#autoplot-can-be-a-helpful-tool",
    "href": "slides/06_SLR_Diagnostics.html#autoplot-can-be-a-helpful-tool",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "autoplot() can be a helpful tool",
    "text": "autoplot() can be a helpful tool\n\nlibrary(ggfortify)\nautoplot(model1) + theme(text=element_text(size=14))"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#influential-points",
    "href": "slides/07_SLR_Diagnostics_02.html#influential-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Influential points",
    "text": "Influential points\n\n\n\n\nOutliers\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\n\n\n\n \n \n\n\n\n\n\n\n\n\nHigh leverage observations\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose predictor \\(X_i\\) has an extreme value\n\\(X_i\\) can be an extremely high or low value compared to the rest of the observations"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#outliers",
    "href": "slides/07_SLR_Diagnostics_02.html#outliers",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\nHow do we determine if a point is an outlier?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nFollowed by evaluation of its residual (and standardized residual)\n\n\n \n\nUse the internally standardized residual (aka studentized residual) to determine if an observation is an outlier"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#outliers-1",
    "href": "slides/07_SLR_Diagnostics_02.html#outliers-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\nInternally standardized residual\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\nggplot() + \n  geom_histogram(data = aug1, aes(x = .std.resid))"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#what-do-we-do-with-our-outliers",
    "href": "slides/07_SLR_Diagnostics_02.html#what-do-we-do-with-our-outliers",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "What do we do with our outliers?",
    "text": "What do we do with our outliers?"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#high-leverage-observations",
    "href": "slides/07_SLR_Diagnostics_02.html#high-leverage-observations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "High leverage observations",
    "text": "High leverage observations\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(X_i\\) is considered “extreme” compared to the other values of \\(X\\)\n\n \n\nHow do we determine if a point has high leverage?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nCalculating the leverage of each observation"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#countries-with-high-leverage-h_i-4n-1",
    "href": "slides/07_SLR_Diagnostics_02.html#countries-with-high-leverage-h_i-4n-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\nLabel only countries with large leverage:\n\nggplot(aug1, aes(x = female_literacy_rate_2011, y = life_expectancy_years_2011,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat &gt; 0.05, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$female_literacy_rate_2011), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$life_expectancy_years_2011), color = \"grey\")"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#cooks-distance-1",
    "href": "slides/07_SLR_Diagnostics_02.html#cooks-distance-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\naug1 = aug1 %&gt;% relocate(.cooksd, .after = female_literacy_rate_2011)\naug1 %&gt;% arrange(desc(.cooksd))\n\n# A tibble: 80 × 10\n   .rownames country       life_expectancy_year…¹ female_literacy_rate…² .cooksd\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;\n 1 33        Central Afri…                   48                     44.2  0.126 \n 2 161       Swaziland                       48.9                   87.3  0.0903\n 3 152       South Africa                    55.8                   92.2  0.0577\n 4 187       Zimbabwe                        51.9                   80.1  0.0531\n 5 114       Morocco                         73.8                   57.6  0.0350\n 6 118       Nepal                           68.7                   46.7  0.0311\n 7 14        Bangladesh                      71                     53.4  0.0280\n 8 23        Botswana                        58.9                   85.6  0.0249\n 9 54        Equatorial G…                   61.4                   91.1  0.0231\n10 62        Gambia                          66                     41.9  0.0228\n# ℹ 70 more rows\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .hat &lt;dbl&gt;, .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;,\n#   .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#how-do-we-deal-with-influential-points",
    "href": "slides/07_SLR_Diagnostics_02.html#how-do-we-deal-with-influential-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "How do we deal with influential points?",
    "text": "How do we deal with influential points?\n\nIt’s always weird to be using numbers to help you diagnose an issue, but the issue kinda gets unresolved\nIf an observation is influential, we can check data errors:\n\nWas there a data entry or collection problem?\nIf you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\nIf an observation is influential, we can check our model:\n\nDid you leave out any important predictors?\nShould you consider adding some interaction terms?\nIs there any nonlinearity that needs to be modeled?\n\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#plotting-cooks-distance",
    "href": "slides/07_SLR_Diagnostics_02.html#plotting-cooks-distance",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Plotting Cook’s Distance",
    "text": "Plotting Cook’s Distance\n\n# plot(model) shows figures similar to autoplot()\n# adds on Cook's distance though\nplot(model1, which = 4)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#transformations",
    "href": "slides/07_SLR_Diagnostics_02.html#transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Transformations",
    "text": "Transformations\n\nWhen we have issues with our LINE (mostly linearity, normality, or equality of variance) assumptions\n\nWe can use transformations to improve the fit of the model\n\nTransformations can…\n\nMake the relationship more linear\nMake the residuals more normal\n“Stabilize” the variance so that it is more constant\nIt can also bring in or reduce outliers\n\nWe can transform the dependent (\\(Y\\)) variable of the independent (\\(X\\)) variable\n\nUsually we want to try transforming the \\(X\\) first\n\n\n \n\nRequires trial and error!!\nMajor drawback: interpreting the model becomes harder!"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#common-transformations-1",
    "href": "slides/07_SLR_Diagnostics_02.html#common-transformations-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Common transformations",
    "text": "Common transformations"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#when-we-have-detected-problems-in-our-model",
    "href": "slides/07_SLR_Diagnostics_02.html#when-we-have-detected-problems-in-our-model",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "When we have detected problems in our model…",
    "text": "When we have detected problems in our model…\n\nWe have talked about influential points\nWe have talked about identifying issues with our LINE assumptions\n\nWhat are our options once we have identified issues in our linear regression model?\n\nSee if we need to add predictors to our model\n\nNicky’s thought for our life expectancy example\n\nTry a transformation if there is an issue with linearity or normality\nTry a transformation if there is unequal variance\nTry a weighted least squares approach if unequal variance (might be lesson at end of course)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#gladder-of-life-expectancy",
    "href": "slides/07_SLR_Diagnostics_02.html#gladder-of-life-expectancy",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "gladder() of life expectancy",
    "text": "gladder() of life expectancy\n\ngladder(gapm$life_expectancy_years_2011)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#ladder-of-life-expectancy",
    "href": "slides/07_SLR_Diagnostics_02.html#ladder-of-life-expectancy",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of life expectancy",
    "text": "ladder() of life expectancy\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$life_expectancy_years_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.963\n0.000\n    square\n0.956\n0.000\n    identity\n0.944\n0.000\n    sqrt\n0.935\n0.000\n    log\n0.924\n0.000\n    1/sqrt\n0.911\n0.000\n    inverse\n0.896\n0.000\n    1/square\n0.860\n0.000\n    1/cubic\n0.815\n0.000"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#gladder-of-female-literacy-rate",
    "href": "slides/07_SLR_Diagnostics_02.html#gladder-of-female-literacy-rate",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "gladder() of female literacy rate",
    "text": "gladder() of female literacy rate\n\ngladder(gapm$female_literacy_rate_2011)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#ladder-of-female-literacy-rate",
    "href": "slides/07_SLR_Diagnostics_02.html#ladder-of-female-literacy-rate",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "ladder() of female literacy rate",
    "text": "ladder() of female literacy rate\n\n\n\nladder() output tests various transformations of the data for normality\nShapiro-Wilkes test is used to assess for normality\n\n\\(H_0\\): data are from a normal population\n\\(H_A\\): data are NOT from a normal population\n\n\n\n\nladder(gapm$female_literacy_rate_2011) %&gt;% \n  gt() %&gt;%\n  tab_options(table.font.size = 40) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      Transformation\n      statistic\n      p.value\n    \n  \n  \n    cubic\n0.850\n0.000\n    square\n0.830\n0.000\n    identity\n0.792\n0.000\n    sqrt\n0.755\n0.000\n    log\n0.693\n0.000\n    1/sqrt\n0.599\n0.000\n    inverse\n0.479\n0.000\n    1/square\n0.264\n0.000\n    1/cubic\n0.159\n0.000"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "href": "slides/07_SLR_Diagnostics_02.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "We are going to compare a few different models with transformations",
    "text": "We are going to compare a few different models with transformations\nWe are going to call life expectancy \\(LE\\) and female literacy rate \\(FLR\\)\n\nModel 1: \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 3: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 4: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\epsilon\\)\nModel 5: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots-does-linearit-improve",
    "href": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots-does-linearit-improve",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Compare Scatterplots: does linearit improve?",
    "text": "Compare Scatterplots: does linearit improve?"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#run-models-with-transformations-examples",
    "href": "slides/07_SLR_Diagnostics_02.html#run-models-with-transformations-examples",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Run models with transformations: examples",
    "text": "Run models with transformations: examples\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2,401.272\n352.070\n6.820\n0.000\n    female_literacy_rate_2011\n31.174\n4.166\n7.484\n0.000\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67,691.796\n149,056.945\n0.454\n0.651\n    female_literacy_rate_2011\n8,092.133\n8,473.154\n0.955\n0.343\n    FLR_2\n−128.596\n147.876\n−0.870\n0.387\n    FLR_3\n0.840\n0.794\n1.059\n0.293"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#reference-all-run-models",
    "href": "slides/07_SLR_Diagnostics_02.html#reference-all-run-models",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Reference: all run models",
    "text": "Reference: all run models\n\n\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2401.27207\n352.069818\n6.820443\n1.726640e-09\n    female_literacy_rate_2011\n31.17351\n4.165624\n7.483514\n9.352191e-11\n  \n  \n  \n\n\n\n\nModel 3: \\(LE^3 \\sim FLR\\)\n\nmodel3 &lt;- lm(LE_3 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n95453.189\n35631.6898\n2.678885\n9.005716e-03\n    female_literacy_rate_2011\n3166.481\n421.5875\n7.510853\n8.285324e-11\n  \n  \n  \n\n\n\n\nModel 4: \\(LE \\sim FLR + FLR^2\\)\n\nmodel4 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2,\n             data = gapm)\n\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n57.030875456\n6.282845592\n9.07723652\n8.512585e-14\n    female_literacy_rate_2011\n0.019348795\n0.201021963\n0.09625215\n9.235704e-01\n    FLR_2\n0.001578649\n0.001472592\n1.07202008\n2.870595e-01\n  \n  \n  \n\n\n\n\n\nModel 5: \\(LE \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel5 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n4.732796e+01\n1.117939e+01\n4.2335001\n6.373341e-05\n    female_literacy_rate_2011\n6.517986e-01\n6.354934e-01\n1.0256576\n3.083065e-01\n    FLR_2\n-9.952763e-03\n1.109080e-02\n-0.8973895\n3.723451e-01\n    FLR_3\n6.245016e-05\n5.953283e-05\n1.0490038\n2.975008e-01\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67691.7963283\n1.490569e+05\n0.4541338\n0.6510268\n    female_literacy_rate_2011\n8092.1325988\n8.473154e+03\n0.9550320\n0.3425895\n    FLR_2\n-128.5960879\n1.478757e+02\n-0.8696230\n0.3872447\n    FLR_3\n0.8404736\n7.937625e-01\n1.0588477\n0.2930229\n  \n  \n  \n\n\n\n\n\n\n\n\nSLR 5"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots-does-linearity-improve",
    "href": "slides/07_SLR_Diagnostics_02.html#compare-scatterplots-does-linearity-improve",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Compare Scatterplots: does linearity improve?",
    "text": "Compare Scatterplots: does linearity improve?"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#without-high-leverage-points-qq-plot-residual-plot",
    "href": "slides/07_SLR_Diagnostics_02.html#without-high-leverage-points-qq-plot-residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Without high leverage points: QQ Plot, Residual plot",
    "text": "Without high leverage points: QQ Plot, Residual plot"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#model-without-influential-points-qq-plot-residual-plot",
    "href": "slides/07_SLR_Diagnostics_02.html#model-without-influential-points-qq-plot-residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Model without influential points: QQ Plot, Residual plot",
    "text": "Model without influential points: QQ Plot, Residual plot\n\nmodel1_lowcd &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                    data = aug1_lowcd)\ntidy(model1_lowcd) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#model-without-those-4-points-qq-plot-residual-plot",
    "href": "slides/07_SLR_Diagnostics_02.html#model-without-those-4-points-qq-plot-residual-plot",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\nmodel1_lowcd &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                    data = aug1_lowcd)\ntidy(model1_lowcd) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n52.388\n2.078\n25.208\n0.000\n    female_literacy_rate_2011\n0.226\n0.024\n9.208\n0.000\n  \n  \n  \n\n\n\ntidy(model1) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.928\n2.660\n19.143\n0.000\n    female_literacy_rate_2011\n0.232\n0.031\n7.377\n0.000"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "href": "slides/07_SLR_Diagnostics_02.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am okay with this!\n\nAnd don’t forget: we may want more variables in our model!\nYou do not need to produce plots with the influential points taken out"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "slides/07_SLR_Diagnostics_02.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Let’s remind ourselves of the model that we have been working with",
    "text": "Let’s remind ourselves of the model that we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "slides/07_SLR_Diagnostics_02.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Our residuals will help us a lot in our diagnostics!",
    "text": "Our residuals will help us a lot in our diagnostics!\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#identifying-outliers",
    "href": "slides/07_SLR_Diagnostics_02.html#identifying-outliers",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Identifying outliers",
    "text": "Identifying outliers\n\n\n\n\nInternally standardized residual\n\n\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\n\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\n\n\n \n\nggplot(data = aug1) + \n  geom_histogram(aes(x = .std.resid))"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#countries-that-are-outliers-r_i-2",
    "href": "slides/07_SLR_Diagnostics_02.html#countries-that-are-outliers-r_i-2",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\n\nWe can identify the countries that are outliers\n\n\naug1 %&gt;% \n  filter(abs(.std.resid) &gt; 2)\n\n# A tibble: 4 × 10\n  .rownames country     life_expectancy_year…¹ female_literacy_rate…² .std.resid\n  &lt;chr&gt;     &lt;chr&gt;                        &lt;dbl&gt;                  &lt;dbl&gt;      &lt;dbl&gt;\n1 33        Central Af…                   48                     44.2      -2.20\n2 152       South Afri…                   55.8                   92.2      -2.71\n3 161       Swaziland                     48.9                   87.3      -3.65\n4 187       Zimbabwe                      51.9                   80.1      -2.89\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#getting-extra-information-on-the-fitted-model",
    "href": "slides/07_SLR_Diagnostics_02.html#getting-extra-information-on-the-fitted-model",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Getting extra information on the fitted model",
    "text": "Getting extra information on the fitted model\n\nWe ran the augment() function to get some diagnostic values, including the residuals\nTakes in a model object\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\nAnd now we run model1 through augment() (model1 is input)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#augment-getting-extra-information-on-the-fitted-model",
    "href": "slides/07_SLR_Diagnostics_02.html#augment-getting-extra-information-on-the-fitted-model",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "augment(): getting extra information on the fitted model",
    "text": "augment(): getting extra information on the fitted model\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#identifying-points-with-high-cooks-distance",
    "href": "slides/07_SLR_Diagnostics_02.html#identifying-points-with-high-cooks-distance",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Identifying points with high Cook’s distance",
    "text": "Identifying points with high Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\naug1 = aug1 %&gt;% relocate(.cooksd, .after = female_literacy_rate_2011)\naug1 %&gt;% arrange(desc(.cooksd))\n\n# A tibble: 80 × 10\n   .rownames country       life_expectancy_year…¹ female_literacy_rate…² .cooksd\n   &lt;chr&gt;     &lt;chr&gt;                          &lt;dbl&gt;                  &lt;dbl&gt;   &lt;dbl&gt;\n 1 33        Central Afri…                   48                     44.2  0.126 \n 2 161       Swaziland                       48.9                   87.3  0.0903\n 3 152       South Africa                    55.8                   92.2  0.0577\n 4 187       Zimbabwe                        51.9                   80.1  0.0531\n 5 114       Morocco                         73.8                   57.6  0.0350\n 6 118       Nepal                           68.7                   46.7  0.0311\n 7 14        Bangladesh                      71                     53.4  0.0280\n 8 23        Botswana                        58.9                   85.6  0.0249\n 9 54        Equatorial G…                   61.4                   91.1  0.0231\n10 62        Gambia                          66                     41.9  0.0228\n# ℹ 70 more rows\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .hat &lt;dbl&gt;, .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;,\n#   .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#summary-of-how-we-identify-influential-points",
    "href": "slides/07_SLR_Diagnostics_02.html#summary-of-how-we-identify-influential-points",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of how we identify influential points",
    "text": "Summary of how we identify influential points\n\nUse scatterplot of \\(Y\\) vs. \\(X\\) to see if any points fall outside of range we expect\nUse standardized residuals, leverage, and Cook’s distance to further identify those points\nLook at the models run with and without the identified points to check for drastic changes\n\nLook at QQ plot and residuals to see if assumptions hold without those points\nLook at coefficient estimates to see if they change in sign and large magnitude\n\n\n \n\nNext: how to handle? It’s a little wishy washy"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#revisiting-our-line-assumptions",
    "href": "slides/07_SLR_Diagnostics_02.html#revisiting-our-line-assumptions",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Revisiting our LINE assumptions",
    "text": "Revisiting our LINE assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#summary-of-transformations",
    "href": "slides/07_SLR_Diagnostics_02.html#summary-of-transformations",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Summary of transformations",
    "text": "Summary of transformations\n\nIf the model without the transformation is blatantly violating a LINE assumption\n\nThen a transformation is a good idea\n\nIf the model without a transformation is not following the LINE assumptions very well, but is mostly okay\n\nThen try to avoid a transformation\nThink about what predictors might need to be added\nEspecially if you keep seeing the same points as influential\n\nIf interpretability is important in your final work, then transformations are not a great solution"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-1",
    "href": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-1",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-2",
    "href": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-2",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-3",
    "href": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-3",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-4",
    "href": "slides/07_SLR_Diagnostics_02.html#poll-everywhere-question-4",
    "title": "SLR: Model Evaluation and Diagnostics",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "extra_resources/Coefficient_interp.html",
    "href": "extra_resources/Coefficient_interp.html",
    "title": "Coefficient interpretations",
    "section": "",
    "text": "Keep in mind that words expected, average, and mean are interchangeable.\nThe following are required parts of the interpretation\n\nUnits of Y\nUnits of X\nDiscussing intercept: Mean or average or expected before Y\nDiscussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nOnly need before difference or Y!!\n\nConfidence interval\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name\n\n\n\nWhen the estimate of the population coefficient is…\n\nPositive: using the word “increase” in the place of “difference” helps with understanding the relationship\nNegative: using the word “decrease” in the place of “difference” helps with understanding the relationship\n\n\n\n\n\n\n\n\n\n\nPopulation Model\nVariable information\nCoefficient estimate interpretations\n\n\n\n\n\\(Y=\\beta_0+\\beta_1X_1+\\epsilon\\)\n\\(X_1\\): continuous covariate\n\n\\(\\widehat\\beta_0\\): Mean \\(Y\\) when \\(X_1\\) is 0\n\nExample: For someone who is 0 years old, the expected peak exercise heart rate is 214.233 beats per minute (95% CI: 204.918, 223.548)\n\n\\(\\widehat\\beta_1\\): Mean difference in \\(Y\\) per 1 unit increase in \\(X_1\\)\n\nExample: For every one year increase in age, the expected peak exercise heart rate decreases 0.834 bpm (95% CI: ….)\n\n\n\n\n\\(Y=\\beta_0+\\beta_1X^c_1+\\epsilon\\)\n\\(X^c_1\\): continuous covariate that is centered around its mean or median\n\n\\(\\widehat\\beta_0\\): Mean \\(Y\\) when \\(X^c_1\\) is at its mean or median\n\\(\\widehat\\beta_1\\): Mean difference in \\(Y\\) per 1 unit increase in \\(X^c_1\\)\n\n\n\n\\(Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\epsilon\\)\n\\(X_1\\): continuous covariate\n\\(X_2\\): continuous covariate"
  },
  {
    "objectID": "slides/00_Quiz_Lab_1.html#coefficient-interpretations",
    "href": "slides/00_Quiz_Lab_1.html#coefficient-interpretations",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Coefficient interpretations",
    "text": "Coefficient interpretations\nPopulation model: \\[\nE[Y|X] = \\beta_0 + \\beta_1X\n\\] What is \\(\\beta_1\\) mean?\n\nLet’s say we have \\(X = x_1\\) and \\(X=x_2\\)\nThe difference between \\(x_1\\) and \\(x_2\\) is 1: \\(x_1 - x_2 = 1\\)\nWe don’t have to know the actual values of the x’s, just that their difference is 1\nNow, let’s look at the expected values for each of those x’s:\n\n\\[ \\begin{aligned}\n    E[Y|x_1] & = \\beta_0 + \\beta_1x_1 \\\\\n    E[Y|x_2] & = \\beta_0 + \\beta_1x_2\n\\end{aligned}\\] - If we take the difference between the expected values, we get: \\[ \\begin{aligned}\n    E[Y|x_1] - E[Y|x_2] & = (\\beta_0 + \\beta_1x_1) - (\\beta_0 + \\beta_1x_2) \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_0 + \\beta_1x_1 - \\beta_0 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1x_1 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1 (x_1 - x_2) \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{x_1 - x_2} \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{1} \\\\\n    \\beta_1 & = E[Y|x_1] - E[Y|x_2]\n\\end{aligned}\\]\n\nSo, we can consider \\(\\beta_1\\) as the difference in the expected Y for every 1 unit increase in X\nOr: we can look at \\(\\beta_1\\) another way: \\[ \\begin{aligned}\n\\beta_1 & = E[Y|x_1] - E[Y|x_2] \\\\\n\\beta_1 & = E\\big[ (Y|x_1) - (Y|x_2) \\big] \\\\\n\\end{aligned}\\]\n\nThis would make \\(beta_1\\) the expected difference in Y for every 1 unit increase in X\n\n\n\n\nQuiz and Lab Notes"
  },
  {
    "objectID": "homework/HW3.html#section",
    "href": "homework/HW3.html#section",
    "title": "Homework 3",
    "section": "(2)",
    "text": "(2)\nWrite out the regression equation for the simple linear regression model."
  },
  {
    "objectID": "homework/HW3.html#questions",
    "href": "homework/HW3.html#questions",
    "title": "Homework 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nA high respiratory rate is a potential diagnostic indicator of respiratory infection in children. To judge whether a respiratory rate is “high” however, a physician must have a clear picture of the distribution of normal rates. To this end, Italian researchers measured the respiratory rates (in breaths/minute) of 618 children between the ages of 15 days and 3 years (measured in months).\nThe data and problem framing came from the Sleuth3 package. Please make sure to run the following code to load the data. You can directly access the dataset ex0824 from the package. I have included a new assignment of the data to q1_data if you would like to use that.\n\nif(!require(Sleuth3)) { install.packages(\"Sleuth3\"); library(Sleuth3) }\nq1_data = ex0824 \n\n\nPart a\nCreate a scatterplot of the dependent and independent variables with both the best-fit line and a smoothed curve through the points. Describe the relationship between the dependent and independent variables, and also comment on whether you think it is reasonable to use a linear regression to model the relationship.\n\n\nPart b\nWrite out the population regression model for the simple linear regression model. Please leave the variables untransformed for now.\n\n\nPart c\nFit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart d\nAssess the normality of the model’s fitted residuals by creating a histogram, density plot, and boxplot of the residuals to visually inspect the distribution of the residuals, and describe any deviations from normality.\n\n\nPart e\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals.\nBonus work, but not required: Compare the QQ plot to 4 such plots simulated from normal data, and discuss why or why not the residuals could have come from a normal distribution.\n\n\nPart f\nTest the normality of the model’s fitted residuals and comment on whether the test’s conclusion is consistent with your visual inspection or not. Make sure to include the hypotheses, needed R code, and a conclusion to the test based on the p-value (as shown in these slides).\n\n\nPart g\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions.\n\n\nPart h\nDetermine whether there are any observations with high leverage. Please use the cutoff, \\(h_i &gt; 6/n\\). If there are observations with high leverage, print the observations and state how many high leverage points there are.\n\n\nPart i\nPrint the 10 observations with highest Cook’s distance. If there are observations with high Cook’s distance (\\(d_i &gt;1\\)), state how many observations have high Cook’s distance.\n\n\nPart j\nCreate a histogram for rate. Describe its distribution shapes.\n\n\nPart k\nUsing the above histogram, and Tukey’s ladder of transformations, discuss the range of transformations that will be appropriate for Rate. Explain your reasoning.\nThen use gladder() to decide on two possible transformations. Explain your reasoning.\nNote: questions below will ask about model fit with the transformations. For now, just explain why you chose the two that you did.\n\n\nPart l\nAdd the two rate transformations you chose above to the dataset. You do not need to print any output, just make sure the code is visible.\n\n\nPart m\nCreate scatterplots using two transformed rates and age. Discuss if either transformation potentially improves the model fit. Explain why or why not. Note: including lines will help!\n\n\nPart n\nUsing one of the transoformed outcomes, fit the regression model, display the regression table, and write out the fitted regression line.\n\n\nPart o\nAssess the normality of the model’s fitted residuals by creating QQ plot of the residuals. Does the transformation improve the QQ plot?\n\n\nPart p\nCreate a residual plot using ggplot and the residuals. Discuss what this means in the context of our model assumptions. Does the transformation improve our model assumptions?\n\n\nPart q\nBetween the model with the untransformed outcome and the transformed outcome, which would you recommend using for analysis? (Hint: there are pros and cons to both models)\n\n\n\nQuestion 2\nThis question uses the same dataset as HW 2, question 1.\nThis question is based on data collected as part of an observational study of patients who suffered from stroke.\nDataset: The main goal was to study various psychological factors: optimism, fatalism, depression, spirituality, and their relationship with stroke severity and other health outcomes among the study participants. Data were collected using questionnaires during a baseline interview and also medical chart review. More information about this study can be found in the article Fatalism, optimism, spirituality, depressive symptoms and stroke outcome: a population based analysis.\nThe dataset that you will work with is called completedata.sas7bdat. The two variables we are interested in are:\n\nCovariate 1: Fatalism (larger values indicate that the individual feels less control of their life)\n\nPotential scores range from 8 to 40\n\nCovariate 2: Optimism (larger values indicate that the individual feels higher levels of optimism)\n\nPotential scores range from 6 to 24\n\nCovariate 3: Spirituality (larger values indicate that the individual has more belief in a higher power)\n\nPotential scores range from 2 to 8\n\nOutcome: Depression (larger values imply increased depression)\n\nPotential scores range from 0 to 27\n\n\nFor our homework purposes we will assume each variable is continuous.\n\ndep_df = read_sas(here(\"./homework/data/completedata.sas7bdat\"))\n\n\nPart a\nFit the regression model with all the covariates (Fatalism, Optimism, Spirituality), display the regression table, and write out the fitted regression line.\n\n\nPart b\nInterpret each coefficient (\\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\)).\nDoes the intercept make sense for the range of values that each covariate can take? Explain.\n\n\nPart c\nRecall in Homework 2, we ran a simple linear regression model for Depression vs. Fatalism with the following interpretation for the coefficient: For every 1 point higher fatalism score, there is an expected difference of 0.25 points higher depression score (95%CI: 0.17, 0.32).\nDoes the addition of Optimism and Spirituality change our coefficient estimate for Fatalism? (No need for an official hypothesis test here. I just want us to note some differences.)\n\n\nPart d\nFrom the fitted regression model, calculate the regression line when Optimism score is 10 and Spirituality score is 6.\n\n\nPart e\nDoes at least one of the covariates contribute significantly to the prediction of Depression? (Note: this is an overall test. Please follow the hypothesis test steps. To complete step 4-6, simply output your ANOVA table.)\n\n\nPart f\nDoes the addition of Spirituality add significantly to the prediction of Depression achieved by Fatalism and Optimism?\n\n\nPart g\nDoes the addition of Spirituality and Optimism add significantly to the prediction of Depression achieved by Fatalism?"
  },
  {
    "objectID": "homework/HW4.html#penguins-flipper-length-vs.-species",
    "href": "homework/HW4.html#penguins-flipper-length-vs.-species",
    "title": "Homework 4",
    "section": "Penguins: Flipper length vs. species",
    "text": "Penguins: Flipper length vs. species\nFor this problem we will be using the penguins dataset from the palmerpenguins R package.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data are at https://allisonhorst.github.io/palmerpenguins/.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\n(1) Outcome averages stratified by category levels\nCalculate the average flipper lengths stratified by each of the penguin species.\n\n\n(2) Visualize the “regression”\nMake a scatterplot of flipper lengths by species, and include diamond-shape points for the averages of the flipper lengths for each of the species.\n\n\n(3) Regression equations\nBefore running the regression in R, we are going to find the regression equation”manually.”\nWrite out the regression equation using LaTeX math markup (see class notes) that models the flipper length by penguin species. Do not yet insert values for the regression coefficients, i.e. us the generic coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1,\\) etc. Use Adelie as the reference level.\n\n\n(4) Interpret coefficients\nHow do we interpret each of the regression coefficients for this model? Write out a separate interpretation for each of the coefficients.\n\n\n(5) Regression coefficients “manually”\n“Manually” calculate the values for each of the coefficients, and update the regression model with the values inserted.\nYou must show your work for this. Do not run the linear model in this step to get the values.\n\n\n(6) Regression table with lm() function\nRun the linear regression of flipper lengths vs. species in R, and display the regression table output. Which species did R choose as the reference level, and how did you determine this?\n\n\n(7) Mean calculation using regression output\nCalculate the mean flipper length of penguins in the Chinstrap and Gentoo species using only the results from the regression table. You must show your work."
  },
  {
    "objectID": "homework/HW4.html#problem-9.7-c",
    "href": "homework/HW4.html#problem-9.7-c",
    "title": "Homework 4",
    "section": "Problem 9.7 (c)",
    "text": "Problem 9.7 (c)\nDo this problem after completing 9.7 (b) above\n\nProvide variables-added-in-order tests for the order \\(X_3, X_1\\), and \\(X_2\\).\n\nThis means that there are 3 tests: (1) test model with just \\(X_3\\), (2) test adding \\(X_1\\) to the model given that \\(X_3\\) is already in the model, and (3) test adding \\(X_2\\) to the model given that \\(X_3, X_1\\) are already in the model. See the subsections below to divide up the work.\n\nTest model with just \\(X_3\\)\n\n\nAdding \\(X_1\\) to the model given that \\(X_3\\) is already in the model\n\n\nAdding \\(X_2\\) to the model given that \\(X_3, X_1\\) are already in the model"
  },
  {
    "objectID": "homework/HW4.html#chapter-10-c-using-the-formula",
    "href": "homework/HW4.html#chapter-10-c-using-the-formula",
    "title": "Homework 4",
    "section": "Chapter 10 (c) using the formula",
    "text": "Chapter 10 (c) using the formula\nDo this after completing Chapter 10 (c) above.\nCalculate \\(r_{YX1|X2}\\) using the formula and check that your answer matches that of Chapter 10 (c) above."
  },
  {
    "objectID": "homework/HW4.html#regression-with-one-categorical-predictor-prequel-to-ch-11-12-change-the-reference-level-to-gentoo",
    "href": "homework/HW4.html#regression-with-one-categorical-predictor-prequel-to-ch-11-12-change-the-reference-level-to-gentoo",
    "title": "Homework 4",
    "section": "Regression with one categorical predictor (Prequel to Ch 11 & 12): Change the reference level to Gentoo",
    "text": "Regression with one categorical predictor (Prequel to Ch 11 & 12): Change the reference level to Gentoo\nAfter completing exercises (1) - (7) in the section Regression with one categorical predictor (Prequel to Ch 11 & 12), do the problems below.\n\n(8)\nWrite out the regression equation using LaTeX math markup (see class notes) that models the flipper length by penguin species. Do not yet insert values for the regression coefficients, i.e. us the generic coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1,\\) etc. Use Gentoo as the reference level.\n\n\n(9)\nHow do we interpret each of the regression coefficients for this model? Write out a separate interpretation for each of the coefficients.\n\n\n(10)\n“Manually” calculate the values for each of the coefficients, and update the regression model with the values inserted. You must show your work for this. Do not run the linear model in this step to get the values."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#lets-start-with-an-example",
    "href": "slides/08_MLR_Intro.html#lets-start-with-an-example",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s start with an example",
    "text": "Let’s start with an example\n\n\n\n\n\n\n\n\n \nAverage life expectancy vs. female literacy rate\n \n\nEach point on the plot is for a different country\n\n \n\n\\(X\\) = country’s adult female literacy rate\n\n \n\n\\(Y\\) = country’s average life expectancy (years)\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#reference-how-did-i-code-that",
    "href": "slides/08_MLR_Intro.html#reference-how-did-i-code-that",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Reference: How did I code that?",
    "text": "Reference: How did I code that?\n\n\nggplot(gapm, aes(x = female_literacy_rate_2011,\n                 y = life_expectancy_years_2011)) +\n  geom_point(size = 4) +\n  geom_smooth(method = \"lm\", se = FALSE, size = 3, colour=\"#F14124\") +\n  labs(x = \"Female literacy rate (%)\", \n       y = \"Life expectancy (years)\",\n       title = \"Relationship between life expectancy and \\n the female literacy rate in 2011\") +\n    theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30))"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#dataset-description",
    "href": "slides/08_MLR_Intro.html#dataset-description",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Dataset description",
    "text": "Dataset description\n\nData files\n\nCleaned: lifeexp_femlit_2011.csv\nNeeds cleaning: lifeexp_femlit_water_2011.csv\n\nData were downloaded from Gapminder\n2011 is the most recent year with the most complete data\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#get-to-know-the-data-12",
    "href": "slides/08_MLR_Intro.html#get-to-know-the-data-12",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Get to know the data (1/2)",
    "text": "Get to know the data (1/2)\n\nLoad data\n\n\ngapm_original &lt;- read_csv(here::here(\"data\", \"lifeexp_femlit_water_2011.csv\"))\n\n\nGlimpse of the data\n\n\nglimpse(gapm_original)\n\nRows: 194\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 92.6, 100.0, 40.3, 97.0, 99.5, …\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q2\", \"Q4\", \"Q1\", \"Q3\", \"Q4\", \"…\n\n\n\nNote the missing values for our variables of interest"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#get-to-know-the-data-22",
    "href": "slides/08_MLR_Intro.html#get-to-know-the-data-22",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Get to know the data (2/2)",
    "text": "Get to know the data (2/2)\n\nGet a sense of the summary statistics\n\n\ngapm_original %&gt;% \n  select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  summary()\n\n life_expectancy_years_2011 female_literacy_rate_2011\n Min.   :47.50              Min.   :13.00            \n 1st Qu.:64.30              1st Qu.:70.97            \n Median :72.70              Median :91.60            \n Mean   :70.66              Mean   :81.65            \n 3rd Qu.:76.90              3rd Qu.:98.03            \n Max.   :82.90              Max.   :99.80            \n NA's   :7                  NA's   :114"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#remove-missing-values-12",
    "href": "slides/08_MLR_Intro.html#remove-missing-values-12",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Remove missing values (1/2)",
    "text": "Remove missing values (1/2)\n\nRemove rows with missing data for life expectancy and female literacy rate\n\n\ngapm &lt;- gapm_original %&gt;% \n  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)\n\nglimpse(gapm)\n\nRows: 80\nColumns: 5\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\", \"Antigu…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ water_basic_source_2011    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8, 96.7, 9…\n$ water_2011_quart           &lt;chr&gt; \"Q1\", \"Q2\", \"Q1\", \"Q3\", \"Q4\", \"Q3\", \"Q3\", \"…\n\n\n\nNo missing values now for our variables of interest"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#remove-missing-values-22",
    "href": "slides/08_MLR_Intro.html#remove-missing-values-22",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Remove missing values (2/2)",
    "text": "Remove missing values (2/2)\n\nAnd no more missing values when we look only at our two variables of interest\n\n\ngapm %&gt;% select(life_expectancy_years_2011, female_literacy_rate_2011) %&gt;% \n  get_summary_stats()\n\n# A tibble: 2 × 13\n  variable        n   min   max median    q1    q3   iqr   mad  mean    sd    se\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 life_expec…    80    48  81.8   72.4  65.9  75.8  9.95  6.30  69.9  7.95 0.889\n2 female_lit…    80    13  99.8   91.6  71.0  98.0 27.0  11.4   81.7 22.0  2.45 \n# ℹ 1 more variable: ci &lt;dbl&gt;\n\n\n\n\nNote\n\n\n\nRemoving the rows with missing data was not needed to run the regression model.\nI did this step since later we will be calculating the standard deviations of the explanatory and response variables for just the values included in the regression model. It’ll be easier to do this if we remove the missing values now."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#poll-everywhere-question-1",
    "href": "slides/08_MLR_Intro.html#poll-everywhere-question-1",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "href": "slides/08_MLR_Intro.html#questions-we-can-ask-with-a-simple-linear-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Questions we can ask with a simple linear regression model",
    "text": "Questions we can ask with a simple linear regression model\n\n\n\n\n\n\n\n\n\nHow do we…\n\ncalculate slope & intercept?\ninterpret slope & intercept?\ndo inference for slope & intercept?\n\nCI, p-value\n\ndo prediction with regression line?\n\nCI for prediction?\n\n\nDoes the model fit the data well?\n\nShould we be using a line to model the data?\n\nShould we add additional variables to the model?\n\nmultiple/multivariable regression\n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#association-vs.-prediction",
    "href": "slides/08_MLR_Intro.html#association-vs.-prediction",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Association vs. prediction",
    "text": "Association vs. prediction\n\n\n\n\nAssociation\n\n\n\nWhat is the association between countries’ life expectancy and female literacy rate?\nUse the slope of the line or correlation coefficient\n\n\n\n\n\n\nPrediction\n\n\n\nWhat is the expected average life expectancy for a country with a specified female literacy rate?    \n\n\n\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#three-types-of-study-design-there-are-more",
    "href": "slides/08_MLR_Intro.html#three-types-of-study-design-there-are-more",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Three types of study design (there are more)",
    "text": "Three types of study design (there are more)\n\n\n\n\nExperiment\n\n\n\nObservational units are randomly assigned to important predictor levels\n\nRandom assignment controls for confounding variables (age, gender, race, etc.)\n“gold standard” for determining causality\nObservational unit is often at the participant-level\n\n\n\n\n\n\n\nQuasi-experiment\n\n\n\nParticipants are assigned to intervention levels without randomization\nNot common study design\n\n\n\n\n\n\nObservational\n\n\n\nNo randomization or assignment of intervention conditions\nIn general cannot infer causality\n\nHowever, there are casual inference methods…"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#lets-revisit-the-regression-analysis-process",
    "href": "slides/08_MLR_Intro.html#lets-revisit-the-regression-analysis-process",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s revisit the regression analysis process",
    "text": "Let’s revisit the regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#poll-everywhere-question-2",
    "href": "slides/08_MLR_Intro.html#poll-everywhere-question-2",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#simple-linear-regression-model",
    "href": "slides/08_MLR_Intro.html#simple-linear-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X\\) is our independent variable\n\nAka predictor, regressor, exposure variable\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are unknown population parameters\n\\(\\epsilon\\) (epsilon) is the error about the line\n\nIt is assumed to be a random variable with a…\n\nNormal distribution with mean 0 and constant variance \\(\\sigma^2\\)\ni.e. \\(\\epsilon \\sim N(0, \\sigma^2)\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#simple-linear-regression-model-another-way-to-view-components",
    "href": "slides/08_MLR_Intro.html#simple-linear-regression-model-another-way-to-view-components",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Simple Linear Regression Model (another way to view components)",
    "text": "Simple Linear Regression Model (another way to view components)\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "href": "slides/08_MLR_Intro.html#if-the-population-parameters-are-unobservable-how-did-we-get-the-line-for-life-expectancy",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "If the population parameters are unobservable, how did we get the line for life expectancy?",
    "text": "If the population parameters are unobservable, how did we get the line for life expectancy?\n\n\n \n\nNote: the population model is the true, underlying model that we are trying to estimate using our sample data\n\nOur goal in simple linear regression is to estimate \\(\\beta_0\\) and \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#poll-everywhere-question-3",
    "href": "slides/08_MLR_Intro.html#poll-everywhere-question-3",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#okay-so-how-do-we-estimate-the-regression-line",
    "href": "slides/08_MLR_Intro.html#okay-so-how-do-we-estimate-the-regression-line",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Okay, so how do we estimate the regression line?",
    "text": "Okay, so how do we estimate the regression line?\n \nAt this point, we are going to move over to an R shiny app that I made.\n \nLet’s see if we can eyeball the best-fit line!"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#regression-line-best-fit-line",
    "href": "slides/08_MLR_Intro.html#regression-line-best-fit-line",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression line = best-fit line",
    "text": "Regression line = best-fit line\n\n\n\\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X \\]\n\n\\(\\widehat{Y}\\) is the predicted outcome for a specific value of \\(X\\)\n\\(\\widehat{\\beta}_0\\) is the intercept of the best-fit line\n\\(\\widehat{\\beta}_1\\) is the slope of the best-fit line, i.e., the increase in \\(\\widehat{Y}\\) for every increase of one (unit increase) in \\(X\\)\n\nslope = rise over run"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#simple-linear-regression-model-1",
    "href": "slides/08_MLR_Intro.html#simple-linear-regression-model-1",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\n\nPopulation regression model\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term\n\n\n\n\nEstimated regression line\n\n\\[\\widehat{Y} =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X\\]\n\n \nComponents\n\n\n\n\n\n\n\n\\(\\widehat{Y}\\)\nestimated expected response given predictor \\(X\\)\n\n\n\\(\\widehat{\\beta}_0\\)\nestimated intercept\n\n\n\\(\\widehat{\\beta}_1\\)\nestimated slope\n\n\n\\(X\\)\npredictor, covariate, independent variable"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "href": "slides/08_MLR_Intro.html#we-get-it-nicky-how-do-we-estimate-the-regression-line",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "We get it, Nicky! How do we estimate the regression line?",
    "text": "We get it, Nicky! How do we estimate the regression line?\nFirst let’s take a break!!"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#it-all-starts-with-a-residual",
    "href": "slides/08_MLR_Intro.html#it-all-starts-with-a-residual",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "It all starts with a residual…",
    "text": "It all starts with a residual…\n\n\n\nRecall, one characteristic of our population model was that the residuals, \\(\\epsilon\\), were Normally distributed: \\(\\epsilon \\sim N(0, \\sigma^2)\\)\nIn our population regression model, we had: \\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\nWe can also take the average (expected) value of the population model\nWe take the expected value of both sides and get:\n\n\\[\\begin{aligned}\n        E[Y] & = E[\\beta_0 + \\beta_1X + \\epsilon] \\\\\n        E[Y] & = E[\\beta_0] + E[\\beta_1X] + E[\\epsilon] \\\\\n        E[Y] & = \\beta_0 + \\beta_1X + E[\\epsilon] \\\\\n        E[Y|X] & = \\beta_0 + \\beta_1X \\\\\n\\end{aligned}\\]\n\nWe call \\(E[Y|X]\\) the expected value of \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#so-now-we-have-two-representations-of-our-population-model",
    "href": "slides/08_MLR_Intro.html#so-now-we-have-two-representations-of-our-population-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "So now we have two representations of our population model",
    "text": "So now we have two representations of our population model\n\n\n\n\nWith observed \\(Y\\) values and residuals:\n\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\n\n\nWith the population expected value of \\(Y\\) given \\(X\\):\n\n\n\\[E[Y|X] = \\beta_0 + \\beta_1X\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our residuals:\n\\[\\begin{aligned}\nY & = (\\beta_0 + \\beta_1X) + \\epsilon \\\\\nY & = E[Y|X] + \\epsilon \\\\\nY - E[Y|X] & = \\epsilon \\\\\n\\epsilon & = Y - E[Y|X]\n\\end{aligned}\\]\nAnd so we have our true, population model, residuals!\n\nThis is an important fact! For the population model, the residuals: \\(\\epsilon = Y - E[Y|X]\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#back-to-our-estimated-model",
    "href": "slides/08_MLR_Intro.html#back-to-our-estimated-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Back to our estimated model",
    "text": "Back to our estimated model\nWe have the same two representations of our estimated/fitted model:\n\n\n\n\nWith observed values:\n\n\n\\[Y =  \\widehat{\\beta}_0 + \\widehat{\\beta}_1X + \\widehat{\\epsilon}\\]\n\n\n\n\n\nWith the estimated expected value of \\(Y\\) given \\(X\\):\n\n\n\\[\\begin{aligned}\n\\widehat{E}[Y|X] & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{E[Y|X]} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\widehat{Y} & = \\widehat{\\beta}_0 + \\widehat{\\beta}_1X \\\\\n\\end{aligned}\\]\n\n\n\n\nUsing the two forms of the model, we can figure out a formula for our estimated residuals:\n\\[\\begin{aligned}\nY & = (\\widehat{\\beta}_0 + \\widehat{\\beta}_1X) + \\widehat\\epsilon \\\\\nY & = \\widehat{Y} + \\widehat\\epsilon \\\\\n\\widehat\\epsilon & = Y - \\widehat{Y}\n\\end{aligned}\\]\n\nThis is an important fact! For the estimated/fitted model, the residuals: \\(\\widehat\\epsilon = Y - \\widehat{Y}\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#individual-i-residuals-in-the-estimatedfitted-model",
    "href": "slides/08_MLR_Intro.html#individual-i-residuals-in-the-estimatedfitted-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each individual \\(i\\): \\(Y_i\\)\n\nValue in the dataset for individual \\(i\\)\n\nFitted value for each individual \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "href": "slides/08_MLR_Intro.html#individual-i-residuals-in-the-estimatedfitted-model-1",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Individual \\(i\\) residuals in the estimated/fitted model",
    "text": "Individual \\(i\\) residuals in the estimated/fitted model\n\n\n\nObserved values for each individual \\(i\\): \\(Y_i\\)\n\nValue in the dataset for individual \\(i\\)\n\nFitted value for each individual \\(i\\): \\(\\widehat{Y}_i\\)\n\nValue that falls on the best-fit line for a specific \\(X_i\\)\nIf two individuals have the same \\(X_i\\), then they have the same \\(\\widehat{Y}_i\\)\n\n\n\n\nResidual for each individual: \\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\nDifference between the observed and fitted value"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#poll-everywhere-question-4",
    "href": "slides/08_MLR_Intro.html#poll-everywhere-question-4",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#so-what-do-we-do-with-the-residuals",
    "href": "slides/08_MLR_Intro.html#so-what-do-we-do-with-the-residuals",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "So what do we do with the residuals?",
    "text": "So what do we do with the residuals?\n\nWe want to minimize the residuals\n\nAka minimize the difference between the observed \\(Y\\) value and the estimated expected response given the predictor ( \\(\\widehat{E}[Y|X]\\) )\n\nWe can use ordinary least squares (OLS) to do this in linear regression!\nIdea behind this: reduce the total error between the fitted line and the observed point (error between is called residuals)\n\nVague use of total error: more precisely, we want to reduce the sum of squared errors\nThink back to my R Shiny app!\nWe need to mathematically define this!\n\n\n \n \n\nNote: there are other ways to estimate the best-fit line!!\n\nExample: Maximum likelihood estimation"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#setting-up-for-ordinary-least-squares",
    "href": "slides/08_MLR_Intro.html#setting-up-for-ordinary-least-squares",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Setting up for ordinary least squares",
    "text": "Setting up for ordinary least squares\n\n\n\nSum of Squared Errors (SSE)\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0+\\widehat{\\beta}_1X_i))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\widehat\\epsilon_i = Y_i - \\widehat{Y}_i\\)\n\\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1X_i\\)\n\n\n\n\n\n\nThen we want to find the estimated coefficient values that minimize the SSE!"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#steps-to-estimate-coefficients-using-ols",
    "href": "slides/08_MLR_Intro.html#steps-to-estimate-coefficients-using-ols",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Steps to estimate coefficients using OLS",
    "text": "Steps to estimate coefficients using OLS\n\nSet up SSE (previous slide)\nMinimize SSE with respect to coefficient estimates\n\nNeed to solve a system of equations\n\nCompute derivative of SSE wrt \\(\\widehat\\beta_0\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\nCompute derivative of SSE wrt \\(\\widehat\\beta_1\\)\nSet derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\nSubstitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#minimize-sse-with-respect-to-coefficients",
    "href": "slides/08_MLR_Intro.html#minimize-sse-with-respect-to-coefficients",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "2. Minimize SSE with respect to coefficients",
    "text": "2. Minimize SSE with respect to coefficients\n\nWant to minimize with respect to (wrt) the potential coefficient estimates ( \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\))\nTake derivative of SSE wrt \\(\\widehat\\beta_0\\) and \\(\\widehat\\beta_1\\) and set equal to zero to find minimum SSE\n\n\\[\n\\dfrac{\\partial SSE}{\\partial \\widehat\\beta_0} = 0 \\text{ and } \\dfrac{\\partial SSE}{\\partial \\widehat\\beta_1} = 0\n\\]\n\nSolve the above system of equations in steps 3-6"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "href": "slides/08_MLR_Intro.html#compute-derivative-of-sse-wrt-widehatbeta_0",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)",
    "text": "3. Compute derivative of SSE wrt \\(\\widehat\\beta_0\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0}& =\\frac{\\partial\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)^2}{\\partial{\\widehat{\\beta}}_0}=\n\\sum_{i=1}^{n}\\frac{{\\partial\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}^2}{\\partial{\\widehat{\\beta}}_0} \\\\\n& =\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\\left(-1\\right)}=\\sum_{i=1}^{n}{-2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & = -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "href": "slides/08_MLR_Intro.html#set-derivative-of-sse-wrt-widehatbeta_0-0",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)",
    "text": "4. Set derivative of SSE wrt \\(\\widehat\\beta_0 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_0} & =0 \\\\ -2\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\\n\\sum_{i=1}^{n}\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right) & =0 \\\\ \\sum_{i=1}^{n}Y_i-n{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\sum_{i=1}^{n}X_i & =0 \\\\\n\\frac{1}{n}\\sum_{i=1}^{n}Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\frac{1}{n}\\sum_{i=1}^{n}X_i & =0 \\\\\n\\overline{Y}-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1\\overline{X} & =0 \\\\\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "href": "slides/08_MLR_Intro.html#compute-derivative-of-sse-wrt-widehatbeta_1",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)",
    "text": "5. Compute derivative of SSE wrt \\(\\widehat\\beta_1\\)\n\n\n\\[\nSSE = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i)^2\n\\]\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1}& =\\frac{\\partial\\sum_{i=1}^{n}{(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1}=\\sum_{i=1}^{n}\\frac{{\\partial(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i)}^2}{\\partial{\\widehat{\\beta}}_1} \\\\\n&=\\sum_{i=1}^{n}{2\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)(-X_i)}=\\sum_{i=1}^{n}{-2X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)} \\\\ &=-2\\sum_{i=1}^{n}{X_i\\left(Y_i-{\\widehat{\\beta}}_0-{\\widehat{\\beta}}_1X_i\\right)}\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\nDerivative rule: derivative of sum is sum of derivative\nDerivative rule: chain rule"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "href": "slides/08_MLR_Intro.html#set-derivative-of-sse-wrt-widehatbeta_1-0",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)",
    "text": "6. Set derivative of SSE wrt \\(\\widehat\\beta_1 = 0\\)\n\n\n\\[\\begin{aligned}\n\\frac{\\partial SSE}{\\partial{\\widehat{\\beta}}_1} & =0 \\\\ \\sum_{i=1}^{n}\\left({X_iY}_i-{\\widehat{\\beta}}_0X_i-{\\widehat{\\beta}}_1{X_i}^2\\right)&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i{\\widehat{\\beta}}_0}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1}&=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\left(\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\right)}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_iY_i}-\\sum_{i=1}^{n}{X_i\\overline{Y}}+\\sum_{i=1}^{n}{{\\widehat{\\beta}}_1X_i\\overline{X}}-\\sum_{i=1}^{n}{{X_i}^2{\\widehat{\\beta}}_1} &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+\\sum_{i=1}^{n}{({\\widehat{\\beta}}_1X_i\\overline{X}}-{X_i}^2{\\widehat{\\beta}}_1) &=0 \\\\\n\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}+{\\widehat{\\beta}}_1\\sum_{i=1}^{n}{X_i(\\overline{X}}-X_i) &=0 \\\\\n\\end{aligned}\\]\n\n\n\nThings to use\n\n\n\n\\({\\widehat{\\beta}}_0=\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X}\\)\n\\(\\overline{Y}=\\frac{1}{n}\\sum_{i=1}^{n}Y_i\\)\n\\(\\overline{X}=\\frac{1}{n}\\sum_{i=1}^{n}X_i\\)\n\n\n\n         \n \n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "href": "slides/08_MLR_Intro.html#substitute-widehatbeta_1-back-into-widehatbeta_0",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)",
    "text": "7. Substitute \\(\\widehat\\beta_1\\) back into \\(\\widehat\\beta_0\\)\nFinal coefficient estimates for SLR\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_1\\)\n\n\n\\[{\\widehat{\\beta}}_1 =\\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})}\\]\n\n\n\n\n\nCoefficient estimate for \\(\\widehat\\beta_0\\)\n\n\n\\[\\begin{aligned}\n{\\widehat{\\beta}}_0 & =\\overline{Y}-{\\widehat{\\beta}}_1\\overline{X} \\\\\n{\\widehat{\\beta}}_0 & = \\overline{Y} - \\frac{\\sum_{i=1}^{n}{X_i(Y_i-\\overline{Y})}}{\\sum_{i=1}^{n}{X_i(}X_i-\\overline{X})} \\overline{X} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#poll-everywhere-question-5",
    "href": "slides/08_MLR_Intro.html#poll-everywhere-question-5",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#do-i-need-to-do-all-that-work-every-time",
    "href": "slides/08_MLR_Intro.html#do-i-need-to-do-all-that-work-every-time",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Do I need to do all that work every time??",
    "text": "Do I need to do all that work every time??"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#regression-in-r-lm",
    "href": "slides/08_MLR_Intro.html#regression-in-r-lm",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression in R: lm()",
    "text": "Regression in R: lm()\n\nLet’s discuss the syntax of this function\n\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#regression-in-r-lm-summary",
    "href": "slides/08_MLR_Intro.html#regression-in-r-lm-summary",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression in R: lm() + summary()",
    "text": "Regression in R: lm() + summary()\n\nmodel1 &lt;- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,\n                 data = gapm)\nsummary(model1)\n\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               50.92790    2.66041  19.143  &lt; 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,    Adjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#regression-in-r-lm-tidy",
    "href": "slides/08_MLR_Intro.html#regression-in-r-lm-tidy",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression in R: lm() + tidy()",
    "text": "Regression in R: lm() + tidy()\n \n\ntidy(model1) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 45)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.9278981\n2.66040695\n19.142898\n3.325312e-31\n    female_literacy_rate_2011\n0.2321951\n0.03147744\n7.376557\n1.501286e-10\n  \n  \n  \n\n\n\n\n \n\nRegression equation for our model (which we saw a looong time ago):\n\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#how-do-we-interpret-the-coefficients",
    "href": "slides/08_MLR_Intro.html#how-do-we-interpret-the-coefficients",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we interpret the coefficients?",
    "text": "How do we interpret the coefficients?\n\n\\[\\widehat{\\text{life expectancy}} =  50.9 + 0.232\\cdot\\text{female literacy rate}\\]\n\n\nIntercept\n\nThe expected outcome for the \\(Y\\)-variable when the \\(X\\)-variable is 0\nExample: The expected/average life expectancy is 50.9 years for a country with 0% female literacy.\n\nSlope\n\nFor every increase of 1 unit in the \\(X\\)-variable, there is an expected increase of, on average, \\(\\widehat\\beta_1\\) units in the \\(Y\\)-variable.\nWe only say that there is an expected increase and not necessarily a causal increase.\nExample: For every 1 percent increase in the female literacy rate, the expected/average life expectancy increases, on average, 0.232 years."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#next-time",
    "href": "slides/08_MLR_Intro.html#next-time",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Next time",
    "text": "Next time\n\nInference of our estimated coefficients\nInference of estimated expected \\(Y\\) given \\(X\\)\nPrediction\nHypothesis testing!\n\n\n\nMLR 1"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#reminder-of-slr",
    "href": "slides/08_MLR_Intro.html#reminder-of-slr",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Reminder of SLR",
    "text": "Reminder of SLR"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#vsabv",
    "href": "slides/08_MLR_Intro.html#vsabv",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "vsabv",
    "text": "vsabv\n\n# Load the data - update code if the csv file is not in the same location on your computer\n# If you need to download the file, please go to ur shared folder under Data &gt; Slides\ngapm &lt;- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n\nglimpse(gapm_sub)\n\nRows: 72\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\",…\n$ CO2emissions                       &lt;dbl&gt; 0.4120, 1.7900, 1.2500, 5.3600, 4.6…\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210, 207, NA, 2900, 1810, 258,…\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 2410, 2370, 3160, 2790,…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 5910, 18600, 19600, 70…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5,…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 2.42e+07, 9.57e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8,…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"ago\", \"atg\", \"arg\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"americ…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"g77\", \"g77…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, -12.50000, 17.0…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 18.50000, -61.8…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#slr-vs.-mlr",
    "href": "slides/08_MLR_Intro.html#slr-vs.-mlr",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "SLR vs. MLR",
    "text": "SLR vs. MLR\n\n\n\nSimple Linear Regression\n\n \n\nWe use one predictor to try to explain the variance of the outcome\n\n\n\nMultiple Linear Regression\n\n \n\nWe use multiple predictors to try to explain the variance of the outcome\n\n\nSometimes referred to as multivariable linear regression, but never multivariate\n\n\n\n \n\nThe models have similar “LINE” assumptions and follow the same general diagnostic procedure"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-kc-ppd-as-an-independent-variable",
    "href": "slides/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-kc-ppd-as-an-independent-variable",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Can we improve our model by adding food supply (kc PPD) as an independent variable?",
    "text": "Can we improve our model by adding food supply (kc PPD) as an independent variable?\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#run-the-multiple-regression-model",
    "href": "slides/08_MLR_Intro.html#run-the-multiple-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Run the multiple regression model",
    "text": "Run the multiple regression model\nStatistical model:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#visualize-the-multiple-regression-model",
    "href": "slides/08_MLR_Intro.html#visualize-the-multiple-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize the multiple regression model",
    "text": "Visualize the multiple regression model\n\n\n\nThe equation \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X_1 + \\widehat{\\beta}_2 \\cdot X_2\\] has three variables (\\(Y, X_1,\\) and \\(X_2\\)) and thus we need 3 dimensions to plot it\n\n \n\nInstead of a regression line, we get a regression plane\n\nSee code in .qmd- file. I hid it from view in the html file."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "href": "slides/08_MLR_Intro.html#regression-lines-for-varying-values-of-food-supply",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression lines for varying values of food supply",
    "text": "Regression lines for varying values of food supply\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157 \\text{ Female literacy rate}\n+ 0.008 \\text{ Food supply}\n\\end{aligned}\\]\n\n\n\nNote: when the food supply is held constant but the female literacy rate varies…\n\nthen the outcome values change along a line\n\nDifferent values of food supply give different lines\n\nThe intercepts change, but\nthe slopes stay the same (parallel lines)\n\n\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#multiple-regression-model",
    "href": "slides/08_MLR_Intro.html#multiple-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Multiple regression model",
    "text": "Multiple regression model\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\] or on the individual (observation) level:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}+ \\ldots + \\beta_k x_{ik} + \\epsilon_i,\\ \\ \\text{for}\\ i = 1, 2, \\ldots, n\\]\n\n\\(Y\\) is the response/dependent variable\n\\(X_1, X_2, \\ldots, X_k\\) are \\(k\\) predictor/independent variables\n\nthese are assumed to be measured without error, meaning that all model uncertainty is from the error \\(\\epsilon\\)\n\n\\(\\epsilon\\) is the random error\n\\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\) are unknown population parameters\n\nWe estimate them based on the sample using \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\nQuestion: how are these estimated?"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#model-assumptions-eline",
    "href": "slides/08_MLR_Intro.html#model-assumptions-eline",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Model assumptions: eLINE",
    "text": "Model assumptions: eLINE\n\ne xistence\n\nFor any any combination of \\(X_1, X_2, \\ldots, X_k\\) values,\n\\(Y\\) is a (univariate) random variable with a probability distribution\nwith finite mean and variance.\n\nL inearity\n\nThe mean value of \\(Y\\) given any combination of \\(X_1, X_2, \\ldots, X_k\\) values, is a linear function of \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\):\n\n\n\\[\\mu_{Y|X_1, X_2, \\ldots, X_k} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k\\]\n\nI ndependence\n\nThe \\(Y\\) values are independent of one another\n\nN ormality\n\n\\(Y\\) has a normal distribution for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\nE quality of variance (homoscedasticity)\n\nThe variance of \\(Y\\) is the same for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\n\n\\[\\sigma^2_{Y|X_1, X_2, \\ldots, X_k} = Var(Y|X_1, X_2, \\ldots, X_k) \\equiv \\sigma^2\\]\n\nEquivalently, the residuals are independently and identically distributed (iid):\n\nnormal\nwith mean 0 and\nconstant variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#anova-table",
    "href": "slides/08_MLR_Intro.html#anova-table",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "ANOVA table",
    "text": "ANOVA table\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\bar{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\bar{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\bar{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\bar{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\bar{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#visualize-relationships",
    "href": "slides/08_MLR_Intro.html#visualize-relationships",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationships",
    "text": "Visualize relationships"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#scatterplot-in-3-d",
    "href": "slides/08_MLR_Intro.html#scatterplot-in-3-d",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Scatterplot in 3-D",
    "text": "Scatterplot in 3-D"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#section-1",
    "href": "slides/08_MLR_Intro.html#section-1",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "summary(mr1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n    data = gapm_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.715  -2.328   1.052   3.022   9.083 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        33.595479   4.472049   7.512 1.56e-10 ***\nFemaleLiteracyRate  0.156699   0.032158   4.873 6.75e-06 ***\nFoodSupplykcPPD     0.008482   0.001795   4.726 1.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.391 on 69 degrees of freedom\nMultiple R-squared:  0.563, Adjusted R-squared:  0.5503 \nF-statistic: 44.44 on 2 and 69 DF,  p-value: 3.958e-13\n\nanova(mr1)\n\nAnalysis of Variance Table\n\nResponse: LifeExpectancyYrs\n                   Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nFemaleLiteracyRate  1 1934.24 1934.24  66.547 1.029e-11 ***\nFoodSupplykcPPD     1  649.32  649.32  22.339 1.167e-05 ***\nResiduals          69 2005.56   29.07                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "href": "slides/08_MLR_Intro.html#simple-linear-regression-vs.-multiple-linear-regression",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Simple Linear Regression vs. Multiple Linear Regression",
    "text": "Simple Linear Regression vs. Multiple Linear Regression\n\n\n\nSimple Linear Regression\n\n \n\nWe use one predictor to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\\]\n\n\n\nMultiple Linear Regression\n\n \n\nWe use multiple predictors to try to explain the variance of the outcome\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_{k}X_{k}+ \\epsilon\n\\]\n\n \n\nHas \\(k+1\\) total coefficients (including intercept) for \\(k\\) predictors/covariates\nSometimes referred to as multivariable linear regression, but never multivariate\n\n\n\n \n\nThe models have similar “LINE” assumptions and follow the same general diagnostic procedure"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "href": "slides/08_MLR_Intro.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "SLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#regression-line-when-food-supply-3000-kc-ppd",
    "href": "slides/08_MLR_Intro.html#regression-line-when-food-supply-3000-kc-ppd",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Regression line when food supply = 3000 kc PPD",
    "text": "Regression line when food supply = 3000 kc PPD\n\n\n \n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\cdot 3000 \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 24 \\\\\n\\widehat{\\text{LE}} &= 57.6 + 0.157\\ \\text{FLR}\n\\end{aligned}\\]\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#how-are-the-model-coefficients-calculated",
    "href": "slides/08_MLR_Intro.html#how-are-the-model-coefficients-calculated",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How are the model coefficients calculated?",
    "text": "How are the model coefficients calculated?\n\nWe need to estimate the model coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\nThis can be done using the least-squares method\n\nFind the \\(\\widehat{\\beta}\\) values that minimize the sum of squares due to error (\\(SSE\\))\n\n\n\\[SSE=\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\\]\n\nThe equations for calculating the \\(\\widehat{\\beta}\\) values is best done using matrix notation (not required for our class)\n\nWe will be using R to get the coefficients instead of the equation\n\nThere are other methods to calculate the best-fit line, such as the minimum-variance method. They give the same results."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "href": "slides/08_MLR_Intro.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Model Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "href": "slides/08_MLR_Intro.html#going-back-to-our-life-expectancy-example",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Going back to our life expectancy example",
    "text": "Going back to our life expectancy example\n\nLet’s say many other variables were measured for each country, including food supply\n\nFood Supply (kilocalories per person per day, kc PPD): the average kilocalories consumed by a person each day.\n\nIn SLR, we only had one predictor and one outcome in the model:\n\nLife expectancy = the average number of years a newborn child would live if current mortality patterns were to stay the same.\nAdult literacy rate is the percentage of people ages 15 and above who can, with understanding, read and write a short, simple statement on their everyday life.\n\n\n \n\nDo we think adult female literacy rate is going to explain a lot of the variance of life expectancy between countries?"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-an-independent-variable",
    "href": "slides/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-an-independent-variable",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Can we improve our model by adding food supply as an independent variable?",
    "text": "Can we improve our model by adding food supply as an independent variable?\n\n\n\n\n\n\nSimple linear regression population model\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\epsilon\n\\end{aligned}\\]\n\n\n\n\nMultiple linear regression population model\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#population-multiple-regression-model",
    "href": "slides/08_MLR_Intro.html#population-multiple-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Population multiple regression model",
    "text": "Population multiple regression model\n\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nor on the individual (observation) level:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}+ \\ldots + \\beta_k x_{ik} + \\epsilon_i,\\ \\ \\text{for}\\ i = 1, 2, \\ldots, n\\]\n\n\n\nObservable sample data\n\n\\(Y\\) is our dependent variable\n\nAka outcome or response variable\n\n\\(X_1, X_2, \\ldots, X_k\\) are our \\(k\\) independent variables\n\nAka predictors or covariates\n\n\n\n\n\nUnobservable population parameters\n\n\\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\) are unknown population parameters\n\nFrom our sample, we find the population parameter estimates: \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\n\n\\(\\epsilon\\) is the random error\n\nAnd is still normally distributed\n\\(\\epsilon \\sim N(0, \\sigma^2)\\) where \\(\\sigma^2\\) is the population parameter of the variance"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "href": "slides/08_MLR_Intro.html#visualize-relationship-between-life-expectancy-female-literacy-rate-and-food-supply",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship between life expectancy, female literacy rate, and food supply",
    "text": "Visualize relationship between life expectancy, female literacy rate, and food supply\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#visualize-relationship-in-3-d",
    "href": "slides/08_MLR_Intro.html#visualize-relationship-in-3-d",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize relationship in 3-D",
    "text": "Visualize relationship in 3-D"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#dont-forget-the-other-functions-we-can-use-to-extract-information",
    "href": "slides/08_MLR_Intro.html#dont-forget-the-other-functions-we-can-use-to-extract-information",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Don’t forget the other functions we can use to extract information!",
    "text": "Don’t forget the other functions we can use to extract information!\n\nsummary(mr1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n    data = gapm_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.715  -2.328   1.052   3.022   9.083 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        33.595479   4.472049   7.512 1.56e-10 ***\nFemaleLiteracyRate  0.156699   0.032158   4.873 6.75e-06 ***\nFoodSupplykcPPD     0.008482   0.001795   4.726 1.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.391 on 69 degrees of freedom\nMultiple R-squared:  0.563, Adjusted R-squared:  0.5503 \nF-statistic: 44.44 on 2 and 69 DF,  p-value: 3.958e-13\n\nanova(mr1)\n\nAnalysis of Variance Table\n\nResponse: LifeExpectancyYrs\n                   Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nFemaleLiteracyRate  1 1934.24 1934.24  66.547 1.029e-11 ***\nFoodSupplykcPPD     1  649.32  649.32  22.339 1.167e-05 ***\nResiduals          69 2005.56   29.07                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "href": "slides/08_MLR_Intro.html#dont-forget-summary-to-extract-information",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Don’t forget summary() to extract information!",
    "text": "Don’t forget summary() to extract information!\n\nsummary(mr1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n    data = gapm_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.715  -2.328   1.052   3.022   9.083 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        33.595479   4.472049   7.512 1.56e-10 ***\nFemaleLiteracyRate  0.156699   0.032158   4.873 6.75e-06 ***\nFoodSupplykcPPD     0.008482   0.001795   4.726 1.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.391 on 69 degrees of freedom\nMultiple R-squared:  0.563, Adjusted R-squared:  0.5503 \nF-statistic: 44.44 on 2 and 69 DF,  p-value: 3.958e-13"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#anova-table-2",
    "href": "slides/08_MLR_Intro.html#anova-table-2",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "ANOVA table 2",
    "text": "ANOVA table 2\nANOVA table (\\(k\\) = # of predictors, \\(n\\) = # of observations)\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\\(F \\sim F_{(k, n-k-1)}\\)\n\n\nError\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)\n\n\n\n\n\n\n\nanova(mr1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n1,934.245\n1,934.245\n66.547\n0.000\n    FoodSupplykcPPD\n1.000\n649.319\n649.319\n22.339\n0.000\n    Residuals\n69.000\n2,005.556\n29.066\nNA\nNA"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#interpretint-the-population-coefficients",
    "href": "slides/08_MLR_Intro.html#interpretint-the-population-coefficients",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpretint the population coefficients",
    "text": "Interpretint the population coefficients"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "href": "slides/08_MLR_Intro.html#interpreting-the-estimated-population-coefficients",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "href": "slides/08_MLR_Intro.html#technical-side-note-not-needed-in-our-class",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Technical side note (not needed in our class)",
    "text": "Technical side note (not needed in our class)\n\nThe equations for calculating the \\(\\boldsymbol{\\widehat{\\beta}}\\) values is best done using matrix notation (not required for our class)\nWe will be using R to get the coefficients instead of the equation (already did this a few slides back!)\nHow we have represented the population regression model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\n\n\n\nHow to represent population model with matrix notation:\n\n\\[\\begin{aligned}\n\\boldsymbol{Y} &= \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon} \\\\\n\\boldsymbol{Y}_{n \\times 1}& = \\boldsymbol{X}_{n \\times (k+1)}\\boldsymbol{\\beta}_{(k+1)\\times 1} + \\boldsymbol{\\epsilon}_{n \\times 1}\n\\end{aligned}\\]\n\n\\[\n\\boldsymbol{Y} = \\left[\\begin{array}{c} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n\n\\end{array} \\right]_{n \\times 1}\n\\] \\[\n\\boldsymbol{\\epsilon} = \\left[\\begin{array}{c} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n\n\\end{array} \\right]_{n \\times 1}  \n\\]\n\n\\[\n\\boldsymbol{X} = \\left[ \\begin{array}{ccccc} 1 &  X_{11} &  X_{12} & \\ldots & X_{1,k} \\\\\n1 &X_{21} &  X_{22} & \\ldots & X_{2,k} \\\\\n\\vdots&\\vdots & \\vdots &  \\ldots & \\vdots \\\\\n1 & X_{n1} &  X_{n2} & \\ldots & X_{n,k} \\end{array} \\right]_{n \\times (k+1)}\n\\]\n\\[\n\\boldsymbol{\\beta}  = \\left[\\begin{array}{c} \\beta_0 \\\\ \\beta_1\\\\  \\vdots \\\\\n\\beta_{k}\n\\end{array} \\right]_{(k+1)\\times 1}\n\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#poll-everwhere-question",
    "href": "slides/08_MLR_Intro.html#poll-everwhere-question",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everwhere Question",
    "text": "Poll Everwhere Question"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#line-model-assumptions",
    "href": "slides/08_MLR_Intro.html#line-model-assumptions",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "LINE model assumptions",
    "text": "LINE model assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nThe mean value of \\(Y\\) given any combination of \\(X_1, X_2, \\ldots, X_k\\) values, is a linear function of \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k\\):\n\\[\\mu_{Y|X_1, \\ldots, X_k} = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k\\]\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nObservations (\\(X_1, X_2, \\ldots, X_k, Y\\)) are independent from one another\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\n\\(Y\\) has a normal distribution for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\nThus, the residuals are normally distributed\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nThe variance of \\(Y\\) is the same for any any combination of \\(X_1, X_2, \\ldots, X_k\\) values\n\\[\\sigma^2_{Y|X_1, X_2, \\ldots, X_k} = Var(Y|X_1, X_2, \\ldots, X_k) = \\sigma^2\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#summary-of-the-line-assumptions",
    "href": "slides/08_MLR_Intro.html#summary-of-the-line-assumptions",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Summary of the LINE assumptions",
    "text": "Summary of the LINE assumptions\n\nEquivalently, the residuals are independently and identically distributed (iid):\n\nnormal\nwith mean 0 and\nconstant variance \\(\\sigma^2\\)"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#loading-the-new-ish-data",
    "href": "slides/08_MLR_Intro.html#loading-the-new-ish-data",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Loading the (new-ish) data",
    "text": "Loading the (new-ish) data\n\n# Load the data - update code if the csv file is not in the same location on your computer\n# If you need to download the file, please go to ur shared folder under Data &gt; Slides\ngapm &lt;- read_excel(\"data/Gapminder_vars_2011.xlsx\", \n                   na = \"NA\")  # important!!!! \n\ngapm_sub &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD)\n\nglimpse(gapm_sub)\n\nRows: 72\nColumns: 18\n$ country                            &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Angola\",…\n$ CO2emissions                       &lt;dbl&gt; 0.4120, 1.7900, 1.2500, 5.3600, 4.6…\n$ ElectricityUsePP                   &lt;dbl&gt; NA, 2210, 207, NA, 2900, 1810, 258,…\n$ FoodSupplykcPPD                    &lt;dbl&gt; 2110, 3130, 2410, 2370, 3160, 2790,…\n$ IncomePP                           &lt;dbl&gt; 1660, 10200, 5910, 18600, 19600, 70…\n$ LifeExpectancyYrs                  &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8,…\n$ FemaleLiteracyRate                 &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5,…\n$ population                         &lt;dbl&gt; 2.97e+07, 2.93e+06, 2.42e+07, 9.57e…\n$ WaterSourcePrct                    &lt;dbl&gt; 52.6, 88.1, 40.3, 97.0, 99.5, 97.8,…\n$ geo                                &lt;chr&gt; \"afg\", \"alb\", \"ago\", \"atg\", \"arg\", …\n$ four_regions                       &lt;chr&gt; \"asia\", \"europe\", \"africa\", \"americ…\n$ eight_regions                      &lt;chr&gt; \"asia_west\", \"europe_east\", \"africa…\n$ six_regions                        &lt;chr&gt; \"south_asia\", \"europe_central_asia\"…\n$ members_oecd_g77                   &lt;chr&gt; \"g77\", \"others\", \"g77\", \"g77\", \"g77…\n$ Latitude                           &lt;dbl&gt; 33.00000, 41.00000, -12.50000, 17.0…\n$ Longitude                          &lt;dbl&gt; 66.00000, 20.00000, 18.50000, -61.8…\n$ `World bank region`                &lt;chr&gt; \"South Asia\", \"Europe & Central Asi…\n$ `World bank, 4 income groups 2017` &lt;chr&gt; \"Low income\", \"Upper middle income\"…"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "href": "slides/08_MLR_Intro.html#can-we-improve-our-model-by-adding-food-supply-as-a-covariate",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Can we improve our model by adding food supply as a covariate?",
    "text": "Can we improve our model by adding food supply as a covariate?\n\n\n\n\n\n\nSimple linear regression population model\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\epsilon\n\\end{aligned}\\]\n\n\n\n\nMultiple linear regression population model (with added Food Supply)\n\n\n\\[\\begin{aligned}\n\\text{Life expectancy} & = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon \\\\\n\\text{LE} & = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "href": "slides/08_MLR_Intro.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we fit a multiple linear regression model in R?",
    "text": "How do we fit a multiple linear regression model in R?\nNew population model for example:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "href": "slides/08_MLR_Intro.html#visualize-the-fitted-multiple-regression-model",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Visualize the fitted multiple regression model",
    "text": "Visualize the fitted multiple regression model\n\n\n\nThe fitted model equation \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X_1 + \\widehat{\\beta}_2 \\cdot X_2\\] has three variables (\\(Y, X_1,\\) and \\(X_2\\)) and thus we need 3 dimensions to plot it\n\n \n\nInstead of a regression line, we get a regression plane\n\nSee code in .qmd- file. I hid it from view in the html file."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "href": "slides/08_MLR_Intro.html#how-do-we-calculate-the-regression-line-for-3000-kc-ppd-food-supply",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we calculate the regression line for 3000 kc PPD food supply?",
    "text": "How do we calculate the regression line for 3000 kc PPD food supply?\n\n\n \n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\cdot 3000 \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 24 \\\\\n\\widehat{\\text{LE}} &= 57.6 + 0.157\\ \\text{FLR}\n\\end{aligned}\\]\n\n\n(mr1_2d = ggPredict(mr1, interactive = T))"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#how-are-the-model-coefficients-estimated-and-calculated",
    "href": "slides/08_MLR_Intro.html#how-are-the-model-coefficients-estimated-and-calculated",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How are the model coefficients estimated and calculated?",
    "text": "How are the model coefficients estimated and calculated?\n\nWe need to estimate the model coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\nThis can be done using the least-squares method\n\nFind the \\(\\widehat{\\beta}\\) values that minimize the sum of squares due to error (\\(SSE\\))\n\n\n\\[SSE=\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = \\sum_{i=1}^n (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\\]\n\nThe equations for calculating the \\(\\widehat{\\beta}\\) values is best done using matrix notation (not required for our class)\n\nWe will be using R to get the coefficients instead of the equation\n\nThere are other methods to calculate the best-fit line, such as the minimum-variance method. They give the same results."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "href": "slides/08_MLR_Intro.html#how-do-we-estimate-the-model-parameters",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "How do we estimate the model parameters?",
    "text": "How do we estimate the model parameters?\n\nWe need to estimate the population model coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, \\ldots, \\widehat{\\beta}_k\\)\nThis can be done using the ordinary least-squares method\n\nFind the \\(\\widehat{\\beta}\\) values that minimize the sum of squares due to error (\\(SSE\\))\n\n\n\\[ \\begin{aligned}\nSSE & = \\displaystyle\\sum^n_{i=1} \\widehat\\epsilon_i^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{Y}_i)^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - (\\widehat{\\beta}_0 +\\widehat{\\beta}_1 X_{i1}+ \\ldots+\\widehat{\\beta}_1 X_{ik}))^2 \\\\\nSSE & = \\displaystyle\\sum^n_{i=1} (Y_i - \\widehat{\\beta}_0 -\\widehat{\\beta}_1 X_{i1}- \\ldots-\\widehat{\\beta}_1 X_{ik})^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#variation-explained-vs.-unexplained",
    "href": "slides/08_MLR_Intro.html#variation-explained-vs.-unexplained",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\bar{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\bar{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\bar{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\bar{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\bar{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\bar{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#building-the-anova-table",
    "href": "slides/08_MLR_Intro.html#building-the-anova-table",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Building the ANOVA table",
    "text": "Building the ANOVA table\nANOVA table (\\(k\\) = # of predictors, \\(n\\) = # of observations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\\(F \\sim F_{(k, n-k-1)}\\)\n\n\nError\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)\n\n\n\n\n\n\n\n\n \n\nanova(mr1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n1,934.245\n1,934.245\n66.547\n0.000\n    FoodSupplykcPPD\n1.000\n649.319\n649.319\n22.339\n0.000\n    Residuals\n69.000\n2,005.556\n29.066\nNA\nNA"
  },
  {
    "objectID": "slides/08_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "href": "slides/08_MLR_Intro.html#getting-these-interpretations-from-our-regression-table",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "slides/08_MLR_Intro.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "slides/08_MLR_Intro.html#poll-everwhere-question-2",
    "href": "slides/08_MLR_Intro.html#poll-everwhere-question-2",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "Poll Everwhere Question 2",
    "text": "Poll Everwhere Question 2"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#overall",
    "href": "slides/Quiz_1_Lab_1.html#overall",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Overall",
    "text": "Overall\n\nGreat job!\nJust a few things that I think are important to review before we move into more complicated models"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#question-7",
    "href": "slides/Quiz_1_Lab_1.html#question-7",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Question 7",
    "text": "Question 7\n\n\nWhich of the following statements is true about the value -0.834 in our regression table?\n\nIt is the estimate of the sample intercept\nIt is the estimate of the population intercept\nIt is the estimate of the sample slope\n\n\n\nIt is the estimate of the population slope\n\n\n\n\n\n\nBecause -0.834 corresponds to the “Age” row of the regression table, this is the slope of our fitted regression line\nThis means \\(\\widehat\\beta_1 = -0.834\\)\n\\(\\widehat\\beta_1\\) is the estimate of the population slope\n\\(-0.834\\) is just the realized value (the result of fitting the population model)\nWe read \\(\\widehat\\beta_1 = -0.834\\) as: the estimate of the population slope is -0.834\nWhile speaking, I might say “coefficient estimate.” If I am saying estimate, then I mean the population estimate\n\nPlease stop and ask me if my language ever feels unclear"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#question-9-and-10",
    "href": "slides/Quiz_1_Lab_1.html#question-9-and-10",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Question 9 and 10",
    "text": "Question 9 and 10\n\nThe following are required parts of the interpretation\n\nUnits of Y\nUnits of X\nMean/average/expected before Y when discussing intercept\nMean/average/expected before difference, increase, or decrease when discussing coefficient for continuous covariate\n\nYou can also have expected/average/mean before the Y, but not necessary\n\nConfidence interval"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#question-9-and-10-12",
    "href": "slides/Quiz_1_Lab_1.html#question-9-and-10-12",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Question 9 and 10 (1/2)",
    "text": "Question 9 and 10 (1/2)\n\nThe following are required parts of the interpretation\n\nUnits of Y\nUnits of X\nMean/average/expected before Y when discussing intercept\nMean/average/expected before difference, increase, or decrease when discussing coefficient for continuous covariate\n\nYou can also have expected/average/mean before the Y, but not necessary\n\nConfidence interval"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#question-9-and-10-22",
    "href": "slides/Quiz_1_Lab_1.html#question-9-and-10-22",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Question 9 and 10 (2/2)",
    "text": "Question 9 and 10 (2/2)\n\nIntercept: For someone 0 years old, the average peak exercise heart rate is 214.233 beats per minute (95% CI: 204.918, 223.548).\nSlope: For every one year increase in age, the peak exercise heart rate is expected to decrease by 0.834 beats per minute (95% CI: -0.982, -0.685).\n\nOR: For every one year increase in age, the expected peak exercise heart rate decreases by 0.834 beats per minute (95% CI: -0.982, -0.685)."
  },
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Quiz\nAnswer key\n\n\n\n\n1\n\n\n\n2\n\n\n\n3\n\n\n\n\n\nQuiz 3 Information\n\nWhat will it cover?\n\nWill cover Lesson 10 - 11: categorical covariates to interactions\nI’m thinking this one will be a litter shorter since there’s less material\nWill still have the 50 min for the quiz\nIncludes HW 4 and 5\n\nWill include interpretations of interactions!\n\n\n\n\n\nQuiz 2 Information\n\nWhat will it cover?\n\nLesson 4 (Week 3, SLR: Inference, starting at mean response) to Lesson 9 (Week 5, MLR: Inference / F-tests)\nHW 4 will not be on it!\n\n\n\n\nQuiz 1 Information\n\nWe will be in RLSB 3A003 B!!!\nGeneral structure\n\nIt will be a maximum of 15 questions\n\n~ 10 multiple choice questions (including T/F)\n~ 3 free response questions\n\n\nWhat will it cover?\n\nLesson 2 (Data Management) to Lesson 4 (SLR: Inference, except the mean response)\n\nSo up to what we covered on Monday 1/22\n\nHW 0 - 1\n\nWhat can you expect?\n\nMostly concept questions\nYou may need to recognize what certain, important functions do\nYou may need to recognize a number from R output (like the regression table on slide 4 in Lesson 4 slides)\n\nInstructions that will be on the quiz:\n\n\nI have written a “30 minute” quiz. However, you have 50 minutes from 2:00 - 2:50pm.\nThe quiz is open book and open notes. You may use books other than the class textbook, you may use anything on our course webpage, and you may use reference websites (like Wikipedia, Googling expected value of specific distribution, etc.).\nNo cheating will be tolerated. Cheating includes:\n\nUsing ChatGPT\nUsing question and answer threads typically seen on sites like StackExchange, WikiHow, Quora, Reddit, StackOverflow, Chegg, etc.\nAsking other students in the room or looking at other students’ quiz work."
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html",
    "href": "slides/Quiz_1_Lab_1.html",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "",
    "text": "Which of the following statements is true about the value -0.834 in our regression table?\n\nIt is the estimate of the sample intercept\nIt is the estimate of the population intercept\nIt is the estimate of the sample slope\n\n\n\nIt is the estimate of the population slope\n\n\n\n\n\n\nBecause -0.834 corresponds to the “Age” row of the regression table, this is the slope of our fitted regression line\nThis means \\(\\widehat\\beta_1 = -0.834\\)\n\\(\\widehat\\beta_1\\) is the estimate of the population slope\n\\(-0.834\\) is just the realized value (the result of fitting the population model)\nWe read \\(\\widehat\\beta_1 = -0.834\\) as: the estimate of the population slope is -0.834\nWhile speaking, I might say “coefficient estimate.” If I am saying estimate, then I mean the population estimate\n\nPlease stop and ask me if my language ever feels unclear\n\n\n\n\n\n\n\n\nThe following are required parts of the interpretation\n\nUnits of Y\nUnits of X\nMean/average/expected before Y when discussing intercept\nMean/average/expected before difference, increase, or decrease when discussing coefficient for continuous covariate\n\nYou can also have expected/average/mean before the Y, but not necessary\n\nConfidence interval\n\n\n\n\n\n\nIntercept: For someone 0 years old, the average peak exercise heart rate is 214.233 beats per minute (95% CI: 204.918, 223.548).\nSlope: For every one year increase in age, the peak exercise heart rate is expected to decrease by 0.834 beats per minute (95% CI: -0.982, -0.685)"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#more-on-coefficient-interpretations",
    "href": "slides/Quiz_1_Lab_1.html#more-on-coefficient-interpretations",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "More on coefficient interpretations",
    "text": "More on coefficient interpretations\n\n\nPopulation model: \\[\nE[Y|X] = \\beta_0 + \\beta_1X\n\\]\nWhat is \\(\\beta_1\\) mean?\n\nLet’s say we have \\(X = x_1\\) and \\(X=x_2\\)\nThe difference between \\(x_1\\) and \\(x_2\\) is 1: \\(x_1 - x_2 = 1\\)\nWe don’t have to know the actual values of the x’s, just that their difference is 1\nNow, let’s look at the expected values for each of those x’s:\n\n\\[ \\begin{aligned}\n    E[Y|x_1] & = \\beta_0 + \\beta_1x_1 \\\\\n    E[Y|x_2] & = \\beta_0 + \\beta_1x_2\n\\end{aligned}\\]\n\n\nIf we take the difference between the expected values, we get:\n\n\\[ \\begin{aligned}\n    E[Y|x_1] - E[Y|x_2] & = (\\beta_0 + \\beta_1x_1) - (\\beta_0 + \\beta_1x_2) \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_0 + \\beta_1x_1 - \\beta_0 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1x_1 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1 (x_1 - x_2) \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{x_1 - x_2} \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{1} \\\\\n    \\beta_1 & = E[Y|x_1] - E[Y|x_2]\n\\end{aligned}\\]\n\nSo, we can consider \\(\\beta_1\\) as the difference in the expected Y for every 1 unit increase in X"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#coefficient-interpretations",
    "href": "slides/Quiz_1_Lab_1.html#coefficient-interpretations",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Coefficient interpretations",
    "text": "Coefficient interpretations\nPopulation model: \\[\nE[Y|X] = \\beta_0 + \\beta_1X\n\\] What is \\(\\beta_1\\) mean?\n\nLet’s say we have \\(X = x_1\\) and \\(X=x_2\\)\nThe difference between \\(x_1\\) and \\(x_2\\) is 1: \\(x_1 - x_2 = 1\\)\nWe don’t have to know the actual values of the x’s, just that their difference is 1\nNow, let’s look at the expected values for each of those x’s:\n\n\\[ \\begin{aligned}\n    E[Y|x_1] & = \\beta_0 + \\beta_1x_1 \\\\\n    E[Y|x_2] & = \\beta_0 + \\beta_1x_2\n\\end{aligned}\\] - If we take the difference between the expected values, we get: \\[ \\begin{aligned}\n    E[Y|x_1] - E[Y|x_2] & = (\\beta_0 + \\beta_1x_1) - (\\beta_0 + \\beta_1x_2) \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_0 + \\beta_1x_1 - \\beta_0 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1x_1 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1 (x_1 - x_2) \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{x_1 - x_2} \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{1} \\\\\n    \\beta_1 & = E[Y|x_1] - E[Y|x_2]\n\\end{aligned}\\]\n\nSo, we can consider \\(\\beta_1\\) as the difference in the expected Y for every 1 unit increase in X\nOr: we can look at \\(\\beta_1\\) another way: \\[ \\begin{aligned}\n\\beta_1 & = E[Y|x_1] - E[Y|x_2] \\\\\n\\beta_1 & = E\\big[ (Y|x_1) - (Y|x_2) \\big] \\\\\n\\end{aligned}\\]\n\nThis would make \\(beta_1\\) the expected difference in Y for every 1 unit increase in X"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#thinking-of-the-expected-value-another-way",
    "href": "slides/Quiz_1_Lab_1.html#thinking-of-the-expected-value-another-way",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Thinking of the expected value another way",
    "text": "Thinking of the expected value another way\n\nOr: we can look at \\(\\beta_1\\) another way: \\[ \\begin{aligned}\n\\beta_1 & = E[Y|x_1] - E[Y|x_2] \\\\\n\\beta_1 & = E\\big[ (Y|x_1) - (Y|x_2) \\big] \\\\\n\\end{aligned}\\]\n\nThis would make \\(\\beta_1\\) the expected difference in Y for every 1 unit increase in X"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#overall-1",
    "href": "slides/Quiz_1_Lab_1.html#overall-1",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Overall",
    "text": "Overall\n\nI really appreciated everyone’s perspective!\nI definitely learned a few things while reading\nBiggest reason why points were lost: the research question was not focused enough\n\nAsked for the IAT score (implicit measure) and one other variable"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#limitations-of-the-iat",
    "href": "slides/Quiz_1_Lab_1.html#limitations-of-the-iat",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Limitations of the IAT",
    "text": "Limitations of the IAT\n\nTaking the test multiple times\n\nA lot of us mentioned learning bias, which can definitely be true\n\nThink about what direction that might bias our results\n\nProblems with independence between observations\n\nGeneralizability\n\nDoes it represent our population? When we just say “population,” is there an unsaid assumption on the population we are referring to?\nCan we start to narrow the definition of our population to give context to our sample?"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#other-notes",
    "href": "slides/Quiz_1_Lab_1.html#other-notes",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Other notes",
    "text": "Other notes\n\nDid not intend for us to get focused on the 3 social theories in the article\n\nIf it helps you contextualize, then go for it!\nBut make sure you are defining the social theories and connecting them to motivation for your\n\nMinor writing notes\n\nWhile folks is a great, inclusive word to describe people, it is a little too informal in reports\n\nGood alternative is “individuals”\n\nDo not use “I” or “think” in report\n\nCan use the royal “We”\n\n\nWhen we talk about our analysis, avoid how “individuals’” scores relate to their other measures.\n\nImportant to note that we are not making conclusions about the individual\nWe are using individual data to make conclusions about the population!"
  },
  {
    "objectID": "slides/Quiz_1_Lab_1.html#moving-forward",
    "href": "slides/Quiz_1_Lab_1.html#moving-forward",
    "title": "A word on Quiz 1 and Lab 1",
    "section": "Moving forward",
    "text": "Moving forward\n\nMake sure you articulate the motivation for your research question\n\nIf you are interested in it, then there is likely some research discussing the relationship\nContextualize why this is a research question worth exploring\n\nIf you want to review your intro, please come to me!\n\nRevising early will be helpful for the report\nI will grade each portion of the report expecting you make the needed changes\nI did not make notes on all edits - tried to identify the bigger things\n\nGood sources for report help\n\nStructuring research articles\nInclusive language practices\nGuide on improving readability\n\n\n\n\nQuiz and Lab 1"
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html",
    "href": "extra_resources/Latex_qmd_formatting.html",
    "title": "LateX and R Markdown Formatting",
    "section": "",
    "text": "This style of coding has a bunch of different yet completely equivalent names. It is a form of coding within markdown files that helps us write equations in an easily readable format.\n\nLatex\nLateX\nLaTeX\nMay occasionally see it written with a K instead of an X\nPronounced as “lay-tek” or “lah-tek”"
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#centered",
    "href": "extra_resources/Latex_qmd_formatting.html#centered",
    "title": "LateX and R Markdown Formatting",
    "section": "3.1 Centered",
    "text": "3.1 Centered\nIf you want your equation to be centered in your output, use the double dollar sign ($$) on either side of your equation: $$\\alpha = 0.05$$\n\\[ \\alpha = 0.05 \\]\nTip: If you put your $$   $$ out first, whatever you type in between them will render as you’re typing! Sometimes this is helpful to catch if you’ve made a typo or misspelled a Greek letter or command you meant to use (more on these later)."
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#in-line",
    "href": "extra_resources/Latex_qmd_formatting.html#in-line",
    "title": "LateX and R Markdown Formatting",
    "section": "3.2 In-Line",
    "text": "3.2 In-Line\nIf you want your equation to be in-line with your markdown text, use the single dollar sign ($) on either side of your equation. Using this, you can write things like $\\alpha$ to output (\\(\\alpha\\) = 0.05)."
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#subscript",
    "href": "extra_resources/Latex_qmd_formatting.html#subscript",
    "title": "LateX and R Markdown Formatting",
    "section": "5.1 Subscript",
    "text": "5.1 Subscript\nOnly the first value (letter or number) following the subscript command will be subscripted, by default. If you want to make a value subscripted, use the underscore (_) like this:\nH_0: \\(H_0\\)\nWhatever you type next will not be subscripted:\nH_12: \\(H_12\\)\nTo subscript more than one value, use curly brackets:\nH_{12}: \\(H_{12}\\)"
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#superscript",
    "href": "extra_resources/Latex_qmd_formatting.html#superscript",
    "title": "LateX and R Markdown Formatting",
    "section": "5.2 Superscript",
    "text": "5.2 Superscript\nSometimes we want values to appear above others as a superscript, such as when we’re squaring or cubing them. As with the subscript, if you want more than one value following the carrot to be superscripted, use curly brackets:\nr^2: \\(r^2\\)\nr^12: \\(r^12\\)\nr^{12}: \\(r^{12}\\)"
  },
  {
    "objectID": "labs/BMI_help.html",
    "href": "labs/BMI_help.html",
    "title": "BMI Variable Help",
    "section": "",
    "text": "Link to github page for qmd file\n\nLoading the needed packages:\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)\nif(!require(lubridate)) { install.packages(\"lubridate\"); library(lubridate) }\n\n\n\nLoading my IAT dataset (as it’s Rda file):\n\nload(file = here(\"../TA_files/Project/data/IAT_data.rda\"))\n\n\n\nSelecting the variables that I want to look at:\n\niat_prep = iat_2021_raw %&gt;% \n  select(IAT_score = D_biep.Thin_Good_all, \n         att7, iam_001, identfat_001, \n         myweight_002, myheight_002,\n         identthin_001, controlother_001, \n         controlyou_001, mostpref_001,\n         important_001, \n         birthmonth, birthyear, month, year, \n         raceomb_002, raceombmulti, ethnicityomb, \n         edu, edu_14, \n         genderIdentity, \n         birthSex)\n\n\n\nSelf-reported BMI\nI started investigating the BMI because I was curious how the paper [@elran-barak2018] used it and just wanted to check reproducibility. There are a few issues with the self-reported BMI that immediately stuck out:\n\nComponents of BMI (weight and height) were self-reported\n\nPeople told they are underweight often add pounds (REFERENCE)\nPeople told they are overweight often subtract pounds (REFERENCE)\n\nRaw data from weight and height are categorical. This is according to the codebook associated with this dataset. Please find your codebook file named Weight_IAT_public_2021_codebook.csv . You can find the value names for myweight_002 and myheight_002.\nFor example, in the weight variable,\n\nmost categories identify a lower limit to the weight in the group. One example group is weight is greater than or equal to 200 pounds and less than 205 pounds (labelled as “200 lb :: 91 kg”).\nthe first category for weight is “below 50lb:: 23kg” with 258 observations\nthe last category for weight is “above 440lb:: above 200kg” with 295 observations\n\nWhile the 5 groups of weight leading up the last category have 33, 28, 34, 20, and 89 observations, respectively.\n\nMy intention here is not the question anyone’s weight, but keep in mind that surveys sometimes have people selecting the first or last option because they are not taking the survey seriously\n\n\n\nMy exact steps\n\nI wanted to get a table of the counts within each weight group. I used the gt package to make a table of what I thought was a categorical variable. It looks like R interprets the numbered categories as numbers.\n\niat_prep %&gt;%\n  dplyr::select(myweight_002) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n\n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    myweight_002\n23 (18, 29)\n        Unknown\n141,326\n  \n\n  \n    \n      1 Median (IQR)\n    \n  \n\n\n\n\nI will first check the class of the variable to make sure R is doing what I think it’s doing.\n\nclass(iat_prep$myweight_002)\n\n[1] \"integer\"\n\n\nSo R is interpreting the values as integers. I will need to make them categories to view them through gt commands.\nLet’s make it a category:\n\niat_prep2 = iat_prep %&gt;% \n  mutate(myweight = as.factor(myweight_002))\n\nNow we make the table:\n\niat_prep2 %&gt;%\n  dplyr::select(myweight) %&gt;%\n  tbl_summary()\n\n\n\n\n\n  \n\n    \n      Characteristic\n      N = 465,8861\n    \n  \n  \n    myweight\n\n        1\n258 (&lt;0.1%)\n        2\n257 (&lt;0.1%)\n        3\n329 (0.1%)\n        4\n363 (0.1%)\n        5\n379 (0.1%)\n        6\n329 (0.1%)\n        7\n327 (0.1%)\n        8\n360 (0.1%)\n        9\n589 (0.2%)\n        10\n1,002 (0.3%)\n        11\n2,180 (0.7%)\n        12\n3,766 (1.2%)\n        13\n6,175 (1.9%)\n        14\n9,038 (2.8%)\n        15\n12,068 (3.7%)\n        16\n15,598 (4.8%)\n        17\n16,007 (4.9%)\n        18\n17,518 (5.4%)\n        19\n19,093 (5.9%)\n        20\n17,794 (5.5%)\n        21\n15,599 (4.8%)\n        22\n16,636 (5.1%)\n        23\n14,854 (4.6%)\n        24\n14,643 (4.5%)\n        25\n13,510 (4.2%)\n        26\n12,778 (3.9%)\n        27\n12,243 (3.8%)\n        28\n11,498 (3.5%)\n        29\n9,414 (2.9%)\n        30\n9,099 (2.8%)\n        31\n7,274 (2.2%)\n        32\n8,775 (2.7%)\n        33\n4,691 (1.4%)\n        34\n5,411 (1.7%)\n        35\n4,595 (1.4%)\n        36\n5,659 (1.7%)\n        37\n3,494 (1.1%)\n        38\n3,938 (1.2%)\n        39\n2,489 (0.8%)\n        40\n2,932 (0.9%)\n        41\n1,941 (0.6%)\n        42\n3,197 (1.0%)\n        43\n1,244 (0.4%)\n        44\n1,794 (0.6%)\n        45\n1,442 (0.4%)\n        46\n1,322 (0.4%)\n        47\n1,251 (0.4%)\n        48\n1,238 (0.4%)\n        49\n900 (0.3%)\n        50\n800 (0.2%)\n        51\n651 (0.2%)\n        52\n1,152 (0.4%)\n        53\n347 (0.1%)\n        54\n436 (0.1%)\n        55\n346 (0.1%)\n        56\n409 (0.1%)\n        57\n295 (&lt;0.1%)\n        58\n384 (0.1%)\n        59\n165 (&lt;0.1%)\n        60\n202 (&lt;0.1%)\n        61\n126 (&lt;0.1%)\n        62\n342 (0.1%)\n        63\n92 (&lt;0.1%)\n        64\n154 (&lt;0.1%)\n        65\n129 (&lt;0.1%)\n        66\n113 (&lt;0.1%)\n        67\n139 (&lt;0.1%)\n        68\n85 (&lt;0.1%)\n        69\n85 (&lt;0.1%)\n        70\n55 (&lt;0.1%)\n        71\n65 (&lt;0.1%)\n        72\n120 (&lt;0.1%)\n        73\n26 (&lt;0.1%)\n        74\n26 (&lt;0.1%)\n        75\n26 (&lt;0.1%)\n        76\n33 (&lt;0.1%)\n        77\n28 (&lt;0.1%)\n        78\n34 (&lt;0.1%)\n        79\n20 (&lt;0.1%)\n        80\n89 (&lt;0.1%)\n        81\n295 (&lt;0.1%)\n        Unknown\n141,326\n  \n\n  \n    \n      1 n (%)\n    \n  \n\n\n\n\nThe table is really long, so a histogram would work much better to visualize how many observations are in each category:\n\nggplot(data = iat_prep, aes(x = myweight_002)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(iat_prep$myweight_002, \n                                   na.rm = T)), \n             color = \"red\", linewidth = 2)\n\nWarning: Use of `iat_prep$myweight_002` is discouraged.\nℹ Use `myweight_002` instead.\n\n\nWarning: Removed 141326 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\nWe need to convert the heights and weights to their cm and kg respectively. Since I only have a number category, I’ve gone into the codebook to find what each numbered category represents. If you put 8, you are 43 inches tall; 16:51 in; and 32:67in. Now I can use a line to see if I can create an equation to convert these values.\n\\[\n\\begin{align}\nin & = m\\times cat+b \\\\\n43 &= m \\times 8 + b \\\\\nb & = 43-8m \\\\\n\\\\\n51 &= 16m + b \\\\\n51 &= 16m + (43-8m) \\\\\nm &=1 \\\\\nb&=43-8m = 43-8=35 \\\\\n\\end{align}\n\\]\nThen we double check with third set of points:\n\\[\n\\begin{align}\n67 & = 1 \\times 32 + 35 \\\\\n67 & = 67 \\\\\n\\end{align}\n\\]\n\niat_prep$myheight_in = 1*iat_prep$myheight_002 + 35\n\nThen we need to convert height to meters since BMI is in \\(kg/m^2\\).\n\niat_prep$myheight_m = 0.0254*iat_prep$myheight_in\n\nOkay, now we need to do something similar for weight. Three more points to find the conversion: 10:90lb; 20:140lb; and 30: 190lb.\n\\[\n\\begin{align}\nlb & = m\\times cat+b \\\\\n90 &= m \\times 10 + b \\\\\nb & = 90-10m \\\\\n\\\\\n140 &= 20m + b \\\\\n140 &= 20m + (90-10m) \\\\\nm &=5 \\\\\nb&=90-10m = 90-50=40 \\\\\n\\end{align}\n\\]\nThen we double check with third set of points:\n\\[\n\\begin{align}\n190 & = 5 \\times 30 + 40 \\\\\n190 & = 190 \\\\\n\\end{align}\n\\]\n\niat_prep$myweight_lb = 5*iat_prep$myweight_002 + 40\n\nThen we need to convert height to meters since BMI is in \\(kg/m^2\\).\n\niat_prep$myweight_kg = 0.453592*iat_prep$myweight_lb\n\n\niat_prep$bmi = iat_prep$myweight_kg/(iat_prep$myheight_m)^2\n\n\nggplot(data = iat_prep, aes(x = bmi)) + \n  geom_histogram(binwidth = 1) +\n  geom_vline(aes(xintercept = mean(bmi, \n                                   na.rm = T)), \n             color = \"red\", linewidth = 2)\n\nWarning: Removed 142470 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nFrom histogram, looks like there are a couple observations at BMIs greater than 200. Let’s double check that.\n\nsummary(iat_prep$bmi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   4.28   20.81   23.71   25.48   28.21  241.41  142470 \n\n\nOkay, so we now know the max is 241.41. I want to see the observations that have BMIs this large. I’ll take a look at their other values to see if there are any other issues.\n\niat_prep_bmi = iat_prep %&gt;% filter(bmi &gt; 200)\nhead(iat_prep_bmi, 10)\n\n     IAT_score att7 iam_001 identfat_001 myweight_002 myheight_002\n1  -0.61544208    4       1            5           81            2\n2   0.54476890    1       6            2           80            1\n3   0.70458996    1       2            3           80            2\n4   0.28206698    6       7            1           81            1\n5   0.33790313    5       7            3           81            2\n6   1.23311171    1       7            4           80            2\n7  -0.02357343    7       1            1           81            2\n8           NA    7       1            1           81            2\n9   0.33704837    4       1            1           81            2\n10 -0.47687442    4       4           NA           81            1\n   identthin_001 controlother_001 controlyou_001 mostpref_001 important_001\n1              1                5              5            4             5\n2              3                4              4            3             1\n3              3                3              2            4             5\n4              5                1              1            6             5\n5              4                3              2            1             4\n6              1                5              1            1             2\n7              5                1              1            2             5\n8              5                1              5            7             5\n9              5                1              1            4             4\n10            NA               NA              1           NA            NA\n   birthmonth birthyear month year raceomb_002    raceombmulti ethnicityomb edu\n1          12      1910     1 2021           8 [1,2,3,4,5,6,7]            1  12\n2          12      2009     1 2021           4                            1   1\n3          10      1916     1 2021           4                            1   1\n4          11      1910     2 2021          NA                            3   1\n5           4      2007     2 2021           6                            3  NA\n6           5      2001     2 2021           6                            2   5\n7           2      1980     2 2021           8 [1,2,3,4,5,6,7]            1   9\n8          NA        NA     2 2021          NA                           NA  NA\n9           5      1976     2 2021           5                            2   4\n10          9        NA     2 2021        -999                           NA  NA\n   edu_14 genderIdentity birthSex myheight_in myheight_m myweight_lb\n1      12            [2]        2          37     0.9398         445\n2       1            [1]        1          36     0.9144         440\n3       1            [1]        1          37     0.9398         440\n4       1            [6]        2          36     0.9144         445\n5      NA            [1]        1          37     0.9398         445\n6       5            [1]        1          37     0.9398         440\n7       9  [1,2,3,4,5,6]        2          37     0.9398         445\n8      NA                      NA          37     0.9398         445\n9       4            [1]        1          37     0.9398         445\n10     NA            [2]        2          36     0.9144         445\n   myweight_kg      bmi\n1     201.8484 228.5359\n2     199.5805 238.6963\n3     199.5805 225.9681\n4     201.8484 241.4087\n5     201.8484 228.5359\n6     199.5805 225.9681\n7     201.8484 228.5359\n8     201.8484 228.5359\n9     201.8484 228.5359\n10    201.8484 241.4087\n\n\nLooking at the subset of individuals with BMIs greater than 200, I am reminded that there is some serious quality control that needs to be done to this dataset. Other variable observations indicate that some of these rows are individuals who did not accurately fill out their survey. Right now, we keep them in our dataset, but we will need to examine them for outliers."
  },
  {
    "objectID": "slides/08_MLR_Intro.html",
    "href": "slides/08_MLR_Intro.html",
    "title": "Introduction to Multiple Linear Regression (MLR)",
    "section": "",
    "text": "Understand equations and visualizations that helps us build multiple linear regression model.\nFit MLR model (in R) and understand the difference between fitted regression plane and regression lines.\nIdentify the population multiple linear regression model and define statistics language for key notation.\nBased off of previous SLR work, understand how the population MLR is estimated.\nInterpret MLR (population) coefficient estimates with additional variable in model\n\n\n\n\nSLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "href": "slides/09_MLR_Inference.html#reminder-of-what-we-learned-in-the-context-of-slr",
    "title": "MLR: Inference",
    "section": "Reminder of what we learned in the context of SLR",
    "text": "Reminder of what we learned in the context of SLR\n\nSLR helped us establish the foundation for a lot of regression\n\nBut we do not usually use SLR in analysis\n\n\nWhat did we learn in SLR??\n\n\n\n\nModel Fitting\n\n\n\nOrdinary least squares (OLS)\nlm() function in R\n\n\n\n\n\n\nModel Use\n\n\n\nInference for variance of residuals\nHypothesis testing for coefficients\nInterpreting population coefficient estimates\nCalculated the expected mean for specific \\(X\\) values\nInterpreted coefficient of determination\n\n\n\n\n\n\nModel Evaluation/Diagnostics\n\n\n\nLINE Assumptions\nInfluential points\nData Transformations"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#lets-map-that-to-our-regression-analysis-process",
    "href": "slides/09_MLR_Inference.html#lets-map-that-to-our-regression-analysis-process",
    "title": "MLR: Inference / F-test",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#interpreting-the-estimated-population-coefficients",
    "href": "slides/09_MLR_Inference.html#interpreting-the-estimated-population-coefficients",
    "title": "MLR: Inference / F-test",
    "section": "Interpreting the estimated population coefficients",
    "text": "Interpreting the estimated population coefficients\n\nFor a population model: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\epsilon\\]\n\nWhere \\(X_1\\) and \\(X_2\\) are continuous variables\nNo need to specify \\(Y\\) because it required to be continuous in linear regression\n\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB)."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#getting-these-interpretations-from-our-regression-table",
    "href": "slides/09_MLR_Inference.html#getting-these-interpretations-from-our-regression-table",
    "title": "MLR: Inference / F-test",
    "section": "Getting these interpretations from our regression table",
    "text": "Getting these interpretations from our regression table\nWe fit the regression model in R and printed the regression table:\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model: \\(\\widehat{\\text{LE}} = 33.595 + 0.157 \\text{ FLR} + 0.008 \\text{ FS}\\)\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "href": "slides/09_MLR_Inference.html#lets-just-examine-the-general-interpretation-vs.-the-example",
    "title": "MLR: Inference / F-test",
    "section": "Let’s just examine the general interpretation vs. the example",
    "text": "Let’s just examine the general interpretation vs. the example\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected \\(Y\\)-variable is (\\(\\widehat\\beta_0\\) units) when the \\(X_1\\)-variable is 0 \\(X_1\\)-units and \\(X_2\\)-variable is 0 \\(X_1\\)-units (95% CI: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every increase of 1 \\(X_1\\)-unit in the \\(X_1\\)-variable, adjusting/controlling for \\(X_2\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_1|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\nGeneral interpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every increase of 1 \\(X_2\\)-unit in the \\(X_2\\)-variable, adjusting/controlling for \\(X_1\\)-variable, there is an expected increase/decrease of \\(|\\widehat\\beta_2|\\) units in the \\(Y\\)-variable (95%: LB, UB).\n\n\n\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_0\\)\n\n\nThe expected life expectancy is 33.595 years when the female literacy rate is 0% and food supply is 0 0 kcal PPD (95% CI: 24.674, 41.517).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_1\\)\n\n\nFor every 1% increase in the female literacy rate, adjusting for food supply, there is an expected increase of 0.157 years in the life expectancy (95%: 0.093, 0.221).\n\n\n\n\n\nInterpretation for \\(\\widehat{\\beta}_2\\)\n\n\nFor every 1 kcal PPD increase in the food supply, adjusting for female literacy rate, there is an expected increase of 0.008 years in life expectancy (95%: 0.005, 0.012)."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#we-must-revisit-our-dear-friend-the-f-test",
    "href": "slides/09_MLR_Inference.html#we-must-revisit-our-dear-friend-the-f-test",
    "title": "MLR: Inference / F-test",
    "section": "We must revisit our dear friend, the F-test!",
    "text": "We must revisit our dear friend, the F-test!\n\nhttps://www.writerswrite.co.za/foreshadowing/"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#three-types-of-tests-that-we-will-build",
    "href": "slides/09_MLR_Inference.html#three-types-of-tests-that-we-will-build",
    "title": "MLR: Inference",
    "section": "Three types of tests that we will build",
    "text": "Three types of tests that we will build\n\n\nOverall test\n\n\nDoes the entire set of independent variables (or at least one of the variables) contribute significantly to the prediction of Y?\n\n\n\n\nTest for addition of a single variable (covariate subset test)\n\n\nDoes the addition of one particular independent variable of interest add significantly to the prediction of Y achieved by other independnet variables already present in the model?\n\n\n\n\nTest for addition of group of variables (covariate subset test)\n\n\nDoes the addition of some group of independent variables of interest add significantly to the prediction of Y obtained through other independent variables already present in the model?"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#variation-explained-vs.-unexplained",
    "href": "slides/09_MLR_Inference.html#variation-explained-vs.-unexplained",
    "title": "MLR: Inference / F-test",
    "section": "Variation: Explained vs. Unexplained",
    "text": "Variation: Explained vs. Unexplained\n\\[\\begin{aligned}\n\\sum_{i=1}^n (Y_i - \\overline{Y})^2 &= \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 + \\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 \\\\\nSSY &= SSR + SSE\n\\end{aligned}\\]\n\n\\(Y_i - \\overline{Y}\\) = the deviation of \\(Y_i\\) around the mean \\(\\overline{Y}\\)\n\n(the total amount deviation unexplained at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(\\widehat{Y}_i- \\overline{Y}\\) = the deviation of the fitted value \\(\\widehat{Y}_i\\) around the mean \\(\\overline{Y}\\)\n\n(the amount deviation explained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) ).\n\n\\(Y_i - \\widehat{Y}_i\\) = the deviation of the observation \\(Y\\) around the fitted regression line\n\n(the amount deviation unexplained by the regression at \\(X_{i1},\\ldots,X_{ik}\\) )"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#building-the-anova-table",
    "href": "slides/09_MLR_Inference.html#building-the-anova-table",
    "title": "MLR: Inference",
    "section": "Building the ANOVA table",
    "text": "Building the ANOVA table\nANOVA table (\\(k\\) = # of predictors, \\(n\\) = # of observations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariation Source\ndf\nSS\nMS\ntest statistic\np-value\n\n\n\n\nRegression\n\\(k\\)\n\\(SSR\\)\n\\(MSR = \\frac{SSR}{k}\\)\n\\(F = \\frac{MSR}{MSE}\\)\n\\(F \\sim F_{(k, n-k-1)}\\)\n\n\nError\n\\(n-k-1\\)\n\\(SSE\\)\n\\(MSE = \\frac{SSE}{n-k-1}\\)\n\n\n\n\nTotal\n\\(n-1\\)\n\\(SSY\\)\n\n\n\n\n\n\n\n\n \n\nanova(mr1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n1,934.245\n1,934.245\n66.547\n0.000\n    FoodSupplykcPPD\n1.000\n649.319\n649.319\n22.339\n0.000\n    Residuals\n69.000\n2,005.556\n29.066\nNA\nNA"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#overall-f-test",
    "href": "slides/09_MLR_Inference.html#overall-f-test",
    "title": "MLR: Inference / F-test",
    "section": "Overall F-test",
    "text": "Overall F-test\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for all the covariate coefficients…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2= \\ldots=\\beta_k=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=1, 2, \\ldots, k\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#addition-of-a-single-variable-partial-f-test",
    "href": "slides/09_MLR_Inference.html#addition-of-a-single-variable-partial-f-test",
    "title": "MLR: Inference",
    "section": "Addition of a single variable: Partial F-test",
    "text": "Addition of a single variable: Partial F-test\nHypotheses\n\n\\(H_0:\\) \\(X^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\\(H_A:\\) \\(X^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\nEquivalently,\n\\[H_0:\\beta^*=0\\ \\ vs.\\ \\ H_A:\\beta^* \\neq 0\\]\n\\[\\text{Full model}: Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_p X_p + \\beta^* X^* + \\epsilon\\\\\n\\text{Reduced model}: Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_p X_p + \\epsilon\\]\nTest statistic and probability distribution\n\\[F_{X^*|X_1, X_2, \\ldots, X_p} = \\frac{\\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}}}{\\frac{SSE_{full}}{df_{full}}}  \n= \\frac{\\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}}}{MSE_{full}}\\sim F_{1, n-k-1}\\]\n\nThe F statistic has an \\(F\\)-distribution with numerator \\(df = 1\\) and denominator \\(df = n-k-1\\).\n\nDecision\n\nIf \\(F &gt; F_{1, n-k-1, 1-\\alpha}\\), then reject \\(H_0\\)\nor, if \\(p\\)-value \\(&lt; \\alpha\\), then reject \\(H_0\\)\n\nConclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that \\(\\beta^* \\neq 0\\)\nSame as: \\(X^*\\) contributes significantly to the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#addition-of-group-of-variables-multiple-partial-f-test",
    "href": "slides/09_MLR_Inference.html#addition-of-group-of-variables-multiple-partial-f-test",
    "title": "MLR: Inference",
    "section": "Addition of group of variables: Multiple Partial F-test",
    "text": "Addition of group of variables: Multiple Partial F-test\nHypotheses\n\n\\(H_0:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\\(H_A:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\nEquivalently,\n\\[H_0:\\beta_1^*=\\beta_2^*=\\ldots = \\beta_s^*=0\\ \\ vs.\\ \\ H_A: \\text{at least one }\\beta_j^* \\neq 0\\]\n\\[\\text{Full model}: Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_q X_q + \\beta_1^* X_1^* + \\beta_2^* X_2^*+ \\ldots + \\beta_s^* X_s ^*+ \\epsilon\\\\\n\\text{Reduced model}: Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_q X_q + \\epsilon\\]\nTest statistic and probability distribution\n\\[F_{X_1^*, X_2^*, \\ldots, X_s^*|X_1, X_2, \\ldots, X_q} = \\frac{\\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}}}{\\frac{SSE_{full}}{df_{full}}}  \n= \\frac{\\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}}}{MSE_{full}}\\sim F_{s, n-(q+s)-1}\\]\n\nThe F statistic has an \\(F\\)-distribution with numerator \\(df = s\\) and denominator \\(df = n-(q+s)-1\\).\n\nDecision\n\nIf \\(F &gt; F_{s, n-(q+s)-1, 1-\\alpha}\\), then reject \\(H_0\\)\nor, if \\(p\\)-value \\(&lt; \\alpha\\), then reject \\(H_0\\)\n\nConclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that at least one \\(\\beta_j^* \\neq 0\\)\nSame as: at least one of \\(X_1^*, X_2^*, \\ldots, X_s^*\\) contributes significantly to the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "href": "slides/09_MLR_Inference.html#remember-from-lesson-5-f-test-vs.-t-test-for-the-population-slope",
    "title": "MLR: Inference / F-test",
    "section": "Remember from Lesson 5: F-test vs. t-test for the population slope",
    "text": "Remember from Lesson 5: F-test vs. t-test for the population slope\nThe square of a \\(t\\)-distribution with \\(df = \\nu\\) is an \\(F\\)-distribution with \\(df = 1, \\nu\\)\n\\[T_{\\nu}^2 \\sim F_{1,\\nu}\\]\n\nWe can use either F-test or t-test to run the following hypothesis test:\n\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\nNote that the F-test does not support one-sided alternative tests, but the t-test does!"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "href": "slides/09_MLR_Inference.html#remember-from-lesson-5-planting-a-seed-about-the-f-test",
    "title": "MLR: Inference / F-test",
    "section": "Remember from Lesson 5: Planting a seed about the F-test",
    "text": "Remember from Lesson 5: Planting a seed about the F-test\nWe can think about the hypothesis test for the slope…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model (\\(\\beta_1=0\\))\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative model (\\(\\beta_1\\neq0\\))\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n\nIn multiple linear regression, we can start using this framework to test multiple coefficient parameters at once\n\nDecide whether or not to reject the smaller reduced model in favor of the larger full model\nCannot do this with the t-test!"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#we-can-extend-this",
    "href": "slides/09_MLR_Inference.html#we-can-extend-this",
    "title": "MLR: Inference / F-test",
    "section": "We can extend this!!",
    "text": "We can extend this!!\nWe can create a hypothesis test for more than one coefficient at a time…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_2=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_1\\neq0\\) and/or \\(\\beta_2\\neq0\\)\n\n\n\n\nin a slightly different way…\n\n\n\n\n\n\nNull model\n\n\n\n\\(Y = \\beta_0 + \\epsilon\\)\nSmaller (reduced) model\n\n\n\n\n\n\nAlternative* model\n\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\)\nLarger (full) model\n\n\n\n\n\n*This is not quite the alternative, but if we reject the null, then this is the model we move forward with"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#example",
    "href": "slides/09_MLR_Inference.html#example",
    "title": "MLR: Inference",
    "section": "Example",
    "text": "Example\n\n# library(GGally)\ngapm_sub %&gt;% \n  select(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD, WaterSourcePrct) %&gt;%  \n  ggpairs()\n\ngapm_sub2 &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD, WaterSourcePrct)\ndim(gapm)\n\n[1] 195  18\n\ndim(gapm_sub2)\n\n[1] 72 18\n\n\nTest whether adding food supply and percent with water source to the model improves the prediction of life expectancy, given that female literacy rate is already in the model.\n\\[\\text{Full model}: \\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_1^* \\text{Food supply} + \\beta_2^* \\text{water source %}+ \\epsilon\\\\\n\\text{Reduced model}: \\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon\\]\nHypotheses\n\\[H_0:\\beta_1^*=\\beta_2^*=0\\ \\ vs.\\ \\ H_A:\\text{at least one }\\beta_j^* \\neq 0\\]\nTest statistic\n\n# full model\nmr2_full &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + \n                 WaterSourcePrct, data = gapm_sub2) \nanova(mr2_full) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n1934.2449\n1934.24488\n86.65083\n9.385591e-14\n    FoodSupplykcPPD\n1\n649.3186\n649.31862\n29.08835\n9.417820e-07\n    WaterSourcePrct\n1\n487.6403\n487.64032\n21.84544\n1.445509e-05\n    Residuals\n68\n1517.9156\n22.32229\nNA\nNA\n  \n  \n  \n\n\n\n#---------------\n# reduced model\nmr2_red &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub2) \nanova(mr2_red) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n1934.245\n1934.24488\n50.99945\n6.894997e-10\n    Residuals\n70\n2654.875\n37.92678\nNA\nNA\n  \n  \n  \n\n\n\n# partial F stat\n(F_2prtl &lt;- (2654.875   - 1517.9156 )/(70 - 68)/22.32229)\n\n[1] 25.46691\n\n\n\nLet \\(X_1\\) = Female literacy rate, \\(X_1^*\\) = Food supply, and \\(X_2^*\\) = WaterSourcePrct\n\n\\[F_{X_1^*, X_2^*|X_1} = \\frac{\\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}}}{\\frac{SSE_{full}}{df_{full}}}  \n= \\frac{\\frac{2654.875 - 1517.9156  }{70 - 68}}{22.32229} = 25.4669077\\]\nDecision\n\n# critical value for F 2,69 with alpha = 0.05:\nqf(.95, df1 = 2, df2 = 68)\n\n[1] 3.131672\n\n# p-value is ALWAYS the right tail for F-test\npf(F_2prtl, df1 = 2, df2 = 68, lower.tail = FALSE)\n\n[1] 5.558032e-09\n\n\n\nSince \\(F = 25.4669077 &gt; 3.131672\\), we reject \\(H_0\\).\nEquivalently, since \\(p\\text{-value} = 5.5580325\\times 10^{-9} &lt; 0.05\\), we reject \\(H_0\\).\n\nConclusion\n\nThere is sufficient evidence that at least one of countries’ food supply and percent with a water source contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#r-overall-f-test",
    "href": "slides/09_MLR_Inference.html#r-overall-f-test",
    "title": "MLR: Inference",
    "section": "R: Overall F-test",
    "text": "R: Overall F-test\n\nWe calculated F statistic = 44.4430061 and \\(p\\text{-value} = 3.9576511\\times 10^{-13}\\)\nWhere are these values in the “usual” R regression output?\n\n\nsummary(mr1)\n\n\nCall:\nlm(formula = LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n    data = gapm_sub)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.715  -2.328   1.052   3.022   9.083 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        33.595479   4.472049   7.512 1.56e-10 ***\nFemaleLiteracyRate  0.156699   0.032158   4.873 6.75e-06 ***\nFoodSupplykcPPD     0.008482   0.001795   4.726 1.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.391 on 69 degrees of freedom\nMultiple R-squared:  0.563, Adjusted R-squared:  0.5503 \nF-statistic: 44.44 on 2 and 69 DF,  p-value: 3.958e-13\n\nglance(mr1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      r.squared\n      adj.r.squared\n      sigma\n      statistic\n      p.value\n      df\n      logLik\n      AIC\n      BIC\n      deviance\n      df.residual\n      nobs\n    \n  \n  \n    0.5629759\n0.5503085\n5.391292\n44.44301\n3.957646e-13\n2\n-221.936\n451.8719\n460.9786\n2005.556\n69\n72\n  \n  \n  \n\n\n\nmr1_red &lt;- lm(LifeExpectancyYrs ~ 1, data = gapm_sub) \nanova(mr1_red, mr1)\n\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ 1\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     71 4589.1                                  \n2     69 2005.6  2    2583.6 44.443 3.958e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#example-1",
    "href": "slides/09_MLR_Inference.html#example-1",
    "title": "MLR: Inference",
    "section": "Example",
    "text": "Example\n\n# library(GGally)\ngapm_sub %&gt;% \n  select(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD, WaterSourcePrct) %&gt;%  \n  ggpairs()\n\ngapm_sub2 &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD, WaterSourcePrct)\ndim(gapm)\n\n[1] 195  18\n\ndim(gapm_sub2)\n\n[1] 72 18\n\n\nTest whether adding food supply and percent with water source to the model improves the prediction of life expectancy, given that female literacy rate is already in the model.\n\\[\\text{Full model}: \\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_1^* \\text{Food supply} + \\beta_2^* \\text{water source %}+ \\epsilon\\\\\n\\text{Reduced model}: \\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon\\]\nHypotheses\n\\[H_0:\\beta_1^*=\\beta_2^*=0\\ \\ vs.\\ \\ H_A:\\text{at least one }\\beta_j^* \\neq 0\\]\nTest statistic\n\n# full model\nmr2_full &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + \n                 WaterSourcePrct, data = gapm_sub2) \nanova(mr2_full) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n1934.2449\n1934.24488\n86.65083\n9.385591e-14\n    FoodSupplykcPPD\n1\n649.3186\n649.31862\n29.08835\n9.417820e-07\n    WaterSourcePrct\n1\n487.6403\n487.64032\n21.84544\n1.445509e-05\n    Residuals\n68\n1517.9156\n22.32229\nNA\nNA\n  \n  \n  \n\n\n\n#---------------\n# reduced model\nmr2_red &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub2) \nanova(mr2_red) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n1934.245\n1934.24488\n50.99945\n6.894997e-10\n    Residuals\n70\n2654.875\n37.92678\nNA\nNA\n  \n  \n  \n\n\n\n# partial F stat\n(F_2prtl &lt;- (2654.875   - 1517.9156 )/(70 - 68)/22.32229)\n\n[1] 25.46691\n\n\n\nLet \\(X_1\\) = Female literacy rate, \\(X_1^*\\) = Food supply, and \\(X_2^*\\) = WaterSourcePrct\n\n\\[F_{X_1^*, X_2^*|X_1} = \\frac{\\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}}}{\\frac{SSE_{full}}{df_{full}}}  \n= \\frac{\\frac{2654.875 - 1517.9156  }{70 - 68}}{22.32229} = 25.4669077\\]\nDecision\n\n# critical value for F 2,69 with alpha = 0.05:\nqf(.95, df1 = 2, df2 = 68)\n\n[1] 3.131672\n\n# p-value is ALWAYS the right tail for F-test\npf(F_2prtl, df1 = 2, df2 = 68, lower.tail = FALSE)\n\n[1] 5.558032e-09\n\n\n\nSince \\(F = 25.4669077 &gt; 3.131672\\), we reject \\(H_0\\).\nEquivalently, since \\(p\\text{-value} = 5.5580325\\times 10^{-9} &lt; 0.05\\), we reject \\(H_0\\).\n\nConclusion\n\nThere is sufficient evidence that at least one of countries’ food supply and percent with a water source contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#r-partial-f-test",
    "href": "slides/09_MLR_Inference.html#r-partial-f-test",
    "title": "MLR: Inference",
    "section": "R: Partial F-test",
    "text": "R: Partial F-test\n\n# anova(reduced_model, full_model)\nanova(mr2_red, mr2_full)\n\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     70 2654.9                                  \n2     68 1517.9  2      1137 25.467 5.558e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#partial-f-test-vs.-t-test",
    "href": "slides/09_MLR_Inference.html#partial-f-test-vs.-t-test",
    "title": "MLR: Inference",
    "section": "Partial F-test vs. t-test",
    "text": "Partial F-test vs. t-test\n\ntidy(mr1) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n33.595478902\n4.472048896\n7.512324\n1.556087e-10\n    FemaleLiteracyRate\n0.156698843\n0.032158473\n4.872708\n6.754271e-06\n    FoodSupplykcPPD\n0.008482097\n0.001794598\n4.726461\n1.167065e-05\n  \n  \n  \n\n\n\nanova(mr1_red, mr1)\n\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     70 2654.9                                  \n2     69 2005.6  1    649.32 22.339 1.167e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n4.726461^2\n\n[1] 22.33943\n\n\n\\[H_0:\\beta^*=0\\ \\ vs.\\ \\ H_A:\\beta^* \\neq 0\\] t-test statistic\n\\[t_{\\widehat{\\beta}^*} = \\frac{\\widehat{\\beta}^* - 0}{SE_{\\widehat{\\beta}^*}}\\\\\n= \\frac{0.008482097}{0.001794598} = 4.726461\\]\n\n(tstat_food &lt;- 0.008482097/0.001794598)\n\n[1] 4.726461\n\n# p-value\n2*pt(tstat_food, 69, lower.tail = FALSE)\n\n[1] 1.167066e-05"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#example-2",
    "href": "slides/09_MLR_Inference.html#example-2",
    "title": "MLR: Inference",
    "section": "Example",
    "text": "Example\n\n# library(GGally)\ngapm_sub %&gt;% \n  select(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD, WaterSourcePrct) %&gt;%  \n  ggpairs()\n\ngapm_sub2 &lt;- gapm %&gt;% \n  drop_na(LifeExpectancyYrs, FemaleLiteracyRate, FoodSupplykcPPD, WaterSourcePrct)\ndim(gapm)\n\n[1] 195  18\n\ndim(gapm_sub2)\n\n[1] 72 18\n\n\nTest whether adding food supply and percent with water source to the model improves the prediction of life expectancy, given that female literacy rate is already in the model.\n\\[\\text{Full model}: \\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_1^* \\text{Food supply} + \\beta_2^* \\text{water source %}+ \\epsilon\\\\\n\\text{Reduced model}: \\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\epsilon\\]\nHypotheses\n\\[H_0:\\beta_1^*=\\beta_2^*=0\\ \\ vs.\\ \\ H_A:\\text{at least one }\\beta_j^* \\neq 0\\]\nTest statistic\n\n# full model\nmr2_full &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + \n                 WaterSourcePrct, data = gapm_sub2) \nanova(mr2_full) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n1934.2449\n1934.24488\n86.65083\n9.385591e-14\n    FoodSupplykcPPD\n1\n649.3186\n649.31862\n29.08835\n9.417820e-07\n    WaterSourcePrct\n1\n487.6403\n487.64032\n21.84544\n1.445509e-05\n    Residuals\n68\n1517.9156\n22.32229\nNA\nNA\n  \n  \n  \n\n\n\n#---------------\n# reduced model\nmr2_red &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub2) \nanova(mr2_red) %&gt;% tidy() %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1\n1934.245\n1934.24488\n50.99945\n6.894997e-10\n    Residuals\n70\n2654.875\n37.92678\nNA\nNA\n  \n  \n  \n\n\n\n# partial F stat\n(F_2prtl &lt;- (2654.875   - 1517.9156 )/(70 - 68)/22.32229)\n\n[1] 25.46691\n\n\n\nLet \\(X_1\\) = Female literacy rate, \\(X_1^*\\) = Food supply, and \\(X_2^*\\) = WaterSourcePrct\n\n\\[F_{X_1^*, X_2^*|X_1} = \\frac{\\frac{SSE_{reduced} - SSE_{full}}{df_{reduced} - df_{full}}}{\\frac{SSE_{full}}{df_{full}}}  \n= \\frac{\\frac{2654.875 - 1517.9156  }{70 - 68}}{22.32229} = 25.4669077\\]\nDecision\n\n# critical value for F 2,69 with alpha = 0.05:\nqf(.95, df1 = 2, df2 = 68)\n\n[1] 3.131672\n\n# p-value is ALWAYS the right tail for F-test\npf(F_2prtl, df1 = 2, df2 = 68, lower.tail = FALSE)\n\n[1] 5.558032e-09\n\n\n\nSince \\(F = 25.4669077 &gt; 3.131672\\), we reject \\(H_0\\).\nEquivalently, since \\(p\\text{-value} = 5.5580325\\times 10^{-9} &lt; 0.05\\), we reject \\(H_0\\).\n\nConclusion\n\nThere is sufficient evidence that at least one of countries’ food supply and percent with a water source contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#r-partial-f-test-1",
    "href": "slides/09_MLR_Inference.html#r-partial-f-test-1",
    "title": "MLR: Inference",
    "section": "R: Partial F-test",
    "text": "R: Partial F-test\n\n# anova(reduced_model, full_model)\nanova(mr2_red, mr2_full)\n\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     70 2654.9                                  \n2     68 1517.9  2      1137 25.467 5.558e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#building-a-very-important-toolkit-three-types-of-tests",
    "href": "slides/09_MLR_Inference.html#building-a-very-important-toolkit-three-types-of-tests",
    "title": "MLR: Inference / F-test",
    "section": "Building a very important toolkit: three types of tests",
    "text": "Building a very important toolkit: three types of tests\n\n\nOverall test\n\n\nDoes at least one of the covariates/predictors contribute significantly to the prediction of Y?\n\n\n\n\nTest for addition of a single variable (covariate subset test)\n\n\nDoes the addition of one particular covariate add significantly to the prediction of Y achieved by other covariates already present in the model?\n\n\n\n\nTest for addition of group of variables (covariate subset test)\n\n\nDoes the addition of some group of covariates add significantly to the prediction of Y achieved by other covariates already present in the model?"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#reminder-or-what-we-need-in-our-interpretations-of-coefficient-attached-to-a-covariate",
    "href": "slides/09_MLR_Inference.html#reminder-or-what-we-need-in-our-interpretations-of-coefficient-attached-to-a-covariate",
    "title": "MLR: Inference",
    "section": "Reminder or what we need in our interpretations of coefficient attached to a covariate",
    "text": "Reminder or what we need in our interpretations of coefficient attached to a covariate\n\n\n\nUnits of Y\nUnits of X\nDiscussing intercept: Mean or average or expected before Y\nDiscussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nOnly need before difference or Y!!\n\nConfidence interval\n\n\n\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "href": "slides/09_MLR_Inference.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1",
    "title": "MLR: Inference",
    "section": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_2 = \\beta_3 = 0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=2,3\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1-1",
    "href": "slides/09_MLR_Inference.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1-1",
    "title": "MLR: Inference",
    "section": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1-2",
    "href": "slides/09_MLR_Inference.html#f-test-general-steps-for-hypothesis-test-for-population-slope-beta_1-2",
    "title": "MLR: Inference",
    "section": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)",
    "text": "F-test: general steps for hypothesis test for population slope \\(\\beta_1\\)\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#overall-f-test-general-steps-for-hypothesis-test-for-all-covariate-coefficients",
    "href": "slides/09_MLR_Inference.html#overall-f-test-general-steps-for-hypothesis-test-for-all-covariate-coefficients",
    "title": "MLR: Inference",
    "section": "Overall F-test: general steps for hypothesis test for all covariate coefficients",
    "text": "Overall F-test: general steps for hypothesis test for all covariate coefficients\n\n\n\n\n\nFor today’s class, we are assuming that we have met the underlying assumptions\n\n\n\n\n\n\nState the null hypothesis.\n\n\n\nOften, we are curious if the coefficient is 0 or not:\n\\[\\begin{align}\nH_0 &: \\beta_1 = 0\\\\\n\\text{vs. } H_A&: \\beta_1 \\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=1\\) and denominator \\(df=n-2\\).\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic for \\(\\widehat\\beta_1\\) is\n\\[F = \\frac{MSR}{MSE}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{1, n-2} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject: \\(P(F_{1, n-2} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis that the slope is 0 at the \\(100\\alpha\\%\\) significiance level. There is (sufficient/insufficient) evidence that there is significant association between (\\(Y\\)) and (\\(X\\)) (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#overall-f-test-1",
    "href": "slides/09_MLR_Inference.html#overall-f-test-1",
    "title": "MLR: Inference",
    "section": "Overall F-test",
    "text": "Overall F-test\nHypotheses\n\\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_k = 0 \\\\\nH_A: \\text{at least one } \\beta_j \\neq 0\\]\nTest statistic and probability distribution\n\\[F = \\frac{MSR_{full}}{MSE_{full}} \\sim F_{k, n-k-1}\\]\n\nThe F statistic has an \\(F\\)-distribution with numerator \\(df = k\\) and denominator \\(df = n-k-1\\).\n\nDecision\n\nIf \\(F &gt; F_{k, n-k-1, 1-\\alpha}\\), then reject \\(H_0\\)\nor, if \\(p\\)-value \\(&lt; \\alpha\\), then reject \\(H_0\\)\n\nConclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: at least one independent variable contributes significantly to the prediction of \\(Y\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#covariate-subset-test-single-variable",
    "href": "slides/09_MLR_Inference.html#covariate-subset-test-single-variable",
    "title": "MLR: Inference / F-test",
    "section": "Covariate subset test: Single variable",
    "text": "Covariate subset test: Single variable\nDoes the addition of one particular covariate of interest add significantly to the prediction of Y achieved by other covariates already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\beta_j X_j +\\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a single \\(j\\) covariate coefficient (where \\(j\\) can be any value \\(1, 2, \\ldots, k\\))…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_j=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_j\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(\\begin{aligned}Y = &\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_j X_j +\\\\ &\\ldots + \\beta_k X_k + \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#covariate-subset-test-group-of-variables",
    "href": "slides/09_MLR_Inference.html#covariate-subset-test-group-of-variables",
    "title": "MLR: Inference / F-test",
    "section": "Covariate subset test: group of variables",
    "text": "Covariate subset test: group of variables\nDoes the addition of some group of covariates of interest add significantly to the prediction of Y obtained through other independent variables already present in the model?\n\nFor a general population MLR model, \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2+ \\ldots + \\beta_k X_k + \\epsilon\\]\n\nWe can create a hypothesis test for a group of covariate coefficients (subset of many)… For example…\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_1=\\beta_3 =0\\) (this can be any coefficients)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\nAt least one \\(\\beta_j\\neq0\\) (for \\(j=2,3\\))\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(Y = \\beta_0 + \\beta_2 X_2 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X + \\beta_3 X_3+\\epsilon\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#overall-f-test-general-steps-for-hypothesis-test",
    "href": "slides/09_MLR_Inference.html#overall-f-test-general-steps-for-hypothesis-test",
    "title": "MLR: Inference / F-test",
    "section": "Overall F-test: general steps for hypothesis test",
    "text": "Overall F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that at least one predictor’s coefficient is not 0 (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "href": "slides/09_MLR_Inference.html#what-we-need-in-our-interpretations-of-coefficients-reference",
    "title": "MLR: Inference / F-test",
    "section": "What we need in our interpretations of coefficients (reference)",
    "text": "What we need in our interpretations of coefficients (reference)\n\n\n\nUnits of Y\nUnits of X\nDiscussing intercept: Mean or average or expected before Y\nDiscussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nOnly need before difference or Y!!\n\nConfidence interval\n\n\n\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#overall-f-test-a-word-on-the-conclusion",
    "href": "slides/09_MLR_Inference.html#overall-f-test-a-word-on-the-conclusion",
    "title": "MLR: Inference / F-test",
    "section": "Overall F-test: a word on the conclusion",
    "text": "Overall F-test: a word on the conclusion\n\nIf \\(H_0\\) is rejected, we conclude there is sufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: at least one independent variable contributes significantly to the prediction of \\(Y\\)\n\n \n\nIf \\(H_0\\) is not rejected, we conclude there is insufficient evidence that at least one predictor’s coefficient is different from zero.\nSame as: Not enough evidence that at least one independent variable contributes significantly to the prediction of \\(Y\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "href": "slides/09_MLR_Inference.html#how-do-we-fit-a-multiple-linear-regression-model-in-r",
    "title": "MLR: Inference",
    "section": "How do we fit a multiple linear regression model in R?",
    "text": "How do we fit a multiple linear regression model in R?\nNew population model for example:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#example-overall-f-test-for",
    "href": "slides/09_MLR_Inference.html#example-overall-f-test-for",
    "title": "MLR: Inference",
    "section": "Example: Overall F-test for",
    "text": "Example: Overall F-test for\n\nanova(mr1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n1,934.245\n1,934.245\n66.547\n0.000\n    FoodSupplykcPPD\n1.000\n649.319\n649.319\n22.339\n0.000\n    Residuals\n69.000\n2,005.556\n29.066\nNA\nNA\n  \n  \n  \n\n\n\n\n\nNotice we have two rows of “regression” values instead of just one row\nWe need to calculate the \\(MSR_{full}\\):\n\n\\[MSR_{full} = \\frac{SSR_{full}}{df} = \\frac{SSR_{lit rate} + SSR_{food}}{1+1} = \\frac{1934.24488 + 649.31862}{2} = 1291.78175\\]\n\n(MSR_full &lt;- (1934.24488 + 649.31862)/2)\n\n[1] 1291.782\n\n(F_full &lt;- (1934.24488 + 649.31862)/2/29.06603)\n\n[1] 44.44301\n\n\n\\[F = \\frac{MSR_{full}}{MSE_{full}} = \\frac{1291.78175}{29.06603} = 44.4430061\\]\nDecision\n\n# critical value for F 2,69 with alpha = 0.05:\nqf(.95, df1 = 2, df2 = 69)\n\n[1] 3.129644\n\n# p-value is ALWAYS the right tail for F-test\npf(F_full, df1 = 2, df2 = 69, lower.tail = FALSE)\n\n[1] 3.957651e-13\n\n\n\nSince \\(F = 44.4430061 &gt; 3.129644\\), we reject \\(H_0\\).\nEquivalently, since \\(p\\text{-value} = 3.9576511\\times 10^{-13} &lt; 0.05\\), we reject \\(H_0\\).\n\nConclusion\n\nThere is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "href": "slides/09_MLR_Inference.html#we-will-keep-working-with-the-mlr-model-from-last-class",
    "title": "MLR: Inference / F-test",
    "section": "We will keep working with the MLR model from last class",
    "text": "We will keep working with the MLR model from last class\nNew population model for example:\n\\[\\text{Life expectancy} = \\beta_0 + \\beta_1 \\text{Female literacy rate} + \\beta_2 \\text{Food supply} + \\epsilon\\]\n\n# Fit regression model:\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\ntidy(mr1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n33.595\n4.472\n7.512\n0.000\n24.674\n42.517\n    FemaleLiteracyRate\n0.157\n0.032\n4.873\n0.000\n0.093\n0.221\n    FoodSupplykcPPD\n0.008\n0.002\n4.726\n0.000\n0.005\n0.012\n  \n  \n  \n\n\n\n\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{Life expectancy}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{Female literacy rate} + \\widehat{\\beta}_2 \\text{Food supply} \\\\\n\\widehat{\\text{Life expectancy}} &= 33.595 + 0.157\\ \\text{Female literacy rate}\n+ 0.008\\ \\text{Food supply}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#when-running-a-f-test-for-linear-models",
    "href": "slides/09_MLR_Inference.html#when-running-a-f-test-for-linear-models",
    "title": "MLR: Inference / F-test",
    "section": "When running a F-test for linear models…",
    "text": "When running a F-test for linear models…\n\nWe need to define a larger, full model (more parameters)\nWe need to define a smaller, reduced model (fewer parameters)\nUse the F-statistic to decide whether or not we reject the smaller model\n\nThe F-statistic compares the SSE of each model to determine if the full model explains a significant amount of additional variance\n\n\n\n\n \n\\[\nF = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\n\\]\n\n\n\\(SSE(R) \\geq SSE(F)\\)\nNumerator measures difference in unexplained variation between the models\n\nBig difference = added parameters greatly reduce the unexplained variation (increase explained variation)\nSmaller difference = added parameters don’t reduce the unexplained variation\n\nTake ratio of difference to the unexplained variation in the full model"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#another-way-to-think-of-ssy-ssr-and-sse",
    "href": "slides/09_MLR_Inference.html#another-way-to-think-of-ssy-ssr-and-sse",
    "title": "MLR: Inference / F-test",
    "section": "Another way to think of SSY, SSR, and SSE",
    "text": "Another way to think of SSY, SSR, and SSE\n\nLet’s create a data frame of each component within the SS’s\n\nDifference in SSY: \\(Y_i - \\overline{Y}\\)\nDifference in SSR: \\(\\widehat{Y}_i- \\overline{Y}\\)\nDifference in SSE: \\(Y_i - \\widehat{Y}_i\\)\n\nUsing our simple linear regression model as an example:\n\n\nslr1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\naug_slr1 = augment(slr1)\nSS_df = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         y_fit = aug_slr1$.fitted, \n         SSR_diff = y_fit - mean(LifeExpectancyYrs), \n         SSE_diff = aug_slr1$.resid)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#plot-the-components-of-each-sum-of-squares",
    "href": "slides/09_MLR_Inference.html#plot-the-components-of-each-sum-of-squares",
    "title": "MLR: Inference / F-test",
    "section": "Plot the components of each sum of squares",
    "text": "Plot the components of each sum of squares\n\nSSY_plot = ggplot(SS_df, aes(SSY_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSR_plot = ggplot(SS_df, aes(SSR_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \nSSE_plot = ggplot(SS_df, aes(SSE_diff)) + geom_histogram() + xlim(-30, 30) + ylim(0, 35) \ngrid.arrange(SSY_plot, SSR_plot, SSE_plot, nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = \\sum_{i=1}^n (Y_i - \\overline{Y})^2 = 64.64\\]\n\\[SSR = \\sum_{i=1}^n (\\widehat{Y}_i- \\overline{Y})^2 = 27.24\\]\n\\[SSE =\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2 = 37.39\\]"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "href": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\nmod_red1 = lm(LifeExpectancyYrs ~ 1, data = gapm_sub)\naug_red1  = augment(mod_red1)\n\nmod_full1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD,\n               data = gapm_sub)\naug_full1  = augment(mod_full1)\n\nSS_df2 = gapm_sub %&gt;% select(LifeExpectancyYrs) %&gt;%\n  mutate(SSY_diff_r1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_r1 = aug_red1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_r1 = aug_red1$.resid, \n         SSY_diff_f1 = LifeExpectancyYrs - mean(LifeExpectancyYrs),\n         SSR_diff_f1 = aug_full1$.fitted - mean(LifeExpectancyYrs), \n         SSE_diff_f1 = aug_full1$.resid)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "href": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-1",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 0\\]\n \n\\[SSE = 64.64\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "href": "slides/09_MLR_Inference.html#lets-think-about-our-mlr-example-for-life-expectancy",
    "title": "MLR: Inference / F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the Overall F-test: Is the regression model containing female literacy rate and food supply useful in estimating countries’ life expectancy?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#poll-everywhere-question",
    "href": "slides/09_MLR_Inference.html#poll-everywhere-question",
    "title": "MLR: Inference",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test",
    "title": "MLR: Inference",
    "section": "So let’s step through our hypothesis test",
    "text": "So let’s step through our hypothesis test\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that at least one predictor’s coefficient is not 0 (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#section",
    "href": "slides/09_MLR_Inference.html#section",
    "title": "MLR: Inference",
    "section": "",
    "text": "anova(mr1) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    FemaleLiteracyRate\n1.000\n1,934.245\n1,934.245\n66.547\n0.000\n    FoodSupplykcPPD\n1.000\n649.319\n649.319\n22.339\n0.000\n    Residuals\n69.000\n2,005.556\n29.066\nNA\nNA\n  \n  \n  \n\n\n\n\n\nNotice we have two rows of “regression” values instead of just one row\nWe need to calculate the \\(MSR_{full}\\):\n\n\\[MSR_{full} = \\frac{SSR_{full}}{df} = \\frac{SSR_{lit rate} + SSR_{food}}{1+1} = \\frac{1934.24488 + 649.31862}{2} = 1291.78175\\]\n\n(MSR_full &lt;- (1934.24488 + 649.31862)/2)\n\n[1] 1291.782\n\n(F_full &lt;- (1934.24488 + 649.31862)/2/29.06603)\n\n[1] 44.44301\n\n\n\\[F = \\frac{MSR_{full}}{MSE_{full}} = \\frac{1291.78175}{29.06603} = 44.4430061\\]\nDecision\n\n# critical value for F 2,69 with alpha = 0.05:\nqf(.95, df1 = 2, df2 = 69)\n\n[1] 3.129644\n\n# p-value is ALWAYS the right tail for F-test\npf(F_full, df1 = 2, df2 = 69, lower.tail = FALSE)\n\n[1] 3.957651e-13\n\n\n\nSince \\(F = 44.4430061 &gt; 3.129644\\), we reject \\(H_0\\).\nEquivalently, since \\(p\\text{-value} = 3.9576511\\times 10^{-13} &lt; 0.05\\), we reject \\(H_0\\).\n\nConclusion\n\nThere is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-12",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-12",
    "title": "MLR: Inference",
    "section": "So let’s step through our hypothesis test (1/2)",
    "text": "So let’s step through our hypothesis test (1/2)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_1\\neq0 or \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-12-1",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-12-1",
    "title": "MLR: Inference",
    "section": "So let’s step through our hypothesis test (1/2)",
    "text": "So let’s step through our hypothesis test (1/2)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}=44.443\\] OR use ANOVA table:\n\nanova(mod_red1, mod_full1) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ 1\n71.000\n4,589.119\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n2.000\n2,583.563\n44.443\n0.000"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-22",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-22",
    "title": "MLR: Inference",
    "section": "So let’s step through our hypothesis test (2/2)",
    "text": "So let’s step through our hypothesis test (2/2)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-13",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-13",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_1\\neq0 \\text{ or } \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-23",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-23",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}=44.443\\] OR use ANOVA table:\n\nanova(mod_red1, mod_full1) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ 1\n71.000\n4,589.119\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n2.000\n2,583.563\n44.443\n0.000"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-33",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-33",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that either countries’ female literacy rate or the food supply (or both) contributes significantly to the prediction of life expectancy (p-value &lt; 0.001)."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#overall-f-test-general-steps-for-hypothesis-test-1",
    "href": "slides/09_MLR_Inference.html#overall-f-test-general-steps-for-hypothesis-test-1",
    "title": "MLR: Inference",
    "section": "Overall F-test: general steps for hypothesis test",
    "text": "Overall F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n\nReject if: \\(P(F_{k, n-k-1} &gt; F) &lt; \\alpha\\)\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that at least one predictor’s coefficient is not 0 (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#covariate-subset-f-test-general-steps-for-hypothesis-test",
    "href": "slides/09_MLR_Inference.html#covariate-subset-f-test-general-steps-for-hypothesis-test",
    "title": "MLR: Inference",
    "section": "Covariate subset F-test: general steps for hypothesis test",
    "text": "Covariate subset F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_j=0\\\\\n\\text{vs. } H_A&: \\beta_j\\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictor/covariate \\(j\\) significantly improves the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#other-ways-to-word-the-hypothesis-tests",
    "href": "slides/09_MLR_Inference.html#other-ways-to-word-the-hypothesis-tests",
    "title": "MLR: Inference",
    "section": "Other ways to word the hypothesis tests",
    "text": "Other ways to word the hypothesis tests\n\nOverall F-test\nSingle covariate subset F-test\n\n\\(H_0:\\) \\(X^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\\(H_A:\\) \\(X^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\nGroup covariate subset F-test\n\n\\(H_0:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\\(H_A:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\n\n\n\nMLR 2"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "href": "slides/09_MLR_Inference.html#lets-think-about-our-mlr-example-for-life-expectancy-1",
    "title": "MLR: Inference / F-test",
    "section": "Let’s think about our MLR example for life expectancy",
    "text": "Let’s think about our MLR example for life expectancy\nOur proposed population model\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\nFitted multiple regression model:\n\\[\\begin{aligned}\n\\widehat{\\text{LE}} &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\text{FLR} + \\widehat{\\beta}_2 \\text{FS} \\\\\n\\widehat{\\text{LE}} &= 33.595 + 0.157\\ \\text{FLR}\n+ 0.008\\ \\text{FS}\n\\end{aligned}\\]\nOur main question for the single covariate subset F-test: Is the regression model containing food supply improve the estimation of countries’ life expectancy, given female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "href": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-2",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 36.39\\]\n \n\\[SSE = 28.25\\]"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-13-1",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-13-1",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-23-1",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-23-1",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red2, mod_full2) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD\n69.000\n2,005.556\n1.000\n649.319\n22.339\n0.000"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-33-1",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-33-1",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply contributes significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#covariate-subset-f-test-general-steps-for-hypothesis-test-1",
    "href": "slides/09_MLR_Inference.html#covariate-subset-f-test-general-steps-for-hypothesis-test-1",
    "title": "MLR: Inference",
    "section": "Covariate subset F-test: general steps for hypothesis test",
    "text": "Covariate subset F-test: general steps for hypothesis test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\nFor example:\n\\[\\begin{align}\nH_0 &: \\beta_1 = \\beta_3 = 0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1,3\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictors/covariates \\(2,3\\) significantly improve the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "href": "slides/09_MLR_Inference.html#we-need-to-slightly-alter-our-mlr-example-for-life-expectancy",
    "title": "MLR: Inference / F-test",
    "section": "We need to slightly alter our MLR example for life expectancy",
    "text": "We need to slightly alter our MLR example for life expectancy\nOur proposed population model to include water source percent (WS):\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\beta_3 WS + \\epsilon\\]\n\nWe don’t have a fitted multiple regression model for this yet!\n\nOur main question for the group covariate subset F-test: Is the regression model containing food supply and water source percent improve the estimation of countries’ life expectancy, given percent female literacy rate is already in the model?\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "href": "slides/09_MLR_Inference.html#comparing-the-ssy-ssr-and-sse-for-reduced-and-full-model-3",
    "title": "MLR: Inference / F-test",
    "section": "Comparing the SSY, SSR, and SSE for reduced and full model",
    "text": "Comparing the SSY, SSR, and SSE for reduced and full model\n\n\nReduced / null model \\[LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 27.24\\]\n \n\\[SSE = 37.39\\]\n\n\n\nFull / Alternative model \\[LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FS + \\beta_3 WS + \\epsilon\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\[SSY = 64.64\\]\n \n\\[SSR = 43.26\\]\n \n\\[SSE = 21.38\\]"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-13-2",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-13-2",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (1/3)",
    "text": "So let’s step through our hypothesis test (1/3)\n\n\n\nMet underlying LINE assumptions\n\n\n\n \n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_2=\\beta_3=0\\\\\n\\text{vs. } H_A&: \\beta_2\\neq0 \\text{ and/or } \\beta_3\\neq0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k =2\\) and denominator \\(df=n-k-1 = 72 - 2-1=69\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-23-2",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-23-2",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (2/3)",
    "text": "So let’s step through our hypothesis test (2/3)\n\n\n\nCompute the value of the test statistic / 6. Calculate the p-value\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\] ANOVA table:\n\nanova(mod_red3, mod_full3) %&gt;% tidy() %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate\n70.000\n2,654.875\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD + WaterSourcePrct\n68.000\n1,517.916\n2.000\n1,136.959\n25.467\n0.000"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-33-2",
    "href": "slides/09_MLR_Inference.html#so-lets-step-through-our-hypothesis-test-33-2",
    "title": "MLR: Inference / F-test",
    "section": "So let’s step through our hypothesis test (3/3)",
    "text": "So let’s step through our hypothesis test (3/3)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\n \nWe reject the null hypothesis at the 5% significance level. There is sufficient evidence that countries’ food supply or water source (or both) contribute significantly to the prediction of life expectancy, given that female literacy rate is already in the model (p-value &lt; 0.001)."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#other-ways-to-word-the-hypothesis-tests-reference",
    "href": "slides/09_MLR_Inference.html#other-ways-to-word-the-hypothesis-tests-reference",
    "title": "MLR: Inference / F-test",
    "section": "Other ways to word the hypothesis tests (reference)",
    "text": "Other ways to word the hypothesis tests (reference)\n\nSingle covariate subset F-test\n\n\\(H_0:\\) \\(X^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\\(H_A:\\) \\(X^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_p\\) are already in the model\n\nGroup covariate subset F-test\n\n\\(H_0:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) does not significantly improve the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\\(H_A:\\) The addition of the \\(s\\) variables \\(X_1^*, X_2^*, \\ldots, X_s^*\\) significantly improves the prediction of \\(Y\\), given that \\(X_1, X_2, \\ldots, X_q\\) are already in the model\n\n\n\n\nMLR 2"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "href": "slides/09_MLR_Inference.html#single-covariate-f-test-general-steps-for-hypothesis-test-reference",
    "title": "MLR: Inference / F-test",
    "section": "Single covariate F-test: general steps for hypothesis test (reference)",
    "text": "Single covariate F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_j=0\\\\\n\\text{vs. } H_A&: \\beta_j\\neq 0\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictor/covariate \\(j\\) significantly improves the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "href": "slides/09_MLR_Inference.html#covariate-subset-f-test-general-steps-for-hypothesis-test-reference",
    "title": "MLR: Inference / F-test",
    "section": "Covariate subset F-test: general steps for hypothesis test (reference)",
    "text": "Covariate subset F-test: general steps for hypothesis test (reference)\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\nFor example:\n\\[\\begin{align}\nH_0 &: \\beta_1 = \\beta_3 = 0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1,3\n\\end{align}\\]\n\n\n\nSpecify the significance level\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level. There is (sufficient/insufficient) evidence that predictors/covariates \\(2,3\\) significantly improve the prediction of Y, given all the other covariates are in the model (p-value = \\(P(F_{1, n-2} &gt; F)\\))."
  },
  {
    "objectID": "slides/09_MLR_Inference.html#poll-everywhere-question-1",
    "href": "slides/09_MLR_Inference.html#poll-everywhere-question-1",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#poll-everywhere-question-2",
    "href": "slides/09_MLR_Inference.html#poll-everywhere-question-2",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#poll-everywhere-question-3",
    "href": "slides/09_MLR_Inference.html#poll-everywhere-question-3",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/09_MLR_Inference.html#poll-everywhere-question-4",
    "href": "slides/09_MLR_Inference.html#poll-everywhere-question-4",
    "title": "MLR: Inference / F-test",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "labs/Lab_03_work.html",
    "href": "labs/Lab_03_work.html",
    "title": "Lab 3 Instructions",
    "section": "",
    "text": "IMPORTANT TO READ\n\n\n\n\nPlease do not delete the rubric from your .qmd file. I will use it to circle the grades!\nThere is an intructions file and a file for you to edit and turn in. Please only work in the latter file!!"
  },
  {
    "objectID": "labs/Lab_03_work.html#directions",
    "href": "labs/Lab_03_work.html#directions",
    "title": "Lab 3 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n1.1 Purpose\nThe main purpose of this lab is to perform some quality control on our data, recode some of the multi-selection categorical variables, continue data exploration, and start analyzing the main relationship of our research question.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like Section 2.4 and explanations in Section 2.5)"
  },
  {
    "objectID": "labs/Lab_03_work.html#lab-activities",
    "href": "labs/Lab_03_work.html#lab-activities",
    "title": "Lab 3 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?\n\n\n2.2 Quality Control\nThere are a few more issues with the data that we need to look into. First, there is another coding for NA values in the race variable: -999. We will need to filter out these observations.\nWe will also need to look at individuals who have potentially answered the survey questions untruthfully. We cannot catch everything, but a good place to start is by looking at individuals who have done more than one of the following:\n\nselected the earliest or latest possible birth year\nselected the lowest or highest possible education\nselected all gender identities (for those using gender identity)\nselected all races (for those using multiple selection race)\nselected the lowest or highest weight (for those looking at BMI)\nselected the lowest or highest height (for those looking at BMI)\n\nI want to take a second to mention that any of the above selections, and combinations of the above selections, are valid. However, we should start to flag the possibility that someone has not gone through the survey properly if we notice that most or all of the respondent’s answers are the first answer choice, last answer choice, or selected all options. Additionally, not all of these carry the same importance in discerning validity. For example, a recorded age of 111 years old is the most striking to me. When paired with other selections that are the maximum or minimum (or first or last) option, then I will record it for future investigation. If this observation looks to be an outlier or high leverage point in our analysis, that is when I’ll decide to remove it.\n\n\n\n\n\n\nTasks\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page.\n\n\n\n\n\n2.3 Working with multi-selection variables\nIn the list of variables that we may choose to work with (in Lab 2), there are two that allowed respondents to select multiple categories. The two variables are genderIdentitiy and raceombmulti. If you did not choose these variables to work with, you may skip this section.\nIf you chose one or both of these variables, then we need to make new variables that correspond to indicators for each possible selection in the respective variable.\nLet’s start with the grepl function. For this function, we can input one of our column names and a value, then it will output, for each row, if the value is in the column. For example, in genderIdentity an individual may identify as a “Trans female/Trans woman” and “Gender queer/Gender nonconforming.” In our dataset in R, this would show as [4,5] in genderIdentity. If we want to create two separate indicators for anyone who identifies as “Trans female/Trans woman” then I need to look for the value 4 in the column genderIdentity. I will run a separate indicator to find individuals who identify as “Gender queer/Gender nonconforming.” Here is an example code of how I would use grepl to do this:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_tf_tw = grepl(4, genderIdentity), \n         ind_gq_gnc = grepl(5, genderIdentity))\n\nYou will need to extend this to all other gender identities.\nFor race, raceombmulti is also the follow up question to raceomb_002. So our indicators need to reflect both variables. In this case, we need to use grepl on both columns at once. For example, if I want to create an indicator for individuals who identify as American Indian/Alaskan Native then I need to find individuals who identify as American Indian/Alaskan Native only and individuals who identify as American Indian/Alaskan Native in addition to another race. For example, my code might look like:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_AIAN = grepl(1, raceomb_002) | grepl(1, raceombmulti))\n\nI suggest only searching for 1-7 in both raceomb_002 and raceombmulti. Note that if raceomb_002 = 8 , then individuals identified as “multiracial” and will select values in raceombmulti.\n\n\n\n\n\n\nTask\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection.\n\n\n\n\n2.4 Thinking about potential confounders and effect modifiers\nBefore we explore more of the data, I want us to take a second to think through potential confounders and effect modifiers from the covariates that we selected in Lab 2. For some of the covariates, we were asked to explain why we chose them. Now I want you to consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship. For example, if our variable of interest is fat group identity, then we may consider that self-perception of size is a confounder since it could be linked with fat group identity and potentially be associated with IAT score.\nFor multi-level, unordered categorical covariates, you might consider if a specific category has an impact. For example, we might consider creating an indicator for white, non-Hispanic/Lantinx respondents since the history of fatphobia is tied with white-centered colonization and white supremacy (Redpath, 2023). Thus IAT scores might look different for White respondents vs. minority respondents (those who answered American Indian/Alaskan Natives; East Asian; South Asian; Native Hawaiian or other Pacific Islander; Black or African American; or Hispanic or Latino). Alternatively, we may not want to center our analysis on whiteness. The same fatphobic history involving white supremacy was particularly targetting Black people. So perhaps we want make an indicator for Black or African American respondents. Another option is leaving race as is - we may have enough data to handle the inclusion of all groups!\nThe purpose of this section is to make sure we are thinking about the relationships between variables in our analysis. I do not want us to make any decisions based solely on the data. I want any changes or manipulations in our variables to be motivated by research-backed evidence.\nFinally, for this project, we are most interested in the relationship we identified in our research question! Other variables are supporting this question, and improving that model fit so that we get as close to the true relationship in our research question as possible!\n\n\n\n\n\n\nTask\n\n\n\nFor each variable, consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship.\n\n\n\n\n2.5 Continuing data exploration\nIn this section, we are going to further explore the variables that we might be adjusting for in our model (potential covariates outside the variable or interest in our research question).\n\n2.5.1 Bivariate exploration\nWe want to look at all other relationships between IAT score and each covariate (outside of the research question variable). Some of you have already made these plots in Lab 2, so you can simply refine them and display them here. There are a few questions that I want you to consider:\n\nFor categorical variables, is there an inherent order? Does the ordered values follow an approximately linear relationship? Are the categories “evenly spaced”? For example, education categories are not necessarily evenly spaced.\nAgain for categorical variables, is there a natural place to divide the categories up? For example, in education, it might be helpful to control for the fact that students in college might be asked to complete this test as an assignment. Thus, we might make an indicator for individuals in college vs. not. This decision can be informed by our plot, but it should not be driven by our plot!!\n\n\n\n\n\n\n\nTask\n\n\n\nFor each variable outside of your research question, create the appropriate plot to visualize the relationship between IAT score and the variable. Comment if there is an obvious trend or not.\n\n\n\n\n2.5.2 Multivariate exploration\nNow we want to extend our plots for Lab 2 where we looked at the outcome (IAT score) and our main variable of interest (as identified by our research question). Here, we will run the same plot, but include another variable. This will help us visualize potential confounders or effect modifiers. Note that if you made indicator variables (for race, gender identity or any other variable), then you should have a plot for each indicator variable.\nYou will need to really think about what kind of plot will best displays these relationships! IAT score is continuous, and many of your variable of interest is categorical. You may consider side-by-side boxplots where the color is the additional variable. You might also consider a jitter plot or only plotting the means. Remember you’re goal for plotting is to get a sense of the relationship only from the plot! Your audience should not have to work hard to understand what the plot is communicating. For example, I wanted to look at IAT score, internalization of societal standards, and race. I might make my plot like such:\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\") +\n  theme(axis.text.x = element_text(angle = 45, size = 8, hjust = 1))\n\n\n\n\n\n\n\nCode to contruct multivariate plot\nggplot(iat_prep2, aes(x = important_001, y=IAT_score, color = as.factor(raceomb_002_f))) +\n  # geom_jitter(size = 2, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, shape = 18) +\n  stat_summary(fun = mean, geom=\"line\") +\n  scale_x_discrete(limits = levels(iat_prep2$important_001_f), labels = function(x) str_wrap(x, width=10)) +\n  labs(x = \"Importance of weight to sense of self \\n (Internalization of societal standards)\", \n       y = \"IAT score\",\n       title = \"Mean IAT scores for importance of weight to sense of self by race\", \n       color = \"Race\")\n\n\n\n\n\nNote that the above plot is specific for these variables!! Other variables may require a different type of visua\nlization!! Also note that I originally had geom_jitter() in my plot, which would make the plot really hard to understand!! Try uncommenting it to see what I mean by “hard to understand.” Also, think about why I connected the mean IAT scores across the different levels of internalization. I had a hard time connecting specific race’s points to identify a trend. Again, try commenting out stat_summary(fun = mean, geom=\"line\") to see what I mean.\n\n\n\n\n\n\nNote\n\n\n\nAn aside: You may see that collapsing groups might wash out differences. If we make an indicator for Black of African American respondents, as we mentioned above, then including White respondents with other minority groups may wash out their association with IAT score and wrongly lead us to a model that says identifying as Black or African American has no association with IAT, where we clearly see that Black or African American respondents have a unique trajectory for IAT scores.\n\n\nPlease make sure that you have made the needed changes to your plot in Lab 2. I noticed many unordered groups in plots where there should be an inherent order and unreadable axes because the text was not tilted. Please see discussion on Slack for what some students did to achieve these plots.\nHere are a few sources that might help you get started with the visualizations:\n\nIntro to R\nModern Data Visualization with R\n\n\n\n\n\n\n\nTask\n\n\n\nFor at least 3 variables outside of your research question, create the appropriate plot to visualize the relationship between IAT score, your main variable (in research question), and the variable outside your research question. Comment whether you can determine anything from the plot or not. If you can, is there any indication that the variable is a confounder or effect modifier?\n\n\n\n\n\n2.6 Fit a simple linear regression\nAs a starting point, it is good to fit a simple linear regression for our primary research question. This is often called the “crude” association. It just means that we are not adjusting for any other variables, and establishing the “starting point” for our analysis. It is likely that the results of the regression will change as we add other variables in the model.\n\n\n\n\n\n\nTask\n\n\n\nRun a simple linear regression model for the relationship in your primary research question. Print the regression table. Interpret the results and comment on the initial trend you see.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nThis is not required in Lab 3. However, if you want to run a multiple linear regression model with one other variable that you plotted in Multivariate Exploration, then you should try it! Do the results align with your ideas in Section 2.4 and/or the visualization you saw in Section 2.5?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "href": "slides/10_Cat_covariates.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#another-way-of-thinking-about-ssy-ssr-and-sse",
    "href": "slides/10_Cat_covariates.html#another-way-of-thinking-about-ssy-ssr-and-sse",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Another way of thinking about SSY, SSR, and SSE",
    "text": "Another way of thinking about SSY, SSR, and SSE"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#chapter-12-prequel",
    "href": "slides/10_Cat_covariates.html#chapter-12-prequel",
    "title": "Categorical Covariates",
    "section": "Chapter 12 prequel",
    "text": "Chapter 12 prequel\n\nChapter 12 is on dummy variables, which I prefer to call indicator variables\nIt covers models that have both a continuous predictor and a categorical predictor\nThe book does not discuss models with just one categorical predictor from a regression perspective, which is what these notes cover\nFor this topic we will continue to use the 2011 Gapminder data with life expectancy as the numeric outcome, but world region (four_regions) as the predictor: Africa, Americas, Asia, and Europe\n\nSuggested resources\n\nModern Dive Section 5.2\n\nhttps://moderndive.com/5-regression.html#model2\nNote that they are using a different Gapminder dataset than we are.\n\nVu & Harrington (text from BSTA 511)\n\nSections 6.3.3 & 7.5"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#suggested-resources",
    "href": "slides/10_Cat_covariates.html#suggested-resources",
    "title": "Categorical Covariates",
    "section": "Suggested resources",
    "text": "Suggested resources\n\nModern Dive Section 5.2\n\nhttps://moderndive.com/5-regression.html#model2\nNote that they are using a different Gapminder dataset than we are.\n\nVu & Harrington (text from BSTA 511)\n\nSections 6.3.3 & 7.5"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#linear-regression-with-one-categorical-explanatory-variable",
    "href": "slides/10_Cat_covariates.html#linear-regression-with-one-categorical-explanatory-variable",
    "title": "Categorical Covariates",
    "section": "Linear regression with one categorical explanatory variable",
    "text": "Linear regression with one categorical explanatory variable\n\n\nBad option for visualizaion:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_point() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualizaion:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualizaion:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_boxplot() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#linear-regression-with-one-categorical-explanatory-variable-1",
    "href": "slides/10_Cat_covariates.html#linear-regression-with-one-categorical-explanatory-variable-1",
    "title": "Categorical Covariates",
    "section": "Linear regression with one categorical explanatory variable",
    "text": "Linear regression with one categorical explanatory variable\n\n\n\nWhen using a categorical explanatory variable,\n\nwe do NOT find a best-fit line.\n\nInstead we model the means of the outcome\n\nfor the different levels of the categorical variable."
  },
  {
    "objectID": "slides/10_Cat_covariates.html#regression-equation-13",
    "href": "slides/10_Cat_covariates.html#regression-equation-13",
    "title": "Categorical Covariates",
    "section": "Regression equation (1/3)",
    "text": "Regression equation (1/3)\n\n\nPreviously: simple linear regression\n\nOutcome \\(Y\\) = numerical variable\nPredictor \\(X\\) = numerical variable\n\nThe regression (best-fit) line is: \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\n\nNew: what if the explanatory variable is categorical?\nNaively, we could write: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\)\nOr, with our variables: \\[\\widehat{\\textrm{LE}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\textrm{WR} \\]\n\nBut what does \\(\\textrm{WR}\\) (world regions) mean in this equation?\n\nWhat values can it take? How do we represent each region?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#regression-equation-23",
    "href": "slides/10_Cat_covariates.html#regression-equation-23",
    "title": "Categorical Covariates",
    "section": "Regression equation (2/3)",
    "text": "Regression equation (2/3)\n\nWe need more variables in our equation to specify each of the multiple levels (categories)\nFor this example, we have:\n\n\\[\\widehat{\\textrm{LE}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\text{Americas} + \\widehat{\\beta}_2 \\cdot \\text{Asia} + \\widehat{\\beta}_3 \\cdot \\text{Europe}\\]\nNote that \\(\\text{Africa}\\) is not in the equation!\n\nWhen interpreting this equation, the world region variables are replaced with 0 or 1,\n\ndepending on which world region we are interpreting the regression for\nwe call these variable indicators, or indicator functions\n\ntextbook calls them dummy variables\n\n\n\n\\[Americas =\n\\begin{cases}\n1 & \\text{if world region =} Americas \\\\\n0         & \\text{else}\n\\end{cases}\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#regression-equation-33",
    "href": "slides/10_Cat_covariates.html#regression-equation-33",
    "title": "Categorical Covariates",
    "section": "Regression equation (3/3)",
    "text": "Regression equation (3/3)\n\\[\\widehat{\\textrm{life expectancy}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Americas + \\widehat{\\beta}_2 \\cdot Asia + \\widehat{\\beta}_3 \\cdot Europe\\]\n\n\n\n\n\n\n\n\nWorld region\nRegression equation for world region\nAverage Life Expectancy for world region\n\n\n\n\nAfrica\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot 0 + \\widehat{\\beta}_2 \\cdot 0 + \\widehat{\\beta}_3 \\cdot 0\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0\\)\n\n\nAmericas\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot 1 + \\widehat{\\beta}_2 \\cdot 0 + \\widehat{\\beta}_3 \\cdot 0\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1\\)\n\n\nAsia\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot 0 + \\widehat{\\beta}_2 \\cdot 1 + \\widehat{\\beta}_3 \\cdot 0\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_2\\)\n\n\nEurope\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot 0 + \\widehat{\\beta}_2 \\cdot 0 + \\widehat{\\beta}_3 \\cdot 1\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_3\\)"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "href": "slides/10_Cat_covariates.html#interpretation-of-regression-equation-coefficients",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Interpretation of regression equation coefficients",
    "text": "Interpretation of regression equation coefficients\n\nRemember: expected, mean, and average are interchangeable\n\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nExpected/mean/average life expectancy of Africa\n\n\n\\(\\widehat{\\beta}_1\\)\nDifference in mean life expectancy of the Americas and Africa -OR-\nMean difference in life expectancy of the Americas and Africa\n\n\n\\(\\widehat{\\beta}_2\\)\nDifference in mean life expectancy between Asia and Africa -OR-\nMean difference in life expectancy between Asia and Africa\n\n\n\\(\\widehat{\\beta}_3\\)\nDifference in mean life expectancy between Europe and Africa -OR-\nMean difference in life expectancy between Europe and Africa"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#manually-calculate-coefficient-values",
    "href": "slides/10_Cat_covariates.html#manually-calculate-coefficient-values",
    "title": "Categorical Covariates",
    "section": "(Manually) calculate coefficient values",
    "text": "(Manually) calculate coefficient values\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\nCode\n# means of each level of `four_regions`\ngapm2_ave &lt;- gapm2 %&gt;% \n  group_by(four_regions) %&gt;% \n  summarise(\n    life_ave = mean(LifeExpectancyYrs))\n\n# mean of `africa`\nmean_africa &lt;- gapm2_ave %&gt;% \n  filter(four_regions == \"Africa\") %&gt;% \n  pull(life_ave)\n\n# differences in means between levels of `four_regions` and `africa`\ngapm2_ave_diff &lt;- gapm2_ave %&gt;% \n  mutate(`Difference with Africa` = life_ave - mean_africa) %&gt;%\n  rename(`World regions` = four_regions, \n         `Average life expectancy` = life_ave)\n\n# At the beginning of the Rmd we loaded knitr, which is where the kable command is from\n# library(knitr)\ngapm2_ave_diff %&gt;% kable(\n  digits = 1,\n  format = \"markdown\"\n  ) \n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Africa\n\n\n\n\nAfrica\n61.3\n0.0\n\n\nAmericas\n74.6\n13.3\n\n\nAsia\n71.7\n10.4\n\n\nEurope\n77.6\n16.3\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#reference-levels",
    "href": "slides/10_Cat_covariates.html#reference-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Reference levels",
    "text": "Reference levels\nWhy is Africa not one of the variables in the regression equation?\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nCategorical variables have to have at least 2 levels. If they have 2 levels, we call them binary\n\n \n\nWe choose one level as our reference level to which all other levels of the categorical variable are compared\n\nThe levels \\(\\text{Americas}, \\text{Asia}, \\text{Europe}\\) are compared to the level \\(\\text{Africa}\\)\n\n\n \n\nThe intercept of the regression equation is the mean of the outcome restricted to the reference level\n\nRecall that the intercept is the mean life expectancy of Africa, which was our reference level\n\n\n \n\nIf the categorical variable has \\(r\\) levels, then we need \\(r-1\\) variables/coefficients to model it!"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#reference-levels-example-change-the-reference-level-to-europe-12",
    "href": "slides/10_Cat_covariates.html#reference-levels-example-change-the-reference-level-to-europe-12",
    "title": "Categorical Covariates",
    "section": "Reference levels example: Change the reference level to Europe (1/2)",
    "text": "Reference levels example: Change the reference level to Europe (1/2)\nSuppose we want to compare the mean life expectancies of world regions to the \\(Europe\\) level instead of \\(Africa\\)\n\nBelow is the regression equation for when \\(Africa\\) is the reference level.\n\n\\[\\widehat{\\textrm{life expectancy}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Americas + \\widehat{\\beta}_2 \\cdot Asia + \\widehat{\\beta}_3 \\cdot Europe\\] * Update the variables to make \\(Europe\\) the reference level:\n\\[\\widehat{\\textrm{life expectancy}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Africa + \\widehat{\\beta}_2 \\cdot Americas + \\widehat{\\beta}_3 \\cdot Asia\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#reference-levels-example-change-the-reference-level-to-europe-22",
    "href": "slides/10_Cat_covariates.html#reference-levels-example-change-the-reference-level-to-europe-22",
    "title": "Categorical Covariates",
    "section": "Reference levels example: Change the reference level to europe (2/2)",
    "text": "Reference levels example: Change the reference level to europe (2/2)\n\nNow update the coefficients of the regression equation using the output below.\n\n\n\n\n\n\nfour_regions\nlife_ave\nDifference with Europe\n\n\n\n\nafrica\n61.3\n-16.3\n\n\namericas\n74.6\n-3.0\n\n\nasia\n71.7\n-5.9\n\n\neurope\n77.6\n0.0\n\n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 77.6+ -16.3 \\cdot Africa + -3 \\cdot Americas + -5.9 \\cdot Asia\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#r-regression-table-with-lm-function",
    "href": "slides/10_Cat_covariates.html#r-regression-table-with-lm-function",
    "title": "Categorical Covariates",
    "section": "R: Regression table with lm() function",
    "text": "R: Regression table with lm() function\n\nmodel1 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model1) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n61.32\n0.76\n80.26\n0.00\n    four_regionsAmericas\n13.32\n1.23\n10.83\n0.00\n    four_regionsAsia\n10.38\n1.08\n9.61\n0.00\n    four_regionsEurope\n16.29\n1.13\n14.37\n0.00\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]\n\nWhich world region did R choose as the reference level?\nHow you would calculate the mean life expectancies of world regions using only the results from the regression table?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "href": "slides/10_Cat_covariates.html#r-change-reference-level-to-europe-12",
    "title": "Lesson 10: Categorical Covariates",
    "section": "R: Change reference level to europe (1/2)",
    "text": "R: Change reference level to europe (1/2)\n\nfour_regions data type was originally a character - check this with str()\n\n\nstr(gapm$four_regions) \n\n chr [1:195] \"asia\" \"europe\" \"africa\" \"europe\" \"africa\" \"americas\" ...\n\n\n\nIn order to change the reference level, we need to convert it to data type factor\n\nI also did this at the beginning to capitalize each region\n\n\n\ngapm_ex = gapm %&gt;% \n mutate(four_regions = factor(four_regions, \n                              levels = c(\"africa\", \"americas\", \"asia\", \"europe\"), \n                              labels = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\")))\nstr(gapm_ex$four_regions) \n\n Factor w/ 4 levels \"Africa\",\"Americas\",..: 3 4 1 4 1 2 2 4 3 4 ...\n\nlevels(gapm_ex$four_regions) # order of factor levels\n\n[1] \"Africa\"   \"Americas\" \"Asia\"     \"Europe\""
  },
  {
    "objectID": "slides/10_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "href": "slides/10_Cat_covariates.html#r-change-reference-level-to-europe-22",
    "title": "Lesson 10: Categorical Covariates",
    "section": "R: Change reference level to europe (2/2)",
    "text": "R: Change reference level to europe (2/2)\n\nNow change the order of the factor levels\nCode below uses fct_relevel() from the forcats package that gets loaded as a part of the tidyverse\nAny levels not mentioned will be left in their existing order, after the explicitly mentioned levels.\n\n\ngapm2 &lt;- gapm2 %&gt;% \n  mutate(four_regions = \n      fct_relevel(four_regions, \"Europe\"))\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\""
  },
  {
    "objectID": "slides/10_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "href": "slides/10_Cat_covariates.html#r-run-model-with-europe-as-the-reference-level",
    "title": "Lesson 10: Categorical Covariates",
    "section": "R: Run model with europe as the reference level",
    "text": "R: Run model with europe as the reference level\n\nlevels(gapm2$four_regions)\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\"    \n\nmodel2 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model2) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n77.61\n0.84\n92.72\n0.00\n    four_regionsAfrica\n−16.29\n1.13\n−14.37\n0.00\n    four_regionsAmericas\n−2.97\n1.28\n−2.33\n0.02\n    four_regionsAsia\n−5.91\n1.13\n−5.21\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia}) \\\\ \\widehat{\\textrm{LE}} &= 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia}) \\end{aligned}\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#fitted-values-residuals",
    "href": "slides/10_Cat_covariates.html#fitted-values-residuals",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Fitted values & residuals",
    "text": "Fitted values & residuals\n\n\nSimilar to as before:\n\nObserved values \\(y\\) are the values in the dataset\nFitted values \\(\\widehat{y}\\) are the values that fall on the best-fit line for a specific value of x are the means of the outcome stratified by the categorical predictor’s levels\nResiduals \\(y - \\widehat{y}\\) are the differences between the two"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#fitted-values",
    "href": "slides/10_Cat_covariates.html#fitted-values",
    "title": "Categorical Covariates",
    "section": "Fitted values",
    "text": "Fitted values\n\nm1_aug &lt;- augment(model1)\n\n \n\n\n\nggplot(m1_aug, aes(x = four_regions, \n                   y = .fitted)) +\n  geom_point() + ylim(60, 80) + theme(axis.text = element_text(size = 22), title = element_text(size = 22))\n\n\n\n\n\n\n\n\n\n\n\nggplot(m1_aug, aes(x = four_regions, \n                   y = .fitted)) +\n  geom_jitter() + ylim(60, 80) + theme(axis.text = element_text(size = 22), title = element_text(size = 22))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#residual-plots",
    "href": "slides/10_Cat_covariates.html#residual-plots",
    "title": "Categorical Covariates",
    "section": "Residual plots",
    "text": "Residual plots\n\n\n\nggplot(m1_aug, aes(x=.resid)) +\n  geom_histogram() + \n  theme(axis.text = element_text(size = 22), \n        title = element_text(size = 22))\n\n\n\n\n\n\n\n\n\n\nggplot(m1_aug, aes(y=.resid)) +\n  geom_boxplot() +\n  theme(axis.text = element_text(size = 22), \n        title = element_text(size = 22))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#different-ways-to-code-categorical-variables",
    "href": "slides/10_Cat_covariates.html#different-ways-to-code-categorical-variables",
    "title": "Categorical Covariates",
    "section": "Different ways to code categorical variables",
    "text": "Different ways to code categorical variables\n\n\n\nReference cell coding\n\nIntercept is the mean of the outcome variable for the reference level of the categorical variable\nk-1 levels are coded as 0, 1\n\n\n\n\n\nWorld region\nVar: Americas\nVar: Asia\nVar: Europe\n\n\n\n\nAfrica (ref)\n0\n0\n0\n\n\nAmericas\n1\n0\n0\n\n\nAsia\n0\n1\n0\n\n\nEurope\n0\n0\n1\n\n\n\n\n\nEffect coding: categorical variables\n\nIntercept is the mean of the k levels’ means of the outcome variable\nThis is the same as the overall mean if each of the levels has the same sample size\nk-1 levels are coded as -1, 0, 1\n\n\n\n\n\nWorld region\nVar: Americas\nVar: Asia\nVar: Europe\n\n\n\n\nAfrica (ref)\n-1\n-1\n-1\n\n\nAmericas\n1\n0\n0\n\n\nAsia\n0\n1\n0\n\n\nEurope\n0\n0\n1\n\n\n\n\n\n\nDifferent statistical software packages have different defaults.\n\nImportant to know what the default is!!"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#regression-equation-with-effect-coding",
    "href": "slides/10_Cat_covariates.html#regression-equation-with-effect-coding",
    "title": "Categorical Covariates",
    "section": "Regression equation with effect coding",
    "text": "Regression equation with effect coding\n\\[\\widehat{\\textrm{life expectancy}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Americas + \\widehat{\\beta}_2 \\cdot Asia + \\widehat{\\beta}_3 \\cdot Europe\\]\n\n\n\n\n\n\n\n\nWorld region\nRegression equation for world region\nAverage Life Expectancy for world region\n\n\n\n\nAfrica\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (-1) + \\widehat{\\beta}_2 \\cdot (-1) + \\widehat{\\beta}_3 \\cdot (-1)\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0 - \\widehat{\\beta}_1 - \\widehat{\\beta}_2 - \\widehat{\\beta}_3\\)\n\n\nAmericas\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot 1 + \\widehat{\\beta}_2 \\cdot 0 + \\widehat{\\beta}_3 \\cdot 0\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1\\)\n\n\nAsia\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot 0 + \\widehat{\\beta}_2 \\cdot 1 + \\widehat{\\beta}_3 \\cdot 0\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_2\\)\n\n\nEurope\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot 0 + \\widehat{\\beta}_2 \\cdot 0 + \\widehat{\\beta}_3 \\cdot 1\\)\n\\(\\widehat{y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_3\\)"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#manually-calculate-coefficient-values-with-effect-coding",
    "href": "slides/10_Cat_covariates.html#manually-calculate-coefficient-values-with-effect-coding",
    "title": "Categorical Covariates",
    "section": "(Manually) calculate coefficient values with effect coding",
    "text": "(Manually) calculate coefficient values with effect coding\n\\[\\widehat{\\textrm{life expectancy}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Americas + \\widehat{\\beta}_2 \\cdot Asia + \\widehat{\\beta}_3 \\cdot Europe\\]\nThe mean of all 4 regions’ mean life expectancy is 71.3 years.\n\n\n\n\n\n\nfour_regions\nlife_ave\nDifference with mean of means\n\n\n\n\nAfrica\n61.3\n-10.0\n\n\nAmericas\n74.6\n3.3\n\n\nAsia\n71.7\n0.4\n\n\nEurope\n77.6\n6.3\n\n\n\n\n\nSee code file for code that created the table above & equation below.\n\\[\\widehat{\\textrm{life expectancy}} = 71.3+ 3.3 \\cdot Americas + 0.4 \\cdot Asia + 6.3 \\cdot Europe\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#example-regression-equation-with-effect-coding",
    "href": "slides/10_Cat_covariates.html#example-regression-equation-with-effect-coding",
    "title": "Categorical Covariates",
    "section": "Example: regression equation with effect coding",
    "text": "Example: regression equation with effect coding\n\\[\\widehat{\\textrm{life expectancy}} = 71.3+ 3.3 \\cdot Americas + 0.4 \\cdot Asia + 6.3 \\cdot Europe\\]\n\n\n\n\n\n\n\n\nWorld region\nRegression equation for world region\nAverage Life Expectancy for world region\n\n\n\n\nAfrica\n\\(\\widehat{y} = 71.3 + 3.3 (-1) + 0.4 (-1) + 6.3 (-1)\\) | \\(\\widehat{y} = 71.3 - 3.3 - 0.4 - 6.3 = 61.3\\) |\n\n\nAmericas\n\\(\\widehat{y} = 71.3 + 3.3 \\cdot 1 + 0.4 \\cdot 0 + 6.3 \\cdot 0\\) | \\(\\widehat{y} = 71.3 + 3.3 = 74.6\\) |\n\n\nAsia\n\\(\\widehat{y} = 71.3 + 3.3 \\cdot 0 + 0.4 \\cdot 1 + 6.3 \\cdot 0\\) | \\(\\widehat{y} = 71.3 + 0.4= 71.7\\) |\n\n\nEurope\n\\(\\widehat{y} = 71.3 + 3.3 \\cdot 0 + 0.4 \\cdot 0 + 6.3 \\cdot 1\\) | \\(\\widehat{y} = 71.3 + 6.3= 77.6\\) |"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-brute-force-way",
    "href": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-brute-force-way",
    "title": "Categorical Covariates",
    "section": "Model in R with effect coding (brute force way)",
    "text": "Model in R with effect coding (brute force way)\n\nCreate variables with the -1, 0, 1 values\n\n\n\n\n\n\n\n\n  \n    \n    \n      americas_eff\n      Europe\n      Africa\n      Americas\n      Asia\n    \n  \n  \n    -1\n0\n54\n0\n0\n    0\n45\n0\n0\n54\n    1\n0\n0\n34\n0\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      asia_eff\n      Europe\n      Africa\n      Americas\n      Asia\n    \n  \n  \n    -1\n0\n54\n0\n0\n    0\n45\n0\n34\n0\n    1\n0\n0\n0\n54\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      europe_eff\n      Europe\n      Africa\n      Americas\n      Asia\n    \n  \n  \n    -1\n0\n54\n0\n0\n    0\n0\n0\n34\n54\n    1\n45\n0\n0\n0"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-brute-force-way-1",
    "href": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-brute-force-way-1",
    "title": "Categorical Covariates",
    "section": "Model in R with effect coding (brute force way)",
    "text": "Model in R with effect coding (brute force way)\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n71.3173366\n0.4179792\n170.6241472\n1.183922e-203\n    americas_eff\n3.3208987\n0.7989377\n4.1566430\n4.954854e-05\n    asia_eff\n0.3845153\n0.6830810\n0.5629131\n5.741830e-01\n    europe_eff\n6.2915523\n0.7245538\n8.6833472\n2.096464e-15\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{life expectancy}} = 71.3+ 3.3 \\cdot Americas + 0.4 \\cdot Asia + 6.3 \\cdot Europe\\]\n\nNote that all three of the effect coded variables we created need to be specified in the model, instead of the one four_regions variable\nHow you would calculate the mean life expectancies of world regions using only the results from the regression table?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-using-contrasts",
    "href": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-using-contrasts",
    "title": "Categorical Covariates",
    "section": "Model in R with effect coding using contrasts",
    "text": "Model in R with effect coding using contrasts\n\nContrasts are used to tell R what “coding system” we want to use when testing hypotheses\n\nReference cell coding is the default type of coding used by R\n\n\n  2 3 4\n1 0 0 0\n2 1 0 0\n3 0 1 0\n4 0 0 1\n\n\nEffect coding: note that R assigns the -1 values to the last level instead of the first level\n\n\n  [,1] [,2] [,3]\n1    1    0    0\n2    0    1    0\n3    0    0    1\n4   -1   -1   -1"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#reorder-levels-so-that-africa-is-last",
    "href": "slides/10_Cat_covariates.html#reorder-levels-so-that-africa-is-last",
    "title": "Categorical Covariates",
    "section": "Reorder levels so that Africa is last",
    "text": "Reorder levels so that Africa is last\n\nDoing this so that R’s lm() output is consistent with what we calculated on previous slides when using effect coding\nR will assign -1 values to the last level of the categorical variable\n\n\n\n[1] \"Europe\"   \"Africa\"   \"Americas\" \"Asia\"    \n\n\n[1] \"Americas\" \"Asia\"     \"Europe\"   \"Africa\"  \n\n\n four_regions Americas Asia Europe Africa\n       Europe        0    0     45      0\n       Africa        0    0      0     54\n     Americas       34    0      0      0\n         Asia        0   54      0      0"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-using-contrasts-1",
    "href": "slides/10_Cat_covariates.html#model-in-r-with-effect-coding-using-contrasts-1",
    "title": "Categorical Covariates",
    "section": "Model in R with effect coding using contrasts",
    "text": "Model in R with effect coding using contrasts\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n71.3173366\n0.4179792\n170.6241472\n1.183922e-203\n    four_regions_africa_last1\n3.3208987\n0.7989377\n4.1566430\n4.954854e-05\n    four_regions_africa_last2\n0.3845153\n0.6830810\n0.5629131\n5.741830e-01\n    four_regions_africa_last3\n6.2915523\n0.7245538\n8.6833472\n2.096464e-15\n  \n  \n  \n\n\n\n\nRegression equation we computed based on group means:\n\\[\\widehat{\\textrm{life expectancy}} = 71.3+ 3.3 \\cdot Americas + 0.4 \\cdot Asia + 6.3 \\cdot Europe\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#reference-cell-coding-vs.-effect-coding",
    "href": "slides/10_Cat_covariates.html#reference-cell-coding-vs.-effect-coding",
    "title": "Categorical Covariates",
    "section": "Reference cell coding vs. effect coding",
    "text": "Reference cell coding vs. effect coding\n\nReference cell coding\n\nR’s default for regression\n\nEffect coding\n\nThis is what one-way ANOVA is from a regression point of view\n\nSee Section 17.4.2 of textbook\n\n\nDifferent statistical software packages have different defaults.\n\nImportant to know what the default is!!\n\n\nSee https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/ for more on different ways to code categorical variables in R\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#topics",
    "href": "slides/10_Cat_covariates.html#topics",
    "title": "Categorical Covariates",
    "section": "Topics",
    "text": "Topics\n\nHow to write out the regression equation\nHow to calculate & interpret the coefficients\nReference levels\n\nChanging the reference level\n\nRunning the model in R\n\nFactor levels\nChanging the reference level in R\nFitted values\nResiduals\n\nDifferent ways to “code” categorical variables\n\nReference cell coding\nEffect coding"
  },
  {
    "objectID": "slides/10_Cat_covariates.html",
    "href": "slides/10_Cat_covariates.html",
    "title": "Categorical Covariates",
    "section": "",
    "text": "Understand why we need a new way to code categorical variables compared to continuous variables\nWrite the regression equation for a categorical variable using reference cell coding\nCalculate and interpret coefficients for reference cell coding\nChange the reference level in a categorical variable for reference cell coding\nCreate new variables and interpret coefficient for ordinal / scoring coding\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "href": "slides/10_Cat_covariates.html#still-looking-at-gapminder-life-expectancy-data",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Still looking at Gapminder Life Expectancy data",
    "text": "Still looking at Gapminder Life Expectancy data\n\nWe will look at life expectancy vs. these world regions\nGapminder uses four world regions\n\nAfrica\nThe Americas\nAsia\nEurope"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "href": "slides/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\nBad option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_point() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\nGood option for visualization:\n\n\nCode\nggplot(gapm2, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_boxplot() +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "href": "slides/10_Cat_covariates.html#linear-regression-with-a-categorical-covariate-1",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Linear regression with a categorical covariate",
    "text": "Linear regression with a categorical covariate\n\n\n\nWhen using a categorical covariate/predictor (that is not ordered),\n\nWe do NOT, technically, find a best-fit line\n\nInstead we model the means of the outcome\n\nFor the different levels of the categorical variable\n\nIn 511, we used Kruskal-Wallis test and our ANOVA table to test if groups means were statistically different from one another\nWe can do this using linear models AND we can include other variable in the model"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#building-the-regression-equation-13",
    "href": "slides/10_Cat_covariates.html#building-the-regression-equation-13",
    "title": "Categorical Covariates",
    "section": "Building the regression equation (1/3)",
    "text": "Building the regression equation (1/3)\n\n\nPreviously: simple linear regression\n\nOutcome \\(Y\\) = numerical variable\nPredictor \\(X\\) = numerical variable\n\nThe regression (best-fit) line is: \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\n\nNew: what if the explanatory variable is categorical?\nNaively, we could write: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\)\nOr, with our variables: \\[\\widehat{\\textrm{LE}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\textrm{WR} \\]\n\nBut what does \\(\\textrm{WR}\\) (world regions) mean in this equation?\n\nWhat values can it take? How do we represent each region?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#building-the-regression-equation-23",
    "href": "slides/10_Cat_covariates.html#building-the-regression-equation-23",
    "title": "Categorical Covariates",
    "section": "Building the regression equation (2/3)",
    "text": "Building the regression equation (2/3)\n\nIn order to represent each region, we need to introduce a new function:\n\nIndicator function:\n\n\\[I(X = x) \\text{ or } I(x) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ X = x \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]\nThis basically a binary yes/no if \\(X\\) is a specific value \\(x\\)\nFor example, if we want to identify a"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "href": "slides/10_Cat_covariates.html#building-the-regression-equation-problem-with-a-single-coefficient",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: problem with a single coefficient",
    "text": "Building the regression equation: problem with a single coefficient\n\n\nPreviously: simple linear regression\n\nOutcome \\(Y\\) = numerical variable\nPredictor \\(X\\) = numerical variable\n\nThe regression (best-fit) line is: \\[\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X \\]\n\nNew: what if the explanatory variable is categorical?\nNaively, we could write: \\(\\widehat{Y} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\)\nOr, with our variables: \\[\\widehat{\\textrm{LE}} = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot \\textrm{WR} \\]\n\nBut what does \\(\\textrm{WR}\\) (world regions) mean in this equation?\n\nWhat values can it take? How do we represent each region?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-the-mean",
    "href": "slides/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-the-mean",
    "title": "Categorical Covariates",
    "section": "Building the regression equation: how do we map categories to the mean?",
    "text": "Building the regression equation: how do we map categories to the mean?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "href": "slides/10_Cat_covariates.html#building-the-regression-equation-how-do-we-map-categories-to-means",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: how do we map categories to means?",
    "text": "Building the regression equation: how do we map categories to means?\n\nIf we only have world region in our model and want to map it to an expected life expectancy…\n\n\n\n\nWe want to create a function that can map each region to life expectancy\n\nIf in Africa: \\(\\widehat{LE} = 61.32\\)\nIf in the Americas: \\(\\widehat{LE} = 74.64\\)\nIf in Asia: \\(\\widehat{LE} = 71.70\\)\nIf in Europe: \\(\\widehat{LE} = 77.61\\)\n\nCan we make one equation for \\(\\widehat{LE}\\) by putting the “if” statements within the equation?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "href": "slides/10_Cat_covariates.html#building-the-regression-equation-indicator-functions",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: Indicator functions",
    "text": "Building the regression equation: Indicator functions\n\nIn order to represent each region in the equation, we need to introduce a new function:\n\nIndicator function:\n\n\\[I(X = x) \\text{ or } I(x) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ X = x \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]\n\nThis basically a binary yes/no if \\(X\\) is a specific value \\(x\\)\n\nFor example, if we want to identify a country as being in the Americas region, we can make:\n\\[I(WR = \\text{Americas}) \\text{ or }I(\\text{Americas}) =\n\\left\\{\n\\begin{array}{@{}ll@{}}\n1, & \\text{if}\\ WR = \\text{Americas} \\\\\n  0, & \\text{else}\n\\end{array}\\right. \\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "href": "slides/10_Cat_covariates.html#building-the-regression-equation-indicators-in-our-equation",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Building the regression equation: Indicators in our equation",
    "text": "Building the regression equation: Indicators in our equation\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 \\cdot I(\\text{Africa}) + 74.64 \\cdot I(\\text{Americas}) + \\\\ &71.7 \\cdot I(\\text{Asia}) + 77.61 \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\nHowever, a linear regression equation still requires an intercept!\n\nSo one of our regions need to become our “reference” group\nWe’ll use Africa as our reference\nThat means we need to adjust all the numbers\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} = & 61.32 + 13.32 \\cdot I(\\text{Americas}) + \\\\ &10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe}) \\\\\n\\widehat{\\textrm{LE}} = & \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\\\ & \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "href": "slides/10_Cat_covariates.html#viewing-the-regression-equation-another-way",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Viewing the regression equation another way",
    "text": "Viewing the regression equation another way\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) + \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\n\n\n\n\n\n\n\n\nWorld region\nRegression equation for WR\nAverage Life Expectancy for WR\n\n\n\n\nAfrica\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0\\)\n\n\nAmericas\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1+ \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_1\\)\n\n\nAsia\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_2\\)\n\n\nEurope\n\\(\\begin{aligned} \\widehat{\\textrm{LE}} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 \\end{aligned}\\)\n\\(\\widehat{\\textrm{LE}} = \\widehat\\beta_0 + \\widehat\\beta_3\\)"
  },
  {
    "objectID": "homework/HW4.html#questions",
    "href": "homework/HW4.html#questions",
    "title": "Homework 4",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nFor this problem we will be using the penguins dataset from the palmerpenguins R package. We will look at the association between flipper length of penguins (measured in mm) and specific species of penguins.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\nPart a\nCalculate the average flipper lengths stratified by each of the penguin species.\n\n\nPart b\nMake a scatterplot (with jigger) of flipper lengths by species, and include diamond-shape points for the averages of the flipper lengths for each of the species.\n\n\nPart c\nWrite out the fitted regression equation that models the flipper length by penguin species. Use LaTeX math markup or insert an image of your equation. Do not yet insert values for the regression coefficients, i.e. use the generic coefficients \\(\\widehat{\\beta}_0, \\widehat{\\beta}_1\\). Use Adelie as the reference level.\n\n\nPart d\nRun the linear regression of flipper lengths vs. species in R, and display the regression table output. Which species did R choose as the reference level, and how did you determine this?\n\n\nPart e\nHow do we interpret each of the regression coefficients for this model? Write out a separate interpretation for each of the coefficients.\n\n\nPart f\nCalculate the mean flipper length (and the 95% CI) of penguins in the Chinstrap and Gentoo species using the predict() function.\n\n\n\nQuestion 2\nA team of nutrition experts investigated the influence of protein content in diet on the relationship between age (explanatory variable) and height (outcome, in centimeters) for children. Using the dataset, CH12Q03.xls, answer the following questions.\nThis question was adapted from this textbook.\n\nPart a\nUsing R, make a variable that is a factor for Diet. Make sure to check what values the original variable for Diet can take. How many indicator functions do you need to represent the categorical variable Diet (protein-rich vs. protein-poor)?\n\n\nPart b\nAt a level of significance \\(\\alpha = 0.10\\), test whether protein diet modifies the effect of age on height. Justify your answer (e.g., perform a hypothesis test for the interaction between diet and age).\nNote: recall that an effect modifier is an interaction.\n\n\nPart c\nIs it possible that diet is a confounder? Note: this will depend on your results from Part b.\n\n\nPart d\nWrite the fitted regression equation for our model in Part b. Write the respective regression lines for each specific diet group: protein rich and protein poor. Interpret the slope of each regression line (no need for a 95% CI here).\n\n\n\nQuestion 3\nAn experiment was conducted regarding a quantitative analysis of factors found in high-density lipoprotein (HDL) in a sample of human blood serum. Three variables thought to be predictive of, or associated with, HDL measurement (Y) were the total cholesterol (X1) and total triglyceride (X2) concentrations in the sample, plus the presence or absence of a certain sticky component of the serum called sinking pre-beta or SPB (X3), coded as 0 if absent and 1 if present. Using the dataset, CH09Q05.xls, answer the following questions.\n\nPart a\nUse \\(\\alpha= 0.05\\), test whether if there is a crude association between HDL measurement and total cholesterol. Note: testing for a crude association means we fit a simple linear regression model and see if the association is significant.\n\n\nPart b\nSometimes simple linear regression leads us to believe that there is no association between two variables, but missing interaction might be obscuring the association. Use \\(\\alpha= 0.1\\) to test whether total triglyceride is an effect modifier of the association between HDL and total cholesterol.\nNote: Since the data frame has the variables named as \\(Y\\), \\(X1\\), and \\(X2\\), you may use those in the regression equations, but when you are making a conclusion, please use the specific names of the variables to identify each. For example, \\(Y\\) is actually HDL.\nNote: The plan is to cover interactions between two continuous covariates on Wednesday 2/21. We will not interpret the interaction coefficient for this, but we can test the interaction coefficient. Similar to the interaction for a binary covariate and a continuous covariate, we only need to test one coefficient, \\(\\beta_3\\).\n\n\nPart c\nIs it possible that total triglyceride is a confounder? No need to test this explicity.\nNote: this will depend on your results from Part b."
  },
  {
    "objectID": "slides/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "href": "slides/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-12",
    "title": "Lesson 10: Categorical Covariates",
    "section": "We can change the reference level to Europe (1/2)",
    "text": "We can change the reference level to Europe (1/2)\n\nSuppose we want to compare the mean life expectancies of world regions to the \\(\\text{Europe}\\) level instead of \\(\\text{Africa}\\)\nBelow is the estimated regression equation for when \\(Africa\\) is the reference level\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\nUpdate the variables to make \\(Europe\\) the reference level:\n\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Africa}) +  \\widehat\\beta_2 \\cdot I(\\text{Americas}) + \\widehat\\beta_3 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "href": "slides/10_Cat_covariates.html#we-can-change-the-reference-level-to-europe-22",
    "title": "Lesson 10: Categorical Covariates",
    "section": "We can change the reference level to Europe (2/2)",
    "text": "We can change the reference level to Europe (2/2)\n\nNow update the coefficients of the regression equation using the output below.\n\n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Europe\n\n\n\n\nAfrica\n61.32\n-16.29\n\n\nAmericas\n74.64\n-2.97\n\n\nAsia\n71.70\n-5.91\n\n\nEurope\n77.61\n0.00\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 77.61 -16.29 \\cdot I(\\text{Africa}) -2.97 \\cdot I(\\text{Americas}) -5.91 \\cdot I(\\text{Asia})\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "href": "slides/10_Cat_covariates.html#there-are-different-ways-to-code-categorical-variables",
    "title": "Lesson 10: Categorical Covariates",
    "section": "There are different ways to code categorical variables",
    "text": "There are different ways to code categorical variables\n\nReference cell coding (sometimes called dummy coding)\n\nCompares each level of a variable to the omitted (reference) level\n\nEffect coding (sometimes called sum coding or deviation coding)\n\nCompares deviations from the grand mean\n\nOrdinal encoding (sometimes called scoring)\n\nCategories have a natural, even spaced ordering\n\n\nIf you want to learn more about these and other coding schemes:\n\nCoding Systems for Categorical Variables in Regression Analysis\nCategorical Data Encoding Techniques\nCoding Schemes for Categorical Variables"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#look-at-an-ordinal-variable",
    "href": "slides/10_Cat_covariates.html#look-at-an-ordinal-variable",
    "title": "Categorical Covariates",
    "section": "Look at an ordinal variable?",
    "text": "Look at an ordinal variable?\n\n\n\n\n\n\n\n\n\n\n\n\nA few changes needed:\n\nPut the income levels in order\n\n\ngapm2 = gapm2 %&gt;%\n mutate(income_levels = factor(income_levels, \n            ordered = T, \n            levels = c(\"Low income\", \n            \"Lower middle income\", \n            \"Upper middle income\", \n            \"High income\")))\n\n\nMake the income levels readble\n\nHow to Rotate Axis Labels in ggplot2?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#section",
    "href": "slides/10_Cat_covariates.html#section",
    "title": "Categorical Covariates",
    "section": "",
    "text": "Categorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#look-at-four-income-levels",
    "href": "slides/10_Cat_covariates.html#look-at-four-income-levels",
    "title": "Categorical Covariates",
    "section": "Look at four income levels",
    "text": "Look at four income levels\nGaominder income levels\n\nLevel 1 is made up of people who earn less than $2 a day and live in extreme poverty\nAt Level 2, people earn between $2 and $8 a day. Almost half the world’s population lives at this income level\nLevel 3 is made up of people who live on between $8 – $32 per day\nThe richest billion people on earth live at Level 4, where their income is more than $32 a day\nGDP per capita"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#look-at-ordinal-variable",
    "href": "slides/10_Cat_covariates.html#look-at-ordinal-variable",
    "title": "Categorical Covariates",
    "section": "Look at ordinal variable?",
    "text": "Look at ordinal variable?\n\n\n\nggplot(gapm2, aes(x = income_levels, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"Income levels\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. income levels\",\n       caption = \"Diamonds = Income level averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20), \n        axis.text.x=element_text(angle = 20, vjust = 1, hjust=1))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#how-can-we-code-this-variable",
    "href": "slides/10_Cat_covariates.html#how-can-we-code-this-variable",
    "title": "Lesson 10: Categorical Covariates",
    "section": "How can we code this variable?",
    "text": "How can we code this variable?\nWe have two options:\n\n\n\n\nTreat the levels as nominal, and use reference cell coding\n\n\n\nLike we did with world regions\nThis option will not break the linearity assumption\nFor \\(g\\) categories of the variable, we will have \\(g-1\\) coefficients to estimate\n\n\n\n\n\n\nUse the ordinal values to score the levels and treat as a numerical variable\n\n\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers\nThis way of coding preserves more power in the model (less coefficients to estimate means more power)\nOnly one coefficient to estimate"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#check-that-linearity",
    "href": "slides/10_Cat_covariates.html#check-that-linearity",
    "title": "Categorical Covariates",
    "section": "Check that linearity",
    "text": "Check that linearity\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#check-that-linearity-hold-for-income-levels",
    "href": "slides/10_Cat_covariates.html#check-that-linearity-hold-for-income-levels",
    "title": "Categorical Covariates",
    "section": "Check that linearity hold for income levels",
    "text": "Check that linearity hold for income levels\n\n\n\nUsing visual assessment, linearity holds for our income levels\nWe can use the ordinal encoding for income levels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "href": "slides/10_Cat_covariates.html#check-that-linearity-holds-for-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Check that linearity holds for income levels",
    "text": "Check that linearity holds for income levels\n\n\n\nUsing visual assessment, linearity holds for our income levels\nWe can use the ordinal encoding for income levels"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#ordinal-encoding-scoring",
    "href": "slides/10_Cat_covariates.html#ordinal-encoding-scoring",
    "title": "Categorical Covariates",
    "section": "Ordinal encoding / Scoring",
    "text": "Ordinal encoding / Scoring\n\nMap each income level to a number\nUsually start at 1\n\n\n\n\nIncome Level\nScore\n\n\n\n\nLow income\n1\n\n\nLower middle income\n2\n\n\nUpper middle income\n3\n\n\nHigh income\n4\n\n\n\n\ngapm2 = gapm2 %&gt;%\n  mutate(income_num = as.numeric(income_levels))\nstr(gapm2$income_num)\n\n num [1:187] 1 3 3 4 2 4 3 2 4 4 ..."
  },
  {
    "objectID": "slides/10_Cat_covariates.html#run-the-model-with-the-scored-income",
    "href": "slides/10_Cat_covariates.html#run-the-model-with-the-scored-income",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Run the model with the scored income",
    "text": "Run the model with the scored income\n\nmod_inc2 = lm(LifeExpectancyYrs ~ income_num, data = gapm2)\ntidy(mod_inc2) %&gt;% gt() %&gt;% tab_options(table.font.size = 37) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n    income_num\n6.25\n0.37\n16.91\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{\\textrm{LE}} &=  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot \\text{Income level} \\\\\n\\widehat{\\textrm{LE}} &=  54.01 + 6.25 \\cdot \\text{Income level}\n\\end{aligned}\\]\n\nKeep in mind: We cannot calculate the expected outcome outside of the scoring values\n\nFor example, we cannot find the mean life expectancy for an income level of 1.5"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#to-keep-in-mind-with-scoring-of-a-categorical-variable",
    "href": "slides/10_Cat_covariates.html#to-keep-in-mind-with-scoring-of-a-categorical-variable",
    "title": "Categorical Covariates",
    "section": "To keep in mind with scoring of a categorical variable",
    "text": "To keep in mind with scoring of a categorical variable\n\nWe cannot calculate the expected outcome outside of the scoring values\n\nFor example, we cannot find the mean life expectancy for an income level of 1.5\n\n\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#what-if",
    "href": "slides/10_Cat_covariates.html#what-if",
    "title": "Categorical Covariates",
    "section": "What if…",
    "text": "What if…\n\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "href": "slides/10_Cat_covariates.html#what-if-life-expectancy-vs.-income-level-looked-like-this",
    "title": "Lesson 10: Categorical Covariates",
    "section": "What if life expectancy vs. income level looked like this?",
    "text": "What if life expectancy vs. income level looked like this?\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo longer maintaining the linearity assumption\nNeed to use reference cell coding\n\n \n\nWe would fit the following model: \\[\\begin{aligned}\n\\textrm{LE} = & \\beta_0 + \\beta_1 \\cdot I(\\text{Lower middle income}) + \\\\ & \\beta_2 \\cdot I(\\text{Upper middle income}) + \\\\ & \\beta_3 \\cdot I(\\text{High income}) + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#next-time-well-start-looking-at-interactions",
    "href": "slides/10_Cat_covariates.html#next-time-well-start-looking-at-interactions",
    "title": "Categorical Covariates",
    "section": "Next time, we’ll start looking at interactions",
    "text": "Next time, we’ll start looking at interactions\n\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#next-time-well-start-looking-at-interactions-v.-additive-effects",
    "href": "slides/10_Cat_covariates.html#next-time-well-start-looking-at-interactions-v.-additive-effects",
    "title": "Categorical Covariates",
    "section": "Next time, we’ll start looking at interactions v. additive effects",
    "text": "Next time, we’ll start looking at interactions v. additive effects\n\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#interpreting-the-model",
    "href": "slides/10_Cat_covariates.html#interpreting-the-model",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Interpreting the model",
    "text": "Interpreting the model\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.01\n1.06\n51.03\n0.00\n51.92\n56.10\n    income_num\n6.25\n0.37\n16.91\n0.00\n5.52\n6.98\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} =  54.01 + 6.25 \\cdot \\text{Income level}\\]\n\nInterpreting the intercept: At an income level of 0, mean life expectancy is 54.01 (95% CI: 51.92, 56.10).\n\nNote: this does not make sense because there is no income level of 0!\n\nInterpreting the coefficient for income: For every 1-level increase in income level, mean life expectancy increases 6.25 years (95% CI: 5.52, 6.98)."
  },
  {
    "objectID": "slides/10_Cat_covariates.html#regression-table-with-lm-function",
    "href": "slides/10_Cat_covariates.html#regression-table-with-lm-function",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Regression table with lm() function",
    "text": "Regression table with lm() function\n\nmodel1 &lt;- lm(LifeExpectancyYrs ~ four_regions, data = gapm2)\ntidy(model1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 38) %&gt;% \n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.32\n0.76\n80.26\n0.00\n59.81\n62.83\n    four_regionsAmericas\n13.32\n1.23\n10.83\n0.00\n10.89\n15.74\n    four_regionsAsia\n10.38\n1.08\n9.61\n0.00\n8.25\n12.51\n    four_regionsEurope\n16.29\n1.13\n14.37\n0.00\n14.05\n18.52\n  \n  \n  \n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]\n\nWhich world region did R choose as the reference level?\nHow you would calculate the mean life expectancies of world regions using only the results from the regression table?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "href": "slides/10_Cat_covariates.html#bringing-in-the-numbersunits95-ci",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Bringing in the numbers/units/95% CI",
    "text": "Bringing in the numbers/units/95% CI\n\n\n\n\n\n\n\nCoefficient\nInterpretation\n\n\n\n\n\\(\\widehat{\\beta}_0\\)\nAverage life expectancy of countries in Africa is 61.32 years (95% CI: 59.81, 62.83).\n\n\n\\(\\widehat{\\beta}_1\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 13.32 (95% CI: 10.89, 15.74).\n\n\n\\(\\widehat{\\beta}_2\\)\nThe difference in mean life expectancy between countries in the Americas and Africa is 10.38 (95% CI: 8.25, 12.51).\n\n\n\\(\\widehat{\\beta}_3\\)\nThe difference in mean life expectancy between countries in Europe and Africa is 18.52 (95% CI: 14.05, 18.52).\n\n\n\n \n\nDon’t forget that we can use the confidence intervals to assess whether the mean difference with Africa is significant or not"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "href": "slides/10_Cat_covariates.html#another-way-to-look-at-coefficient-values",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Another way to look at coefficient values",
    "text": "Another way to look at coefficient values\n\\[\\widehat{\\textrm{LE}} =  \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{Americas}) +  \\widehat\\beta_2 \\cdot I(\\text{Asia}) + \\widehat\\beta_3 \\cdot I(\\text{Europe})\\]\n\n\nCode\n# means of each level of `four_regions`\ngapm2_ave &lt;- gapm2 %&gt;% \n  group_by(four_regions) %&gt;% \n  summarise(\n    life_ave = mean(LifeExpectancyYrs))\n\n# mean of `africa`\nmean_africa &lt;- gapm2_ave %&gt;% \n  filter(four_regions == \"Africa\") %&gt;% \n  pull(life_ave)\n\n# differences in means between levels of `four_regions` and `africa`\ngapm2_ave_diff &lt;- gapm2_ave %&gt;% \n  mutate(`Difference with Africa` = life_ave - mean_africa) %&gt;%\n  rename(`World regions` = four_regions, \n         `Average life expectancy` = life_ave)\n\n# At the beginning of the Rmd we loaded knitr, which is where the kable command is from\n# library(knitr)\ngapm2_ave_diff %&gt;% kable(\n  digits = 1,\n  format = \"markdown\"\n  ) \n\n\n\n\n\nWorld regions\nAverage life expectancy\nDifference with Africa\n\n\n\n\nAfrica\n61.3\n0.0\n\n\nAmericas\n74.6\n13.3\n\n\nAsia\n71.7\n10.4\n\n\nEurope\n77.6\n16.3\n\n\n\n\n\n\\[\\widehat{\\textrm{LE}} = 61.32 + 13.32 \\cdot I(\\text{Americas}) + 10.38 \\cdot I(\\text{Asia}) + 16.29 \\cdot I(\\text{Europe})\\]"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "href": "slides/10_Cat_covariates.html#we-can-also-use-r-to-report-each-regions-average-life-expectancy",
    "title": "Lesson 10: Categorical Covariates",
    "section": "We can also use R to report each region’s average life expectancy",
    "text": "We can also use R to report each region’s average life expectancy\nFind the 95% CI’s for the mean life expectancy for the Americas, Asia, and Europe\n\nUse the base R predict() function (see Lesson 4 for more info)\nRequires specification of a newdata “value”\n\n\nnewdata &lt;- data.frame(four_regions = c(\"Africa\", \"Americas\", \"Asia\", \"Europe\")) \n\n\n\n\n(pred = predict(model1, \n                newdata=newdata, \n                interval=\"confidence\"))\n\n       fit      lwr      upr\n1 61.32037 59.81287 62.82787\n2 74.63824 72.73841 76.53806\n3 71.70185 70.19435 73.20935\n4 77.60889 75.95751 79.26027\n\n\n\n\n\nInterpretations\n\n\n\nThe average life expectancy for countries in the Americas is 74.64 years (95% CI: 72.74, 76.54).\nThe average life expectancy for countries in Asia is 71.7 years (95% CI: 70.19, 73.21).\nThe average life expectancy for countries in Europe is 77.61 years (95% CI: 75.96, 79.26)."
  },
  {
    "objectID": "slides/10_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "href": "slides/10_Cat_covariates.html#fitted-values-are-the-same-as-the-means",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Fitted values are the same as the means",
    "text": "Fitted values are the same as the means\n\nm1_aug &lt;- augment(model1)\n\nggplot(m1_aug, aes(x = four_regions, y = .fitted)) + geom_point() +\n  theme(axis.text = element_text(size = 22), axis.title = element_text(size = 22))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "href": "slides/10_Cat_covariates.html#residual-plots-now-the-spread-within-each-region",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Residual plots (now the spread within each region)",
    "text": "Residual plots (now the spread within each region)\n\nggplot(m1_aug, aes(x=.resid)) + geom_histogram() + \n  theme(axis.text = element_text(size = 22), title = element_text(size = 22))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "href": "slides/10_Cat_covariates.html#lets-look-at-life-expectancy-vs.-four-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Let’s look at life expectancy vs. four income levels",
    "text": "Let’s look at life expectancy vs. four income levels\n\nGapminder discusses individual income levels\n\n \n\nIncome levels for a country is based on average GDP per capita, and grouped into:\n\nLow income\nLower middle income\nUpper middle income\nHigh income"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "href": "slides/10_Cat_covariates.html#visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Visualizing the ordinal variable, income levels",
    "text": "Visualizing the ordinal variable, income levels\n\n\n\n\n\n\n\n\n\n\n\n\nA few changes needed:\n\nPut the income levels in order\n\n\ngapm2 = gapm2 %&gt;%\n mutate(income_levels = factor(income_levels, \n            ordered = T, \n            levels = c(\"Low income\", \n            \"Lower middle income\", \n            \"Upper middle income\", \n            \"High income\")))\n\n\nMake the income levels readable\n\nHow to Rotate Axis Labels in ggplot2?"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "href": "slides/10_Cat_covariates.html#much-better-visualizing-the-ordinal-variable-income-levels",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Much better: Visualizing the ordinal variable, income levels",
    "text": "Much better: Visualizing the ordinal variable, income levels\n\n\n\nggplot(gapm2, aes(x = income_levels, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"Income levels\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. income levels\",\n       caption = \"Diamonds = Income level averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20), \n        axis.text.x=element_text(angle = 20, vjust = 1, hjust=1))"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "href": "slides/10_Cat_covariates.html#some-important-considerations-when-scoring-ordinal-variables",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Some important considerations when scoring ordinal variables",
    "text": "Some important considerations when scoring ordinal variables\n\nEven if a variable is inherently ordered, we need to check that linearity holds if categories are represented as numbers\n\n \n\nAssumes differences between adjacent groups are equal\n\nIncome levels are pre-set groups by Gapminder\nMight be hard to interpret “every 1-level increase in income level”\n\n\n \n\nIs the variable part of the main relationship that you are investigating? (even if linearity holds)\n\nIf yes, consider leaving as reference cell coding unless the interpretation makes sense\nIf no, and just needed as an adjustment in your model, then power benefit of scoring might be worth it!"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#poll-everywhere-question",
    "href": "slides/10_Cat_covariates.html#poll-everywhere-question",
    "title": "Categorical Covariates",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#poll-everywhere-question-1",
    "href": "slides/10_Cat_covariates.html#poll-everywhere-question-1",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#poll-everywhere-question-2",
    "href": "slides/10_Cat_covariates.html#poll-everywhere-question-2",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#poll-everywhere-question-3",
    "href": "slides/10_Cat_covariates.html#poll-everywhere-question-3",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#ordinal-coding-scoring",
    "href": "slides/10_Cat_covariates.html#ordinal-coding-scoring",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Ordinal coding / Scoring",
    "text": "Ordinal coding / Scoring\n\nMap each income level to a number\nUsually start at 1\n\n\n\n\nIncome Level\nScore\n\n\n\n\nLow income\n1\n\n\nLower middle income\n2\n\n\nUpper middle income\n3\n\n\nHigh income\n4\n\n\n\n\ngapm2 = gapm2 %&gt;%\n  mutate(income_num = as.numeric(income_levels))\nstr(gapm2$income_num)\n\n num [1:187] 1 3 3 4 2 4 3 2 4 4 ..."
  },
  {
    "objectID": "slides/10_Cat_covariates.html#if-time",
    "href": "slides/10_Cat_covariates.html#if-time",
    "title": "Lesson 10: Categorical Covariates",
    "section": "If time…",
    "text": "If time…\nLet’s walk through categorical variables that have multiple selections\n\nSo each group is not mutually exclusive\nWe could make an indicator for each category, but individuals could be a part of multiple categories\n\n \n\nAlso, thinking about income levels - can we combine two groups to make one??\n\n\n\nCategorical Covariates"
  },
  {
    "objectID": "slides/10_Cat_covariates.html#poll-everywhere-question-4",
    "href": "slides/10_Cat_covariates.html#poll-everywhere-question-4",
    "title": "Lesson 10: Categorical Covariates",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "slides/11_Interactions.html#lets-map-that-to-our-regression-analysis-process",
    "href": "slides/11_Interactions.html#lets-map-that-to-our-regression-analysis-process",
    "title": "Lesson 11: Interactions",
    "section": "Let’s map that to our regression analysis process",
    "text": "Let’s map that to our regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#multiple-regression-interaction",
    "href": "slides/11_Interactions.html#multiple-regression-interaction",
    "title": "Interactions",
    "section": "Multiple Regression: Interaction",
    "text": "Multiple Regression: Interaction\nConsider a linear regression model with two covariates: \\[\\begin{aligned}\n    Y_i & = & \\beta_0 + \\beta_1X_{i1}+ \\beta_2X_{i2} + \\epsilon_i \\nonumber\n    \\\\\n    E[Y_i|X_{i1},X_{i2}] & = & \\beta_0 + \\beta_1X_{i1}+ \\beta_2X_{i2}\n    \\nonumber\n\\end{aligned}\\]\nRecall: interpretation of coefficients:\n\\(\\beta_0=\\) \\(E[Y]\\), when all covariates equal 0 \\(\\beta_1=\\) mean change in \\(Y\\), per unit increase in \\(X_1\\), with \\(X_2\\) held constant\nNote: to interpret \\(\\beta_1\\), we did not specify any value of \\(X_2\\); only specified that it be held constant\nimplicit assumption: effect of \\(X_1\\) is equal across all values of \\(X_2\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-description",
    "href": "slides/11_Interactions.html#interaction-description",
    "title": "Interactions",
    "section": "Interaction: Description",
    "text": "Interaction: Description\nWe can incorporate interactions into our model through product terms: \\[Y  =  \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} +\n    \\beta_3X_{1}X_{2} + \\epsilon\\]\nTerminology:\n\nmain effect parameters: \\(\\beta_1,\\beta_2\\)\ninteraction parameter: \\(\\beta_3\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#types-of-interactions",
    "href": "slides/11_Interactions.html#types-of-interactions",
    "title": "Interactions",
    "section": "Types of Interactions",
    "text": "Types of Interactions\n\n\n\nCommon types of interactions:\n\nsynergism: \\(X_{2}\\) strengthens the \\(X_{1}\\) effect\nantagonism:\\(X_{2}\\) weakens the \\(X_{1}\\) effect\n\nTo determine synergism vs. antagonism use model with interaction term\n\nlook sign of \\(X_{1}\\times X_{2}\\) parameter\nand test significance of the interaction coefficient\n\nIf not significant: no evidence of effect modification, i.e., the effect of \\(X_{1}\\) does not vary with \\(X_{2}\\)\nThe main effect models estimate the average \\(X_{1}\\) and \\(X_{2}\\) effects"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-models-interpretation",
    "href": "slides/11_Interactions.html#interaction-models-interpretation",
    "title": "Interactions",
    "section": "Interaction Models: Interpretation",
    "text": "Interaction Models: Interpretation\nQ: How to interpret the parameters in the interaction model?\nRecall: main effects model, \\(E[Y_i\\mid X_{i1},X_{i2} ]=\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}\\) \\[\\frac{\\partial E[Y_i\\mid X_{i1},X_{i2} ]}{\\partial X_{i1}} = \\beta_1\\] interpretation of \\(\\beta_1\\) does not involve \\(X_{i2}\\)\nInteraction model: \\(E[Y_i\\mid X_{i1},X_{i2} ]=\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\beta_3X_{i1}X_{i2}\\) \\[\\frac{\\partial E[Y_i\\mid X_{i1},X_{i2} ]}{\\partial X_{i1}} = \\beta_1 +\n\\beta_3X_{i2}\\] effect of \\(X_{i1}\\) depends on value of \\(X_{i2}\\) involves more than one coefficient and other covariates"
  },
  {
    "objectID": "slides/11_Interactions.html#interactions-interpretation",
    "href": "slides/11_Interactions.html#interactions-interpretation",
    "title": "Interactions",
    "section": "Interactions: Interpretation",
    "text": "Interactions: Interpretation\nThe “main effect” in main effects model: \\[\\begin{aligned}\n\\beta_1 & = & \\frac{\\partial E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i1}} \\nonumber\n\\end{aligned}\\]\nThe “main effect” in interaction model: \\[\\begin{aligned}\n\\beta_1 & \\neq & \\frac{\\partial E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i1}} \\nonumber\n\\end{aligned}\\]\nin fact, \\(\\beta_1=\\) change in \\(E[Y]\\) per unit increase in \\(X_{i1}\\), when \\(X_{i2}=0\\)\ndepending on the nature of \\(X_{i2}\\), this may be an undesirable interpretation Thus, for interaction model, we have a clear expression for the effect of \\(X_{i1}\\); but, \\(\\beta_1\\) itself may have an awkward interpretation Be extremely careful when interpreting the coefficients for the “main effects” when there is an interaction in the model"
  },
  {
    "objectID": "slides/11_Interactions.html#interactions-interpretation-continued",
    "href": "slides/11_Interactions.html#interactions-interpretation-continued",
    "title": "Interactions",
    "section": "Interactions: Interpretation (continued)",
    "text": "Interactions: Interpretation (continued)\n\\(E[Y_i\\mid X_{i1},X_{i2} ]=\\beta_0 + \\underbrace{(\\beta_1+\\beta_3X_{i2}) }_\\text{$X_{i1}$'s effect} X_{i1}+ \\underbrace{\\beta_2X_{i2}}_\\text{$X_{i2}$ held constant}\\)\n\\({\\color{white}{E[Y_i\\mid X_{i1},X_{i2} ]}}=\\beta_0 + \\underbrace{(\\beta_2+\\beta_3X_{i1}) }_\\text{$X_{i2}$'s effect}X_{i3} + \\underbrace{\\beta_1X_{i1}}_\\text{$X_{i1}$ held constant}\\)\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in \\(X_{i1}\\)’s effect, per unit increase in \\(X_{i2}\\);\n\n\\(\\beta_3\\) = mean change in \\(X_{i2}\\)’s effect, per unit increase in \\(X_{i1}\\);\nwhere the “\\(X_{i1}\\) effect” equals the change in \\(E[Y]\\) per unit increase in \\(X_{i1}\\) with \\(X_{i2}\\) held constant, i.e. “adjusted \\(X_{i1}\\) effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted \\(X_1\\) (or \\(X_2\\)) effect per unit increase in \\(X_2\\) (or \\(X_1\\))”"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-between-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#interaction-between-categorical-and-continuous-variables",
    "title": "Interactions",
    "section": "Interaction between Categorical and Continuous Variables",
    "text": "Interaction between Categorical and Continuous Variables\nExample: Pediatric Hypertension: A clinician wishes to examine the relationship between systolic blood pressure (SBP) and covariates age and gender\nQuestions of interest: For males and females of the same age, is SBP different, on average? Is the gender effect on SBP independent of age? Does mean SBP change with age? Is the age effect on SBP the same for males and females?"
  },
  {
    "objectID": "slides/11_Interactions.html#example-main-effects-model",
    "href": "slides/11_Interactions.html#example-main-effects-model",
    "title": "Interactions",
    "section": "Example: Main Effects Model",
    "text": "Example: Main Effects Model\nMain effects model: \\(SBP_i=\\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\epsilon_i\\) \\(A_i=\\) age (years) \\(F_i=I(\\)subject \\(i\\) is female) Interpretation of parameters: \\(\\beta_0 =E[SBP|A=0,F=0]\\) \\(\\beta_1 = E[SBP|A=a+1,F=1] -E[SBP|A=a,F=1]\\) \\({\\color{white}{\\beta_1}}= E[SBP|A=a+1,F=0] -E[SBP|A=a,F=0]\\) \\(\\beta_2 = E[SBP|A=a,F=1] - E[SBP|A=a,F=0]\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#example-main-effects-model-1",
    "href": "slides/11_Interactions.html#example-main-effects-model-1",
    "title": "Interactions",
    "section": "Example: Main Effects Model",
    "text": "Example: Main Effects Model\nThe main effects model implies the following relationship: lines have same slope, different intercepts The main effects model assumes no interaction between age and gender w.r.t. SBP"
  },
  {
    "objectID": "slides/11_Interactions.html#example-interaction-model",
    "href": "slides/11_Interactions.html#example-interaction-model",
    "title": "Interactions",
    "section": "Example: Interaction Model",
    "text": "Example: Interaction Model\nInteraction model:\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) Can pose the question of interaction in two ways (symmetric): Is the age-adjusted difference in mean SBP between males and females constant across all ages? \\(SBP_i = \\beta_0 + \\underbrace{\\beta_1 A_i}_\\text{age adjusted} + \\underbrace{(\\beta_2 +\\beta_3 A_i)}_\\text{sex effect} F_i+ \\epsilon_i\\)\nIs the sex-adjusted mean change in SBP per year of age equal for males and females? \\(SBP_i = \\beta_0 + \\underbrace{\\beta_2 F_i}_\\text{sex adjusted} + \\underbrace{(\\beta_1+\\beta_3F_i)}_\\text{age effect} A_i + \\epsilon_i\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#example-interaction-model-1",
    "href": "slides/11_Interactions.html#example-interaction-model-1",
    "title": "Interactions",
    "section": "Example: Interaction Model",
    "text": "Example: Interaction Model\nThe interaction model implies two separate regression lines\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) For male (\\(F=0\\)): \\(SBP_i = \\beta_0 + \\beta_1 A_i + \\epsilon_i\\)\nFor female (\\(F=1\\)):\n\\(SBP_i = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)\\; A_i + \\epsilon_i\\)\nThe two lines have different slope, different intercepts"
  },
  {
    "objectID": "slides/11_Interactions.html#example-interaction-model-2",
    "href": "slides/11_Interactions.html#example-interaction-model-2",
    "title": "Interactions",
    "section": "Example: Interaction Model",
    "text": "Example: Interaction Model\nThe interaction model implies the following relationship: lines have distinct intercepts and distinct slopes"
  },
  {
    "objectID": "slides/11_Interactions.html#example-interaction-model-3",
    "href": "slides/11_Interactions.html#example-interaction-model-3",
    "title": "Interactions",
    "section": "Example: Interaction Model",
    "text": "Example: Interaction Model\nHypothesis testing\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) Question 1: Is the adjusted gender effect on SBP independent of age? Is the adjusted age effect on SBP the same for males and females? How to test for significance of interaction term? \\(H_0:\\) \\(H_1:\\) \\(F=\\)\n\\(T=\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#example-interaction-model-4",
    "href": "slides/11_Interactions.html#example-interaction-model-4",
    "title": "Interactions",
    "section": "Example: Interaction Model",
    "text": "Example: Interaction Model\nHypothesis testing\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) Question 2: Does mean SBP change with age? What about sex? How to test for significance of a covariate, e.g., age? \\(SBP_i = \\beta_0 + (\\beta_1+\\beta_3F_i) A_i + \\beta_2 F_i + \\epsilon_i\\) \\(H_0:\\) \\(H_1:\\) \\(F=\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#example-interaction-model-5",
    "href": "slides/11_Interactions.html#example-interaction-model-5",
    "title": "Interactions",
    "section": "Example: Interaction Model",
    "text": "Example: Interaction Model\nRecall the interaction model implies two separate regression lines\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) For male (\\(F=0\\)): \\(SBP_i = \\beta_0 + \\beta_1 A_i + \\epsilon_i\\)\nFor female (\\(F=1\\)): \\(SBP_i = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)\\; A_i + \\epsilon_i\\)\nHow to calculate 95% CI for age effect among females? Point estimate: \\(\\widehat{\\beta}_1+\\widehat{\\beta}_3\\) Standard error: \\(\\text{SE}(\\widehat{\\beta}_1+\\widehat{\\beta}_3)=\\sqrt{\\widehat{Var}(\\widehat{\\beta}_1+\\widehat{\\beta}_3)}=\\sqrt{\\widehat{Var}(\\widehat{\\beta}_1)+\\widehat{Var}(\\widehat{\\beta}_3)+2\\widehat{Cov}(\\widehat{\\beta}_1,\\widehat{\\beta}_3)}\\) :::"
  },
  {
    "objectID": "slides/11_Interactions.html#example",
    "href": "slides/11_Interactions.html#example",
    "title": "Interactions",
    "section": "Example:",
    "text": "Example:\n\nWe return to the data set containing age (measured in days) and birth weight (BWT in ounces) as predictors of infant systolic blood pressure (SBP, mm Hg).\nRead in the data file (systolic1.csv). (n = 16)\n\n\ndata = read.csv(file=\"systolic1.csv\",header=T)\nattach(data)\nhead(data)\n\n  X idno bwt age sbp\n1 1    1 135   3  89\n2 2    2 120   4  90\n3 3    3 100   3  83\n4 4    4 105   2  77\n5 5    5 130   4  92\n6 6    6 125   5  98"
  },
  {
    "objectID": "slides/11_Interactions.html#fit-sbp-on-age-x-bwt-age-bwt-in-this-order",
    "href": "slides/11_Interactions.html#fit-sbp-on-age-x-bwt-age-bwt-in-this-order",
    "title": "Interactions",
    "section": "Fit SBP on AGE x BWT, AGE, BWT in this order",
    "text": "Fit SBP on AGE x BWT, AGE, BWT in this order\n\nm1 = lm(sbp~I(age*bwt)+I(age)+I(bwt))\nsummary(m1)\n\n\nCall:\nlm(formula = sbp ~ I(age * bwt) + I(age) + I(bwt))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2412 -0.7871 -0.0262  0.8000  3.8433 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   2.55158   20.83776   0.122  0.90457   \nI(age * bwt) -0.12821    0.05159  -2.485  0.02869 * \nI(age)       21.28730    6.22363   3.420  0.00507 **\nI(bwt)        0.55123    0.17373   3.173  0.00803 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.097 on 12 degrees of freedom\nMultiple R-squared:  0.9214,    Adjusted R-squared:  0.9017 \nF-statistic: 46.87 on 3 and 12 DF,  p-value: 6.692e-07"
  },
  {
    "objectID": "slides/11_Interactions.html#compute-the-sequential-and-partial-ss",
    "href": "slides/11_Interactions.html#compute-the-sequential-and-partial-ss",
    "title": "Interactions",
    "section": "Compute the sequential and partial SS",
    "text": "Compute the sequential and partial SS\n\n( SeqSS.m1 = anova(m1) ) #type I\n\nAnalysis of Variance Table\n\nResponse: sbp\n             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nI(age * bwt)  1 563.17  563.17 128.1049 9.252e-08 ***\nI(age)        1  10.75   10.75   2.4459  0.143809    \nI(bwt)        1  44.26   44.26  10.0674  0.008026 ** \nResiduals    12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n( ParSS.m1 = car::Anova(m1,type=\"III\") ) #type III\n\nAnova Table (Type III tests)\n\nResponse: sbp\n             Sum Sq Df F value   Pr(&gt;F)   \n(Intercept)   0.066  1  0.0150 0.904570   \nI(age * bwt) 27.148  1  6.1753 0.028693 * \nI(age)       51.432  1 11.6991 0.005075 **\nI(bwt)       44.258  1 10.0674 0.008026 **\nResiduals    52.754 12                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#you-have-two-f-tests-of-the-interaction.-are-either-of-them-valid-tests",
    "href": "slides/11_Interactions.html#you-have-two-f-tests-of-the-interaction.-are-either-of-them-valid-tests",
    "title": "Interactions",
    "section": "You have two F tests of the interaction. Are either of them valid tests?",
    "text": "You have two F tests of the interaction. Are either of them valid tests?"
  },
  {
    "objectID": "slides/11_Interactions.html#change-ordering-of-covariates-and-recompute-ss",
    "href": "slides/11_Interactions.html#change-ordering-of-covariates-and-recompute-ss",
    "title": "Interactions",
    "section": "Change ordering of covariates and recompute SS",
    "text": "Change ordering of covariates and recompute SS\n\n( anova(lm(sbp~I(age)+I(bwt)+I(age*bwt))) ) #type I\n\nAnalysis of Variance Table\n\nResponse: sbp\n             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nI(age)        1 508.82  508.82 115.7405 1.617e-07 ***\nI(bwt)        1  82.22   82.22  18.7024  0.000988 ***\nI(age * bwt)  1  27.15   27.15   6.1753  0.028693 *  \nResiduals    12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n( car::Anova(m1,type=\"III\") ) #type III\n\nAnova Table (Type III tests)\n\nResponse: sbp\n             Sum Sq Df F value   Pr(&gt;F)   \n(Intercept)   0.066  1  0.0150 0.904570   \nI(age * bwt) 27.148  1  6.1753 0.028693 * \nI(age)       51.432  1 11.6991 0.005075 **\nI(bwt)       44.258  1 10.0674 0.008026 **\nResiduals    52.754 12                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#save-the-fitted-values",
    "href": "slides/11_Interactions.html#save-the-fitted-values",
    "title": "Interactions",
    "section": "Save the fitted values",
    "text": "Save the fitted values\n\ndata = cbind(data,fit.m1 = m1$fitted.values)\nsummary(data)\n\n       X              idno            bwt             age       \n Min.   : 1.00   Min.   : 1.00   Min.   : 90.0   Min.   :2.000  \n 1st Qu.: 4.75   1st Qu.: 4.75   1st Qu.:105.0   1st Qu.:3.000  \n Median : 8.50   Median : 8.50   Median :120.0   Median :3.000  \n Mean   : 8.50   Mean   : 8.50   Mean   :120.3   Mean   :3.312  \n 3rd Qu.:12.25   3rd Qu.:12.25   3rd Qu.:126.2   3rd Qu.:4.000  \n Max.   :16.00   Max.   :16.00   Max.   :160.0   Max.   :5.000  \n      sbp            fit.m1     \n Min.   :77.00   Min.   :76.08  \n 1st Qu.:82.75   1st Qu.:82.87  \n Median :88.50   Median :88.07  \n Mean   :88.06   Mean   :88.06  \n 3rd Qu.:92.75   3rd Qu.:92.79  \n Max.   :98.00   Max.   :98.21"
  },
  {
    "objectID": "slides/11_Interactions.html#hypothesis-test-of-interaction-term",
    "href": "slides/11_Interactions.html#hypothesis-test-of-interaction-term",
    "title": "Interactions",
    "section": "Hypothesis test of interaction term",
    "text": "Hypothesis test of interaction term\n\nsummary(lm(sbp~age*bwt))$coef\n\n              Estimate  Std. Error    t value    Pr(&gt;|t|)\n(Intercept)  2.5515809 20.83776184  0.1224499 0.904569568\nage         21.2872960  6.22362825  3.4203997 0.005074825\nbwt          0.5512330  0.17373097  3.1729116 0.008026120\nage:bwt     -0.1282085  0.05159272 -2.4850114 0.028693323\n\nanova(lm(sbp~age*bwt))\n\nAnalysis of Variance Table\n\nResponse: sbp\n          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nage        1 508.82  508.82 115.7405 1.617e-07 ***\nbwt        1  82.22   82.22  18.7024  0.000988 ***\nage:bwt    1  27.15   27.15   6.1753  0.028693 *  \nResiduals 12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#hypothesis-test-of-age",
    "href": "slides/11_Interactions.html#hypothesis-test-of-age",
    "title": "Interactions",
    "section": "Hypothesis test of age",
    "text": "Hypothesis test of age\n\nnull.model = lm(sbp~bwt)\nalt.model = lm(sbp~age*bwt)\nanova(null.model,alt.model)\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ bwt\nModel 2: sbp ~ age * bwt\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     14 540.40                                  \n2     12  52.75  2    487.65 55.462 8.655e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#compare-to-the-adjusted-age-effect-in-an-additive-model",
    "href": "slides/11_Interactions.html#compare-to-the-adjusted-age-effect-in-an-additive-model",
    "title": "Interactions",
    "section": "Compare to the adjusted age effect in an additive model",
    "text": "Compare to the adjusted age effect in an additive model\n\n# car::Anova(lm(sbp~age),type=\"III\") #crude effect\nanova(lm(sbp~bwt),lm(sbp~age+bwt))  #adjusted effect in additive model\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ bwt\nModel 2: sbp ~ age + bwt\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     14 540.4                                  \n2     13  79.9  1     460.5 74.923 9.342e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm(sbp~age+bwt),type=\"III\") #adjusted effect in additive model\n\nAnova Table (Type III tests)\n\nResponse: sbp\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept) 854.98  1 139.104 2.571e-08 ***\nage         460.50  1  74.923 9.342e-07 ***\nbwt          82.22  1  13.377  0.002896 ** \nResiduals    79.90 13                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#hypothesis-test-of-age-by-hand",
    "href": "slides/11_Interactions.html#hypothesis-test-of-age-by-hand",
    "title": "Interactions",
    "section": "Hypothesis test of age: by hand",
    "text": "Hypothesis test of age: by hand\n\nSeqSS = data.frame(anova(lm(sbp~I(bwt)+I(age)+I(bwt*age))))\ndf1 = SeqSS[\"I(age)\",\"Df\"]+SeqSS[\"I(bwt * age)\",\"Df\"]\nF1 = (SeqSS[\"I(age)\",\"Sum.Sq\"]+SeqSS[\"I(bwt * age)\",\"Sum.Sq\"])/df1\ndf2 = SeqSS[\"Residuals\",\"Df\"]\nF2 = SeqSS[\"Residuals\",\"Mean.Sq\"]\nc(F.val = F1/F2,\n  P.val = 1 - pf(F1/F2,df1=df1,df2=df2))\n\n       F.val        P.val \n5.546241e+01 8.654657e-07 \n\nanova(null.model,alt.model) ##validate\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ bwt\nModel 2: sbp ~ age * bwt\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     14 540.40                                  \n2     12  52.75  2    487.65 55.462 8.655e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#hypothesis-test-of-bwt",
    "href": "slides/11_Interactions.html#hypothesis-test-of-bwt",
    "title": "Interactions",
    "section": "Hypothesis test of bwt",
    "text": "Hypothesis test of bwt\n\nnull.model = lm(sbp~age)\nalt.model = lm(sbp~age*bwt)\nanova(null.model,alt.model)\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age * bwt\n  Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     14 162.121                                \n2     12  52.754  2    109.37 12.439 0.001187 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#center-the-covariates",
    "href": "slides/11_Interactions.html#center-the-covariates",
    "title": "Interactions",
    "section": "Center the covariates",
    "text": "Center the covariates\n\nc(mean(age), sd(age))\n\n[1] 3.3125000 0.9464847\n\nc(mean(bwt), sd(bwt))\n\n[1] 120.3125  18.7500\n\nage.c = age - mean(age)\nbwt.c = bwt - mean(bwt)"
  },
  {
    "objectID": "slides/11_Interactions.html#fit-sbp-on-age-x-bwt-age-bwt-this-time-using-the-centered-versions-of-the-covariates.",
    "href": "slides/11_Interactions.html#fit-sbp-on-age-x-bwt-age-bwt-this-time-using-the-centered-versions-of-the-covariates.",
    "title": "Interactions",
    "section": "Fit SBP on AGE x BWT, AGE, BWT, this time using the centered versions of the covariates.",
    "text": "Fit SBP on AGE x BWT, AGE, BWT, this time using the centered versions of the covariates.\n\nm2 = lm(sbp~I(age.c*bwt.c)+I(age.c)+I(bwt.c))\nsummary(m2)\n\n\nCall:\nlm(formula = sbp ~ I(age.c * bwt.c) + I(age.c) + I(bwt.c))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2412 -0.7871 -0.0262  0.8000  3.8433 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      88.29037    0.53214 165.917  &lt; 2e-16 ***\nI(age.c * bwt.c) -0.12821    0.05159  -2.485 0.028693 *  \nI(age.c)          5.86221    0.57536  10.189 2.93e-07 ***\nI(bwt.c)          0.12654    0.02904   4.357 0.000933 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.097 on 12 degrees of freedom\nMultiple R-squared:  0.9214,    Adjusted R-squared:  0.9017 \nF-statistic: 46.87 on 3 and 12 DF,  p-value: 6.692e-07"
  },
  {
    "objectID": "slides/11_Interactions.html#compute-the-sequential-and-partial-ss-1",
    "href": "slides/11_Interactions.html#compute-the-sequential-and-partial-ss-1",
    "title": "Interactions",
    "section": "Compute the sequential and partial SS",
    "text": "Compute the sequential and partial SS\n\n( SeqSS.m2 = anova(m2) ) #type I\n\nAnalysis of Variance Table\n\nResponse: sbp\n                 Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nI(age.c * bwt.c)  1  29.83   29.83   6.7843 0.0230268 *  \nI(age.c)          1 504.89  504.89 114.8481 1.687e-07 ***\nI(bwt.c)          1  83.46   83.46  18.9858 0.0009328 ***\nResiduals        12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n( ParSS.m2 = car::Anova(m2,type=\"III\") ) #type III\n\nAnova Table (Type III tests)\n\nResponse: sbp\n                 Sum Sq Df    F value    Pr(&gt;F)    \n(Intercept)      121019  1 27528.3083 &lt; 2.2e-16 ***\nI(age.c * bwt.c)     27  1     6.1753 0.0286933 *  \nI(age.c)            456  1   103.8108 2.925e-07 ***\nI(bwt.c)             83  1    18.9858 0.0009328 ***\nResiduals            53 12                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/11_Interactions.html#save-the-fitted-values-1",
    "href": "slides/11_Interactions.html#save-the-fitted-values-1",
    "title": "Interactions",
    "section": "Save the fitted values",
    "text": "Save the fitted values\n\ndata = cbind(data,fit.m2 = m2$fitted.values)\nsummary(data)\n\n       X              idno            bwt             age       \n Min.   : 1.00   Min.   : 1.00   Min.   : 90.0   Min.   :2.000  \n 1st Qu.: 4.75   1st Qu.: 4.75   1st Qu.:105.0   1st Qu.:3.000  \n Median : 8.50   Median : 8.50   Median :120.0   Median :3.000  \n Mean   : 8.50   Mean   : 8.50   Mean   :120.3   Mean   :3.312  \n 3rd Qu.:12.25   3rd Qu.:12.25   3rd Qu.:126.2   3rd Qu.:4.000  \n Max.   :16.00   Max.   :16.00   Max.   :160.0   Max.   :5.000  \n      sbp            fit.m1          fit.m2     \n Min.   :77.00   Min.   :76.08   Min.   :76.08  \n 1st Qu.:82.75   1st Qu.:82.87   1st Qu.:82.87  \n Median :88.50   Median :88.07   Median :88.07  \n Mean   :88.06   Mean   :88.06   Mean   :88.06  \n 3rd Qu.:92.75   3rd Qu.:92.79   3rd Qu.:92.79  \n Max.   :98.00   Max.   :98.21   Max.   :98.21"
  },
  {
    "objectID": "slides/11_Interactions.html#compare-the-output-from-m1-and-m2-what-is-the-same-what-is-different",
    "href": "slides/11_Interactions.html#compare-the-output-from-m1-and-m2-what-is-the-same-what-is-different",
    "title": "Interactions",
    "section": "Compare the output from m1 and m2, what is the same? what is different?",
    "text": "Compare the output from m1 and m2, what is the same? what is different?\n\nsummary(m1)$coef\n\n               Estimate  Std. Error    t value    Pr(&gt;|t|)\n(Intercept)   2.5515809 20.83776184  0.1224499 0.904569568\nI(age * bwt) -0.1282085  0.05159272 -2.4850114 0.028693323\nI(age)       21.2872960  6.22362825  3.4203997 0.005074825\nI(bwt)        0.5512330  0.17373097  3.1729116 0.008026120\n\nsummary(m2)$coef\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      88.2903706 0.53213715 165.916570 1.544073e-21\nI(age.c * bwt.c) -0.1282085 0.05159272  -2.485011 2.869332e-02\nI(age.c)          5.8622095 0.57536064  10.188757 2.925058e-07\nI(bwt.c)          0.1265423 0.02904168   4.357266 9.328388e-04"
  },
  {
    "objectID": "slides/11_Interactions.html#compare-the-output-from-m1-and-m2-what-is-the-same-what-is-different-1",
    "href": "slides/11_Interactions.html#compare-the-output-from-m1-and-m2-what-is-the-same-what-is-different-1",
    "title": "Interactions",
    "section": "Compare the output from m1 and m2, what is the same? what is different?",
    "text": "Compare the output from m1 and m2, what is the same? what is different?\n\nrbind(data.frame(SeqSS.m1),data.frame(SeqSS.m2))\n\n                 Df    Sum.Sq    Mean.Sq    F.value       Pr..F.\nI(age * bwt)      1 563.17274 563.172743 128.104944 9.252455e-08\nI(age)            1  10.75258  10.752575   2.445889 1.438088e-01\nI(bwt)            1  44.25799  44.257989  10.067368 8.026120e-03\nResiduals        12  52.75419   4.396183         NA           NA\nI(age.c * bwt.c)  1  29.82506  29.825065   6.784310 2.302678e-02\nI(age.c)          1 504.89335 504.893347 114.848126 1.686879e-07\nI(bwt.c)          1  83.46490  83.464895  18.985766 9.328388e-04\nResiduals1       12  52.75419   4.396183         NA           NA\n\nrbind(data.frame(ParSS.m1)[-1,],data.frame(ParSS.m2)[-1,])\n\n                    Sum.Sq Df    F.value       Pr..F.\nI(age * bwt)      27.14767  1   6.175282 2.869332e-02\nI(age)            51.43153  1  11.699134 5.074825e-03\nI(bwt)            44.25799  1  10.067368 8.026120e-03\nResiduals         52.75419 12         NA           NA\nI(age.c * bwt.c)  27.14767  1   6.175282 2.869332e-02\nI(age.c)         456.37107  1 103.810761 2.925058e-07\nI(bwt.c)          83.46490  1  18.985766 9.328388e-04\nResiduals1        52.75419 12         NA           NA"
  },
  {
    "objectID": "slides/11_Interactions.html#verify-the-models-are-numerically-equivalent",
    "href": "slides/11_Interactions.html#verify-the-models-are-numerically-equivalent",
    "title": "Interactions",
    "section": "Verify the models are numerically equivalent",
    "text": "Verify the models are numerically equivalent\n\nsummary(data)\n\n       X              idno            bwt             age       \n Min.   : 1.00   Min.   : 1.00   Min.   : 90.0   Min.   :2.000  \n 1st Qu.: 4.75   1st Qu.: 4.75   1st Qu.:105.0   1st Qu.:3.000  \n Median : 8.50   Median : 8.50   Median :120.0   Median :3.000  \n Mean   : 8.50   Mean   : 8.50   Mean   :120.3   Mean   :3.312  \n 3rd Qu.:12.25   3rd Qu.:12.25   3rd Qu.:126.2   3rd Qu.:4.000  \n Max.   :16.00   Max.   :16.00   Max.   :160.0   Max.   :5.000  \n      sbp            fit.m1          fit.m2     \n Min.   :77.00   Min.   :76.08   Min.   :76.08  \n 1st Qu.:82.75   1st Qu.:82.87   1st Qu.:82.87  \n Median :88.50   Median :88.07   Median :88.07  \n Mean   :88.06   Mean   :88.06   Mean   :88.06  \n 3rd Qu.:92.75   3rd Qu.:92.79   3rd Qu.:92.79  \n Max.   :98.00   Max.   :98.21   Max.   :98.21"
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-in-uncentered-and-centered-models.",
    "href": "slides/11_Interactions.html#interpretation-in-uncentered-and-centered-models.",
    "title": "Interactions",
    "section": "Interpretation in uncentered and centered models.",
    "text": "Interpretation in uncentered and centered models.\n\nsummary(lm(sbp~age*bwt))$coef\n\n              Estimate  Std. Error    t value    Pr(&gt;|t|)\n(Intercept)  2.5515809 20.83776184  0.1224499 0.904569568\nage         21.2872960  6.22362825  3.4203997 0.005074825\nbwt          0.5512330  0.17373097  3.1729116 0.008026120\nage:bwt     -0.1282085  0.05159272 -2.4850114 0.028693323\n\nsummary(lm(sbp~age.c*bwt.c))$coef\n\n              Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 88.2903706 0.53213715 165.916570 1.544073e-21\nage.c        5.8622095 0.57536064  10.188757 2.925058e-07\nbwt.c        0.1265423 0.02904168   4.357266 9.328388e-04\nage.c:bwt.c -0.1282085 0.05159272  -2.485011 2.869332e-02\n\n\n\n\nGive interpretations for the main effect coefficient for AGE\n\n\nInterpret the interaction"
  },
  {
    "objectID": "slides/11_Interactions.html#give-interpretations-for-the-main-effect-coefficient-for-age",
    "href": "slides/11_Interactions.html#give-interpretations-for-the-main-effect-coefficient-for-age",
    "title": "Interactions",
    "section": "(1) Give interpretations for the main effect coefficient for AGE",
    "text": "(1) Give interpretations for the main effect coefficient for AGE"
  },
  {
    "objectID": "slides/11_Interactions.html#interpret-the-interaction",
    "href": "slides/11_Interactions.html#interpret-the-interaction",
    "title": "Interactions",
    "section": "(2) Interpret the interaction",
    "text": "(2) Interpret the interaction"
  },
  {
    "objectID": "slides/11_Interactions.html#dichotomize-bwt.-refit-the-interaction-model.",
    "href": "slides/11_Interactions.html#dichotomize-bwt.-refit-the-interaction-model.",
    "title": "Interactions",
    "section": "Dichotomize BWT. Refit the interaction model.",
    "text": "Dichotomize BWT. Refit the interaction model.\n\nhighbwt = 1*(bwt &gt; mean(bwt)) #1*(bwt.c &gt; 0)\nmgrp = lm(sbp~age*highbwt)\nsummary(mgrp)\n\n\nCall:\nlm(formula = sbp ~ age * highbwt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2353 -1.2452 -0.7353  1.8750  4.3235 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.9118     3.2693  19.855 1.52e-10 ***\nage           6.4412     0.9759   6.600 2.54e-05 ***\nhighbwt       8.4882     5.1498   1.648    0.125    \nage:highbwt  -1.2662     1.4872  -0.851    0.411    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.683 on 12 degrees of freedom\nMultiple R-squared:  0.8713,    Adjusted R-squared:  0.8391 \nF-statistic: 27.08 on 3 and 12 DF,  p-value: 1.257e-05"
  },
  {
    "objectID": "slides/11_Interactions.html#scatterplot-with-subjects-stratified-by-age-category",
    "href": "slides/11_Interactions.html#scatterplot-with-subjects-stratified-by-age-category",
    "title": "Interactions",
    "section": "Scatterplot with subjects stratified by age category",
    "text": "Scatterplot with subjects stratified by age category\n\nplot(age[highbwt == 0], sbp[highbwt == 0], pch = 15, col=\"black\",\n     xlab = \"Age (years)\", ylab = \"SBP (mm Hg)\",xlim=range(age),ylim=range(sbp))\npoints(age[highbwt == 1], sbp[highbwt == 1], pch = 16, col=\"red\")\nlines(age[highbwt == 0],mgrp$fitted.values[highbwt == 0],col=\"black\")\nlines(age[highbwt == 1],mgrp$fitted.values[highbwt == 1],col=\"red\")\nlegend(\"bottomright\",bty=\"n\",legend=c(\"higher bwt\",\"lower bwt\"),\n       col=c(\"red\",\"black\"),pch=c(16,15))"
  },
  {
    "objectID": "slides/11_Interactions.html#fit-regression-by-subgroup",
    "href": "slides/11_Interactions.html#fit-regression-by-subgroup",
    "title": "Interactions",
    "section": "Fit regression by subgroup",
    "text": "Fit regression by subgroup\n\nmgrp1 = lm(sbp~age,data=data.frame(sbp,age)[highbwt==1,])\nmgrp0 = lm(sbp~age,data=data.frame(sbp,age)[highbwt==0,])\nsummary(mgrp)$coef\n\n             Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 64.911765  3.2693233 19.8548012 1.518613e-10\nage          6.441176  0.9759295  6.6000429 2.537604e-05\nhighbwt      8.488235  5.1497686  1.6482751 1.252107e-01\nage:highbwt -1.266176  1.4872033 -0.8513809 4.112259e-01\n\nsummary(mgrp1)$coef\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   73.400  3.4989999 20.977423 4.560651e-06\nage            5.175  0.9868511  5.243952 3.342896e-03\n\nsummary(mgrp0)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 64.911765   3.524068 18.419555 3.445266e-07\nage          6.441176   1.051974  6.122945 4.801170e-04"
  },
  {
    "objectID": "slides/11_Interactions.html#fit-regression-by-subgroup-1",
    "href": "slides/11_Interactions.html#fit-regression-by-subgroup-1",
    "title": "Interactions",
    "section": "Fit regression by subgroup",
    "text": "Fit regression by subgroup\n\nrbind( mgrp0=coef(mgrp0), mgrp=coef(mgrp)[c(\"(Intercept)\",\"age\")] )\n\n      (Intercept)      age\nmgrp0    64.91176 6.441176\nmgrp     64.91176 6.441176\n\nrbind( mgrp1=coef(mgrp1), \n   mgrp=c(\n   coef(mgrp)[\"(Intercept)\"]+coef(mgrp)[\"highbwt\"],\n   coef(mgrp)[\"age\"]+coef(mgrp)[\"age:highbwt\"]\n   )\n)\n\n      (Intercept)   age\nmgrp1        73.4 5.175\nmgrp         73.4 5.175"
  },
  {
    "objectID": "slides/11_Interactions.html#get-95-ci-for-age-effect-among-high-bwt-subjects.",
    "href": "slides/11_Interactions.html#get-95-ci-for-age-effect-among-high-bwt-subjects.",
    "title": "Interactions",
    "section": "Get 95% CI for age effect among high BWT subjects.",
    "text": "Get 95% CI for age effect among high BWT subjects.\n\nmgrp = lm(sbp~age*highbwt)\nvcov(mgrp)\n\n            (Intercept)        age    highbwt age:highbwt\n(Intercept)   10.688475 -3.0689681 -10.688475   3.0689681\nage           -3.068968  0.9524384   3.068968  -0.9524384\nhighbwt      -10.688475  3.0689681  26.520117  -7.3866887\nage:highbwt    3.068968 -0.9524384  -7.386689   2.2117735\n\nage.eff = c(  c(0,1,0,1)%*%coef(mgrp)  )\nage.var = c(  c(0,1,0,1)%*%vcov(mgrp)%*%c(0,1,0,1)  )\nt = qt(p=0.975,df=nrow(data)-4) #or qnorm(0.975) if n=nrow(data) is large\n(age.confint = age.eff + c(-1,1)*t*sqrt(age.var))\n\n[1] 2.729934 7.620066\n\nconfint(mgrp1)\n\n                2.5 %    97.5 %\n(Intercept) 64.405535 82.394465\nage          2.638219  7.711781"
  },
  {
    "objectID": "slides/11_Interactions.html#shifting-covariates-in-main-effects-model",
    "href": "slides/11_Interactions.html#shifting-covariates-in-main-effects-model",
    "title": "Interactions",
    "section": "Shifting Covariates in Main Effects Model",
    "text": "Shifting Covariates in Main Effects Model\nMain effects model: \\(E[Y_i]=\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}\\)\nShifting Covariates: set \\(X_{i1}^*=X_{i1}-a\\), \\(X_{i2}^*=X_{i2}-b\\)\nRevised model: $$\n\\[\\begin{aligned}\n        E[Y_i] & = &\\beta_0^* + \\beta_1^*X_{i1}^* + \\beta_2^*X_{i2}^*\n        \\nonumber \\\\ & = & \\beta_0^* + \\beta_1^*(X_{i1}-a) +\n        \\beta_2^*(X_{i2}-b)\n        \\nonumber\n    \n\\end{aligned}\\]\n$$\nExamining the parameters: $$\n\\[\\begin{aligned}\n        \\beta_1^* & = & \\frac{\\partial E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i1}}\n        \\;\\;\\;(=\\beta_1)\\nonumber \\\\\n        \\beta_2^* & = & \\frac{\\partial E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i2}}\n        \\;\\;\\;(=\\beta_2) \\nonumber \\\\\n        \\beta_0^* & = &\n        E[Y_i|X_{i1}=a,X_{i1}=b] \\;(\\neq \\beta_0:  E[Y_i|X_{i1}=0,X_{i1}=0]) \\nonumber\n    \n\\end{aligned}\\]\n$$\nTherefore, in the main effects model shifting covariates impacts the intercept, but not the regression parameters \\(\\{\\beta_j;j&gt;0\\}\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#shifting-covariates-in-interaction-model",
    "href": "slides/11_Interactions.html#shifting-covariates-in-interaction-model",
    "title": "Interactions",
    "section": "Shifting Covariates in Interaction Model",
    "text": "Shifting Covariates in Interaction Model\nInteraction model: \\(E[Y_i]=\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}+ \\beta_3X_{i1}X_{i2}\\)\nShifting Covariates: set \\(X_{i1}^*=X_{i1}-a\\), \\(X_{i2}^*=X_{i2}-b\\)\nRevised model: $$\n\\[\\begin{aligned}\n        E[Y_i] &= & \\beta_0^* + \\beta_1^*X_{i1}^* + \\beta_2^*X_{i2}^* +\n        \\beta_3^*X_{i1}^*X_{i2}^* \\nonumber \\\\\n        & = &\n        \\beta_0^* + \\beta_1^*(X_{i1}-a) + \\beta_2^*(X_{i2}-b) + \\beta_3^*(X_{i1}-a)(X_{i2}-b)\n        \\nonumber\n    \n\\end{aligned}\\]\n$$ Examining the parameters: Clearly, \\(\\beta_0^*\\neq \\beta_0\\) $ = _1^* + _3^*(X_{i2}-b)_1^*$ recall: $_1$, therefore \\(\\beta_1^*\\neq \\beta_1\\) (unless \\(b=0\\)) Similarly, \\(\\beta_2^*\\neq \\beta_2\\) However, \\(\\beta_3^* = \\frac{\\partial^2 E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i1}\\partial X_{i2}} = \\beta_3\\)\nTherefore, in the interaction model shifting covariates impacts everything but the interaction parameters"
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-in-interaction-model",
    "href": "slides/11_Interactions.html#interpretation-in-interaction-model",
    "title": "Interactions",
    "section": "Interpretation in Interaction Model",
    "text": "Interpretation in Interaction Model\nConclusion: interaction coefficient does not depend on shifting/centering\nIn a model with interaction, interpret the main effect with caution: you need to know where the covariates are centered and this centering value should be meaningful to your data\nFor example: \\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\)\n\\(\\beta_1=\\) \\(\\beta_2=\\)\n\n\nInteractions"
  },
  {
    "objectID": "slides/11_Interactions.html#what-is-a-confounder",
    "href": "slides/11_Interactions.html#what-is-a-confounder",
    "title": "Lesson 11: Interactions",
    "section": "What is a confounder?",
    "text": "What is a confounder?\n\nA confounding variable, or confounder, is a factor/variable that wholly or partially accounts for the observed effect of the risk factor on the outcome\nA confounder must be…\n\nRelated to the outcome Y, but not a consequence of Y\nRelated to the explanatory variable X, but not a consequence of X"
  },
  {
    "objectID": "slides/11_Interactions.html#when-variable-is-in-the-model-without-interaction",
    "href": "slides/11_Interactions.html#when-variable-is-in-the-model-without-interaction",
    "title": "Interactions",
    "section": "When variable is in the model without interaction",
    "text": "When variable is in the model without interaction\n\nIn the following model we have two variables, \\(X_1\\) and \\(X_2\\)\n\n\\[Y= \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} + \\epsilon\\]\n\nAnd we assume that every level of the confounder, there is parallel slopes\nNote: to interpret \\(\\beta_1\\), we did not specify any value of \\(X_2\\); only specified that it be held constant\n\nimplicit assumption: effect of \\(X_1\\) is equal across all values of \\(X_2\\)\n\nRecall the food supply plot\nThe above model assumes that \\(X_{1}\\) and \\(X_{2}\\) do not interact (with respect to their effect on \\(Y\\))\n\nepidemiology: no “effect modification”\nmeaning the effect of \\(X_{1}\\) is the same regardless of the values of \\(X_{2}\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#what-is-an-effect-modifier",
    "href": "slides/11_Interactions.html#what-is-an-effect-modifier",
    "title": "Lesson 11: Interactions",
    "section": "What is an effect modifier?",
    "text": "What is an effect modifier?\n\n\n\nAn additional variable in the model\n\nOutside of the main relationship between \\(Y\\) and \\(X_1\\) that we are studying\n\nAn effect modifier will change the effect of \\(X_1\\) on \\(Y\\) depending on its value\n\nAka: as the effect modifier’s values change, so does the association between \\(Y\\) and \\(X_1\\)\nSo the coefficient estimating the relationship between \\(Y\\) and \\(X_1\\) changes with another variable"
  },
  {
    "objectID": "slides/11_Interactions.html#how-do-we-test-if-a-variable-is-an-effect-modifier",
    "href": "slides/11_Interactions.html#how-do-we-test-if-a-variable-is-an-effect-modifier",
    "title": "Interactions",
    "section": "How do we test if a variable is an effect modifier?",
    "text": "How do we test if a variable is an effect modifier?\n\nInteractions!!"
  },
  {
    "objectID": "slides/11_Interactions.html#including-a-confounder-in-the-model",
    "href": "slides/11_Interactions.html#including-a-confounder-in-the-model",
    "title": "Lesson 11: Interactions",
    "section": "Including a confounder in the model",
    "text": "Including a confounder in the model\n\nIn the following model we have two variables, \\(X_1\\) and \\(X_2\\)\n\n\\[Y= \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} + \\epsilon\\]\n\nAnd we assume that every level of the confounder, there is parallel slopes\nNote: to interpret \\(\\beta_1\\), we did not specify any value of \\(X_2\\); only specified that it be held constant\n\nImplicit assumption: effect of \\(X_1\\) is equal across all values of \\(X_2\\)\n\nThe above model assumes that \\(X_{1}\\) and \\(X_{2}\\) do not interact (with respect to their effect on \\(Y\\))\n\nepidemiology: no “effect modification”\nmeaning the effect of \\(X_{1}\\) is the same regardless of the values of \\(X_{2}\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#how-do-we-include-an-effect-modifier-in-the-model",
    "href": "slides/11_Interactions.html#how-do-we-include-an-effect-modifier-in-the-model",
    "title": "Lesson 11: Interactions",
    "section": "How do we include an effect modifier in the model?",
    "text": "How do we include an effect modifier in the model?\n\nInteractions!!\nWe can incorporate interactions into our model through product terms: \\[Y  =  \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} +\n\\beta_3X_{1}X_{2} + \\epsilon\\]\nTerminology:\n\nmain effect parameters: \\(\\beta_1,\\beta_2\\)\n\nThe main effect models estimate the average \\(X_{1}\\) and \\(X_{2}\\) effects\n\ninteraction parameter: \\(\\beta_3\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#types-of-interactions-non-interactions",
    "href": "slides/11_Interactions.html#types-of-interactions-non-interactions",
    "title": "Lesson 11: Interactions",
    "section": "Types of interactions / non-interactions",
    "text": "Types of interactions / non-interactions\n\n\n\nCommon types of interactions:\n\nSynergism: \\(X_{2}\\) strengthens the \\(X_{1}\\) effect\nAntagonism:\\(X_{2}\\) weakens the \\(X_{1}\\) effect\n\n\n \n\nIf the interaction coefficient is not significant\n\nNo evidence of effect modification, i.e., the effect of \\(X_{1}\\) does not vary with \\(X_{2}\\)\n\n\n \n\nIf the main effect of \\(X_2\\) is also not significant\n\nNo evidence that \\(X_2\\) is a confounder"
  },
  {
    "objectID": "slides/11_Interactions.html#how-to-interpret-the-parameters-in-the-interaction-model",
    "href": "slides/11_Interactions.html#how-to-interpret-the-parameters-in-the-interaction-model",
    "title": "Interactions",
    "section": "How to interpret the parameters in the interaction model?",
    "text": "How to interpret the parameters in the interaction model?\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot I(\\text{high income})\\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot I(\\text{high income}) \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, comparing higher income to lower income levels\nwhere the “female literacy rate effect” equals the change in mean life expectancy per percent increase in female literacy with income level held constant, i.e. “adjusted female literacy rate effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect comparing higher income to lower income levels”"
  },
  {
    "objectID": "slides/11_Interactions.html#where-did-we-see-this-before",
    "href": "slides/11_Interactions.html#where-did-we-see-this-before",
    "title": "Interactions",
    "section": "Where did we see this before?",
    "text": "Where did we see this before?\n\n\n\nWe have seen a plot of Life expectancy vs. female literacy rate with different levels of food supply colored\nIf food supply is a confounder in the relationship between life expectancy and female literacy rate, then we only use main effects in the model:\n\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#next-time-well-start-looking-at-interactions-v.-additive-effects",
    "href": "slides/11_Interactions.html#next-time-well-start-looking-at-interactions-v.-additive-effects",
    "title": "Interactions",
    "section": "Next time, we’ll start looking at interactions v. additive effects",
    "text": "Next time, we’ll start looking at interactions v. additive effects"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-between-two-continuous-variables",
    "href": "slides/11_Interactions.html#interaction-between-two-continuous-variables",
    "title": "Interactions",
    "section": "Interaction between two continuous variables",
    "text": "Interaction between two continuous variables\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n32.471\n16.885\n1.923\n0.059\n−1.222\n66.165\n    FemaleLiteracyRate\n0.170\n0.199\n0.854\n0.396\n−0.228\n0.568\n    FoodSupplykcPPD\n0.009\n0.007\n1.325\n0.190\n−0.005\n0.022\n    FemaleLiteracyRate:FoodSupplykcPPD\n0.000\n0.000\n−0.069\n0.945\n0.000\n0.000"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-between-categorical-and-continuous-variables-1",
    "href": "slides/11_Interactions.html#interaction-between-categorical-and-continuous-variables-1",
    "title": "Interactions",
    "section": "Interaction between Categorical and Continuous Variables",
    "text": "Interaction between Categorical and Continuous Variables\nExample: Pediatric Hypertension: A clinician wishes to examine the relationship between systolic blood pressure (SBP) and covariates age and gender\nQuestions of interest: For males and females of the same age, is SBP different, on average? Is the gender effect on SBP independent of age? Does mean SBP change with age? Is the age effect on SBP the same for males and females?"
  },
  {
    "objectID": "slides/11_Interactions.html#how-do-we-decide-if-an-interaction-belongs-in-our-model",
    "href": "slides/11_Interactions.html#how-do-we-decide-if-an-interaction-belongs-in-our-model",
    "title": "Interactions",
    "section": "How do we decide if an interaction belongs in our model?",
    "text": "How do we decide if an interaction belongs in our model?"
  },
  {
    "objectID": "slides/11_Interactions.html#where-have-we-modeled-a-confounder-before",
    "href": "slides/11_Interactions.html#where-have-we-modeled-a-confounder-before",
    "title": "Lesson 11: Interactions",
    "section": "Where have we modeled a confounder before?",
    "text": "Where have we modeled a confounder before?\n\n\n\nWe have seen a plot of Life expectancy vs. female literacy rate with different levels of food supply colored (Lesson 8)\nIn our plot and the model, we treat food supply as a confounder\nIf food supply is a confounder in the relationship between life expectancy and female literacy rate, then we only use main effects in the model:\n\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#interactions-involving-categorical-covariates",
    "href": "slides/11_Interactions.html#interactions-involving-categorical-covariates",
    "title": "Interactions",
    "section": "Interactions?? involving categorical covariates!!",
    "text": "Interactions?? involving categorical covariates!!\n\nMight also show FLR with income"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-between-binary-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#interaction-between-binary-categorical-and-continuous-variables",
    "title": "Interactions",
    "section": "Interaction between binary categorical and continuous variables",
    "text": "Interaction between binary categorical and continuous variables\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\\(LE\\) as life expectancy,\n\\(FLR\\) as female literacy rate\n\\(I(\\text{high income})\\) as the indicator that income level is “high income”\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n46.525\n7.682\n6.056\n0.000\n31.195\n61.854\n    FemaleLiteracyRate\n0.271\n0.082\n3.301\n0.002\n0.107\n0.434\n    income_levels2.L\n−11.772\n10.864\n−1.084\n0.282\n−33.452\n9.907\n    FemaleLiteracyRate:income_levels2.L\n0.161\n0.116\n1.392\n0.168\n−0.070\n0.393"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-between-multi-level-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#interaction-between-multi-level-categorical-and-continuous-variables",
    "title": "Interactions",
    "section": "Interaction between multi-level categorical and continuous variables",
    "text": "Interaction between multi-level categorical and continuous variables\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.053\n4.374\n13.729\n0.000\n51.315\n68.792\n    FemaleLiteracyRate\n−0.052\n0.101\n−0.516\n0.607\n−0.253\n0.149\n    income_levels1Lower middle income\n1.992\n6.597\n0.302\n0.764\n−11.186\n15.171\n    income_levels1Upper middle income\n−25.766\n17.298\n−1.490\n0.141\n−60.323\n8.791\n    income_levels1High income\n17.128\n31.603\n0.542\n0.590\n−46.007\n80.263\n    FemaleLiteracyRate:income_levels1Lower middle income\n0.134\n0.117\n1.144\n0.257\n−0.100\n0.369\n    FemaleLiteracyRate:income_levels1Upper middle income\n0.469\n0.205\n2.283\n0.026\n0.059\n0.879\n    FemaleLiteracyRate:income_levels1High income\n0.051\n0.338\n0.151\n0.880\n−0.623\n0.725"
  },
  {
    "objectID": "slides/11_Interactions.html#centering-continuous-variables-when-we-are-including-interactions",
    "href": "slides/11_Interactions.html#centering-continuous-variables-when-we-are-including-interactions",
    "title": "Lesson 11: Interactions",
    "section": "Centering continuous variables when we are including interactions",
    "text": "Centering continuous variables when we are including interactions\n\n\nFor Europe, the mean life expectancy had a regression line with a large intercept\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\widehat{LE} = & (58.23 + 63.63) + (0.051 - 0.519)FLR \\\\\n\\widehat{LE} = & 121.86 -0.468FLR \\\\\n\\end{aligned}\\]\n\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues\n\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression"
  },
  {
    "objectID": "slides/11_Interactions.html",
    "href": "slides/11_Interactions.html",
    "title": "Interactions",
    "section": "",
    "text": "What is confounding/interactions?\nModeling using interactions\nInterpreting parameters of interaction models\nInteractions involving categorical covariates\n\n\nDetermine if an additional covariate is a not a confounder nor effect modifier; is a confounder but not effect modifier; or is an effect modifier.\nInterpret coefficient parameters for interaction models\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\nA confounding variable, or confounder, is a factor/variable that wholly or partially accounts for the observed effect of the risk factor on the outcome\nA confounder must be…\n\nRelated to the outcome Y, but not a consequence of Y\nRelated to the explanatory variable X, but not a consequence of X\n\n\n\n\n\n\n\n\n\n\n\nIn the following model we have two variables, \\(X_1\\) and \\(X_2\\)\n\n\\[Y= \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} + \\epsilon\\]\n\nAnd we assume that every level of the confounder, there is parallel slopes\nNote: to interpret \\(\\beta_1\\), we did not specify any value of \\(X_2\\); only specified that it be held constant\n\nimplicit assumption: effect of \\(X_1\\) is equal across all values of \\(X_2\\)\n\nRecall the food supply plot\nThe above model assumes that \\(X_{1}\\) and \\(X_{2}\\) do not interact (with respect to their effect on \\(Y\\))\n\nepidemiology: no “effect modification”\nmeaning the effect of \\(X_{1}\\) is the same regardless of the values of \\(X_{2}\\)\n\n\n\n\n\n\n\n\nWe have seen a plot of Life expectancy vs. female literacy rate with different levels of food supply colored\nIn our plot and the model, we treat food supply as a confounder\nIf food supply is a confounder in the relationship between life expectancy and female literacy rate, then we only use main effects in the model:\n\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]\n\n\nmr1 &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + FoodSupplykcPPD, \n          data = gapm_sub)\n(mr1_2d = ggPredict(mr1, interactive = T))\n\n\n\n\n\n\n\n\n\n\n\nAn additional variable in the model\n\nOutside of the main relationship between \\(Y\\) and \\(X_1\\) that we are studying\n\nAn effect modifier will change the effect of \\(X_1\\) on \\(Y\\) depending on its value\n\nAka: as the effect modifier’s values change, so does the association between \\(Y\\) and \\(X_1\\)\nSo the coefficient estimating the relationship between \\(Y\\) and \\(X_1\\) changes with another variable\n\n\n\n\n\n\nInteractions!!\nWe can incorporate interactions into our model through product terms: \\[Y  =  \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} +\n\\beta_3X_{1}X_{2} + \\epsilon\\]\nTerminology:\n\nmain effect parameters: \\(\\beta_1,\\beta_2\\)\ninteraction parameter: \\(\\beta_3\\)\n\n\n\n\n\n\n\n\nCommon types of interactions:\n\nsynergism: \\(X_{2}\\) strengthens the \\(X_{1}\\) effect\nantagonism:\\(X_{2}\\) weakens the \\(X_{1}\\) effect\n\nTo determine synergism vs. antagonism use model with interaction term\n\nlook sign of \\(X_{1}\\times X_{2}\\) parameter\nand test significance of the interaction coefficient\n\nIf the interaction coefficient is not significant\n\nNo evidence of effect modification, i.e., the effect of \\(X_{1}\\) does not vary with \\(X_{2}\\)\n\nThe main effect models estimate the average \\(X_{1}\\) and \\(X_{2}\\) effects\n\n\n\n\n\n\n\n\n\n\n\n\n\nMight also show FLR with income\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\\(E[Y_i\\mid X_{i1},X_{i2} ]=\\beta_0 + \\underbrace{(\\beta_1+\\beta_3X_{i2}) }_\\text{$X_{i1}$'s effect} X_{i1}+ \\underbrace{\\beta_2X_{i2}}_\\text{$X_{i2}$ held constant}\\)\n\\({\\color{white}{E[Y_i\\mid X_{i1},X_{i2} ]}}=\\beta_0 + \\underbrace{(\\beta_2+\\beta_3X_{i1}) }_\\text{$X_{i2}$'s effect}X_{i3} + \\underbrace{\\beta_1X_{i1}}_\\text{$X_{i1}$ held constant}\\)\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in \\(X_{i1}\\)’s effect, per unit increase in \\(X_{i2}\\);\n\n\\(\\beta_3\\) = mean change in \\(X_{i2}\\)’s effect, per unit increase in \\(X_{i1}\\);\nwhere the “\\(X_{i1}\\) effect” equals the change in \\(E[Y]\\) per unit increase in \\(X_{i1}\\) with \\(X_{i2}\\) held constant, i.e. “adjusted \\(X_{i1}\\) effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted \\(X_1\\) (or \\(X_2\\)) effect per unit increase in \\(X_2\\) (or \\(X_1\\))”\n\n\n\n\n\nIdentify outcome (Y) and primary explanatory (X) variables\nDecide which other variables might be important and could be potential confounders. Add these to the model.\n\nThis is often done by indentifying variables that have been identifies in previous research as being important, or researchers beliebe they could be important\nFrom a statistical perspective, we often include variables that are significantly associated with the outcome (in their respective SLR)\n\n(Optional step) Test 3 way interactions\n\nThis makes our model incredibly hard to interpret. Our class will not cover this!!\nWe will skip to testing 2 way interactions\n\nTest 2 way interactions\n\nWhen testing a 2 way interaction, make sure the full and reduced models contain the main effects\nFirst test all the 2 way interactions together using a partial F-test\n\nIf this test not significant, do not test 2-way interactions individually\nIf partial F-test is significant, then test each of the 2-way interactions\n\n\nRemaining main effects - to\n\n\n\n\n\nm_int_inc2 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*income_levels2, data = gapm_sub)\ntidy(m_int_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n46.525\n7.682\n6.056\n0.000\n31.195\n61.854\n    FemaleLiteracyRate\n0.271\n0.082\n3.301\n0.002\n0.107\n0.434\n    income_levels2.L\n−11.772\n10.864\n−1.084\n0.282\n−33.452\n9.907\n    FemaleLiteracyRate:income_levels2.L\n0.161\n0.116\n1.392\n0.168\n−0.070\n0.393\n  \n  \n  \n\n\n\n\n\n\n\n\nm_int_inc1 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*income_levels1, data = gapm_sub)\ntidy(m_int_inc1, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n58.392\n9.025\n6.470\n0.000\n40.362\n76.422\n    FemaleLiteracyRate\n0.112\n0.097\n1.153\n0.253\n−0.082\n0.305\n    income_levels1.L\n5.283\n21.556\n0.245\n0.807\n−37.781\n48.346\n    income_levels1.Q\n20.451\n18.050\n1.133\n0.261\n−15.609\n56.510\n    income_levels1.C\n22.451\n13.673\n1.642\n0.106\n−4.864\n49.766\n    FemaleLiteracyRate:income_levels1.L\n0.109\n0.230\n0.473\n0.638\n−0.351\n0.569\n    FemaleLiteracyRate:income_levels1.Q\n−0.276\n0.193\n−1.428\n0.158\n−0.663\n0.110\n    FemaleLiteracyRate:income_levels1.C\n−0.213\n0.148\n−1.445\n0.153\n−0.508\n0.082\n  \n  \n  \n\n\n\n\n\n\n\n\nm_int_fs = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*FoodSupplykcPPD, data = gapm_sub)\ntidy(m_int_fs, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n32.471\n16.885\n1.923\n0.059\n−1.222\n66.165\n    FemaleLiteracyRate\n0.170\n0.199\n0.854\n0.396\n−0.228\n0.568\n    FoodSupplykcPPD\n0.009\n0.007\n1.325\n0.190\n−0.005\n0.022\n    FemaleLiteracyRate:FoodSupplykcPPD\n0.000\n0.000\n−0.069\n0.945\n0.000\n0.000\n  \n  \n  \n\n\n\n\n\n\n\n\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues\n\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression # Extra\n\n\n\n\n\nExample: Pediatric Hypertension: A clinician wishes to examine the relationship between systolic blood pressure (SBP) and covariates age and gender\nQuestions of interest: For males and females of the same age, is SBP different, on average? Is the gender effect on SBP independent of age? Does mean SBP change with age? Is the age effect on SBP the same for males and females?\n\n\n\nMain effects model: \\(SBP_i=\\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\epsilon_i\\) \\(A_i=\\) age (years) \\(F_i=I(\\)subject \\(i\\) is female) Interpretation of parameters: \\(\\beta_0 =E[SBP|A=0,F=0]\\) \\(\\beta_1 = E[SBP|A=a+1,F=1] -E[SBP|A=a,F=1]\\) \\({\\color{white}{\\beta_1}}= E[SBP|A=a+1,F=0] -E[SBP|A=a,F=0]\\) \\(\\beta_2 = E[SBP|A=a,F=1] - E[SBP|A=a,F=0]\\)\n\n\n\nThe main effects model implies the following relationship: lines have same slope, different intercepts The main effects model assumes no interaction between age and gender w.r.t. SBP\n\n\n\nInteraction model:\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) Can pose the question of interaction in two ways (symmetric): Is the age-adjusted difference in mean SBP between males and females constant across all ages? \\(SBP_i = \\beta_0 + \\underbrace{\\beta_1 A_i}_\\text{age adjusted} + \\underbrace{(\\beta_2 +\\beta_3 A_i)}_\\text{sex effect} F_i+ \\epsilon_i\\)\nIs the sex-adjusted mean change in SBP per year of age equal for males and females? \\(SBP_i = \\beta_0 + \\underbrace{\\beta_2 F_i}_\\text{sex adjusted} + \\underbrace{(\\beta_1+\\beta_3F_i)}_\\text{age effect} A_i + \\epsilon_i\\)\n\n\n\nThe interaction model implies two separate regression lines\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) For male (\\(F=0\\)): \\(SBP_i = \\beta_0 + \\beta_1 A_i + \\epsilon_i\\)\nFor female (\\(F=1\\)):\n\\(SBP_i = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)\\; A_i + \\epsilon_i\\)\nThe two lines have different slope, different intercepts\n\n\n\nThe interaction model implies the following relationship: lines have distinct intercepts and distinct slopes\n\n\n\nHypothesis testing\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) Question 1: Is the adjusted gender effect on SBP independent of age? Is the adjusted age effect on SBP the same for males and females? How to test for significance of interaction term? \\(H_0:\\) \\(H_1:\\) \\(F=\\)\n\\(T=\\)\n\n\n\nHypothesis testing\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) Question 2: Does mean SBP change with age? What about sex? How to test for significance of a covariate, e.g., age? \\(SBP_i = \\beta_0 + (\\beta_1+\\beta_3F_i) A_i + \\beta_2 F_i + \\epsilon_i\\) \\(H_0:\\) \\(H_1:\\) \\(F=\\)\n\n\n\nRecall the interaction model implies two separate regression lines\n\\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\) For male (\\(F=0\\)): \\(SBP_i = \\beta_0 + \\beta_1 A_i + \\epsilon_i\\)\nFor female (\\(F=1\\)): \\(SBP_i = (\\beta_0+\\beta_2) + (\\beta_1+\\beta_3)\\; A_i + \\epsilon_i\\)\nHow to calculate 95% CI for age effect among females? Point estimate: \\(\\widehat{\\beta}_1+\\widehat{\\beta}_3\\) Standard error: \\(\\text{SE}(\\widehat{\\beta}_1+\\widehat{\\beta}_3)=\\sqrt{\\widehat{Var}(\\widehat{\\beta}_1+\\widehat{\\beta}_3)}=\\sqrt{\\widehat{Var}(\\widehat{\\beta}_1)+\\widehat{Var}(\\widehat{\\beta}_3)+2\\widehat{Cov}(\\widehat{\\beta}_1,\\widehat{\\beta}_3)}\\) :::\n\n\n\n\nWe return to the data set containing age (measured in days) and birth weight (BWT in ounces) as predictors of infant systolic blood pressure (SBP, mm Hg).\nRead in the data file (systolic1.csv). (n = 16)\n\n\ndata = read.csv(file=\"systolic1.csv\",header=T)\nattach(data)\nhead(data)\n\n  X idno bwt age sbp\n1 1    1 135   3  89\n2 2    2 120   4  90\n3 3    3 100   3  83\n4 4    4 105   2  77\n5 5    5 130   4  92\n6 6    6 125   5  98\n\n\n\n\n\n\nm1 = lm(sbp~I(age*bwt)+I(age)+I(bwt))\nsummary(m1)\n\n\nCall:\nlm(formula = sbp ~ I(age * bwt) + I(age) + I(bwt))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2412 -0.7871 -0.0262  0.8000  3.8433 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   2.55158   20.83776   0.122  0.90457   \nI(age * bwt) -0.12821    0.05159  -2.485  0.02869 * \nI(age)       21.28730    6.22363   3.420  0.00507 **\nI(bwt)        0.55123    0.17373   3.173  0.00803 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.097 on 12 degrees of freedom\nMultiple R-squared:  0.9214,    Adjusted R-squared:  0.9017 \nF-statistic: 46.87 on 3 and 12 DF,  p-value: 6.692e-07\n\n\n\n\n\n\n( SeqSS.m1 = anova(m1) ) #type I\n\nAnalysis of Variance Table\n\nResponse: sbp\n             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nI(age * bwt)  1 563.17  563.17 128.1049 9.252e-08 ***\nI(age)        1  10.75   10.75   2.4459  0.143809    \nI(bwt)        1  44.26   44.26  10.0674  0.008026 ** \nResiduals    12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n( ParSS.m1 = car::Anova(m1,type=\"III\") ) #type III\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\n\nAnova Table (Type III tests)\n\nResponse: sbp\n             Sum Sq Df F value   Pr(&gt;F)   \n(Intercept)   0.066  1  0.0150 0.904570   \nI(age * bwt) 27.148  1  6.1753 0.028693 * \nI(age)       51.432  1 11.6991 0.005075 **\nI(bwt)       44.258  1 10.0674 0.008026 **\nResiduals    52.754 12                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n( anova(lm(sbp~I(age)+I(bwt)+I(age*bwt))) ) #type I\n\nAnalysis of Variance Table\n\nResponse: sbp\n             Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nI(age)        1 508.82  508.82 115.7405 1.617e-07 ***\nI(bwt)        1  82.22   82.22  18.7024  0.000988 ***\nI(age * bwt)  1  27.15   27.15   6.1753  0.028693 *  \nResiduals    12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n( car::Anova(m1,type=\"III\") ) #type III\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\n\nAnova Table (Type III tests)\n\nResponse: sbp\n             Sum Sq Df F value   Pr(&gt;F)   \n(Intercept)   0.066  1  0.0150 0.904570   \nI(age * bwt) 27.148  1  6.1753 0.028693 * \nI(age)       51.432  1 11.6991 0.005075 **\nI(bwt)       44.258  1 10.0674 0.008026 **\nResiduals    52.754 12                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\ndata = cbind(data,fit.m1 = m1$fitted.values)\nsummary(data)\n\n       X              idno            bwt             age       \n Min.   : 1.00   Min.   : 1.00   Min.   : 90.0   Min.   :2.000  \n 1st Qu.: 4.75   1st Qu.: 4.75   1st Qu.:105.0   1st Qu.:3.000  \n Median : 8.50   Median : 8.50   Median :120.0   Median :3.000  \n Mean   : 8.50   Mean   : 8.50   Mean   :120.3   Mean   :3.312  \n 3rd Qu.:12.25   3rd Qu.:12.25   3rd Qu.:126.2   3rd Qu.:4.000  \n Max.   :16.00   Max.   :16.00   Max.   :160.0   Max.   :5.000  \n      sbp            fit.m1     \n Min.   :77.00   Min.   :76.08  \n 1st Qu.:82.75   1st Qu.:82.87  \n Median :88.50   Median :88.07  \n Mean   :88.06   Mean   :88.06  \n 3rd Qu.:92.75   3rd Qu.:92.79  \n Max.   :98.00   Max.   :98.21  \n\n\n\n\n\n\nsummary(lm(sbp~age*bwt))$coef\n\n              Estimate  Std. Error    t value    Pr(&gt;|t|)\n(Intercept)  2.5515809 20.83776184  0.1224499 0.904569568\nage         21.2872960  6.22362825  3.4203997 0.005074825\nbwt          0.5512330  0.17373097  3.1729116 0.008026120\nage:bwt     -0.1282085  0.05159272 -2.4850114 0.028693323\n\nanova(lm(sbp~age*bwt))\n\nAnalysis of Variance Table\n\nResponse: sbp\n          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nage        1 508.82  508.82 115.7405 1.617e-07 ***\nbwt        1  82.22   82.22  18.7024  0.000988 ***\nage:bwt    1  27.15   27.15   6.1753  0.028693 *  \nResiduals 12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nnull.model = lm(sbp~bwt)\nalt.model = lm(sbp~age*bwt)\nanova(null.model,alt.model)\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ bwt\nModel 2: sbp ~ age * bwt\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     14 540.40                                  \n2     12  52.75  2    487.65 55.462 8.655e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# car::Anova(lm(sbp~age),type=\"III\") #crude effect\nanova(lm(sbp~bwt),lm(sbp~age+bwt))  #adjusted effect in additive model\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ bwt\nModel 2: sbp ~ age + bwt\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     14 540.4                                  \n2     13  79.9  1     460.5 74.923 9.342e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(lm(sbp~age+bwt),type=\"III\") #adjusted effect in additive model\n\nAnova Table (Type III tests)\n\nResponse: sbp\n            Sum Sq Df F value    Pr(&gt;F)    \n(Intercept) 854.98  1 139.104 2.571e-08 ***\nage         460.50  1  74.923 9.342e-07 ***\nbwt          82.22  1  13.377  0.002896 ** \nResiduals    79.90 13                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nSeqSS = data.frame(anova(lm(sbp~I(bwt)+I(age)+I(bwt*age))))\ndf1 = SeqSS[\"I(age)\",\"Df\"]+SeqSS[\"I(bwt * age)\",\"Df\"]\nF1 = (SeqSS[\"I(age)\",\"Sum.Sq\"]+SeqSS[\"I(bwt * age)\",\"Sum.Sq\"])/df1\ndf2 = SeqSS[\"Residuals\",\"Df\"]\nF2 = SeqSS[\"Residuals\",\"Mean.Sq\"]\nc(F.val = F1/F2,\n  P.val = 1 - pf(F1/F2,df1=df1,df2=df2))\n\n       F.val        P.val \n5.546241e+01 8.654657e-07 \n\nanova(null.model,alt.model) ##validate\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ bwt\nModel 2: sbp ~ age * bwt\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     14 540.40                                  \n2     12  52.75  2    487.65 55.462 8.655e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nnull.model = lm(sbp~age)\nalt.model = lm(sbp~age*bwt)\nanova(null.model,alt.model)\n\nAnalysis of Variance Table\n\nModel 1: sbp ~ age\nModel 2: sbp ~ age * bwt\n  Res.Df     RSS Df Sum of Sq      F   Pr(&gt;F)   \n1     14 162.121                                \n2     12  52.754  2    109.37 12.439 0.001187 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nc(mean(age), sd(age))\n\n[1] 3.3125000 0.9464847\n\nc(mean(bwt), sd(bwt))\n\n[1] 120.3125  18.7500\n\nage.c = age - mean(age)\nbwt.c = bwt - mean(bwt)\n\n\n\n\n\nm2 = lm(sbp~I(age.c*bwt.c)+I(age.c)+I(bwt.c))\nsummary(m2)\n\n\nCall:\nlm(formula = sbp ~ I(age.c * bwt.c) + I(age.c) + I(bwt.c))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2412 -0.7871 -0.0262  0.8000  3.8433 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      88.29037    0.53214 165.917  &lt; 2e-16 ***\nI(age.c * bwt.c) -0.12821    0.05159  -2.485 0.028693 *  \nI(age.c)          5.86221    0.57536  10.189 2.93e-07 ***\nI(bwt.c)          0.12654    0.02904   4.357 0.000933 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.097 on 12 degrees of freedom\nMultiple R-squared:  0.9214,    Adjusted R-squared:  0.9017 \nF-statistic: 46.87 on 3 and 12 DF,  p-value: 6.692e-07\n\n\n\n\n\n\n( SeqSS.m2 = anova(m2) ) #type I\n\nAnalysis of Variance Table\n\nResponse: sbp\n                 Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \nI(age.c * bwt.c)  1  29.83   29.83   6.7843 0.0230268 *  \nI(age.c)          1 504.89  504.89 114.8481 1.687e-07 ***\nI(bwt.c)          1  83.46   83.46  18.9858 0.0009328 ***\nResiduals        12  52.75    4.40                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n( ParSS.m2 = car::Anova(m2,type=\"III\") ) #type III\n\nWarning in printHypothesis(L, rhs, names(b)): one or more coefficients in the hypothesis include\n     arithmetic operators in their names;\n  the printed representation of the hypothesis will be omitted\n\n\nAnova Table (Type III tests)\n\nResponse: sbp\n                 Sum Sq Df    F value    Pr(&gt;F)    \n(Intercept)      121019  1 27528.3083 &lt; 2.2e-16 ***\nI(age.c * bwt.c)     27  1     6.1753 0.0286933 *  \nI(age.c)            456  1   103.8108 2.925e-07 ***\nI(bwt.c)             83  1    18.9858 0.0009328 ***\nResiduals            53 12                         \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\ndata = cbind(data,fit.m2 = m2$fitted.values)\nsummary(data)\n\n       X              idno            bwt             age       \n Min.   : 1.00   Min.   : 1.00   Min.   : 90.0   Min.   :2.000  \n 1st Qu.: 4.75   1st Qu.: 4.75   1st Qu.:105.0   1st Qu.:3.000  \n Median : 8.50   Median : 8.50   Median :120.0   Median :3.000  \n Mean   : 8.50   Mean   : 8.50   Mean   :120.3   Mean   :3.312  \n 3rd Qu.:12.25   3rd Qu.:12.25   3rd Qu.:126.2   3rd Qu.:4.000  \n Max.   :16.00   Max.   :16.00   Max.   :160.0   Max.   :5.000  \n      sbp            fit.m1          fit.m2     \n Min.   :77.00   Min.   :76.08   Min.   :76.08  \n 1st Qu.:82.75   1st Qu.:82.87   1st Qu.:82.87  \n Median :88.50   Median :88.07   Median :88.07  \n Mean   :88.06   Mean   :88.06   Mean   :88.06  \n 3rd Qu.:92.75   3rd Qu.:92.79   3rd Qu.:92.79  \n Max.   :98.00   Max.   :98.21   Max.   :98.21  \n\n\n\n\n\n\nsummary(m1)$coef\n\n               Estimate  Std. Error    t value    Pr(&gt;|t|)\n(Intercept)   2.5515809 20.83776184  0.1224499 0.904569568\nI(age * bwt) -0.1282085  0.05159272 -2.4850114 0.028693323\nI(age)       21.2872960  6.22362825  3.4203997 0.005074825\nI(bwt)        0.5512330  0.17373097  3.1729116 0.008026120\n\nsummary(m2)$coef\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      88.2903706 0.53213715 165.916570 1.544073e-21\nI(age.c * bwt.c) -0.1282085 0.05159272  -2.485011 2.869332e-02\nI(age.c)          5.8622095 0.57536064  10.188757 2.925058e-07\nI(bwt.c)          0.1265423 0.02904168   4.357266 9.328388e-04\n\n\n\n\n\n\nrbind(data.frame(SeqSS.m1),data.frame(SeqSS.m2))\n\n                 Df    Sum.Sq    Mean.Sq    F.value       Pr..F.\nI(age * bwt)      1 563.17274 563.172743 128.104944 9.252455e-08\nI(age)            1  10.75258  10.752575   2.445889 1.438088e-01\nI(bwt)            1  44.25799  44.257989  10.067368 8.026120e-03\nResiduals        12  52.75419   4.396183         NA           NA\nI(age.c * bwt.c)  1  29.82506  29.825065   6.784310 2.302678e-02\nI(age.c)          1 504.89335 504.893347 114.848126 1.686879e-07\nI(bwt.c)          1  83.46490  83.464895  18.985766 9.328388e-04\nResiduals1       12  52.75419   4.396183         NA           NA\n\nrbind(data.frame(ParSS.m1)[-1,],data.frame(ParSS.m2)[-1,])\n\n                    Sum.Sq Df    F.value       Pr..F.\nI(age * bwt)      27.14767  1   6.175282 2.869332e-02\nI(age)            51.43153  1  11.699134 5.074825e-03\nI(bwt)            44.25799  1  10.067368 8.026120e-03\nResiduals         52.75419 12         NA           NA\nI(age.c * bwt.c)  27.14767  1   6.175282 2.869332e-02\nI(age.c)         456.37107  1 103.810761 2.925058e-07\nI(bwt.c)          83.46490  1  18.985766 9.328388e-04\nResiduals1        52.75419 12         NA           NA\n\n\n\n\n\n\nsummary(data)\n\n       X              idno            bwt             age       \n Min.   : 1.00   Min.   : 1.00   Min.   : 90.0   Min.   :2.000  \n 1st Qu.: 4.75   1st Qu.: 4.75   1st Qu.:105.0   1st Qu.:3.000  \n Median : 8.50   Median : 8.50   Median :120.0   Median :3.000  \n Mean   : 8.50   Mean   : 8.50   Mean   :120.3   Mean   :3.312  \n 3rd Qu.:12.25   3rd Qu.:12.25   3rd Qu.:126.2   3rd Qu.:4.000  \n Max.   :16.00   Max.   :16.00   Max.   :160.0   Max.   :5.000  \n      sbp            fit.m1          fit.m2     \n Min.   :77.00   Min.   :76.08   Min.   :76.08  \n 1st Qu.:82.75   1st Qu.:82.87   1st Qu.:82.87  \n Median :88.50   Median :88.07   Median :88.07  \n Mean   :88.06   Mean   :88.06   Mean   :88.06  \n 3rd Qu.:92.75   3rd Qu.:92.79   3rd Qu.:92.79  \n Max.   :98.00   Max.   :98.21   Max.   :98.21  \n\n\n\n\n\n\nsummary(lm(sbp~age*bwt))$coef\n\n              Estimate  Std. Error    t value    Pr(&gt;|t|)\n(Intercept)  2.5515809 20.83776184  0.1224499 0.904569568\nage         21.2872960  6.22362825  3.4203997 0.005074825\nbwt          0.5512330  0.17373097  3.1729116 0.008026120\nage:bwt     -0.1282085  0.05159272 -2.4850114 0.028693323\n\nsummary(lm(sbp~age.c*bwt.c))$coef\n\n              Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 88.2903706 0.53213715 165.916570 1.544073e-21\nage.c        5.8622095 0.57536064  10.188757 2.925058e-07\nbwt.c        0.1265423 0.02904168   4.357266 9.328388e-04\nage.c:bwt.c -0.1282085 0.05159272  -2.485011 2.869332e-02\n\n\n\n\nGive interpretations for the main effect coefficient for AGE\n\n\nInterpret the interaction\n\n\n\n\n\n\n\n\n\n\n\n\nhighbwt = 1*(bwt &gt; mean(bwt)) #1*(bwt.c &gt; 0)\nmgrp = lm(sbp~age*highbwt)\nsummary(mgrp)\n\n\nCall:\nlm(formula = sbp ~ age * highbwt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2353 -1.2452 -0.7353  1.8750  4.3235 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  64.9118     3.2693  19.855 1.52e-10 ***\nage           6.4412     0.9759   6.600 2.54e-05 ***\nhighbwt       8.4882     5.1498   1.648    0.125    \nage:highbwt  -1.2662     1.4872  -0.851    0.411    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.683 on 12 degrees of freedom\nMultiple R-squared:  0.8713,    Adjusted R-squared:  0.8391 \nF-statistic: 27.08 on 3 and 12 DF,  p-value: 1.257e-05\n\n\n\n\n\n\nplot(age[highbwt == 0], sbp[highbwt == 0], pch = 15, col=\"black\",\n     xlab = \"Age (years)\", ylab = \"SBP (mm Hg)\",xlim=range(age),ylim=range(sbp))\npoints(age[highbwt == 1], sbp[highbwt == 1], pch = 16, col=\"red\")\nlines(age[highbwt == 0],mgrp$fitted.values[highbwt == 0],col=\"black\")\nlines(age[highbwt == 1],mgrp$fitted.values[highbwt == 1],col=\"red\")\nlegend(\"bottomright\",bty=\"n\",legend=c(\"higher bwt\",\"lower bwt\"),\n       col=c(\"red\",\"black\"),pch=c(16,15))\n\n\n\n\n\n\n\n\nmgrp1 = lm(sbp~age,data=data.frame(sbp,age)[highbwt==1,])\nmgrp0 = lm(sbp~age,data=data.frame(sbp,age)[highbwt==0,])\nsummary(mgrp)$coef\n\n             Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 64.911765  3.2693233 19.8548012 1.518613e-10\nage          6.441176  0.9759295  6.6000429 2.537604e-05\nhighbwt      8.488235  5.1497686  1.6482751 1.252107e-01\nage:highbwt -1.266176  1.4872033 -0.8513809 4.112259e-01\n\nsummary(mgrp1)$coef\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   73.400  3.4989999 20.977423 4.560651e-06\nage            5.175  0.9868511  5.243952 3.342896e-03\n\nsummary(mgrp0)$coef\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 64.911765   3.524068 18.419555 3.445266e-07\nage          6.441176   1.051974  6.122945 4.801170e-04\n\n\n\n\n\n\nrbind( mgrp0=coef(mgrp0), mgrp=coef(mgrp)[c(\"(Intercept)\",\"age\")] )\n\n      (Intercept)      age\nmgrp0    64.91176 6.441176\nmgrp     64.91176 6.441176\n\nrbind( mgrp1=coef(mgrp1), \n   mgrp=c(\n   coef(mgrp)[\"(Intercept)\"]+coef(mgrp)[\"highbwt\"],\n   coef(mgrp)[\"age\"]+coef(mgrp)[\"age:highbwt\"]\n   )\n)\n\n      (Intercept)   age\nmgrp1        73.4 5.175\nmgrp         73.4 5.175\n\n\n\n\n\n\nmgrp = lm(sbp~age*highbwt)\nvcov(mgrp)\n\n            (Intercept)        age    highbwt age:highbwt\n(Intercept)   10.688475 -3.0689681 -10.688475   3.0689681\nage           -3.068968  0.9524384   3.068968  -0.9524384\nhighbwt      -10.688475  3.0689681  26.520117  -7.3866887\nage:highbwt    3.068968 -0.9524384  -7.386689   2.2117735\n\nage.eff = c(  c(0,1,0,1)%*%coef(mgrp)  )\nage.var = c(  c(0,1,0,1)%*%vcov(mgrp)%*%c(0,1,0,1)  )\nt = qt(p=0.975,df=nrow(data)-4) #or qnorm(0.975) if n=nrow(data) is large\n(age.confint = age.eff + c(-1,1)*t*sqrt(age.var))\n\n[1] 2.729934 7.620066\n\nconfint(mgrp1)\n\n                2.5 %    97.5 %\n(Intercept) 64.405535 82.394465\nage          2.638219  7.711781\n\n\n\n\n\nMain effects model: \\(E[Y_i]=\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}\\)\nShifting Covariates: set \\(X_{i1}^*=X_{i1}-a\\), \\(X_{i2}^*=X_{i2}-b\\)\nRevised model: $$\n\\[\\begin{aligned}\n        E[Y_i] & = &\\beta_0^* + \\beta_1^*X_{i1}^* + \\beta_2^*X_{i2}^*\n        \\nonumber \\\\ & = & \\beta_0^* + \\beta_1^*(X_{i1}-a) +\n        \\beta_2^*(X_{i2}-b)\n        \\nonumber\n    \n\\end{aligned}\\]\n$$\nExamining the parameters: $$\n\\[\\begin{aligned}\n        \\beta_1^* & = & \\frac{\\partial E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i1}}\n        \\;\\;\\;(=\\beta_1)\\nonumber \\\\\n        \\beta_2^* & = & \\frac{\\partial E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i2}}\n        \\;\\;\\;(=\\beta_2) \\nonumber \\\\\n        \\beta_0^* & = &\n        E[Y_i|X_{i1}=a,X_{i1}=b] \\;(\\neq \\beta_0:  E[Y_i|X_{i1}=0,X_{i1}=0]) \\nonumber\n    \n\\end{aligned}\\]\n$$\nTherefore, in the main effects model shifting covariates impacts the intercept, but not the regression parameters \\(\\{\\beta_j;j&gt;0\\}\\)\n\n\n\nInteraction model: \\(E[Y_i]=\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2}+ \\beta_3X_{i1}X_{i2}\\)\nShifting Covariates: set \\(X_{i1}^*=X_{i1}-a\\), \\(X_{i2}^*=X_{i2}-b\\)\nRevised model: $$\n\\[\\begin{aligned}\n        E[Y_i] &= & \\beta_0^* + \\beta_1^*X_{i1}^* + \\beta_2^*X_{i2}^* +\n        \\beta_3^*X_{i1}^*X_{i2}^* \\nonumber \\\\\n        & = &\n        \\beta_0^* + \\beta_1^*(X_{i1}-a) + \\beta_2^*(X_{i2}-b) + \\beta_3^*(X_{i1}-a)(X_{i2}-b)\n        \\nonumber\n    \n\\end{aligned}\\]\n$$ Examining the parameters: Clearly, \\(\\beta_0^*\\neq \\beta_0\\) $ = _1^* + _3^*(X_{i2}-b)_1^*$ recall: $_1$, therefore \\(\\beta_1^*\\neq \\beta_1\\) (unless \\(b=0\\)) Similarly, \\(\\beta_2^*\\neq \\beta_2\\) However, \\(\\beta_3^* = \\frac{\\partial^2 E[Y_i\\mid X_{i1},X_{i2}]}{\\partial X_{i1}\\partial X_{i2}} = \\beta_3\\)\nTherefore, in the interaction model shifting covariates impacts everything but the interaction parameters\n\n\n\nConclusion: interaction coefficient does not depend on shifting/centering\nIn a model with interaction, interpret the main effect with caution: you need to know where the covariates are centered and this centering value should be meaningful to your data\nFor example: \\(SBP_i = \\beta_0 + \\beta_1 A_i + \\beta_2 F_i + \\beta_3 A_iF_i + \\epsilon_i\\)\n\\(\\beta_1=\\) \\(\\beta_2=\\)"
  },
  {
    "objectID": "slides/11_Interactions.html#general-steps-how-do-we-decide-if-an-interaction-belongs-in-our-model",
    "href": "slides/11_Interactions.html#general-steps-how-do-we-decide-if-an-interaction-belongs-in-our-model",
    "title": "Interactions",
    "section": "General steps: How do we decide if an interaction belongs in our model?",
    "text": "General steps: How do we decide if an interaction belongs in our model?\n\nIdentify outcome (Y) and primary explanatory (X) variables\nDecide which other variables might be important and could be potential confounders. Add these to the model.\n\nThis is often done by indentifying variables that previous research deemed important, or researchers believe could be important\nFrom a statistical perspective, we often include variables that are significantly associated with the outcome (in their respective SLR)\n\n(Optional step) Test 3 way interactions\n\nThis makes our model incredibly hard to interpret. Our class will not cover this!!\nWe will skip to testing 2 way interactions\n\nTest 2 way interactions\n\nWhen testing a 2 way interaction, make sure the full and reduced models contain the main effects\nFirst test all the 2 way interactions together using a partial F-test (with \\(alpha = 0.10\\))\n\nIf this test not significant, do not test 2-way interactions individually\nIf partial F-test is significant, then test each of the 2-way interactions\n\n\nRemaining main effects - to include of not to include?\n\nFor variables that are included in any interactions, they will be automatically included as main effects and thus not checked for confounding\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable(s) changes any of the coefficient of the primary explanatory variable (including interactions) X by more than 10%\n\nIf any of X’s coefficients change when removing the potential confounder, then keep it in the model"
  },
  {
    "objectID": "slides/11_Interactions.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "href": "slides/11_Interactions.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "title": "Interactions",
    "section": "A glimpse at how interactions might be incorporated into model selection",
    "text": "A glimpse at how interactions might be incorporated into model selection\n\nIdentify outcome (Y) and primary explanatory (X) variables\nDecide which other variables might be important and could be potential confounders. Add these to the model.\n\nThis is often done by indentifying variables that previous research deemed important, or researchers believe could be important\nFrom a statistical perspective, we often include variables that are significantly associated with the outcome (in their respective SLR)\n\n(Optional step) Test 3 way interactions\n\nThis makes our model incredibly hard to interpret. Our class will not cover this!!\nWe will skip to testing 2 way interactions\n\nTest 2 way interactions\n\nWhen testing a 2 way interaction, make sure the full and reduced models contain the main effects\nFirst test all the 2 way interactions together using a partial F-test (with \\(alpha = 0.10\\))\n\nIf this test not significant, do not test 2-way interactions individually\nIf partial F-test is significant, then test each of the 2-way interactions\n\n\nRemaining main effects - to include of not to include?\n\nFor variables that are included in any interactions, they will be automatically included as main effects and thus not checked for confounding\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable(s) changes any of the coefficient of the primary explanatory variable (including interactions) X by more than 10%\n\nIf any of X’s coefficients change when removing the potential confounder, then keep it in the model\n\n\n\n\n\n\nInteractions"
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Interpretation for interaction between binary categorical and continuous variables",
    "text": "Interpretation for interaction between binary categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot I(\\text{high income})\\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot I(\\text{high income}) \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, comparing higher income to lower income levels\nwhere the “female literacy rate effect” equals the change in mean life expectancy per percent increase in female literacy with income level held constant, i.e. “adjusted female literacy rate effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect comparing higher income to lower income levels”\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Interpretation for interaction between multi-level categorical and continuous variables",
    "text": "Interpretation for interaction between multi-level categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe})\\bigg] + \\\\ &\\underbrace{\\bigg[\\widehat\\beta_1 +  \\widehat\\beta_5 \\cdot I(\\text{Americas}) + \\widehat\\beta_6 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot I(\\text{Europe}) \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_5\\) = mean change in female literacy rate’s effect, comparing countries in the Americas to countries in Africa\n\\(\\beta_6\\) = mean change in female literacy rate’s effect, comparing countries in Asia to countries in Africa\n\\(\\beta_7\\) = mean change in female literacy rate’s effect, comparing countries in Europe to countries in Africa\n\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "slides/11_Interactions.html#interaction-between-two-categorical-variables",
    "href": "slides/11_Interactions.html#interaction-between-two-categorical-variables",
    "title": "Interactions",
    "section": "Interaction between two categorical variables",
    "text": "Interaction between two categorical variables"
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-for-interaction-between-two-categorical-variables",
    "href": "slides/11_Interactions.html#interpretation-for-interaction-between-two-categorical-variables",
    "title": "Lesson 11: Interactions",
    "section": "Interpretation for interaction between two categorical variables",
    "text": "Interpretation for interaction between two categorical variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income})\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot I(\\text{high income})\\bigg]  + \\bigg[\\widehat\\beta_2 + \\widehat\\beta_5 \\cdot I(\\text{high income})\\bigg] I(\\text{Americas}) + \\\\ & \\bigg[\\widehat\\beta_3 + \\widehat\\beta_6 \\cdot I(\\text{high income})\\bigg]  I(\\text{Asia}) +  \\bigg[\\widehat\\beta_4 + \\widehat\\beta_7 \\cdot I(\\text{high income})\\bigg]  I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_1\\) = mean change in the Africa’s life expectancy, comparing high income to low income countries\n\\(\\beta_5\\) = mean change in the Americas’ effect, comparing high income to low income countries\n\\(\\beta_6\\) = mean change in Asia’s effect, comparing high income to low income countries\n\\(\\beta_7\\) = mean change in Europe’s effect, comparing high income to low income countries"
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-for-interaction-between-two-continuous-variables",
    "href": "slides/11_Interactions.html#interpretation-for-interaction-between-two-continuous-variables",
    "title": "Interactions",
    "section": "Interpretation for interaction between two continuous variables",
    "text": "Interpretation for interaction between two continuous variables"
  },
  {
    "objectID": "slides/11_Interactions.html#now-centered-interpretation-for-interaction-between-two-continuous-variables",
    "href": "slides/11_Interactions.html#now-centered-interpretation-for-interaction-between-two-continuous-variables",
    "title": "Interactions",
    "section": "Now centered: interpretation for interaction between two continuous variables",
    "text": "Now centered: interpretation for interaction between two continuous variables"
  },
  {
    "objectID": "slides/11_Interactions.html#deciding-between-confounder-and-effect-modifier",
    "href": "slides/11_Interactions.html#deciding-between-confounder-and-effect-modifier",
    "title": "Interactions",
    "section": "Deciding between confounder and effect modifier",
    "text": "Deciding between confounder and effect modifier\n\nThis is more of a model selection question (in coming lectures)\nBut if we had a model with only TWO covariates, we could step through the following process:\n\nTest the interaction (of potential effect modifier): use a partial F-test to test if interaction term(s) explain enough variation compared to model without interaction\n\nRecall that for two continuous covariates, we will test a single coefficient\nFor a binary and continuous covariate, we will test a single coefficient\nFor two binary categorical covariates, we will test a single coefficient\nFor a multi-level categorical covariate (with any other type of covariate), we must test a group of coefficients!!\n\nThen look at the main effect (or potential confounder)\n\nIf interaction already included, then automatically included as main effect (and thus not checked for confounding)\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%"
  },
  {
    "objectID": "slides/11_Interactions.html#poll-everywhere-question-1",
    "href": "slides/11_Interactions.html#poll-everywhere-question-1",
    "title": "Lesson 11: Interactions",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "slides/11_Interactions.html#looks-like-income-level-might-be-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions.html#looks-like-income-level-might-be-an-effect-modifier-for-female-literacy-rate",
    "title": "Interactions",
    "section": "Looks like income level might be an effect modifier for female literacy rate…",
    "text": "Looks like income level might be an effect modifier for female literacy rate…"
  },
  {
    "objectID": "slides/11_Interactions.html#do-we-think-income-level-can-be-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions.html#do-we-think-income-level-can-be-an-effect-modifier-for-female-literacy-rate",
    "title": "Interactions",
    "section": "Do we think income level can be an effect modifier for female literacy rate?",
    "text": "Do we think income level can be an effect modifier for female literacy rate?\n\n\n\nLet’s say we only have two income groups: low income and high income\nWe can start by visualizing the relationship between life expectancy and female literacy rate by income level\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions.html#do-we-think-world-region-can-be-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions.html#do-we-think-world-region-can-be-an-effect-modifier-for-female-literacy-rate",
    "title": "Interactions",
    "section": "Do we think world region can be an effect modifier for female literacy rate?",
    "text": "Do we think world region can be an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by world region\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "href": "slides/11_Interactions.html#do-we-think-income-level-can-be-an-effect-modifier-for-world-region",
    "title": "Lesson 11: Interactions",
    "section": "Do we think income level can be an effect modifier for world region?",
    "text": "Do we think income level can be an effect modifier for world region?\n\n\n\nTaking a break from female literacy rate to demonstrate interactions for two categorical variables\nWe can start by visualizing the relationship between life expectancy and world region by income level\nQuestions of interest: Does the effect of world region on life expectancy differ depending on income level?\n\nThis is the same as: Is income level an effect modifier for world region?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions.html#do-we-think-food-supply-can-be-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions.html#do-we-think-food-supply-can-be-an-effect-modifier-for-female-literacy-rate",
    "title": "Interactions",
    "section": "Do we think food supply can be an effect modifier for female literacy rate?",
    "text": "Do we think food supply can be an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions.html#do-we-think-income-level-is-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions.html#do-we-think-income-level-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions",
    "section": "Do we think income level is an effect modifier for female literacy rate?",
    "text": "Do we think income level is an effect modifier for female literacy rate?\n\n\n\nLet’s say we only have two income groups: low income and high income\nWe can start by visualizing the relationship between life expectancy and female literacy rate by income level\n\n \n\nQuestions of interest: Is the effect of female literacy rate on life expectancy differ depending on income level?\n\nThis is the same as: Is income level is an effect modifier for female literacy rate?\n\n\n \n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions.html#recall-our-data-and-the-main-relationship",
    "href": "slides/11_Interactions.html#recall-our-data-and-the-main-relationship",
    "title": "Lesson 11: Interactions",
    "section": "Recall our data and the main relationship",
    "text": "Recall our data and the main relationship"
  },
  {
    "objectID": "slides/11_Interactions.html#model-with-interaction-between-a-binary-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#model-with-interaction-between-a-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Model with interaction between a binary categorical and continuous variables",
    "text": "Model with interaction between a binary categorical and continuous variables\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR\\) as female literacy rate (continuous variable)\n\\(I(\\text{high income})\\) as the indicator that income level is “high income” (binary categorical variable)\n\nIn R:\n\nm_int_inc2 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 +\n                  FemaleLiteracyRate*income_levels2, data = gapm_sub)\n\nOR\n\nm_int_inc2 = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*income_levels2, \n                data = gapm_sub)"
  },
  {
    "objectID": "slides/11_Interactions.html#model-fit",
    "href": "slides/11_Interactions.html#model-fit",
    "title": "Interactions",
    "section": "Model fit",
    "text": "Model fit\n\ntidy(m_int_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.849\n2.846\n19.270\n0.000\n49.169\n60.529\n    FemaleLiteracyRate\n0.156\n0.039\n3.990\n0.000\n0.078\n0.235\n    income_levels2Higher income\n−16.649\n15.364\n−1.084\n0.282\n−47.308\n14.011\n    FemaleLiteracyRate:income_levels2Higher income\n0.228\n0.164\n1.392\n0.168\n−0.099\n0.555\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#displaying-the-regressiont-table-and-writing-the-fitted-regression-equation",
    "href": "slides/11_Interactions.html#displaying-the-regressiont-table-and-writing-the-fitted-regression-equation",
    "title": "Interactions",
    "section": "Displaying the regressiont table and writing the fitted regression equation",
    "text": "Displaying the regressiont table and writing the fitted regression equation\n\ntidy(m_int_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.849\n2.846\n19.270\n0.000\n49.169\n60.529\n    FemaleLiteracyRate\n0.156\n0.039\n3.990\n0.000\n0.078\n0.235\n    income_levels2Higher income\n−16.649\n15.364\n−1.084\n0.282\n−47.308\n14.011\n    FemaleLiteracyRate:income_levels2Higher income\n0.228\n0.164\n1.392\n0.168\n−0.099\n0.555\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#comparing-income-levels",
    "href": "slides/11_Interactions.html#comparing-income-levels",
    "title": "Interactions",
    "section": "Comparing income levels",
    "text": "Comparing income levels\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 0 + \\\\\n& 0.228 \\cdot FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 FLR \\cdot 1 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 1 + \\\\ & 0.228 \\cdot FLR \\cdot 1 \\\\\n\\widehat{LE} = & (54.85 - 16.65 \\cdot 1) + \\\\ & (0.156 \\cdot FLR + 0.228 \\cdot FLR \\cdot 1) \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-income-level",
    "href": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-income-level",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression lines for each income level",
    "text": "Comparing fitted regression lines for each income level\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 0 + \\\\\n& 0.228 \\cdot FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 FLR \\cdot 1 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 1 + \\\\ & 0.228 \\cdot FLR \\cdot 1 \\\\\n\\widehat{LE} = & (54.85 - 16.65 \\cdot 1) + \\\\ & (0.156 \\cdot FLR + 0.228 \\cdot FLR \\cdot 1) \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#general-interpretation-of-the-interaction-term-reference",
    "href": "slides/11_Interactions.html#general-interpretation-of-the-interaction-term-reference",
    "title": "Interactions",
    "section": "General interpretation of the interaction term (reference)",
    "text": "General interpretation of the interaction term (reference)\n\\(E[Y\\mid X_{1},X_{2} ]=\\beta_0 + \\underbrace{(\\beta_1+\\beta_3X_{2}) }_\\text{$X_{1}$'s effect} X_{1}+ \\underbrace{\\beta_2X_{2}}_\\text{$X_{2}$ held constant}\\)\n\\({\\color{white}{E[Y\\mid X_{1},X_{2} ]}}=\\beta_0 + \\underbrace{(\\beta_2+\\beta_3X_{1}) }_\\text{$X_{2}$'s effect}X_{2} + \\underbrace{\\beta_1X_{1}}_\\text{$X_{1}$ held constant}\\)\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in \\(X_{1}\\)’s effect, per unit increase in \\(X_{2}\\);\n\n\\(\\beta_3\\) = mean change in \\(X_{2}\\)’s effect, per unit increase in \\(X_{1}\\);\nwhere the “\\(X_{1}\\) effect” equals the change in \\(E[Y]\\) per unit increase in \\(X_{1}\\) with \\(X_{2}\\) held constant, i.e. “adjusted \\(X_{1}\\) effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted \\(X_1\\) (or \\(X_2\\)) effect per unit increase in \\(X_2\\) (or \\(X_1\\))”"
  },
  {
    "objectID": "slides/11_Interactions.html#general-interpretation-of-the-interaction-term-reference-1",
    "href": "slides/11_Interactions.html#general-interpretation-of-the-interaction-term-reference-1",
    "title": "Interactions",
    "section": "General interpretation of the interaction term (reference)",
    "text": "General interpretation of the interaction term (reference)\n\\(E[Y\\mid X_{1},X_{2} ]=\\beta_0 + \\underbrace{(\\beta_1+\\beta_3X_{2}) }_\\text{$X_{1}$'s effect} X_{1}+ \\underbrace{\\beta_2X_{2}}_\\text{$X_{2}$ held constant}\\)\n\\({\\color{white}{E[Y\\mid X_{1},X_{2} ]}}=\\beta_0 + \\underbrace{(\\beta_2+\\beta_3X_{1}) }_\\text{$X_{2}$'s effect}X_{2} + \\underbrace{\\beta_1X_{1}}_\\text{$X_{1}$ held constant}\\)\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in \\(X_{1}\\)’s effect, per unit increase in \\(X_{2}\\);\n\n\\(\\beta_3\\) = mean change in \\(X_{2}\\)’s effect, per unit increase in \\(X_{1}\\);\nwhere the “\\(X_{1}\\) effect” equals the change in \\(E[Y]\\) per unit increase in \\(X_{1}\\) with \\(X_{2}\\) held constant, i.e. “adjusted \\(X_{1}\\) effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted \\(X_1\\) (or \\(X_2\\)) effect per unit increase in \\(X_2\\) (or \\(X_1\\))”"
  },
  {
    "objectID": "slides/11_Interactions.html#testing-the-interaction-between-binary-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#testing-the-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Interactions",
    "section": "Testing the interaction between binary categorical and continuous variables",
    "text": "Testing the interaction between binary categorical and continuous variables\n\nWe run an F-test for a single coefficient (\\(\\beta_3\\)) in the below model (see lesson 9, MLR: Inference / F-test)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_3=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_3\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\\\ &\\epsilon\n\\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\\\ &\\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables-1",
    "href": "slides/11_Interactions.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables-1",
    "title": "Interactions",
    "section": "Interpretation for interaction between binary categorical and continuous variables",
    "text": "Interpretation for interaction between binary categorical and continuous variables"
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-binary-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#test-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between binary categorical and continuous variables",
    "text": "Test interaction between binary categorical and continuous variables\n\nWe run an F-test for a single coefficient (\\(\\beta_3\\)) in the below model (see lesson 9, MLR: Inference / F-test)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_3=0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_3\\neq0\\)\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\\\ &\\epsilon\n\\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{high income}) + \\\\ &\\beta_3 FLR \\cdot I(\\text{high income}) + \\epsilon\n\\end{aligned}\\]\n\n\n\n\n\n\n\nI’m going to be skipping steps so please look back at Lesson 9 for full steps (required in HW 4)"
  },
  {
    "objectID": "slides/11_Interactions.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "href": "slides/11_Interactions.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "title": "Lesson 11: Interactions",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_inc2, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n54.849\n2.846\n19.270\n0.000\n49.169\n60.529\n    FemaleLiteracyRate\n0.156\n0.039\n3.990\n0.000\n0.078\n0.235\n    income_levels2Higher income\n−16.649\n15.364\n−1.084\n0.282\n−47.308\n14.011\n    FemaleLiteracyRate:income_levels2Higher income\n0.228\n0.164\n1.392\n0.168\n−0.099\n0.555\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-binary-categorical-and-continuous-variables-1",
    "href": "slides/11_Interactions.html#test-interaction-between-binary-categorical-and-continuous-variables-1",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between binary categorical and continuous variables",
    "text": "Test interaction between binary categorical and continuous variables\n\nFit the reduced and full model\n\n\nm_int_inc_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2, \n                   data = gapm_sub)\nm_int_inc_full = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 +\n                  FemaleLiteracyRate*income_levels2, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2\n69.000\n2,407.667\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 + FemaleLiteracyRate * income_levels2\n68.000\n2,340.948\n1.000\n66.719\n1.938\n0.168\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.168).\n\nIf significant, we say more: For higher income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.384 years. For lower income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.156 years. Thus, the female literacy rate almost doubles comparing high income to low income levels."
  },
  {
    "objectID": "slides/11_Interactions.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions",
    "section": "Do we think world region is an effect modifier for female literacy rate?",
    "text": "Do we think world region is an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by world region\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on world region?\n\nThis is the same as: Is world region is an effect modifier for female literacy rate?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 FLR \\cdot I(\\text{Americas}) + \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(FLR\\) as female literacy rate (continuous variable)\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\nm_int_wr = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + four_regions +\n                  FemaleLiteracyRate*four_regions, data = gapm_sub)\n\nOR\n\nm_int_wr = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "slides/11_Interactions.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "href": "slides/11_Interactions.html#displaying-the-regression-table-and-writing-fitted-regression-equation-1",
    "title": "Lesson 11: Interactions",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_wr, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n58.225\n3.377\n17.240\n0.000\n51.478\n64.972\n    FemaleLiteracyRate\n0.051\n0.053\n0.957\n0.342\n−0.055\n0.157\n    four_regionsAmericas\n−2.406\n17.913\n−0.134\n0.894\n−38.191\n33.379\n    four_regionsAsia\n2.283\n5.410\n0.422\n0.674\n−8.525\n13.091\n    four_regionsEurope\n63.628\n46.414\n1.371\n0.175\n−29.095\n156.350\n    FemaleLiteracyRate:four_regionsAmericas\n0.164\n0.197\n0.830\n0.410\n−0.231\n0.558\n    FemaleLiteracyRate:four_regionsAsia\n0.061\n0.073\n0.830\n0.410\n−0.086\n0.208\n    FemaleLiteracyRate:four_regionsEurope\n−0.519\n0.476\n−1.090\n0.280\n−1.471\n0.432\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 58.23 + 0.051 \\cdot FLR    −2.41 \\cdot I(\\text{Americas}) + 2.28 \\cdot I(\\text{Asia}) + 63.63 \\cdot I(\\text{Europe}) + \\\\ & 0.164  \\cdot FLR \\cdot I(\\text{Americas}) + 0.061 \\cdot FLR \\cdot I(\\text{Asia}) −0.519    \\cdot FLR \\cdot I(\\text{Europe})\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-world-region",
    "href": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-world-region",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression lines for each world region",
    "text": "Comparing fitted regression lines for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 58.23 + 0.051 \\cdot FLR    −2.41 \\cdot I(\\text{Americas}) + 2.28 \\cdot I(\\text{Asia}) + 63.63 \\cdot I(\\text{Europe}) + \\\\ & 0.164  \\cdot FLR \\cdot I(\\text{Americas}) + 0.061 \\cdot FLR \\cdot I(\\text{Asia}) −0.519    \\cdot FLR \\cdot I(\\text{Europe})\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 1 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)FLR \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\\\ & \\widehat\\beta_4 \\cdot 0 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 1+ \\widehat\\beta_7 FLR \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)FLR \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\\\ & \\widehat\\beta_4 \\cdot 1 + \\widehat\\beta_5 FLR \\cdot 0 + \\\\ & \\widehat\\beta_6 FLR \\cdot 0+ \\widehat\\beta_7 FLR \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-and-continuous-variables",
    "href": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-and-continuous-variables",
    "title": "Interactions",
    "section": "Test interaction between multi-level categorical and continuous variables",
    "text": "Test interaction between multi-level categorical and continuous variables\n\nFit the reduced and full model\n\n\nm_int_inc_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2, \n                   data = gapm_sub)\nm_int_inc_full = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 +\n                  FemaleLiteracyRate*income_levels2, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2\n69.000\n2,407.667\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 + FemaleLiteracyRate * income_levels2\n68.000\n2,340.948\n1.000\n66.719\n1.938\n0.168\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.168).\n\nIf significant, we say more: For higher income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.384 years. For lower income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.156 years. Thus, the female literacy rate almost doubles comparing high income to low income levels."
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-and-continuous-variables-1",
    "href": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-and-continuous-variables-1",
    "title": "Interactions",
    "section": "Test interaction between multi-level categorical and continuous variables",
    "text": "Test interaction between multi-level categorical and continuous variables\n\nFit the reduced and full model\n\n\nm_int_inc_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2, \n                   data = gapm_sub)\nm_int_inc_full = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 +\n                  FemaleLiteracyRate*income_levels2, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2\n69.000\n2,407.667\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + income_levels2 + FemaleLiteracyRate * income_levels2\n68.000\n2,340.948\n1.000\n66.719\n1.938\n0.168\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.168).\n\nIf significant, we say more: For higher income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.384 years. For lower income levels, for every one percent increase in female literacy rate, the mean life expectancy increases 0.156 years. Thus, the female literacy rate almost doubles comparing high income to low income levels."
  },
  {
    "objectID": "slides/11_Interactions.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables-1",
    "href": "slides/11_Interactions.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables-1",
    "title": "Interactions",
    "section": "Interpretation for interaction between multi-level categorical and continuous variables",
    "text": "Interpretation for interaction between multi-level categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6 I(\\text{high income}) \\cdot I(\\text{Asia})+ \\widehat\\beta_7 I(\\text{high income}) \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe})\\bigg] + \\\\ &\\underbrace{\\bigg[\\widehat\\beta_1 +  \\widehat\\beta_5 \\cdot I(\\text{Americas}) + \\widehat\\beta_6 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot I(\\text{Europe}) \\bigg]}_\\text{I(\\text{high income})'s effect} I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_5\\) = mean change in female literacy rate’s effect, comparing countries in the Americas to countries in Africa\n\\(\\beta_6\\) = mean change in female literacy rate’s effect, comparing countries in Asia to countries in Africa\n\\(\\beta_7\\) = mean change in female literacy rate’s effect, comparing countries in Europe to countries in Africa\n\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "href": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 FLR \\cdot I(\\text{Americas}) + \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\\\ & \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 FLR \\cdot I(\\text{Americas}) + \\\\ & \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "href": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_red = lm(LifeExpectancyYrs ~ FLR_c + four_regions, \n                   data = gapm_sub)\nm_int_wr_full = lm(LifeExpectancyYrs ~ FLR_c + four_regions+\n                  FLR_c*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + four_regions\n67.000\n1,705.881\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + four_regions + FLR_c * four_regions\n64.000\n1,641.151\n3.000\n64.731\n0.841\n0.476\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.478)."
  },
  {
    "objectID": "slides/11_Interactions.html#centering-continuous-variables-when-we-are-including-interactions-1",
    "href": "slides/11_Interactions.html#centering-continuous-variables-when-we-are-including-interactions-1",
    "title": "Interactions",
    "section": "Centering continuous variables when we are including interactions",
    "text": "Centering continuous variables when we are including interactions\n\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues\n\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression"
  },
  {
    "objectID": "slides/11_Interactions.html#now-centered-interpretation-for-interaction-between-two-continuous-variables-1",
    "href": "slides/11_Interactions.html#now-centered-interpretation-for-interaction-between-two-continuous-variables-1",
    "title": "Interactions",
    "section": "Now centered: interpretation for interaction between two continuous variables",
    "text": "Now centered: interpretation for interaction between two continuous variables"
  },
  {
    "objectID": "slides/11_Interactions.html#itll-be-helpful-to-center-female-literacy-rate",
    "href": "slides/11_Interactions.html#itll-be-helpful-to-center-female-literacy-rate",
    "title": "Lesson 11: Interactions",
    "section": "It’ll be helpful to center female literacy rate",
    "text": "It’ll be helpful to center female literacy rate\n\n\nCentering female literacy rate: \\[ FLR^c = FLR - \\overline{FLR}\\]\nCentering in R:\n\n\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate))\n\n\nI’m going to print the mean so I can use it for my interpretations\n\n\n(mean_FLR = mean(gapm_sub$FemaleLiteracyRate))\n\n[1] 82.03056\n\n\n\nNow all intercept values (in each respective world region) will be the mean life expectancy when female literacy rate is 82.03%\nWe will used center FLR for the rest of the lecture"
  },
  {
    "objectID": "slides/11_Interactions.html#now-we-refit-the-model-with-the-centered-flr",
    "href": "slides/11_Interactions.html#now-we-refit-the-model-with-the-centered-flr",
    "title": "Lesson 11: Interactions",
    "section": "Now we refit the model with the centered FLR",
    "text": "Now we refit the model with the centered FLR\n\n\nm_int_wr_flrc = lm(LifeExpectancyYrs ~ FLR_c*four_regions, \n                data = gapm_sub)\ntidy(m_int_wr_flrc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n62.387\n1.626\n38.358\n0.000\n59.138\n65.637\n    FLR_c\n0.051\n0.053\n0.957\n0.342\n−0.055\n0.157\n    four_regionsAmericas\n11.032\n2.918\n3.781\n0.000\n5.203\n16.862\n    four_regionsAsia\n7.287\n2.042\n3.568\n0.001\n3.207\n11.367\n    four_regionsEurope\n21.038\n7.698\n2.733\n0.008\n5.659\n36.417\n    FLR_c:four_regionsAmericas\n0.164\n0.197\n0.830\n0.410\n−0.231\n0.558\n    FLR_c:four_regionsAsia\n0.061\n0.073\n0.830\n0.410\n−0.086\n0.208\n    FLR_c:four_regionsEurope\n−0.519\n0.476\n−1.090\n0.280\n−1.471\n0.432\n  \n  \n  \n\n\n\n\n\nWhat changed? What stayed the same? What’s the new intercept for Europe?"
  },
  {
    "objectID": "slides/11_Interactions.html#lets-take-a-look-back-at-the-plot",
    "href": "slides/11_Interactions.html#lets-take-a-look-back-at-the-plot",
    "title": "Lesson 11: Interactions",
    "section": "Let’s take a look back at the plot",
    "text": "Let’s take a look back at the plot\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR  \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 +\\widehat\\beta_2) + (\\widehat\\beta_1 +\\widehat\\beta_3) FLR \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables-1",
    "href": "slides/11_Interactions.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables-1",
    "title": "Lesson 11: Interactions",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(I(\\text{high income})\\) as indicator of high income\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\n# gapm_sub = gapm_sub %&gt;% mutate(income_levels2 = relevel(income_levels2, ref = \"Higher income\")) # for poll everywhere\n\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                  income_levels2*four_regions, data = gapm_sub)\nm_int_wr_inc = lm(LifeExpectancyYrs ~ income_levels2*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "slides/11_Interactions.html#displaying-the-regression-table-and-writing-fitted-regression-equation-2",
    "href": "slides/11_Interactions.html#displaying-the-regression-table-and-writing-fitted-regression-equation-2",
    "title": "Lesson 11: Interactions",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy(m_int_wr_inc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 25) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.850\n1.281\n47.488\n0.000\n58.290\n63.410\n    income_levels2Higher income\n2.100\n2.865\n0.733\n0.466\n−3.624\n7.824\n    four_regionsAmericas\n10.800\n3.844\n2.810\n0.007\n3.121\n18.479\n    four_regionsAsia\n7.467\n1.957\n3.815\n0.000\n3.556\n11.377\n    four_regionsEurope\n11.500\n2.865\n4.014\n0.000\n5.776\n17.224\n    income_levels2Higher income:four_regionsAmericas\n2.640\n4.896\n0.539\n0.592\n−7.141\n12.421\n    income_levels2Higher income:four_regionsAsia\n1.543\n3.956\n0.390\n0.698\n−6.360\n9.447\n    income_levels2Higher income:four_regionsEurope\n2.382\n4.020\n0.592\n0.556\n−5.649\n10.412\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-world-region-1",
    "href": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-world-region-1",
    "title": "Interactions",
    "section": "Comparing fitted regression lines for each world region",
    "text": "Comparing fitted regression lines for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\& \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income})\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ &  \\widehat\\beta_5 I(\\text{high income}) \\cdot 1 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 1+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 1 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables-2",
    "href": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables-2",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.928)."
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables-3",
    "href": "slides/11_Interactions.html#test-interaction-between-multi-level-categorical-continuous-variables-3",
    "title": "Interactions",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_wr_red = lm(LifeExpectancyYrs ~ FLR_c + four_regions, \n                   data = gapm_sub)\nm_int_wr_full = lm(LifeExpectancyYrs ~ FLR_c + four_regions+\n                  FLR_c*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + four_regions\n67.000\n1,705.881\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + four_regions + FLR_c * four_regions\n64.000\n1,641.151\n3.000\n64.731\n0.841\n0.476\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.478)."
  },
  {
    "objectID": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-income-level-1",
    "href": "slides/11_Interactions.html#comparing-fitted-regression-lines-for-each-income-level-1",
    "title": "Interactions",
    "section": "Comparing fitted regression lines for each income level",
    "text": "Comparing fitted regression lines for each income level\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{high income}) + \\widehat\\beta_3 FLR \\cdot I(\\text{high income}) \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot I(\\text{high income}) + 0.228 \\cdot FLR \\cdot I(\\text{high income})\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 0 + \\\\\n& 0.228 \\cdot FLR \\cdot 0 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR\\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 FLR \\cdot 1 \\\\\n\\widehat{LE} = & 54.85 + 0.156 \\cdot FLR - 16.65 \\cdot 1 + \\\\ & 0.228 \\cdot FLR \\cdot 1 \\\\\n\\widehat{LE} = & (54.85 - 16.65 \\cdot 1) + \\\\ & (0.156 \\cdot FLR + 0.228 \\cdot FLR \\cdot 1) \\\\\n\\widehat{LE} = & (54.85 - 16.65) + (0.156 + 0.228) \\cdot FLR\\\\\n\\widehat{LE} = & 38.2 + 0.384 \\cdot FLR\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#comparing-fitted-regression-means-for-each-world-region",
    "href": "slides/11_Interactions.html#comparing-fitted-regression-means-for-each-world-region",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression means for each world region",
    "text": "Comparing fitted regression means for each world region\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nAfrica\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\& \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income})\\\\\n\\end{aligned}\\]\n\n\n\n\n\nThe Americas\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 1 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 0 + \\\\ &  \\widehat\\beta_5 I(\\text{high income}) \\cdot 1 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_2\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_5\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nAsia\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 1 + \\widehat\\beta_4 \\cdot 0 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 1+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 0 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_3\\big) + \\\\ &\\big(\\widehat\\beta_1 + \\widehat\\beta_6\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nEurope\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\widehat\\beta_3 \\cdot 0 + \\widehat\\beta_4 \\cdot 1 + \\\\ & \\widehat\\beta_5 I(\\text{high income}) \\cdot 0 + \\\\ & \\widehat\\beta_6 I(\\text{high income}) \\cdot 0+ \\\\ & \\widehat\\beta_7 I(\\text{high income}) \\cdot 1 \\\\\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)I(\\text{high income}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#comparing-fitted-regression-means-for-each-income-level",
    "href": "slides/11_Interactions.html#comparing-fitted-regression-means-for-each-income-level",
    "title": "Lesson 11: Interactions",
    "section": "Comparing fitted regression means for each income level",
    "text": "Comparing fitted regression means for each income level\n\\[\\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{high income}) + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\widehat\\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & 60.85 + 2.10 \\cdot I(\\text{high income}) + 10.8 \\cdot I(\\text{Americas}) + 7.47\\cdot  I(\\text{Asia}) + 11.50 \\cdot I(\\text{Europe}) + \\\\ & 2.64 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + 1.54 \\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & 2.38 \\cdot I(\\text{high income})\\cdot I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 0\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 0 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 0\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 \\cdot 1 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 \\cdot 1\\cdot I(\\text{Americas}) + \\widehat\\beta_6\\cdot 1 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot 1\\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) + \\\\ & (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#lets-take-a-look-back-at-the-plot-1",
    "href": "slides/11_Interactions.html#lets-take-a-look-back-at-the-plot-1",
    "title": "Lesson 11: Interactions",
    "section": "Let’s take a look back at the plot",
    "text": "Let’s take a look back at the plot\n\n\n\n\nFor lower income countries: \\(I(\\text{high income}) =0\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\\\ & \\widehat\\beta_4 I(\\text{Europe}) \\\\\n\\end{aligned}\\]\n\n\n\n\nFor higher income countries: \\(I(\\text{high income}) =1\\)\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & (\\widehat\\beta_0 + \\widehat\\beta_1)  + (\\widehat\\beta_2 + \\widehat\\beta_5) I(\\text{Americas}) + \\\\& (\\widehat\\beta_3 + \\widehat\\beta_6)  I(\\text{Asia}) +  (\\widehat\\beta_4 + \\widehat\\beta_7)  I(\\text{Europe}) \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#test-interaction-between-two-categorical-variables",
    "href": "slides/11_Interactions.html#test-interaction-between-two-categorical-variables",
    "title": "Lesson 11: Interactions",
    "section": "Test interaction between two categorical variables",
    "text": "Test interaction between two categorical variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\n\n\nNull \\(H_0\\)\n\n\n\\(\\beta_5= \\beta_6 = \\beta_7 =0\\)\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\(\\beta_5\\neq0\\) and/or \\(\\beta_6\\neq0\\) and/or \\(\\beta_7\\neq0\\)\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\\\& \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\epsilon \\end{aligned}\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{high income}) + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\\\ & \\beta_4 I(\\text{Europe}) + \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\\\ & \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions.html#next-time",
    "href": "slides/11_Interactions.html#next-time",
    "title": "Interactions",
    "section": "Next time",
    "text": "Next time"
  },
  {
    "objectID": "slides/11_Interactions.html#next-time-hour-before-quiz",
    "href": "slides/11_Interactions.html#next-time-hour-before-quiz",
    "title": "Interactions",
    "section": "Next time (hour before quiz)",
    "text": "Next time (hour before quiz)\nGo back to the remaining learning objectives:\n\nInterpret the interaction component of a model with two continuous covariates, and how the main variable’s effect changes.\nWhen there are only two covariates in the model, test whether one is a confounder or effect modifier.\n\n\n\nInteractions"
  },
  {
    "objectID": "slides/11_Interactions.html#poll-everywhere-question-2",
    "href": "slides/11_Interactions.html#poll-everywhere-question-2",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/11_Interactions.html#poll-everywhere-question-3",
    "href": "slides/11_Interactions.html#poll-everywhere-question-3",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/11_Interactions.html#poll-everywhere-question-4",
    "href": "slides/11_Interactions.html#poll-everywhere-question-4",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "slides/11_Interactions_2.html",
    "href": "slides/11_Interactions_2.html",
    "title": "Lesson 12: Interactions Continued",
    "section": "",
    "text": "Interpret the interaction component of a model with two continuous covariates, and how the main variable’s effect changes.\nWhen there are only two covariates in the model, test whether one is a confounder or effect modifier.\n\n\n\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on food supply?\n\nThis is the same as: Is food supply is an effect modifier for female literacy rate? Is food supply an effect modifier of the association between life expectancy and female literacy rate?\n\nLet’s run an interaction model to see!\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR^c\\) as the centered around the mean female literacy rate (continuous variable)\n\\(FS^c\\) as the centered around the mean food supply (continuous variable)\n\n\n\nCode to center FLR and FS\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate), \n         FS_c = FoodSupplykcPPD - mean(FoodSupplykcPPD))\nmean_FS = mean(gapm_sub$FoodSupplykcPPD) %&gt;% round(digits = 0)\nmean_FLR = mean(gapm_sub$FemaleLiteracyRate) %&gt;% round(digits = 2)\n\n\nIn R:\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c*FS_c, data = gapm_sub)\n\nOR\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c*FS_c, data = gapm_sub)\n\n\n\n\n\ntidy_m_fs = tidy(m_int_fs, conf.int=T) \ntidy_m_fs %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.32060\n0.72393\n97.13721\n0.00000\n68.87601\n71.76518\n    FLR_c\n0.15532\n0.03808\n4.07905\n0.00012\n0.07934\n0.23130\n    FS_c\n0.00849\n0.00182\n4.67908\n0.00001\n0.00487\n0.01212\n    FLR_c:FS_c\n−0.00001\n0.00008\n−0.06908\n0.94513\n−0.00016\n0.00015\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\nTo identify different lines, we need to pick example values of Food Supply:\n\n\n\n\nFood Supply of 1812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot (-1000) + \\\\ & \\widehat\\beta_3 FLR^c \\cdot (-1000) \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 - 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 - 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 2812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 0 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 \\big)+ \\\\ & \\big(\\widehat\\beta_1 \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 3812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 1000 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 1000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot FS^c \\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot FS^c \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, for every one kcal PPD increase in food supply\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect for every 1 kcal PPD increase in food supply”\nIt will be helpful to test the interaction to round out this interpretation!!\n\n\n\n\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\[\\beta_3=0\\]\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\[\\beta_3\\neq0\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\\\ & \\beta_3 FLR^c \\cdot FS^c + \\epsilon\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\nFit the reduced and full model\n\n\nm_int_fs_red = lm(LifeExpectancyYrs ~ FLR_c + FS_c, \n                   data = gapm_sub)\nm_int_fs_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c +\n                  FLR_c*FS_c, data = gapm_sub)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nanova(m_int_fs_red, m_int_fs_full) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + FS_c\n69.000\n2,005.556\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c * FS_c\n68.000\n2,005.415\n1.000\n0.141\n0.005\n0.945\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and food supply (p = 0.945). Food supply is not an effect modifier of the association between female literacy rate and life expectancy."
  },
  {
    "objectID": "slides/11_Interactions_2.html#do-we-think-food-supply-can-be-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions_2.html#do-we-think-food-supply-can-be-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 12: Interactions Continued",
    "section": "Do we think food supply can be an effect modifier for female literacy rate?",
    "text": "Do we think food supply can be an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on food supply?\n\nThis is the same as: Is food supply is an effect modifier for female literacy rate? Is food supply an effect modifier of the association between life expectancy and female literacy rate?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions_2.html#interaction-between-two-continuous-variables",
    "href": "slides/11_Interactions_2.html#interaction-between-two-continuous-variables",
    "title": "Lesson 12: Interactions Continued",
    "section": "Interaction between two continuous variables",
    "text": "Interaction between two continuous variables\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n32.471\n16.885\n1.923\n0.059\n−1.222\n66.165\n    FemaleLiteracyRate\n0.170\n0.199\n0.854\n0.396\n−0.228\n0.568\n    FoodSupplykcPPD\n0.009\n0.007\n1.325\n0.190\n−0.005\n0.022\n    FemaleLiteracyRate:FoodSupplykcPPD\n0.000\n0.000\n−0.069\n0.945\n0.000\n0.000"
  },
  {
    "objectID": "slides/11_Interactions_2.html#deciding-between-confounder-and-effect-modifier",
    "href": "slides/11_Interactions_2.html#deciding-between-confounder-and-effect-modifier",
    "title": "Lesson 11: Interactions Continued",
    "section": "Deciding between confounder and effect modifier",
    "text": "Deciding between confounder and effect modifier\n\nThis is more of a model selection question (in coming lectures)\nBut if we had a model with only TWO covariates, we could step through the following process:\n\nTest the interaction (of potential effect modifier): use a partial F-test to test if interaction term(s) explain enough variation compared to model without interaction\n\nRecall that for two continuous covariates, we will test a single coefficient\nFor a binary and continuous covariate, we will test a single coefficient\nFor two binary categorical covariates, we will test a single coefficient\nFor a multi-level categorical covariate (with any other type of covariate), we must test a group of coefficients!!\n\nThen look at the main effect (or potential confounder)\n\nIf interaction already included, then automatically included as main effect (and thus not checked for confounding)\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%"
  },
  {
    "objectID": "slides/11_Interactions_2.html#general-interpretation-of-the-interaction-term-reference",
    "href": "slides/11_Interactions_2.html#general-interpretation-of-the-interaction-term-reference",
    "title": "Lesson 11: Interactions Continued",
    "section": "General interpretation of the interaction term (reference)",
    "text": "General interpretation of the interaction term (reference)\n\\(E[Y\\mid X_{1},X_{2} ]=\\beta_0 + \\underbrace{(\\beta_1+\\beta_3X_{2}) }_\\text{$X_{1}$'s effect} X_{1}+ \\underbrace{\\beta_2X_{2}}_\\text{$X_{2}$ held constant}\\)\n\\({\\color{white}{E[Y\\mid X_{1},X_{2} ]}}=\\beta_0 + \\underbrace{(\\beta_2+\\beta_3X_{1}) }_\\text{$X_{2}$'s effect}X_{2} + \\underbrace{\\beta_1X_{1}}_\\text{$X_{1}$ held constant}\\)\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in \\(X_{1}\\)’s effect, per unit increase in \\(X_{2}\\);\n\n\\(\\beta_3\\) = mean change in \\(X_{2}\\)’s effect, per unit increase in \\(X_{1}\\);\nwhere the “\\(X_{1}\\) effect” equals the change in \\(E[Y]\\) per unit increase in \\(X_{1}\\) with \\(X_{2}\\) held constant, i.e. “adjusted \\(X_{1}\\) effect”\n\nIn summary, the interaction term can be interpreted as “difference in adjusted \\(X_1\\) (or \\(X_2\\)) effect per unit increase in \\(X_2\\) (or \\(X_1\\))”"
  },
  {
    "objectID": "slides/11_Interactions_2.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "href": "slides/11_Interactions_2.html#a-glimpse-at-how-interactions-might-be-incorporated-into-model-selection",
    "title": "Lesson 11: Interactions Continued",
    "section": "A glimpse at how interactions might be incorporated into model selection",
    "text": "A glimpse at how interactions might be incorporated into model selection\n\nIdentify outcome (Y) and primary explanatory (X) variables\nDecide which other variables might be important and could be potential confounders. Add these to the model.\n\nThis is often done by indentifying variables that previous research deemed important, or researchers believe could be important\nFrom a statistical perspective, we often include variables that are significantly associated with the outcome (in their respective SLR)\n\n(Optional step) Test 3 way interactions\n\nThis makes our model incredibly hard to interpret. Our class will not cover this!!\nWe will skip to testing 2 way interactions\n\nTest 2 way interactions\n\nWhen testing a 2 way interaction, make sure the full and reduced models contain the main effects\nFirst test all the 2 way interactions together using a partial F-test (with \\(alpha = 0.10\\))\n\nIf this test not significant, do not test 2-way interactions individually\nIf partial F-test is significant, then test each of the 2-way interactions\n\n\nRemaining main effects - to include of not to include?\n\nFor variables that are included in any interactions, they will be automatically included as main effects and thus not checked for confounding\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable(s) changes any of the coefficient of the primary explanatory variable (including interactions) X by more than 10%\n\nIf any of X’s coefficients change when removing the potential confounder, then keep it in the model\n\n\n\n\n\n\nInteractions 2"
  },
  {
    "objectID": "slides/11_Interactions_2.html#interpretation-for-interaction-between-two-continuous-variables",
    "href": "slides/11_Interactions_2.html#interpretation-for-interaction-between-two-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "Interpretation for interaction between two continuous variables",
    "text": "Interpretation for interaction between two continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot FS^c \\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot FS^c \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, for every one kcal PPD increase in food supply\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect for every 1 kcal PPD increase in food supply”\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#overall",
    "href": "slides/Quiz_2_Lab_2.html#overall",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Overall",
    "text": "Overall\n\nGreat job!\nJust a few things to review"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#question-7",
    "href": "slides/Quiz_2_Lab_2.html#question-7",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "",
    "text": "Which of the following statements is true about the value -0.834 in our regression table?\n\nIt is the estimate of the sample intercept\nIt is the estimate of the population intercept\nIt is the estimate of the sample slope\n\n\n\nIt is the estimate of the population slope\n\n\n\n\n\n\nBecause -0.834 corresponds to the “Age” row of the regression table, this is the slope of our fitted regression line\nThis means \\(\\widehat\\beta_1 = -0.834\\)\n\\(\\widehat\\beta_1\\) is the estimate of the population slope\n\\(-0.834\\) is just the realized value (the result of fitting the population model)\nWe read \\(\\widehat\\beta_1 = -0.834\\) as: the estimate of the population slope is -0.834\nWhile speaking, I might say “coefficient estimate.” If I am saying estimate, then I mean the population estimate\n\nPlease stop and ask me if my language ever feels unclear"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#question-9-and-10-12",
    "href": "slides/Quiz_2_Lab_2.html#question-9-and-10-12",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "Question 9 and 10 (1/2)",
    "text": "Question 9 and 10 (1/2)\n\nThe following are required parts of the interpretation\n\nUnits of Y\nUnits of X\nMean/average/expected before Y when discussing intercept\nMean/average/expected before difference, increase, or decrease when discussing coefficient for continuous covariate\n\nYou can also have expected/average/mean before the Y, but not necessary\n\nConfidence interval"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#question-9-and-10-22",
    "href": "slides/Quiz_2_Lab_2.html#question-9-and-10-22",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "Question 9 and 10 (2/2)",
    "text": "Question 9 and 10 (2/2)\n\nIntercept: For someone 0 years old, the average peak exercise heart rate is 214.233 beats per minute (95% CI: 204.918, 223.548).\nSlope: For every one year increase in age, the peak exercise heart rate is expected to decrease by 0.834 beats per minute (95% CI: -0.982, -0.685).\n\nOR: For every one year increase in age, the expected peak exercise heart rate decreases by 0.834 beats per minute (95% CI: -0.982, -0.685)."
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#more-on-coefficient-interpretations",
    "href": "slides/Quiz_2_Lab_2.html#more-on-coefficient-interpretations",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "More on coefficient interpretations",
    "text": "More on coefficient interpretations\n\n\nPopulation model: \\[\nE[Y|X] = \\beta_0 + \\beta_1X\n\\]\nWhat is \\(\\beta_1\\) mean?\n\nLet’s say we have \\(X = x_1\\) and \\(X=x_2\\)\nThe difference between \\(x_1\\) and \\(x_2\\) is 1: \\(x_1 - x_2 = 1\\)\nWe don’t have to know the actual values of the x’s, just that their difference is 1\nNow, let’s look at the expected values for each of those x’s:\n\n\\[ \\begin{aligned}\n    E[Y|x_1] & = \\beta_0 + \\beta_1x_1 \\\\\n    E[Y|x_2] & = \\beta_0 + \\beta_1x_2\n\\end{aligned}\\]\n\n\nIf we take the difference between the expected values, we get:\n\n\\[ \\begin{aligned}\n    E[Y|x_1] - E[Y|x_2] & = (\\beta_0 + \\beta_1x_1) - (\\beta_0 + \\beta_1x_2) \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_0 + \\beta_1x_1 - \\beta_0 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1x_1 - \\beta_1x_2 \\\\\n    E[Y|x_1] - E[Y|x_2] & = \\beta_1 (x_1 - x_2) \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{x_1 - x_2} \\\\\n    \\beta_1 & = \\frac{E[Y|x_1] - E[Y|x_2]}{1} \\\\\n    \\beta_1 & = E[Y|x_1] - E[Y|x_2]\n\\end{aligned}\\]\n\nSo, we can consider \\(\\beta_1\\) as the difference in the expected Y for every 1 unit increase in X"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#thinking-of-the-expected-value-another-way",
    "href": "slides/Quiz_2_Lab_2.html#thinking-of-the-expected-value-another-way",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "Thinking of the expected value another way",
    "text": "Thinking of the expected value another way\n\nOr: we can look at \\(\\beta_1\\) another way: \\[ \\begin{aligned}\n\\beta_1 & = E[Y|x_1] - E[Y|x_2] \\\\\n\\beta_1 & = E\\big[ (Y|x_1) - (Y|x_2) \\big] \\\\\n\\end{aligned}\\]\n\nThis would make \\(\\beta_1\\) the expected difference in Y for every 1 unit increase in X"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#overall-1",
    "href": "slides/Quiz_2_Lab_2.html#overall-1",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Overall",
    "text": "Overall\n\nOverall, good job! I feel like we’re making progress towards our analysis with thoughtful considerations for the data\nI definitely steered us towards thinking about the defined population\n\nMake sure you also think about over-representation\nOver-representation does NOT mean we cannot make claims about the under-represented, but present, groups\n\nWhen visualizing, making tables, displaying information from the data: always keep in the back of your mind:\n\nWhat can a reader get from this if they have never seen the data?\nIs it easy for the reader to understand the plot?\nDoes everything in the plot have a purpose?\nIs the main thing I’m trying to communicate also the thing that stands out?\n\nBivariate visualization was supposed to visualize our research question"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#notes-on-coding",
    "href": "slides/Quiz_2_Lab_2.html#notes-on-coding",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Notes on coding",
    "text": "Notes on coding\n\n#| message: false Please use after troubleshooting your code\n\nEspecially when loading libraries! Makes everything neater\n\nWhen assigning category names, capitalization of the first word in each category is customary"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#limitations-of-the-iat",
    "href": "slides/Quiz_2_Lab_2.html#limitations-of-the-iat",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "Limitations of the IAT",
    "text": "Limitations of the IAT\n\nTaking the test multiple times\n\nA lot of us mentioned learning bias, which can definitely be true\n\nThink about what direction that might bias our results\n\nProblems with independence between observations\n\nGeneralizability\n\nDoes it represent our population? When we just say “population,” is there an unsaid assumption on the population we are referring to?\nCan we start to narrow the definition of our population to give context to our sample?"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#other-notes",
    "href": "slides/Quiz_2_Lab_2.html#other-notes",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "Other notes",
    "text": "Other notes\n\nDid not intend for us to get focused on the 3 social theories in the article\n\nIf it helps you contextualize, then go for it!\nBut make sure you are defining the social theories and connecting them to motivation for your\n\nMinor writing notes\n\nWhile folks is a great, inclusive word to describe people, it is a little too informal in reports\n\nGood alternative is “individuals”\n\nDo not use “I” or “think” in report\n\nCan use the royal “We”\n\n\nWhen we talk about our analysis, avoid how “individuals’” scores relate to their other measures.\n\nImportant to note that we are not making conclusions about the individual\nWe are using individual data to make conclusions about the population!"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#moving-forward",
    "href": "slides/Quiz_2_Lab_2.html#moving-forward",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "Moving forward",
    "text": "Moving forward\n\nMake sure you articulate the motivation for your research question\n\nIf you are interested in it, then there is likely some research discussing the relationship\nContextualize why this is a research question worth exploring\n\nIf you want to review your intro, please come to me!\n\nRevising early will be helpful for the report\nI will grade each portion of the report expecting you make the needed changes\nI did not make notes on all edits - tried to identify the bigger things\n\nGood sources for report help\n\nStructuring research articles\nInclusive language practices\nGuide on improving readability\n\n\n\n\nQuiz and Lab 2"
  },
  {
    "objectID": "weeks/week_07_sched.html",
    "href": "weeks/week_07_sched.html",
    "title": "Week 7",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #213c96; background-color: #fff; padding: 10px; }\n.description{ font-size: 20px; color: #fff; background-color: #213c96; padding: 10px; } ```"
  },
  {
    "objectID": "weeks/week_07_sched.html#announcements",
    "href": "weeks/week_07_sched.html#announcements",
    "title": "Week 7",
    "section": "Announcements",
    "text": "Announcements\n\nWednesday 2/21\n\nMy office hours tomorrow are from 11am - 12pm\n\nI have a faculty meeting at 12pm\n\nHW 4 is due on Sunday (2/25)\nLab 3 is due Sunday (3/3)\nNext week is virtual!!"
  },
  {
    "objectID": "slides/11_Interactions_2.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions_2.html#do-we-think-world-region-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 12: Interactions Continued",
    "section": "Do we think world region is an effect modifier for female literacy rate?",
    "text": "Do we think world region is an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by world region\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on world region?\n\nThis is the same as: Is world region is an effect modifier for female literacy rate?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions_2.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "href": "slides/11_Interactions_2.html#model-with-interaction-between-a-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 12: Interactions Continued",
    "section": "Model with interaction between a multi-level categorical and continuous variables",
    "text": "Model with interaction between a multi-level categorical and continuous variables\nModel we are fitting:\n\\[\\begin{aligned}LE = &\\beta_0 + \\beta_1 FLR + \\beta_2 I(\\text{Americas}) + \\beta_3 I(\\text{Asia}) + \\beta_4 I(\\text{Europe}) + \\\\ & \\beta_5 FLR \\cdot I(\\text{Americas}) + \\beta_6 FLR \\cdot I(\\text{Asia})+ \\beta_7 FLR \\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\]\n\n\\(LE\\) as life expectancy\n\\(FLR\\) as female literacy rate (continuous variable)\n\\(I(\\text{Americas})\\), \\(I(\\text{Asia})\\), \\(I(\\text{Europe})\\) as the indicator for each world region\n\nIn R:\n\nm_int_wr = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + four_regions +\n                  FemaleLiteracyRate*four_regions, data = gapm_sub)\n\nOR\n\nm_int_wr = lm(LifeExpectancyYrs ~ FemaleLiteracyRate*four_regions, \n                data = gapm_sub)"
  },
  {
    "objectID": "slides/11_Interactions_2.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "href": "slides/11_Interactions_2.html#displaying-the-regression-table-and-writing-fitted-regression-equation",
    "title": "Lesson 11: Interactions Continued",
    "section": "Displaying the regression table and writing fitted regression equation",
    "text": "Displaying the regression table and writing fitted regression equation\n\ntidy_m_fs = tidy(m_int_fs, conf.int=T) \ntidy_m_fs %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 5)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.32060\n0.72393\n97.13721\n0.00000\n68.87601\n71.76518\n    FLR_c\n0.15532\n0.03808\n4.07905\n0.00012\n0.07934\n0.23130\n    FS_c\n0.00849\n0.00182\n4.67908\n0.00001\n0.00487\n0.01212\n    FLR_c:FS_c\n−0.00001\n0.00008\n−0.06908\n0.94513\n−0.00016\n0.00015\n  \n  \n  \n\n\n\n\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#comparing-fitted-regression-lines-for-each-world-region",
    "href": "slides/11_Interactions_2.html#comparing-fitted-regression-lines-for-each-world-region",
    "title": "Lesson 12: Interactions Continued",
    "section": "Comparing fitted regression lines for each world region",
    "text": "Comparing fitted regression lines for each world region\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 FS + \\widehat\\beta_3 FLR \\cdot FS \\\\\n\\widehat{LE} = & 32.47 + 0.17 \\cdot FLR + 0.01 \\cdot FS - 0.00001 \\cdot FLR \\cdot FS\n\\end{aligned}\\]\n\n\n\n\nFood Supply of 2,000 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 2000 + \\\\ & \\widehat\\beta_3 FLR \\cdot 2000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot 2000 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot 2000 \\big) FLR\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 3,000 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 3000 + \\\\ & \\widehat\\beta_3 FLR \\cdot 3000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot 3000 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot 3000 \\big) FLR\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 4,000 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\\\ & \\widehat\\beta_2 \\cdot 4000 + \\\\ & \\widehat\\beta_3 FLR \\cdot 4000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot 4000 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot 4000 \\big) FLR\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#poll-everywhere-question-3",
    "href": "slides/11_Interactions_2.html#poll-everywhere-question-3",
    "title": "Lesson 12: Interactions Continued",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/11_Interactions_2.html#centering-continuous-variables-when-we-are-including-interactions",
    "href": "slides/11_Interactions_2.html#centering-continuous-variables-when-we-are-including-interactions",
    "title": "Lesson 12: Interactions Continued",
    "section": "Centering continuous variables when we are including interactions",
    "text": "Centering continuous variables when we are including interactions\n\n\nFor Europe, the mean life expectancy had a regression line with a large intercept\n\n\\[\\begin{aligned}\n\\widehat{LE} = &\\big(\\widehat\\beta_0+\\widehat\\beta_4\\big) + \\big(\\widehat\\beta_1 + \\widehat\\beta_7\\big)FLR \\\\\n\\widehat{LE} = & (58.23 + 63.63) + (0.051 - 0.519)FLR \\\\\n\\widehat{LE} = & 121.86 -0.468FLR \\\\\n\\end{aligned}\\]\n\nCentering the continuous variables in a model (when they are involved in interactions) helps with:\n\nInterpretations of the coefficient estimates\nCorrelation between the main effect for the variable and the interaction that it is involved with\n\nTo be discussed in future lecture: leads to multicollinearity issues\n\n\nOther online sources about when and when not to center:\n\nThe why and when of centering continuous predictors in regression modeling\nWhen not to center a predictor variable in regression"
  },
  {
    "objectID": "slides/11_Interactions_2.html#itll-be-helpful-to-center-female-literacy-rate",
    "href": "slides/11_Interactions_2.html#itll-be-helpful-to-center-female-literacy-rate",
    "title": "Lesson 12: Interactions Continued",
    "section": "It’ll be helpful to center female literacy rate",
    "text": "It’ll be helpful to center female literacy rate\n\n\nCentering female literacy rate: \\[ FLR^c = FLR - \\overline{FLR}\\]\nCentering in R:\n\n\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate))\n\n\nI’m going to print the mean so I can use it for my interpretations\n\n\n(mean_FLR = mean(gapm_sub$FemaleLiteracyRate))\n\n[1] 82.03056\n\n\n\nNow all intercept values (in each respective world region) will be the mean life expectancy when female literacy rate is 82.03%\nWe will used center FLR for the rest of the lecture"
  },
  {
    "objectID": "slides/11_Interactions_2.html#now-we-refit-the-model-with-the-centered-flr",
    "href": "slides/11_Interactions_2.html#now-we-refit-the-model-with-the-centered-flr",
    "title": "Lesson 12: Interactions Continued",
    "section": "Now we refit the model with the centered FLR",
    "text": "Now we refit the model with the centered FLR\n\n\nm_int_wr_flrc = lm(LifeExpectancyYrs ~ FLR_c*four_regions, \n                data = gapm_sub)\ntidy(m_int_wr_flrc, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n62.387\n1.626\n38.358\n0.000\n59.138\n65.637\n    FLR_c\n0.051\n0.053\n0.957\n0.342\n−0.055\n0.157\n    four_regionsAmericas\n11.032\n2.918\n3.781\n0.000\n5.203\n16.862\n    four_regionsAsia\n7.287\n2.042\n3.568\n0.001\n3.207\n11.367\n    four_regionsEurope\n21.038\n7.698\n2.733\n0.008\n5.659\n36.417\n    FLR_c:four_regionsAmericas\n0.164\n0.197\n0.830\n0.410\n−0.231\n0.558\n    FLR_c:four_regionsAsia\n0.061\n0.073\n0.830\n0.410\n−0.086\n0.208\n    FLR_c:four_regionsEurope\n−0.519\n0.476\n−1.090\n0.280\n−1.471\n0.432\n  \n  \n  \n\n\n\n\n\nWhat changed? What stayed the same? What’s the new intercept for Europe?"
  },
  {
    "objectID": "slides/11_Interactions_2.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "href": "slides/11_Interactions_2.html#interpretation-for-interaction-between-multi-level-categorical-and-continuous-variables",
    "title": "Lesson 12: Interactions Continued",
    "section": "Interpretation for interaction between multi-level categorical and continuous variables",
    "text": "Interpretation for interaction between multi-level categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 FLR + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe}) + \\\\ & \\widehat\\beta_5 FLR \\cdot I(\\text{Americas}) + \\widehat\\beta_6 FLR \\cdot I(\\text{Asia})+ \\widehat\\beta_7 FLR \\cdot I(\\text{Europe}) \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 I(\\text{Americas}) + \\widehat\\beta_3 I(\\text{Asia}) + \\widehat\\beta_4 I(\\text{Europe})\\bigg] + \\\\ &\\underbrace{\\bigg[\\widehat\\beta_1 +  \\widehat\\beta_5 \\cdot I(\\text{Americas}) + \\widehat\\beta_6 \\cdot I(\\text{Asia})+ \\widehat\\beta_7 \\cdot I(\\text{Europe}) \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_5\\) = mean change in female literacy rate’s effect, comparing countries in the Americas to countries in Africa\n\\(\\beta_6\\) = mean change in female literacy rate’s effect, comparing countries in Asia to countries in Africa\n\\(\\beta_7\\) = mean change in female literacy rate’s effect, comparing countries in Europe to countries in Africa\n\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "slides/11_Interactions_2.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "href": "slides/11_Interactions_2.html#test-interaction-between-multi-level-categorical-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nWe run an F-test for a group of coefficients (\\(\\beta_5\\), \\(\\beta_6\\), \\(\\beta_7\\)) in the below model (see lesson 9)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\[\\beta_3=0\\]\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\[\\beta_3\\neq0\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\\\ & \\beta_3 FLR^c \\cdot FS^c + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "href": "slides/11_Interactions_2.html#test-interaction-between-multi-level-categorical-continuous-variables-1",
    "title": "Lesson 11: Interactions Continued",
    "section": "Test interaction between multi-level categorical & continuous variables",
    "text": "Test interaction between multi-level categorical & continuous variables\n\nFit the reduced and full model\n\n\nm_int_fs_red = lm(LifeExpectancyYrs ~ FLR_c + FS_c, \n                   data = gapm_sub)\nm_int_fs_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c +\n                  FLR_c*FS_c, data = gapm_sub)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nanova(m_int_fs_red, m_int_fs_full) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + FS_c\n69.000\n2,005.556\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c * FS_c\n68.000\n2,005.415\n1.000\n0.141\n0.005\n0.945\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and food supply (p = 0.945). Food supply is not an effect modifier of the association between female literacy rate and life expectancy."
  },
  {
    "objectID": "slides/11_Interactions_2.html#do-we-think-food-supply-is-an-effect-modifier-for-female-literacy-rate",
    "href": "slides/11_Interactions_2.html#do-we-think-food-supply-is-an-effect-modifier-for-female-literacy-rate",
    "title": "Lesson 11: Interactions Continued",
    "section": "Do we think food supply is an effect modifier for female literacy rate?",
    "text": "Do we think food supply is an effect modifier for female literacy rate?\n\n\n\nWe can start by visualizing the relationship between life expectancy and female literacy rate by food supply\nQuestions of interest: Does the effect of female literacy rate on life expectancy differ depending on food supply?\n\nThis is the same as: Is food supply is an effect modifier for female literacy rate? Is food supply an effect modifier of the association between life expectancy and female literacy rate?\n\nLet’s run an interaction model to see!"
  },
  {
    "objectID": "slides/11_Interactions_2.html#model-with-interaction-between-two-continuous-variables",
    "href": "slides/11_Interactions_2.html#model-with-interaction-between-two-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "Model with interaction between two continuous variables",
    "text": "Model with interaction between two continuous variables\nModel we are fitting:\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\\(LE\\) as life expectancy\n\\(FLR^c\\) as the centered around the mean female literacy rate (continuous variable)\n\\(FS^c\\) as the centered around the mean food supply (continuous variable)\n\n\n\nCode to center FLR and FS\ngapm_sub = gapm_sub %&gt;% \n  mutate(FLR_c = FemaleLiteracyRate - mean(FemaleLiteracyRate), \n         FS_c = FoodSupplykcPPD - mean(FoodSupplykcPPD))\nmean_FS = mean(gapm_sub$FoodSupplykcPPD) %&gt;% round(digits = 0)\nmean_FLR = mean(gapm_sub$FemaleLiteracyRate) %&gt;% round(digits = 2)\n\n\nIn R:\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c*FS_c, data = gapm_sub)\n\nOR\n\nm_int_fs = lm(LifeExpectancyYrs ~ FLR_c*FS_c, data = gapm_sub)"
  },
  {
    "objectID": "slides/11_Interactions_2.html#comparing-fitted-regression-lines-for-various-food-supply-values",
    "href": "slides/11_Interactions_2.html#comparing-fitted-regression-lines-for-various-food-supply-values",
    "title": "Lesson 11: Interactions Continued",
    "section": "Comparing fitted regression lines for various food supply values",
    "text": "Comparing fitted regression lines for various food supply values\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & 70.32 + 0.16 \\cdot FLR^c + 0.01 \\cdot FS^c - 0.00001 \\cdot FLR^c \\cdot FS^c\n\\end{aligned}\\]\nTo identify different lines, we need to pick example values of Food Supply:\n\n\n\n\nFood Supply of 1812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot (-1000) + \\\\ & \\widehat\\beta_3 FLR^c \\cdot (-1000) \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 - 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 - 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 2812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 0 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 0 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 \\big)+ \\\\ & \\big(\\widehat\\beta_1 \\big) FLR^c\n\\end{aligned}\\]\n\n\n\n\n\nFood Supply of 3812 kcal PPD\n\n\n\\[\\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\\\ & \\widehat\\beta_2 \\cdot 1000 + \\\\ & \\widehat\\beta_3 FLR^c \\cdot 1000 \\\\\n\\widehat{LE} = & \\big(\\widehat\\beta_0 + 1000 \\widehat\\beta_2 \\big)+ \\\\ & \\big(\\widehat\\beta_1 + 1000 \\widehat\\beta_3  \\big) FLR^c\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#poll-everywhere-question",
    "href": "slides/11_Interactions_2.html#poll-everywhere-question",
    "title": "Lesson 11: Interactions Continued",
    "section": "Poll Everywhere Question??",
    "text": "Poll Everywhere Question??"
  },
  {
    "objectID": "slides/11_Interactions_2.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "href": "slides/11_Interactions_2.html#interpretation-for-interaction-between-binary-categorical-and-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "Interpretation for interaction between binary categorical and continuous variables",
    "text": "Interpretation for interaction between binary categorical and continuous variables\n\\[ \\begin{aligned}\n\\widehat{LE} = & \\widehat\\beta_0 + \\widehat\\beta_1 FLR^c + \\widehat\\beta_2 FS^c + \\widehat\\beta_3 FLR^c \\cdot FS^c \\\\\n\\widehat{LE} = & \\bigg[\\widehat\\beta_0 + \\widehat\\beta_2 \\cdot FS^c \\bigg] + \\underbrace{\\bigg[\\widehat\\beta_1 + \\widehat\\beta_3 \\cdot FS^c \\bigg]}_\\text{FLR's effect} FLR \\\\\n\\end{aligned}\\]\n\nInterpretation:\n\n\\(\\beta_3\\) = mean change in female literacy rate’s effect, for every one kcal PPD increase in food supply\n\nIn summary, the interaction term can be interpreted as “difference in adjusted female literacy rate effect for every 1 kcal PPD increase in food supply”\nIt will be helpful to test the interaction to round out this interpretation!!"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-1-testing-the-interaction",
    "href": "slides/11_Interactions_2.html#step-1-testing-the-interaction",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 1: Testing the interaction",
    "text": "Step 1: Testing the interaction\n\nWe test with \\(\\alpha = 0.10\\)\nFollow the F-test procedure in Lesson 9 (MLR: Inference/F-test)\n\nThis means we need to follow the 7 steps of the general F-test in previous slide (taken from Lesson 9)\n\nUse the hypothesis tests for the specific variable combo:\n\n\n\n\n\nBinary & continuous variable (Lesson 11, LOB 2)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full model to reduced model\n\n\n\n\n\nMulti-level & continuous variables (Lesson 11, LOB 3)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\n\n\n\nBinary & multi-level variable (Lesson 11, LOB 4)\n\n\nTesting group of coefficients for the interaction terms using F-test comparing full to reduced model\n\n\n\n\n\nTwo continuous variables (Lesson 11, LOB 5)\n\n\nTesting a single coefficient for the interaction term using F-test comparing full to reduced model"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-testing-a-confounder",
    "href": "slides/11_Interactions_2.html#step-2-testing-a-confounder",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: Testing a confounder",
    "text": "Step 2: Testing a confounder\n\nIf interaction already included:\n\nMeaning: F-test showed evidence for alternative/full model\nThen the variable is an effect modifier and we don’t need to consider it as a confounder\nThen automatically included as main effect (and thus not checked for confounding)\n\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders\nOne way to do this is by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%\n\nIf the main effect of the primary explanatory variable changes by less than 10%, then the additional variable is neither an effect modifier nor a confounder\n\nWe leave the variable out of the model"
  },
  {
    "objectID": "weeks/week_10_sched.html",
    "href": "weeks/week_10_sched.html",
    "title": "Week 10",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #213c96; background-color: #fff; padding: 10px; }\n.description{ font-size: 20px; color: #fff; background-color: #213c96; padding: 10px; } ```"
  },
  {
    "objectID": "weeks/week_10_sched.html#announcements",
    "href": "weeks/week_10_sched.html#announcements",
    "title": "Week 10",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 3/11\n\nCorrection in Model selection slides: larger adjusted R-squared is better!!\nMy office hours will be changed from 11:30am - 1pm to 11am - 12:30pm\n\nOffice hours on 3/14 are 11am - 12:30pm\n\n\n\n\nWednesday 3/13\n\nLast lecture!!!\nQuiz will be returned on Monday\nLab 3 feedback slides\n\nSorry for using “Other” instead of “A different identity” which was exactly what I was saying NOT to do\nI should have looked more closely at the codebook before making those slides\n\nMy Thursday office hours will be changed from 11:30am - 1pm to 11am - 12:30pm\n\nOffice hours on 3/14 are 11am - 12:30pm\n\nLab 4 feedback: aiming to give it back to you by Sunday evening\nProject report instructions are up!\n\nNeed to update rubric\nMay need to alter the figure requirement for the forest plot / coefficient estimate table"
  },
  {
    "objectID": "weeks/week_09_sched.html",
    "href": "weeks/week_09_sched.html",
    "title": "Week 9",
    "section": "",
    "text": "Room Locations for the week\n\n\n\n\nOn Monday, 3/4,        we will be in RLSB 3A003 B\nOn Wednesday, 3/6,  we will be in RLSB 3A003 A"
  },
  {
    "objectID": "weeks/week_09_sched.html#announcements",
    "href": "weeks/week_09_sched.html#announcements",
    "title": "Week 9",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 3/4\n\nMissing a couple mid-term reviews: 32 reviews and only 29 names\nWe’ll go through Lesson 12 on Model selection, then dive into purposeful selection (a model selection strategy)\n\nI will be adding more slides to Lesson 13 Purposeful Selection for Wednesday’s class\n\n\n\n\nWednesday 3/6\n\nIs there going to be a specific strategy/type of model selection we’re supposed to use for the lab/final project? Will that be the focus of Lab 4?\nLab 3: new formatting was confusing for some\n\nI am sorry that I did not communicate the new formatting in multiple areas\nLab 4 will follow the same format\n\nThe course website will contain thorough, guided instructions\nThe file that you will download and edit will only contain the direct tasks that I want you to complete.\n\n\nQuiz 3 info\n\nWill cover Lesson 10 - 11: categorical covariates to interactions\nI’m thinking this one will be a litter shorter since there’s less material\nWill still have the 50 min for the quiz\nIncludes HW 4 and 5\n\nWill include interpretations of interactions!\n\n\nNext week\n\nQuiz\nFinish Purposeful Selection (probably)\nOne more lecture on Diagnostics in MLR\n\nLast week (3/18)\n\nMeeting ONLY on 3/18\nThat class will be fully dedicated to help on the Project report!"
  },
  {
    "objectID": "weeks/week_08_sched.html#announcements",
    "href": "weeks/week_08_sched.html#announcements",
    "title": "Week 8",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 2/26\n\nWednesday class: finish interactions + office hours for lab\nMidterm feedback is still open! Please complete it by the end of this week!!\n\n\n\nWednesday 2/28"
  },
  {
    "objectID": "slides/13_Model_selection.html#model-selection-basics",
    "href": "slides/13_Model_selection.html#model-selection-basics",
    "title": "Lesson 13: Model Selection",
    "section": "Model Selection basics",
    "text": "Model Selection basics\n\n“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nMoore text includes methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches"
  },
  {
    "objectID": "slides/13_Model_selection.html#model-building-and-selection-slide-from-jodi-lapidus",
    "href": "slides/13_Model_selection.html#model-building-and-selection-slide-from-jodi-lapidus",
    "title": "Lesson 13: Model Selection",
    "section": "Model building and selection (slide from Jodi Lapidus)",
    "text": "Model building and selection (slide from Jodi Lapidus)\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size\n\nStrategies include, but are not limited to:\n\nPre-specification of multivariable model\nPurposeful model selection (H/L text)\n\n“Risk factor modeling”\n\nStatistical significance-based “canned” strategies\n\nBackward/forward/stepwise selection\nCommonly taught but NOT commonly used\n\nChange in Estimate (CIE) approaches\n\n\n\n\nModel Selection"
  },
  {
    "objectID": "slides/13_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "href": "slides/13_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size"
  },
  {
    "objectID": "slides/13_Model_selection.html#prediction-vs.-association",
    "href": "slides/13_Model_selection.html#prediction-vs.-association",
    "title": "Lesson 13: Model Selection",
    "section": "Prediction vs. association",
    "text": "Prediction vs. association\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nOnly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals"
  },
  {
    "objectID": "slides/13_Model_selection.html#association-one-variables-effect",
    "href": "slides/13_Model_selection.html#association-one-variables-effect",
    "title": "Lesson 13: Model Selection",
    "section": "Association / One variable’s effect",
    "text": "Association / One variable’s effect\n\nGoal: Understand one variable’s effect on the response after adjusting for other factors\nOnly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model"
  },
  {
    "objectID": "slides/13_Model_selection.html#model-selection-strategies-slide-adjusted-from-jodi-lapidus",
    "href": "slides/13_Model_selection.html#model-selection-strategies-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Model selection strategies (slide adjusted from Jodi Lapidus)",
    "text": "Model selection strategies (slide adjusted from Jodi Lapidus)\n\nPre-specification of multivariable model\nPurposeful model selection (will learn in BSTA 513)\n\n“Risk factor modeling”\n\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\nChange in Estimate (CIE) approaches\n\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nPre-specification of multivariable model\nPurposeful model selection (will learn in BSTA 513)\n\n“Risk factor modeling”\n\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\nChange in Estimate (CIE) approaches\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals"
  },
  {
    "objectID": "slides/13_Model_selection.html#some-model-selection-criteria",
    "href": "slides/13_Model_selection.html#some-model-selection-criteria",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Some model selection criteria",
    "text": "Some model selection criteria"
  },
  {
    "objectID": "slides/13_Model_selection.html#more",
    "href": "slides/13_Model_selection.html#more",
    "title": "Lesson 13: Model Selection",
    "section": "more",
    "text": "more\nMore information on the two analysis goals:\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics"
  },
  {
    "objectID": "slides/13_Model_selection.html#the-goals-of-association-vs.-prediction",
    "href": "slides/13_Model_selection.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Association / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?"
  },
  {
    "objectID": "slides/13_Model_selection.html#model-building-for-association-vs.-prediction",
    "href": "slides/13_Model_selection.html#model-building-for-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "More information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html",
    "href": "slides/Quiz_2_Lab_2.html",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "",
    "text": "Great job!\nJust a few things to review\n\n\n\n\n\n\n\n\n\\[\nSSY = SST = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiggest mistake: not including the hat on SBP!\n\n\n\n\n\n\n\n\n\n\n\nBiggest mistakes\n\nNot adjusting for age\nForgetting units"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#notes-on-interpretationsconsiderations",
    "href": "slides/Quiz_2_Lab_2.html#notes-on-interpretationsconsiderations",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Notes on interpretations/considerations",
    "text": "Notes on interpretations/considerations\n\nRemember that multiple linear regression will adjust for variables outside of our research question!\n\nSo we either adjust with a main effect or an interaction!"
  },
  {
    "objectID": "slides/13_Model_selection.html",
    "href": "slides/13_Model_selection.html",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\nExplain the general process or idea behind different model selection techniques\nKnow the formula for and function for various model fit statistics\n\n\n\n\nFirst, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions\n\n\n\n\n\n\n\nSuppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\n\n\n\n\n\nRecall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html\n\n\n\n\n\n\n\n\n“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size\n\n\n\n\n\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?\n\n\n\n\n\n\n\n\nMore information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics\n\n\n\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "slides/13_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "href": "slides/13_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "First, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions"
  },
  {
    "objectID": "slides/13_Model_selection.html#general-process-for-association",
    "href": "slides/13_Model_selection.html#general-process-for-association",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "General process for association",
    "text": "General process for association\nModel selection methods, for the purpose of association, generally consist of the same essential steps:\n\nConstruct a list of all candidate covariates"
  },
  {
    "objectID": "slides/13_Model_selection.html#bias-variance-trade-off",
    "href": "slides/13_Model_selection.html#bias-variance-trade-off",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Recall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "slides/11_Interactions_2.html#test-interaction-between-two-continuous-variables",
    "href": "slides/11_Interactions_2.html#test-interaction-between-two-continuous-variables",
    "title": "Lesson 11: Interactions Continued",
    "section": "Test interaction between two continuous variables",
    "text": "Test interaction between two continuous variables\n\nWe run an F-test for a single coefficients (\\(\\beta_3\\)) in the below model (see lesson 9)\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\beta_3 FLR^c \\cdot FS^c + \\epsilon\\]\n\n\n\n\n\n\nNull \\(H_0\\)\n\n\n\\[\\beta_3=0\\]\n\n\n\n\n\nAlternative \\(H_1\\)\n\n\n\\[\\beta_3\\neq0\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNull / Smaller / Reduced model\n\n\n\\[ LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\]\n\n\n\n\n\nAlternative / Larger / Full model\n\n\n\\[\\begin{aligned}\nLE = & \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\\\ & \\beta_3 FLR^c \\cdot FS^c + \\epsilon\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#test-interaction-between-two-continuous-variables-1",
    "href": "slides/11_Interactions_2.html#test-interaction-between-two-continuous-variables-1",
    "title": "Lesson 11: Interactions Continued",
    "section": "Test interaction between two continuous variables",
    "text": "Test interaction between two continuous variables\n\nFit the reduced and full model\n\n\nm_int_fs_red = lm(LifeExpectancyYrs ~ FLR_c + FS_c, \n                   data = gapm_sub)\nm_int_fs_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c +\n                  FLR_c*FS_c, data = gapm_sub)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nanova(m_int_fs_red, m_int_fs_full) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FLR_c + FS_c\n69.000\n2,005.556\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FLR_c + FS_c + FLR_c * FS_c\n68.000\n2,005.415\n1.000\n0.141\n0.005\n0.945\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and food supply (p = 0.945). Food supply is not an effect modifier of the association between female literacy rate and life expectancy."
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#how-should-a-table-look",
    "href": "slides/Quiz_2_Lab_2.html#how-should-a-table-look",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "How should a table look?",
    "text": "How should a table look?"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#how-should-a-plot-look",
    "href": "slides/Quiz_2_Lab_2.html#how-should-a-plot-look",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "How should a plot look?",
    "text": "How should a plot look?"
  },
  {
    "objectID": "labs/Lab_03_work.html#quality-control",
    "href": "labs/Lab_03_work.html#quality-control",
    "title": "Lab 3 Instructions",
    "section": "2.2 Quality Control",
    "text": "2.2 Quality Control\nThere are a few more issues with the data that we need to look into. First, there is another coding for NA values in the race variable: -999. We will need to filter out these observations.\nWe will also need to look at individuals who have potentially answered the survey questions untruthfully. We cannot catch everything, but a good place to start is by looking at individuals who have done more than one of the following:\n\nselected the earliest or latest possible birth year\nselected the lowest or highest possible education\nselected all gender identities (for those using gender identity)\nselected all races (for those using multiple selection race)\nselected the lowest or highest weight (for those looking at BMI)\nselected the lowest or highest height (for those looking at BMI)\n\nI want to take a second to mention that any of the above selections, and combinations of the above selections, are valid. However, we should start to flag the possibility that someone has not gone through the survey properly if we notice that most or all of the respondent’s answers are the first answer choice, last answer choice, or selected all options. Additionally, not all of these carry the same importance in discerning validity. For example, a recorded age of 111 years old is the most striking to me. When paired with other selections that are the maximum or minimum (or first or last) option, then I will record it for future investigation. If this observation looks to be an outlier or high leverage point in our analysis, that is when I’ll decide to remove it.\n\n\n\n\n\n\nTasks for 1.1\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page."
  },
  {
    "objectID": "labs/Lab_03_work.html#purpose",
    "href": "labs/Lab_03_work.html#purpose",
    "title": "Lab 3 Instructions",
    "section": "1.1 Purpose",
    "text": "1.1 Purpose\nThe main purpose of this lab is to perform some quality control on our data, recode some of the multi-selection categorical variables, continue data exploration, and start analyzing the main relationship of our research question."
  },
  {
    "objectID": "labs/Lab_03_work.html#grading",
    "href": "labs/Lab_03_work.html#grading",
    "title": "Lab 3 Instructions",
    "section": "1.2 Grading",
    "text": "1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nSome tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like target population, choosing variables, revisiting research question)"
  },
  {
    "objectID": "labs/Lab_03_work.html#restate-your-research-question",
    "href": "labs/Lab_03_work.html#restate-your-research-question",
    "title": "Lab 3 Instructions",
    "section": "2.1 0. Restate your research question",
    "text": "2.1 0. Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the following format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?"
  },
  {
    "objectID": "labs/Lab_03_work.html#working-with-multi-selection-variables",
    "href": "labs/Lab_03_work.html#working-with-multi-selection-variables",
    "title": "Lab 3 Instructions",
    "section": "2.3 Working with multi-selection variables",
    "text": "2.3 Working with multi-selection variables\nIn the list of variables that we may choose to work with (in Lab 2), there are two that allowed respondents to select multiple categories. The two variables are genderIdentitiy and raceombmulti. If you did not choose these variables to work with, you may skip this section.\nIf you chose one or both of these variables, then we need to make new variables that correspond to indicators for each possible selection in the respective variable.\nLet’s start with the grepl function. For this function, we can input one of our column names and a value, then it will output, for each row, if the value is in the column. For example, in genderIdentity an individual may identify as a “Trans female/Trans woman” and “Gender queer/Gender nonconforming.” In our dataset in R, this would show as [4,5] in genderIdentity. If we want to create two separate indicators for anyone who identifies as “Trans female/Trans woman” then I need to look for the value 4 in the column genderIdentity. I will run a separate indicator to find individuals who identify as “Gender queer/Gender nonconforming.” Here is an example code of how I would use grepl to do this:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_tf_tw = grepl(4, genderIdentity), \n         ind_gq_gnc = grepl(5, genderIdentity))\n\nYou will need to extend this to all other gender identities.\nFor race, raceombmulti is also the follow up question to raceomb_002. So our indicators need to reflect both variables. In this case, we need to use grepl on both columns at once. For example, if I want to create an indicator for individuals who identify as American Indian/Alaskan Native then I need to find individuals who identify as American Indian/Alaskan Native only and individuals who identify as American Indian/Alaskan Native in addition to another race. For example, my code might look like:\n\niat_prep_new = iat_prep_old %&gt;%\n  mutate(ind_AIAN = grepl(1, raceomb_002) | grepl(1, raceombmulti))\n\nI suggest only searching for 1-7 in both raceomb_002 and raceombmulti. Note that if raceomb_002 = 8 , then individuals identified as “multiracial” and will select values in raceombmulti.\n\n\n\n\n\n\nTask for 1.2\n\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection."
  },
  {
    "objectID": "labs/Lab_03_work.html#continuing-data-exploration",
    "href": "labs/Lab_03_work.html#continuing-data-exploration",
    "title": "Lab 3 Instructions",
    "section": "2.4 Continuing data exploration",
    "text": "2.4 Continuing data exploration\n\n2.4.1 2.1\nIn this section\n\nLook at all other relationships between IAT score and each covariate.\n\nFor categorical variables, is there an inherent order? Does the ordered values follow a linear relationship? Are the categories evenly spaced? Think education - is there a natural place to divide the categories up?? multivariate data exploration"
  },
  {
    "objectID": "labs/Lab_03_work.html#make-a-table-1",
    "href": "labs/Lab_03_work.html#make-a-table-1",
    "title": "Lab 3 Instructions",
    "section": "2.5 Make a Table 1",
    "text": "2.5 Make a Table 1"
  },
  {
    "objectID": "labs/Lab_03_work.html#fit-the-simple-linear-regression",
    "href": "labs/Lab_03_work.html#fit-the-simple-linear-regression",
    "title": "Lab 3 Instructions",
    "section": "2.6 Fit the simple linear regression",
    "text": "2.6 Fit the simple linear regression"
  },
  {
    "objectID": "labs/Lab_03_work.html#bibliography",
    "href": "labs/Lab_03_work.html#bibliography",
    "title": "Lab 3 Instructions",
    "section": "3 Bibliography",
    "text": "3 Bibliography\nRedpath, F. (2023). Abolish the Body Mass Index: A Historical and Current Analysis of the Traumatizing Nature of the BMI. Tapestries:  Interwoven Voices of Local and Global Identities, 12(1). https://digitalcommons.macalester.edu/tapestries/vol12/iss1/12"
  },
  {
    "objectID": "labs/Lab_03.html",
    "href": "labs/Lab_03.html",
    "title": "Lab 3",
    "section": "",
    "text": "# PLEASE DO NOT REMOVE THIS CODE CHUNK!!!\n### ADD YOUR LIBRARIES HERE!!! ####\n\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(here)"
  },
  {
    "objectID": "labs/Lab_03.html#directions",
    "href": "labs/Lab_03.html#directions",
    "title": "Lab 3",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThis is your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.1.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning (like Section 2.4 and explanations in Section 2.5)"
  },
  {
    "objectID": "labs/Lab_03.html#lab-activities",
    "href": "labs/Lab_03.html#lab-activities",
    "title": "Lab 3",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\n\n\n2.2 Quality Control\n\n\n\n\n\n\nTasks\n\n\n\n\nFilter out observations with a value of -999 in the race variable.\nGlimpse at the observations that may indicate a respondent who has not properly completed the survey portion. This will require filtering for specific answer choices. Please see examples of filter() on it’s documentation page.\n\n\n\n\n\n2.3 Working with multi-selection variables\n\n\n\n\n\n\nTask\n\n\n\nIf you are using genderIdentity or raceombmulti, create indicator variables for each possible selection.\n\n\n\n\n2.4 Thinking about potential confounders and effect modifiers\n\n\n\n\n\n\nTask\n\n\n\nFor each variable, consider how each could alter the relationship between IAT score and your variable of interest (from your research question). For each covariate, explain how it might or might not change the relationship.\n\n\n\n\n2.5 Continuing data exploration\n\n2.5.1 Bivariate exploration\n\n\n\n\n\n\nTask\n\n\n\nFor each variable outside of your research question, create the appropriate plot to visualize the relationship between IAT score and the variable. Comment if there is an obvious trend or not.\n\n\n\n\n2.5.2 Multivariate exploration\n\n\n\n\n\n\nTask\n\n\n\nFor at least 3 variables outside of your research question, create the appropriate plot to visualize the relationship between IAT score, your main variable (in research question), and the variable outside your research question. Comment whether you can determine anything from the plot or not. If you can, is there any indication that the variable is a confounder or effect modifier?\n\n\n\n\n\n2.6 Fit a simple linear regression\n\n\n\n\n\n\nTask\n\n\n\nRun a simple linear regression model for the relationship in your primary research question. Print the regression table. Interpret the results and comment on the initial trend you see.\n\n\n\n\n\n\n\n\nBonus Task\n\n\n\nThis is not required in Lab 3. However, if you want to run a multiple linear regression model with one other variable that you plotted in Multivariate Exploration, then you should try it! Do the results align with your ideas in Section 2.4 and/or the visualization you saw in Section 2.5?"
  },
  {
    "objectID": "slides/11_Interactions_2.html#reminder-from-lesson-9-general-steps-for-f-test",
    "href": "slides/11_Interactions_2.html#reminder-from-lesson-9-general-steps-for-f-test",
    "title": "Lesson 11: Interactions Continued",
    "section": "Reminder from Lesson 9: General steps for F-test",
    "text": "Reminder from Lesson 9: General steps for F-test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level."
  },
  {
    "objectID": "slides/11_Interactions_2.html#poll-everywhere-question-1",
    "href": "slides/11_Interactions_2.html#poll-everywhere-question-1",
    "title": "Lesson 11: Interactions Continued",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question\nWhat are other options for combinations of variables that can have an interaction? Please write your answer in the format like “continuous and continuous”\nFor two binary variables, how many coefficients do we need to test for an interaction? 1\nFor two multi-level categorical variables that have 3 and 4 categories, respectively, how many coefficients do we need to test for an interaction?"
  },
  {
    "objectID": "slides/11_Interactions_2.html#lets-try-this-out-on-one-of-our-potential-effect-modifiers-or-confounders",
    "href": "slides/11_Interactions_2.html#lets-try-this-out-on-one-of-our-potential-effect-modifiers-or-confounders",
    "title": "Lesson 11: Interactions Continued",
    "section": "Let’s try this out on one of our potential effect modifiers or confounders",
    "text": "Let’s try this out on one of our potential effect modifiers or confounders\n\n\n\nLook back at income level and world region: is income level an effect modifier, confounder, or has no effect on the association between life expectancy and world region?\nWe can start by visualizing the relationship between life expectancy and world region by income level\nSo we’ll need to revisit the work we did in previous slides on the interaction, then check fo condounding"
  },
  {
    "objectID": "slides/11_Interactions_2.html#testing-for-percent-change-delta-in-a-coefficient",
    "href": "slides/11_Interactions_2.html#testing-for-percent-change-delta-in-a-coefficient",
    "title": "Lesson 11: Interactions Continued",
    "section": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient",
    "text": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient\n\nLet’s say we have \\(X_1\\) and \\(X_2\\), and we specifically want to see if \\(X_2\\) is a confounder for \\(X_1\\) (the explanatory variable or variable of interest)\nIf we are only considering \\(X_1\\) and \\(X_2\\), then we need to run the following two models:\n\nFitted model 1 / reduced model (mod1): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1\\)\n\nWe call the above \\(\\widehat\\beta_1\\) the reduced model coefficient: \\(\\widehat\\beta_{1, \\text{mod1}}\\) or \\(\\widehat\\beta_{1, \\text{red}}\\)\n\nFitted model 2 / Full model (mod2): \\(\\widehat{Y} = \\widehat\\beta_0 + \\widehat\\beta_1X_1 +\\widehat\\beta_2X_2\\)\n\nWe call this \\(\\widehat\\beta_1\\) the full model coefficient: \\(\\widehat\\beta_{1, \\text{mod2}}\\) or \\(\\widehat\\beta_{1, \\text{full}}\\)\n\n\n\n\n\n\n\n\n\nCalculation for % change in coefficient\n\n\n\\[\n\\Delta\\% = 100\\% \\cdot\\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{red}} - \\widehat\\beta_{1, \\text{full}}}{\\widehat\\beta_{1, \\text{full}}}\n\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate",
    "href": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate?",
    "text": "Is food supply a confounder for female literacy rate?\n\nRun model with and without food supply:\n\n\nmod1_red = lm(LifeExpectancyYrs ~ FLR_c, data = gapm_sub)\nmod2_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c, data = gapm_sub)\n\n\nNote that the full model when testing for confounding was the reduced model for testing an interaction\nFull and reduced are always relative qualifiers of the models that we are testing\n\n\nRecord the coefficient estimate for centered female literacy rate in both:\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.72578\n96.85709\n0.00000\n68.84969\n71.74475\n    FLR_c\n0.22990\n0.03219\n7.14139\n0.00000\n0.16570\n0.29411\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.63537\n110.63985\n0.00000\n69.02969\n71.56475\n    FLR_c\n0.15670\n0.03216\n4.87271\n0.00001\n0.09254\n0.22085\n    FS_c\n0.00848\n0.00179\n4.72646\n0.00001\n0.00490\n0.01206\n  \n  \n  \n\n\n\n\n\nCalculate the percent change:\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{0.22990 - 0.15670}{0.15670} = 46.71\n\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-12",
    "href": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-12",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (1/2)",
    "text": "Is food supply a confounder for female literacy rate? (1/2)\n\nRun model with and without food supply:\n\n\nModel 1 (reduced): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\epsilon\\)\nModel 2 (full): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\)\n\n \n\nmod1_red = lm(LifeExpectancyYrs ~ FLR_c, data = gapm_sub)\nmod2_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c, data = gapm_sub)\n\n \n\nNote that the full model when testing for confounding was the reduced model for testing an interaction\nFull and reduced are always relative qualifiers of the models that we are testing"
  },
  {
    "objectID": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-22",
    "href": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-22",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (2/2)",
    "text": "Is food supply a confounder for female literacy rate? (2/2)\n\nRecord the coefficient estimate for centered female literacy rate in both:\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.72578\n96.85709\n0.00000\n68.84969\n71.74475\n    FLR_c\n0.22990\n0.03219\n7.14139\n0.00000\n0.16570\n0.29411\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.63537\n110.63985\n0.00000\n69.02969\n71.56475\n    FLR_c\n0.15670\n0.03216\n4.87271\n0.00001\n0.09254\n0.22085\n    FS_c\n0.00848\n0.00179\n4.72646\n0.00001\n0.00490\n0.01206\n  \n  \n  \n\n\n\n\n\nCalculate the percent change:\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{0.22990 - 0.15670}{0.15670} = 46.71\n\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-13",
    "href": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-13",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (1/3)",
    "text": "Is food supply a confounder for female literacy rate? (1/3)\n\nRun models with and without food supply:\n \n\nModel 1 (reduced): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\epsilon\\)\n\n\nmod1_red = lm(LifeExpectancyYrs ~ FLR_c, data = gapm_sub)\n\n \n\nModel 2 (full): \\(LE = \\beta_0 + \\beta_1 FLR^c + \\beta_2 FS^c + \\epsilon\\)\n\n\nmod2_full = lm(LifeExpectancyYrs ~ FLR_c + FS_c, data = gapm_sub)\n\n\n \n\nNote that the full model when testing for confounding was the reduced model for testing an interaction\nFull and reduced are always relative qualifiers of the models that we are testing"
  },
  {
    "objectID": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-23",
    "href": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-23",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (2/3)",
    "text": "Is food supply a confounder for female literacy rate? (2/3)\n\nRecord the coefficient estimate for centered female literacy rate in both models:\n\n\nModel 1 (reduced):\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.72578\n96.85709\n0.00000\n68.84969\n71.74475\n    FLR_c\n0.22990\n0.03219\n7.14139\n0.00000\n0.16570\n0.29411\n  \n  \n  \n\n\n\n\n\nModel 2 (full):\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n70.29722\n0.63537\n110.63985\n0.00000\n69.02969\n71.56475\n    FLR_c\n0.15670\n0.03216\n4.87271\n0.00001\n0.09254\n0.22085\n    FS_c\n0.00848\n0.00179\n4.72646\n0.00001\n0.00490\n0.01206\n  \n  \n  \n\n\n\n\n\nCalculate the percent change:\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{0.22990 - 0.15670}{0.15670} = 46.71\\%\n\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#if-you-want-extra-practice",
    "href": "slides/11_Interactions_2.html#if-you-want-extra-practice",
    "title": "Lesson 11: Interactions Continued",
    "section": "If you want extra practice",
    "text": "If you want extra practice\n\nTry out this procedure to determine if a variable is an effect modifier or confounder or nothing on the other interactions we tested out in Lesson 11"
  },
  {
    "objectID": "slides/11_Interactions_2.html#process",
    "href": "slides/11_Interactions_2.html#process",
    "title": "Lesson 11: Interactions Continued",
    "section": "Process",
    "text": "Process\n\nStep 1: Testing the interaction/effect modifier\n\nCompare model with and without interaction using F-test to see if interaction is significant (aka income is an effect modifier)\nModels\n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\\\& \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\\\& \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})++ \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\\\ & \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\)\n\n\nStep 2: Testing a confounder (only if not an effect modifier)\n\nCompare model with and without main effect for additional variable (income level) using F-test to see if additional variable (income level) is a confounder\nModels\n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\\\& \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\epsilon \\end{aligned}\\)\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\\\& \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "slides/11_Interactions_2.html#results-from-step-1-in-lesson-11-lob-4",
    "href": "slides/11_Interactions_2.html#results-from-step-1-in-lesson-11-lob-4",
    "title": "Lesson 11: Interactions Continued",
    "section": "Results from step 1 in Lesson 11 LOB 4",
    "text": "Results from step 1 in Lesson 11 LOB 4\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between female literacy rate and income level (p = 0.928)."
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-since-not-an-effect-modifier",
    "href": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-since-not-an-effect-modifier",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder (since not an effect modifier)",
    "text": "Step 2: See if income is a confounder (since not an effect modifier)\n\nFit the reduced and full model\n\n\nmod1_wr_inc_red = lm(LifeExpectancyYrs ~ four_regions, \n                   data = gapm_sub)\nmod1_wr_inc_full = lm(LifeExpectancyYrs ~ four_regions + income_levels2, \n                   data = gapm_sub)"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-1-results-from-lesson-11-lob-4",
    "href": "slides/11_Interactions_2.html#step-1-results-from-lesson-11-lob-4",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 1: Results from Lesson 11 LOB 4",
    "text": "Step 1: Results from Lesson 11 LOB 4\n\nFit the reduced and full model\n\n\nm_int_wr_inc_red = lm(LifeExpectancyYrs ~ income_levels2 + four_regions, \n                   data = gapm_sub)\nm_int_wr_inc_full = lm(LifeExpectancyYrs ~ income_levels2 + four_regions +\n                          income_levels2*four_regions, data = gapm_sub)\n\n\nDisplay the ANOVA table with F-statistic and p-value\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ income_levels2 + four_regions\n67.000\n1,693.242\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ income_levels2 + four_regions + income_levels2 * four_regions\n64.000\n1,681.304\n3.000\n11.938\n0.151\n0.928\n  \n  \n  \n\n\n\n\n\nConclusion: There is not a significant interaction between world region and income level (p = 0.928).\nThus, income level is not an effect modifier of world region. However, we can continue to test if income level is a confounder."
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-since-not-an-effect-modifier-1",
    "href": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-since-not-an-effect-modifier-1",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder (since not an effect modifier)",
    "text": "Step 2: See if income is a confounder (since not an effect modifier)\n\nRecord the coefficient estimate for centered female literacy rate in both models:\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.27000\n1.16508\n52.58870\n0.00000\n58.94512\n63.59488\n    four_regionsAmericas\n14.33000\n1.90257\n7.53193\n0.00000\n10.53349\n18.12651\n    four_regionsAsia\n8.11824\n1.71883\n4.72313\n0.00001\n4.68837\n11.54810\n    four_regionsEurope\n14.78217\n1.59304\n9.27924\n0.00000\n11.60332\n17.96103\n  \n  \n  \n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.54716\n1.16190\n52.11048\n0.00000\n58.22800\n62.86632\n    four_regionsAmericas\n12.04102\n2.05816\n5.85038\n0.00000\n7.93292\n16.14912\n    four_regionsAsia\n7.77808\n1.66414\n4.67394\n0.00001\n4.45645\n11.09971\n    four_regionsEurope\n12.51938\n1.79139\n6.98864\n0.00000\n8.94375\n16.09501\n    income_levels2Higher income\n3.61419\n1.46967\n2.45917\n0.01651\n0.68070\n6.54767"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-since-not-an-effect-modifier-2",
    "href": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-since-not-an-effect-modifier-2",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder (since not an effect modifier)",
    "text": "Step 2: See if income is a confounder (since not an effect modifier)\n\nCalculate the percent change for \\(\\widehat\\beta_1\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{14.33000 - 12.04102}{12.04102} = 19.01\n\\]\nCalculate the percent change for \\(\\widehat\\beta_2\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{2, \\text{mod1}} - \\widehat\\beta_{2, \\text{mod2}}}{\\widehat\\beta_{2, \\text{mod2}}} = 100\\% \\cdot \\frac{8.11824 - 7.77808}{7.77808} = 4.37\n\\]\nCalculate the percent change for \\(\\widehat\\beta_3\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{3, \\text{mod1}} - \\widehat\\beta_{3, \\text{mod2}}}{\\widehat\\beta_{3, \\text{mod2}}} = 100\\% \\cdot \\frac{14.78217 - 12.51938}{12.51938} = 18.07\n\\]"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder",
    "href": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nFit the reduced and full model for testing the confounder\n\n \n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\epsilon \\end{aligned}\\)\n\n\nmod1_wr_inc_red = lm(LifeExpectancyYrs ~ four_regions, \n                   data = gapm_sub)\n\n \n\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)\n\n\nmod1_wr_inc_full = lm(LifeExpectancyYrs ~ four_regions + income_levels2, \n                   data = gapm_sub)"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-1",
    "href": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-1",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nRecord the coefficient estimate for centered female literacy rate in both models:\nModel 1 (reduced):\\(\\begin{aligned}\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{Americas}) + \\widehat\\beta_2 I(\\text{Asia}) + \\widehat\\beta_3 I(\\text{Europe}) \\end{aligned}\\)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n61.27000\n1.16508\n52.58870\n0.00000\n58.94512\n63.59488\n    four_regionsAmericas\n14.33000\n1.90257\n7.53193\n0.00000\n10.53349\n18.12651\n    four_regionsAsia\n8.11824\n1.71883\n4.72313\n0.00001\n4.68837\n11.54810\n    four_regionsEurope\n14.78217\n1.59304\n9.27924\n0.00000\n11.60332\n17.96103\n  \n  \n  \n\n\n\n\n\nModel 2 (full): \\(\\begin{aligned}\\widehat{LE} = &\\widehat\\beta_0 + \\widehat\\beta_1 I(\\text{Americas}) + \\widehat\\beta_2 I(\\text{Asia}) + \\widehat\\beta_3 I(\\text{Europe}) + \\widehat\\beta_4 I(\\text{high income})\\end{aligned}\\)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n60.54716\n1.16190\n52.11048\n0.00000\n58.22800\n62.86632\n    four_regionsAmericas\n12.04102\n2.05816\n5.85038\n0.00000\n7.93292\n16.14912\n    four_regionsAsia\n7.77808\n1.66414\n4.67394\n0.00001\n4.45645\n11.09971\n    four_regionsEurope\n12.51938\n1.79139\n6.98864\n0.00000\n8.94375\n16.09501\n    income_levels2Higher income\n3.61419\n1.46967\n2.45917\n0.01651\n0.68070\n6.54767"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-2",
    "href": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-2",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\nCalculate the percent change for \\(\\widehat\\beta_1\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{14.33000 - 12.04102}{12.04102} = 19.01\n\\]\nCalculate the percent change for \\(\\widehat\\beta_2\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{2, \\text{mod1}} - \\widehat\\beta_{2, \\text{mod2}}}{\\widehat\\beta_{2, \\text{mod2}}} = 100\\% \\cdot \\frac{8.11824 - 7.77808}{7.77808} = 4.37\n\\]\nCalculate the percent change for \\(\\widehat\\beta_3\\): \\[\n\\Delta\\%  = 100\\% \\cdot \\frac{\\widehat\\beta_{3, \\text{mod1}} - \\widehat\\beta_{3, \\text{mod2}}}{\\widehat\\beta_{3, \\text{mod2}}} = 100\\% \\cdot \\frac{14.78217 - 12.51938}{12.51938} = 18.07\n\\]\nNote that two of these % changes are greater than 10%, and one is less than 10%…"
  },
  {
    "objectID": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-3",
    "href": "slides/11_Interactions_2.html#step-2-see-if-income-is-a-confounder-3",
    "title": "Lesson 11: Interactions Continued",
    "section": "Step 2: See if income is a confounder",
    "text": "Step 2: See if income is a confounder\n\n\n\nThere is no set rule when we have more than one estimated coefficient that we examine for confoundeing\nIn this, I would consider\n\nThe majority of coefficients (2/3 coefficients) changes more than 10%\nThe change in coefficients for all three are in the same direction\nThe plot of life expectancy vs world region by income level have a shift in mean life expectancy from lower to higher income level\n\nThus, I would conclude that income level is a confounder, so we would leave income level’s main effect in the model"
  },
  {
    "objectID": "slides/12_In_class_activities.html#example",
    "href": "slides/12_In_class_activities.html#example",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Example",
    "text": "Example\n\nThe following example comes from this textbook\n\n\nSuppose that we are conducting an observational study of adults to assess whether physical activity level (PAL) is associated with systolic blood pressure (SBP), accounting (i.e., controlling) for AGE and SAB (sex assigned at birth). The extraneous variable here is AGE and SAB, while the explanatory variable (variable of interest) is PAL. We need to determine whether we can ignore AGE and/or SAB in our analysis and still correctly assess the PAL–SBP association."
  },
  {
    "objectID": "slides/11_Interactions_2.html#poll-everywhere-questions",
    "href": "slides/11_Interactions_2.html#poll-everywhere-questions",
    "title": "Lesson 11: Interactions Continued",
    "section": "Poll Everywhere Questions",
    "text": "Poll Everywhere Questions\nWhat are other options for combinations of variables that can have an interaction? Please write your answer in the format like “continuous and continuous”\nFor two binary variables, how many coefficients do we need to test for an interaction? 1\nFor two multi-level categorical variables that have 3 and 4 categories, respectively, how many coefficients do we need to test for an interaction?"
  },
  {
    "objectID": "slides/11_Interactions_2.html#poll-everywhere-questions-2-4",
    "href": "slides/11_Interactions_2.html#poll-everywhere-questions-2-4",
    "title": "Lesson 11: Interactions Continued",
    "section": "Poll Everywhere Questions 2-4",
    "text": "Poll Everywhere Questions 2-4"
  },
  {
    "objectID": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-33",
    "href": "slides/11_Interactions_2.html#is-food-supply-a-confounder-for-female-literacy-rate-33",
    "title": "Lesson 11: Interactions Continued",
    "section": "Is food supply a confounder for female literacy rate? (3/3)",
    "text": "Is food supply a confounder for female literacy rate? (3/3)\nThe percent change in female literacy rate’s coefficient estimate was 46.71%.\nThus, food supply is a confounder of female literacy rate in the association between life expectancy and female literacy rate."
  },
  {
    "objectID": "slides/11_Interactions_2.html#determining-if-income-level-is-an-effect-modifier-confounder-or-neither",
    "href": "slides/11_Interactions_2.html#determining-if-income-level-is-an-effect-modifier-confounder-or-neither",
    "title": "Lesson 11: Interactions Continued",
    "section": "Determining if income level is an effect modifier, confounder, or neither",
    "text": "Determining if income level is an effect modifier, confounder, or neither\n\nStep 1: Testing the interaction/effect modifier\n\nCompare model with and without interaction using F-test to see if interaction is significant\nModels\n\nModel 1 (red): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)\n\n \n\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\\\ & \\beta_5 \\cdot I(\\text{high income}) \\cdot I(\\text{Americas}) + \\beta_6\\cdot I(\\text{high income}) \\cdot I(\\text{Asia})+ \\\\ & \\beta_7 \\cdot I(\\text{high income})\\cdot I(\\text{Europe})+ \\epsilon \\end{aligned}\\)\n\n\nStep 2: Testing a confounder (only if not an effect modifier)\n\nCompare model with and without main effect for additional variable (income level) using F-test to see if additional variable (income level) is a confounder\nModels\n\nModel 1 (reduced): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\epsilon \\end{aligned}\\)\nModel 2 (full): \\(\\begin{aligned}LE = &\\beta_0 + \\beta_1 I(\\text{Americas}) + \\beta_2 I(\\text{Asia}) + \\beta_3 I(\\text{Europe}) + \\beta_4 I(\\text{high income})+ \\epsilon \\end{aligned}\\)"
  },
  {
    "objectID": "slides/12_In_class_activities.html#explore-the-data-first-thing-to-do",
    "href": "slides/12_In_class_activities.html#explore-the-data-first-thing-to-do",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Explore the data: First thing to do",
    "text": "Explore the data: First thing to do\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Selection"
  },
  {
    "objectID": "slides/12_In_class_activities.html#example-1-palmer-penguins",
    "href": "slides/12_In_class_activities.html#example-1-palmer-penguins",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Example 1: Palmer Penguins",
    "text": "Example 1: Palmer Penguins\n\nRevisit the Palmer Penguins dataset that we say in HW 4"
  },
  {
    "objectID": "slides/12_In_class_activities.html#lets-take-a-look-at-the-variables",
    "href": "slides/12_In_class_activities.html#lets-take-a-look-at-the-variables",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Let’s take a look at the variables",
    "text": "Let’s take a look at the variables\n\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "slides/12_In_class_activities.html#questions-we-can-ask-and-answer-so-far",
    "href": "slides/12_In_class_activities.html#questions-we-can-ask-and-answer-so-far",
    "title": "Lesson 12: In-class exercise!!",
    "section": "Questions we can ask and answer so far…",
    "text": "Questions we can ask and answer so far…\n\nUsing SLR, does each variable predict flipper length significantly?"
  },
  {
    "objectID": "homework/HW5.html#question-1",
    "href": "homework/HW5.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\nWe are going to revisit the Palmer Penguins dataset from Homework 4. Choosing what to test, interpretations of coefficients, F-test conclusions, and interactions\nFor this problem we will be using the penguins dataset from the palmerpenguins R package. We will look at the association between flipper length of penguins (measured in mm) and specific species of penguins.\nDescription from help file:\n\nIncludes measurements for penguin species, island in Palmer Archipelago, size (flipper length, body mass, bill dimensions), and sex.\n\nMore info about the data.\n\n# first install the palmerpenguins package\n# install.packages(\"palmerpenguins\")\nlibrary(palmerpenguins)\ndata(penguins)\n\n# run the command below to learn more about the variables in the penguins dataset\n# ?penguins\n\n\nPart a\nMake a plot of flipper length (outcome) and body mass (explanatory variable). Discuss what you see in the plot.\n\n\nPart b\nWrite the simple linear regression model that we will fit for the association between body mass and flipper length. If you use any short hand, please write it out. For example: Let \\(BD\\) represent bill depth.\n\n\nPart c\nRun the simple linear regression model for the association between body mass and flipper length. Display the regression table output.\n\n\nPart d\nInterpret the coefficient for body mass. Note that as we move forward with a multivariate model, we will refer to this is estimate at the the crude or unadjusted coefficient estimate.\n\n\nPart e\nDiscuss how centering body mass might help with interpretability. Then, center body mass around the mean, run the model again, and display the regression table. Does the intercept and/or slope change from Part c?\n\n\nPart f\nMake a plot of flipper length (outcome) and body mass (explanatory variable) by bill depth. Discuss what you see in the plot. (Hint: bill depth will be the color in the plot.)\n\n\nPart g\nMake a plot of flipper length (outcome) and body mass (explanatory variable) by penguin species. Discuss what you see in the plot and relate it back to the plot in Part f.\n\n\nPart h\nUsing only body mass and bill depth as covariates, write out the model that we would fit including the main effects of body mass and bill depth and their interaction. How many coefficients are tested when we test for a significant interaction?\n\n\n\n\n\n\nNote\n\n\n\nBoth covariates should be centered. For the rest of the homework, we will use the centered body mass and bill depth.\n\n\n\n\nPart i\nCenter bill depth.\n\n\nPart j\nUsing only body mass and bill depth as covariates, test if bill depth is an effect modifier or confounder of body mass, or if it is not in the model at all.\n\n\nPart k\nUsing only body mass and species as covariates, write out the model that we would fit including the main effects of body mass and species and their interaction. How many coefficients are tested when we test for a significant interaction?\nHint: Homework 4 can help guide us with the species’ categories.\n\n\nPart l\nUsing only body mass and species as covariates, test if species is an effect modifier or confounder of body mass, or if it is not in the model at all. Note that\n\n\nPart m\nUsing the results in the above parts, we will move forward with the following model:\n\\[\\begin{aligned}\nFL = & \\beta_0 + \\beta_1 BM^c + \\beta_2 BD^c +  \\beta_3 I(\\textrm{Chinstrap}) + \\beta_4 I(\\textrm{Gentoo}) +  \\\\ & \\beta_5 BM^c \\cdot I(\\textrm{Chinstrap}) + \\beta_6 BM^c \\cdot I(\\textrm{Gentoo}) + \\epsilon\n\\end{aligned}\\]\nRun the above model and display the regression table output.\nPlease note that this is not exactly the best method for selecting a model. I just wanted to step us through a similar thought process.\n\n\nPart n\nInterpret each coefficient in the model above. There should be 7 total interpretations."
  },
  {
    "objectID": "slides/13_Model_selection.html#stepwise-selection",
    "href": "slides/13_Model_selection.html#stepwise-selection",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Stepwise selection",
    "text": "Stepwise selection"
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Lessons",
    "section": "",
    "text": "Lesson\nTopic\nSlides\nAnnotated Slides\nRecording(s)\n\n\n\n\n\nIntro\n\n\n\n\n\n1\nReview\n\n\n\n\n\n\n2\nData Management\n\n\n\n\n\n3\nSimple Linear Regression\n\n\n\n\n\n\n4\nSLR: Inference and Prediction\n\n\n\n\n\n\n\n5\nSLR: More Inference\n\n\n\n\n\n6\nSLR: Model Diagnostics 1\n\n\n\n\n\n7\nSLR: Model Diagnostics 2\n\n\n\n\n\n8\nIntroduction to Multiple Linear Regression\n\n\n\n\n\n9\nMLR: Inference\n\n\n\n\n\n10\nCategorical Covariates\n\n\n\n\n\n\n11.1\nInteractions\n\n\n\n\n\n\n\n11.2\nInteractions continued\n\n\n\n\n\n\n12\nModel selection 1\n\n\n\n\n\n\n13\nPurposeful Model Selection\n\n\n\n\n\n\n\n14\nMLR Diagnostics"
  },
  {
    "objectID": "slides/13_Model_selection.html#words-to-know",
    "href": "slides/13_Model_selection.html#words-to-know",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Words to know?",
    "text": "Words to know?\n\n\nModel Selection"
  },
  {
    "objectID": "homework/HW5.html#part-m-1",
    "href": "homework/HW5.html#part-m-1",
    "title": "Homework 5",
    "section": "Part m",
    "text": "Part m\nInterpret each coefficient in the model above. There should be 7 total interpretations."
  },
  {
    "objectID": "slides/13_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "href": "slides/13_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Association / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "slides/13_Model_selection.html#model-selection-strategies-for-other-types-of-outcomes",
    "href": "slides/13_Model_selection.html#model-selection-strategies-for-other-types-of-outcomes",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Model selection strategies for other types of outcomes",
    "text": "Model selection strategies for other types of outcomes"
  },
  {
    "objectID": "slides/13_Model_selection.html#lets-quickly-discuss-some-of-the-strategies",
    "href": "slides/13_Model_selection.html#lets-quickly-discuss-some-of-the-strategies",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Let’s quickly discuss some of the strategies",
    "text": "Let’s quickly discuss some of the strategies"
  },
  {
    "objectID": "slides/13_Model_selection.html#pre-specification-of-multivariable-model",
    "href": "slides/13_Model_selection.html#pre-specification-of-multivariable-model",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Pre-specification of multivariable model",
    "text": "Pre-specification of multivariable model\n\nIn a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts.\nIf we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance.\n\nExample: ACTG320 data comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\nVariables such as study site, as well as any randomization stratification variables are common covariates.\n\nIn these cases, only a limited number of multivariable models are fit and reported Do not perform all the model building steps outlined in Hosmer and Lemeshow texts."
  },
  {
    "objectID": "slides/13_Model_selection.html#change-in-estimate-cie-approach",
    "href": "slides/13_Model_selection.html#change-in-estimate-cie-approach",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Change in estimate (CIE) approach",
    "text": "Change in estimate (CIE) approach\n\nCIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\nObserved change is presumed to measure confounding by the covariate.\n\nWhat estimate?\n\nH/L text suggest using coefficients from the model\n\nWhat magnitude change is ”important”?\n\nH/L text suggest 10%\n\nOne must choose an effect measure to judge change importance, where “importance” needs to be evaluated along a contextually meaningful scale."
  },
  {
    "objectID": "slides/13_Model_selection.html#best-subset",
    "href": "slides/13_Model_selection.html#best-subset",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Best subset",
    "text": "Best subset"
  },
  {
    "objectID": "slides/13_Model_selection.html#regularization-techniques",
    "href": "slides/13_Model_selection.html#regularization-techniques",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Regularization techniques",
    "text": "Regularization techniques\n\nRegularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n\n\n\n\n\n\n\n\n\n\n\nLASSO (Least About Shrinkage and Selection Operator)\nRidge\nElastic Net\n\n\nPenalization\nL-1 Norm, uses absolute value\nL-2 Norm, uses squared value\nBest of both worlds, L-1 and L-2 used\n\n\nPro’s\nReduces overfitting, will shrink coefficient to zero\nReduces overfitting, handles collinearity, can handle k&gt;n\nReduces overfitting, handles collinearity, handles k&gt;n, shrinks coefficients to zero\n\n\nCon’s\nCannot handle k&gt;n, doesn’t handle multicollinearity well\nDoes not shrink coefficients to zero, difficult to interpret\nMore difficult for R to do than the other two (but not really that bad)"
  },
  {
    "objectID": "slides/13_Model_selection.html#some-model-fit-statistics",
    "href": "slides/13_Model_selection.html#some-model-fit-statistics",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Some model fit statistics",
    "text": "Some model fit statistics\n\nSo far we have compared models using the F-test\nThe F-test is a great way to compare models that are nested\n\nBasically, this means that the “full” model contains all the covariates that the “reduced” model contains\nThe full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\nWhat if we want to compare models that are not nested?\n\nThere is a special group of fit statistics that can help us compare models\nNote: these are sometimes used in the model building process (within one strategy)\n\nHelpful if we want to compare selected models across strategies\nHelpful if we have a few “final” models with different covariates that we want to compare"
  },
  {
    "objectID": "slides/13_Model_selection.html#purposeful-model-selection",
    "href": "slides/13_Model_selection.html#purposeful-model-selection",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Purposeful model selection",
    "text": "Purposeful model selection\n\nCan use this type of model selection for any type of regression"
  },
  {
    "objectID": "slides/13_Model_selection.html#introduction-to-model-fit-statistics",
    "href": "slides/13_Model_selection.html#introduction-to-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Introduction to model fit statistics",
    "text": "Introduction to model fit statistics\n\nSo far we have compared models using the F-test\nThe F-test is a great way to compare models that are nested\n\nBasically, this means that the “full” model contains all the covariates that the “reduced” model contains\nThe full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\nWhat if we want to compare models that are not nested?\n\nThere is a special group of fit statistics that can help us compare models\nNote: these are sometimes used in the model building process (within one strategy)\n\nHelpful if we want to compare selected models across strategies\nHelpful if we have a few “final” models with different covariates that we want to compare"
  },
  {
    "objectID": "slides/13_Model_selection.html#common-model-fit-statistics",
    "href": "slides/13_Model_selection.html#common-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThe following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\nFor these fit statistics, smaller values indicate better model fit!\n\n\n\n\n\n\n\n\nFit statistic\nEquation\n\n\n\n\nR-squared / Adjusted R-squared\n\\(Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}\\)\n\n\nMallow’s \\(C_p\\)\n\\(C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p\\)\n\n\nAkaike information criterion (AIC)\n\\(AIC = n\\log(SSE) - n \\log(n) + 2(p+1)\\)\n\n\nBayesian information criterion (BIC)\n\\(BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)\\)\n\n\n\n \n\nWe don’t need to know the exact formulas for them!"
  },
  {
    "objectID": "slides/13_Model_selection.html#stepwise-selection-slides-adjusted-from-adrianna-westbrook",
    "href": "slides/13_Model_selection.html#stepwise-selection-slides-adjusted-from-adrianna-westbrook",
    "title": "Lesson 13: Model/Variable Selection",
    "section": "Stepwise selection (slides adjusted from Adrianna Westbrook)",
    "text": "Stepwise selection (slides adjusted from Adrianna Westbrook)\n\nPredictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)\n\nForward: Run y=x1 through y=xk-1 and enter the xi with the lowest p-value (assuming it is below the threshold) Now run y=xi + x1 through y=xi + xk-1 and enter the next xi with the lowest p-value Continue process until no more predictors come back with a p-value below the threshold\nBackward: Start with a full model (y=x1 + … + xk-1) and remove predictor with the highest p-value (assuming it is above the threshold) Repeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)"
  },
  {
    "objectID": "slides/13_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "href": "slides/13_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection (slide adjusted from Adrianna Westbrook)",
    "text": "Stepwise selection (slide adjusted from Adrianna Westbrook)\n\nThis is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\nBUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\nMajor disadvantages to stepwise selection:\n\nProne to overfitting\nBiased estimates\nCements the wrong idea that we are looking for our “most significant” covariates\n\nPredictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)"
  },
  {
    "objectID": "slides/13_Model_selection.html#stepwise-selection-two-common-approaches",
    "href": "slides/13_Model_selection.html#stepwise-selection-two-common-approaches",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection: two common approaches",
    "text": "Stepwise selection: two common approaches\n\nI will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\nForward selection:\n\nFor \\(p\\) cvariates potential covariates, run all simple linear regressions:\n\n\\(Y= \\beta_0 + \\beta_1 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon\\)\nInclude the \\(X_i\\) with the lowest p-value (assuming it is below the threshold)\n\nNow run \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon\\) and enter the next \\(X_j\\) with the lowest p-value\nContinue process until no more predictors come back with a p-value below the threshold\n\nBackward selection:\n\nStart with a full model (\\(Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon\\)) and remove predictor with the highest p-value (assuming it is above the threshold)\nRepeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)"
  },
  {
    "objectID": "slides/13_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "href": "slides/13_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Best subset (slide adjusted from Adrianna Westbrook)",
    "text": "Best subset (slide adjusted from Adrianna Westbrook)\n\nI don’t see this approach very often\nQuite literally making subsets of the data and using the “best” one\nGeneral steps:\n\nRun every possible model fitting 1 to all possible \\(p\\) predictors/covariates\nYou can limit number of potential predictors\n\\(2^p\\) = total number of models where \\(p\\) = number of predictors\nYou will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,…, \\(p\\) predictor model)\nThen have to find the best fitting model between the best models from each category\n\nMajor disadvantages to best subset:\n\nDoes not account for interactions\nNeeds to run a lot of models (takes A LOT of time)"
  },
  {
    "objectID": "slides/13_Model_selection.html#model-complexity-vs.-parsimony",
    "href": "slides/13_Model_selection.html#model-complexity-vs.-parsimony",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Suppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "slides/13_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "href": "slides/13_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)",
    "text": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)\n\nIn a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\nIf we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\nExample: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\nVariables such as study site, as well as any randomization stratification variables are common covariates.\n\nIn these cases, only a limited number of multivariable models are fit and reported\n\nDo not perform all the model building steps outlined in Hosmer and Lemeshow texts"
  },
  {
    "objectID": "slides/13_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "href": "slides/13_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Purposeful model selection (slide adjusted from Jodi Lapidus)",
    "text": "Purposeful model selection (slide adjusted from Jodi Lapidus)\n\nCan use this type of model selection for any type of regression\nCareful, well-thought out variable selection process\n\nConsiders both confounding and interaction, as well as checking model assumptions, fit, etc.\n\nOften a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\nHowever, not always appropriate!\nE.g. clinical trials with model specified in advance. (pre-specified model)\n\n\n \n\nThis is the selection process that we will focus on in this class!"
  },
  {
    "objectID": "slides/13_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "href": "slides/13_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)",
    "text": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)\n\nCIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\nObserved change is presumed to measure confounding by the covariate.\n\nWhat estimate?\n\nH/L text suggest using coefficients from the model\nWe typically use the coefficient estimate from the explanatory variable that we are most interested in\n\nWhat magnitude change is ”important”?\n\nH/L text suggest 10%\n\nOne must choose an effect measure to judge change importance, where “importance” needs to be evaluated along a contextually meaningful scale\nAccurate assessment of confounding may require examining changes from removing entire sets of covariates\n\nAdd in or eliminate candidate confounders one at time?\nAdd in or eliminate candidate confounders in sets?"
  },
  {
    "objectID": "slides/13_Model_selection.html#common-model-fit-statistics-1",
    "href": "slides/13_Model_selection.html#common-model-fit-statistics-1",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThere is no hypothesis testing for these fit statistics\n\nOnly helpful if you are comparing models\nWorks for nested and non-nested models\n\nCommon to report all or some of them\nAll of the fit statistics will not necessarily reach a consensus about the best fitting model\n\nEach weigh SSE, number of parameters, and number of observations differently"
  },
  {
    "objectID": "slides/12_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "href": "slides/12_Model_selection.html#why-cant-i-just-throw-in-all-the-variables-into-my-model",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Why can’t I just throw in all the variables into my model?",
    "text": "Why can’t I just throw in all the variables into my model?\n\nFirst, let’s think about the number of observations in our dataset\nFor example: In the Gapminder dataset, I can use an indicator for each country\n\nRemember that each country is an observation\nSo we have a perfectly fit model - a covariate for each observation\nBut we cannot generalize this to any other countries\nAnd we haven’t identified any meaningful relationships between life expectancy and other measured characteristics\n\nMore covariates in the model is not always better\n\nOverfitting the data limits our generalizability and prevents us from answering research questions"
  },
  {
    "objectID": "slides/12_Model_selection.html#model-complexity-vs.-parsimony",
    "href": "slides/12_Model_selection.html#model-complexity-vs.-parsimony",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model Complexity vs. Parsimony",
    "text": "Model Complexity vs. Parsimony\n\n\nSuppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "slides/12_Model_selection.html#bias-variance-trade-off",
    "href": "slides/12_Model_selection.html#bias-variance-trade-off",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Bias-variance trade off",
    "text": "Bias-variance trade off\n\n\n\nRecall mean square error is a function of SSE (sum of squared residuals)\n\\[\nMSE = \\dfrac{1}{n} \\sum_{i=1}^{n} \\big(Y_i - \\widehat{Y}_i \\big)^2\n\\]\nMSE can also be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\nFor the same data:\n\nMore covariates in model: less bias, more variance\nLess covariates in model: more bias, less variance\n\nOut goal: find a model with just the right amount of covariates to balance bias and vairance\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "slides/12_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "href": "slides/12_Model_selection.html#model-selection-basics-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model Selection basics (slide adjusted from Jodi Lapidus)",
    "text": "Model Selection basics (slide adjusted from Jodi Lapidus)\n\n“Because models always fall far short of the complex reality under study, there are no best or optimal strategies for modeling.”\n\nFrom: Statistical Foundations for Model-Based Adjustments\n\nNot all statistical texts provide practical advice on model development\n\nA lot of resources include methods/code to compare models, but does not include much advice re: selecting which model to ultimately use.\nOther texts are sparse on details or incorporate simplistic approaches\n\nModel development strategy should align with research goals\n\nPrediction vs. Estimating Association\nStrategy may depend on study design and data set size"
  },
  {
    "objectID": "slides/12_Model_selection.html#the-goals-of-association-vs.-prediction",
    "href": "slides/12_Model_selection.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "The goals of association vs. prediction",
    "text": "The goals of association vs. prediction\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret the coefficient of the variable that is the focus of the study\n\nInterpreting the coefficients of the other variables is not important, but can help bring context\n\nAny variables not selected for the final model have still been adjusted for, since they had a chance to be in the model\nExample: How is body mass of a penguin associated with flipper length?\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals\n\nExample: What is the flipper length of a penguin with body mass of 3000 g (and all its other characteristics)?"
  },
  {
    "objectID": "slides/12_Model_selection.html#model-building-for-association-vs.-prediction",
    "href": "slides/12_Model_selection.html#model-building-for-association-vs.-prediction",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model building for association vs. prediction",
    "text": "Model building for association vs. prediction\nMore information on the two analysis goals:\n\n\n\n\n\nIf you ever get the chance, check out Dr. Kristin Sainani’s series on Statistics"
  },
  {
    "objectID": "slides/12_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "href": "slides/12_Model_selection.html#model-selection-strategies-for-continuous-outcomes",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Model selection strategies for continuous outcomes",
    "text": "Model selection strategies for continuous outcomes\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "slides/12_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "href": "slides/12_Model_selection.html#pre-specification-of-multivariable-model-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)",
    "text": "Pre-specification of multivariable model (slide adjusted from Jodi Lapidus)\n\nIn a clinical trial, we often have to write and finalize a statistical analysis plan (SAP) before the trial starts\nIf we wish to compare treatment effects adjusted for covariates, all covariates typically specified in advance\n\nExample: Comparing effectiveness of 3-drug vs. 2-drug regimen for delaying AIDS onset or death. Covariates such as severity of HIV infection at baseline would have been specified in advance.\nVariables such as study site, as well as any randomization stratification variables are common covariates.\n\nIn these cases, only a limited number of multivariable models are fit and reported\n\nDo not perform all the model building steps outlined in Hosmer and Lemeshow texts"
  },
  {
    "objectID": "slides/12_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "href": "slides/12_Model_selection.html#purposeful-model-selection-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Purposeful model selection (slide adjusted from Jodi Lapidus)",
    "text": "Purposeful model selection (slide adjusted from Jodi Lapidus)\n\nCan use this type of model selection for any type of regression\nCareful, well-thought out variable selection process\n\nConsiders both confounding and interaction, as well as checking model assumptions, fit, etc.\n\nOften a reasonable strategy, especially in epidemiology and more exploratory clinical studies\n\nHowever, not always appropriate!\nE.g. clinical trials with model specified in advance. (pre-specified model)\n\n\n \n\nThis is the selection process that we will focus on in this class!"
  },
  {
    "objectID": "slides/12_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "href": "slides/12_Model_selection.html#change-in-estimate-cie-approach-slide-adjusted-from-jodi-lapidus",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)",
    "text": "Change in estimate (CIE) approach (slide adjusted from Jodi Lapidus)\n\nCIE strategies select covariates on the basis of how much their control changes exposure effect estimates\n\nObserved change is presumed to measure confounding by the covariate.\n\nWhat estimate?\n\nH/L text suggest using coefficients from the model\nWe typically use the coefficient estimate from the explanatory variable that we are most interested in\n\nWhat magnitude change is ”important”?\n\nH/L text suggest 10%\n\nOne must choose an effect measure to judge change importance, where “importance” needs to be evaluated along a contextually meaningful scale\nAccurate assessment of confounding may require examining changes from removing entire sets of covariates\n\nAdd in or eliminate candidate confounders one at time?\nAdd in or eliminate candidate confounders in sets?"
  },
  {
    "objectID": "slides/12_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "href": "slides/12_Model_selection.html#stepwise-selection-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection (slide adjusted from Adrianna Westbrook)",
    "text": "Stepwise selection (slide adjusted from Adrianna Westbrook)\n\nThis is an incredibly common approach that statisticians use, often because it is an older and more recognized method\n\nBUT IT IS ALSO ONE OF THE WORST MODEL SELECTION STRATEGIES!!\n\nMajor disadvantages to stepwise selection:\n\nProne to overfitting\nBiased estimates\nCements the wrong idea that we are looking for our “most significant” covariates\n\nPredictors/covariates are added or removed one at time if they are below a certain threshold (usually p-value below 0.10 to 0.20)"
  },
  {
    "objectID": "slides/12_Model_selection.html#stepwise-selection-two-common-approaches",
    "href": "slides/12_Model_selection.html#stepwise-selection-two-common-approaches",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Stepwise selection: two common approaches",
    "text": "Stepwise selection: two common approaches\n\nI will introduce two of the approaches so that you understand the general process if a collaborator ever mentions stepwise selection\nForward selection:\n\nFor \\(p\\) cvariates potential covariates, run all simple linear regressions:\n\n\\(Y= \\beta_0 + \\beta_1 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_{p} + \\epsilon\\)\nInclude the \\(X_i\\) with the lowest p-value (assuming it is below the threshold)\n\nNow run \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_1 + \\epsilon\\) through \\(Y= \\beta_0 + \\beta_1 X_i + \\beta_2 X_{p} + \\epsilon\\) and enter the next \\(X_j\\) with the lowest p-value\nContinue process until no more predictors come back with a p-value below the threshold\n\nBackward selection:\n\nStart with a full model (\\(Y= \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p + \\epsilon\\)) and remove predictor with the highest p-value (assuming it is above the threshold)\nRepeatedly remove the variable with the highest p-value until all remaining variables meet the stopping criteria (are below the threshold)"
  },
  {
    "objectID": "slides/12_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "href": "slides/12_Model_selection.html#best-subset-slide-adjusted-from-adrianna-westbrook",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Best subset (slide adjusted from Adrianna Westbrook)",
    "text": "Best subset (slide adjusted from Adrianna Westbrook)\n\nI don’t see this approach very often\nQuite literally making subsets of the data and using the “best” one\nGeneral steps:\n\nRun every possible model fitting 1 to all possible \\(p\\) predictors/covariates\nYou can limit number of potential predictors\n\\(2^p\\) = total number of models where \\(p\\) = number of predictors\nYou will get the best fitting model within each category (i.e., 1 predictor model, 2 predictor model,…, \\(p\\) predictor model)\nThen have to find the best fitting model between the best models from each category\n\nMajor disadvantages to best subset:\n\nDoes not account for interactions\nNeeds to run a lot of models (takes A LOT of time)"
  },
  {
    "objectID": "slides/12_Model_selection.html#regularization-techniques",
    "href": "slides/12_Model_selection.html#regularization-techniques",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Regularization techniques",
    "text": "Regularization techniques\n\nRegularization techniques (LASSO, ridge, elastic net) adds a penalization that shrinks (or regularizes) coefficients down to reduce overfitting\n\n\n\n\n\n\n\n\n\n\n\nLASSO (Least About Shrinkage and Selection Operator)\nRidge\nElastic Net\n\n\nPenalization\nL-1 Norm, uses absolute value\nL-2 Norm, uses squared value\nBest of both worlds, L-1 and L-2 used\n\n\nPro’s\nReduces overfitting, will shrink coefficient to zero\nReduces overfitting, handles collinearity, can handle k&gt;n\nReduces overfitting, handles collinearity, handles k&gt;n, shrinks coefficients to zero\n\n\nCon’s\nCannot handle k&gt;n, doesn’t handle multicollinearity well\nDoes not shrink coefficients to zero, difficult to interpret\nMore difficult for R to do than the other two (but not really that bad)"
  },
  {
    "objectID": "slides/12_Model_selection.html#introduction-to-model-fit-statistics",
    "href": "slides/12_Model_selection.html#introduction-to-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Introduction to model fit statistics",
    "text": "Introduction to model fit statistics\n\nSo far we have compared models using the F-test\nThe F-test is a great way to compare models that are nested\n\nBasically, this means that the “full” model contains all the covariates that the “reduced” model contains\nThe full model will have additional covariates, but the covariates in the reduced is a subset of the covariates in the full\n\nWhat if we want to compare models that are not nested?\n\nThere is a special group of fit statistics that can help us compare models\nNote: these are sometimes used in the model building process (within one strategy)\n\nHelpful if we want to compare selected models across strategies\nHelpful if we have a few “final” models with different covariates that we want to compare"
  },
  {
    "objectID": "slides/12_Model_selection.html#common-model-fit-statistics",
    "href": "slides/12_Model_selection.html#common-model-fit-statistics",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThe following model fit statistics combine information about the SSE, the number of parameters in the model, and the sample size\nFor Mallow’s Cp, AIC, and BIC: smaller values indicate better model fit!\nFor Adjusted R-squared: larger values indicate better model fit!\n\n\n\n\nFit statistic\nEquation\nR code\n\n\n\n\nR-squared / Adjusted R-squared\n\\(Adj. R^2 = 1 - \\frac{SSE/(n-p-1)}{SSY/(n-1)}\\)\nWithin summary(model_name)\n\n\nMallow’s \\(C_p\\)\n\\(C_p = \\Bigg[ \\dfrac{\\widehat\\sigma^2_p}{\\widehat\\sigma^2_{max}} - 1 \\Bigg](n-p) + p\\)\nols_mallows_cp()\n\n\nAkaike information criterion (AIC)\n\\(AIC = n\\log(SSE) - n \\log(n) + 2(p+1)\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(BIC = n\\log(SSE) - n\\log(n) + log(n)\\cdot(p+1)\\)\nBIC(model_name)\n\n\n\n \n\nWe don’t need to know the exact formulas for them!"
  },
  {
    "objectID": "slides/12_Model_selection.html#common-model-fit-statistics-1",
    "href": "slides/12_Model_selection.html#common-model-fit-statistics-1",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Common model fit statistics",
    "text": "Common model fit statistics\n\nThere is no hypothesis testing for these fit statistics\n\nOnly helpful if you are comparing models\nWorks for nested and non-nested models\n\nCommon to report all or some of them\nAll of the fit statistics will not necessarily reach a consensus about the best fitting model\n\nEach weigh SSE, number of parameters, and number of observations differently\n\n\n\n\n\nhttps://www.researchgate.net/figure/Model-Fit-Statistics_tbl1_308844501"
  },
  {
    "objectID": "slides/12_Model_selection.html#poll-everywhere-question",
    "href": "slides/12_Model_selection.html#poll-everywhere-question",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/12_Model_selection.html#poll-everywhere-question-1",
    "href": "slides/12_Model_selection.html#poll-everywhere-question-1",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/12_Model_selection.html#poll-everywhere-question-2",
    "href": "slides/12_Model_selection.html#poll-everywhere-question-2",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html",
    "href": "slides/13_Purposeful_Selection.html",
    "title": "Lesson 13: Purposeful model selection",
    "section": "",
    "text": "Understand the overall steps for purposeful selection as a model building strategy\nApply purposeful selection to a dataset using R\nConnect purposeful selection steps back to tests of coefficients in Class 8\nUse different approaches to assess the linear scale of continuous variables in logistic regression\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#process",
    "href": "slides/13_Purposeful_Selection.html#process",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Process",
    "text": "Process\n\nExploratory data analysis\nCheck unadjusted associations in simple linear regression\nEnter all covariates in model that meet some threshold\n\nH/L suggest \\(p&lt;0.2\\) or \\(p&lt;0.25\\): great for modest sized datasets\nPLEASE keep in mind sample size in your study\nCan also use magnitude of association rather than, or along with, p-value\n\nRemove those that no longer reach some threshold\n\nCompare magnitude of associations to unadjusted version (univariable)\n\nCheck scaling of continuous and coding of categorical covariates\nFinalize main effect model\nCheck for interactions\nAssess model fit\n\nModel assumptions, diagnostics, overall fit\n\n\n\n\nPurposeful Selection"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#overall-process",
    "href": "slides/13_Purposeful_Selection.html#overall-process",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Overall Process",
    "text": "Overall Process\n\nExploratory data analysis\nCheck unadjusted associations in simple linear regression\nEnter all covariates in model that meet some threshold\n\nOne textbook suggest \\(p&lt;0.2\\) or \\(p&lt;0.25\\): great for modest sized datasets\nPLEASE keep in mind sample size in your study\nCan also use magnitude of association rather than, or along with, p-value\n\nRemove those that no longer reach some threshold\n\nCompare magnitude of associations to unadjusted version (univariable)\n\nCheck scaling of continuous and coding of categorical covariates\nCheck for interactions\nAssess model fit\n\nModel assumptions, diagnostics, overall fit"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#with-snappier-step-names",
    "href": "slides/13_Purposeful_Selection.html#with-snappier-step-names",
    "title": "Lesson 13: Purposeful model selection",
    "section": "With snappier step names",
    "text": "With snappier step names\n\n\nPre-step:\nStep 1:\nStep 2:\nStep 3:\n\nExploratory data analysis\nSimple linear regressions\nPreliminary variable selection\nAssess change in coefficients\nAssess scale for continuous variables\nFinalize main effect model\nCheck for interactions\nAssess model fit\n\n\n\n\nPurposeful Selection"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#process-with-snappier-step-names",
    "href": "slides/13_Purposeful_Selection.html#process-with-snappier-step-names",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Process with snappier step names",
    "text": "Process with snappier step names\n\n\nPre-step:\n \nStep 1:\n \nStep 2:\n \nStep 3:\n \nStep 4:\n \nStep 5:\n \nStep 6:\n\nExploratory data analysis (EDA)\n \nSimple linear regressions / analysis\n \nPreliminary variable selection\n \nAssess change in coefficients\n \nAssess scale for continuous variables\n \nCheck for interactions\n \nAssess model fit"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis",
    "text": "Pre-step: Exploratory data analysis\n\nThings we have been doing over the quarter in class and in our project\nI will not discuss some of the methods mentioned in our lab and data management class\n\nI am only going to introduce additional exploratory functions\n\n\n \nA few things we can do:\n\nCheck the data\nStudy your variables\nMissing data?\nExplore simple relationships and assumptions"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions",
    "href": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions",
    "text": "Step 1: Simple linear regressions"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection",
    "href": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nIdentify candidates for your first multivariable model by performing an F-test on each covariate’s SLR\n\nUsing p-values from previous slide\nIf the p-value of the test is less than 0.25, then consider the variable a candidate\n\n\n \n\nCandidates for first multivariable model\n\nAll clinically important variables (regardless of p-value)\nVariables with univariate test with p-value &lt; 0.25\n\n\n \n\nWith more experience, you won’t need to rely on these strict rules as much"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficients",
    "href": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficients",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficients",
    "text": "Step 3: Assess change in coefficients"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\nWe assume the linear regression model is linear for each continuous variable\nWe need to assess linearity for continuous variables in the model\n\nDo this through smoothed scatterplots that we introduced in Lesson 6 (SLR Diagnostics)\nResidual plots (can be used in SLR) does not help us in MLR\nEach term in MLR model needs to have linearity with outcome\n\nThree methods/approaches to address the violation of linearity assumption:\n\nApproach 1: Categorize continuous variable\nApproach 2: Fractional Polynomials\nApproach 3: Spline functions\n\nApproach will depend on the covariate!!\nFor our class, only implement Approach 1 or 2\nModel at the end of Step 4 is the main effects model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-5-finalize-main-effect-model",
    "href": "slides/13_Purposeful_Selection.html#step-5-finalize-main-effect-model",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Finalize main effect model",
    "text": "Step 5: Finalize main effect model\n\nModel at the end of Step 5 is the main effects model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-6-check-for-interactions",
    "href": "slides/13_Purposeful_Selection.html#step-6-check-for-interactions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Check for interactions",
    "text": "Step 6: Check for interactions\n\nCreate a list of interaction terms from variables in the “main effects model” that has clinical plausibility\nAdd the interaction variables, one at a time, to the main effects model, and assess the significance using a likelihood ratio test or Wald test\n\nMay keep interaction terms with p-value &lt; 0.05\n\nKeep the main effects untouched, only simplify the interaction terms – locked!\nUse methods from Step 2 (comparing model with all interactions to a smaller model with interactions) to determine which interactions to keep\nThe model by the end of Step 6 is called the preliminary final model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-7-assess-model-fit",
    "href": "slides/13_Purposeful_Selection.html#step-7-assess-model-fit",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 7: Assess model fit",
    "text": "Step 7: Assess model fit\n\nAssess the adequacy of the model and check its fit\nMethods will be discussed later class\nIf the model is adequate and fits well, then it is the Final model\n\n\n\nPurposeful Selection"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#section",
    "href": "slides/13_Purposeful_Selection.html#section",
    "title": "Lesson 13: Purposeful model selection",
    "section": "",
    "text": "“Successful modeling of a complex data set is part science, part statistical methods, and part experience and common sense.”\n\n \n\nHosmer, Lemeshow, and Sturdivant Textbook, pg. 101"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-1",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis (1/)",
    "text": "Pre-step: Exploratory data analysis (1/)\n\nThings we have been doing over the quarter in class and in our project\n\n \nA few things we can do:\n\nCheck the data\nStudy your variables\nMissing data?\nExplore simple relationships and assumptions"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nGet to know the potential values for the data\n\nCategories\nUnits\n\nThen make sure the summary of values makes sense\n\nIf minimum or maximum look outside appropriate range\nFor example: a negative value for a measurement that is inherently positive (like population or income)\n\n\n\n\n\n\nhttps://www.gapminder.org/data/documentation/"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data-1",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nLook at a summary for the raw data\nTypical use:\n\n\nlibrary(skimr)\nskim(gapm)\n\n\nSome skim() help"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data-2",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\n\n\nLook at a summary for the raw data\nTypical use:\n\n\nlibrary(skimr)\nskim(gapm)\n\n\nSome skim() help\nNote that skim(gapm) looks different because I had to create factors\nI am breaking down the skim() function into the categorical and continuous variables only because I want to show them on the slides\n\n\n\n\n\nskim(gapm_sub1) %&gt;% yank(\"factor\")\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nfour_regions\n0\n1.00\nFALSE\n4\nAsi: 57, Afr: 54, Eur: 49, Ame: 35\n\n\nincome_levels1\n1\n0.99\nFALSE\n4\nHig: 56, Upp: 55, Low: 52, Low: 31\n\n\nincome_levels2\n1\n0.99\nFALSE\n2\nHig: 111, Low: 83"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data-3",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-check-the-data-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Check the data",
    "text": "Pre-step: Exploratory data analysis: Check the data\n\nskim(gapm_sub1) %&gt;% yank(\"numeric\")\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nCO2emissions\n4\n0.98\n4.55\n6.10\n0.03\n0.64\n2.41\n6.22\n41.20\n▇▁▁▁▁\n\n\nElectricityUsePP\n58\n0.70\n4220.92\n5964.07\n31.10\n699.00\n2410.00\n5600.00\n52400.00\n▇▁▁▁▁\n\n\nFoodSupplykcPPD\n27\n0.86\n2825.06\n443.59\n1910.00\n2490.00\n2775.00\n3172.50\n3740.00\n▅▇▇▇▅\n\n\nIncomePP\n2\n0.99\n16704.45\n19098.61\n614.00\n3370.00\n10100.00\n22700.00\n129000.00\n▇▂▁▁▁\n\n\nLifeExpectancyYrs\n8\n0.96\n70.66\n8.44\n47.50\n64.30\n72.70\n76.90\n82.90\n▁▃▃▇▇\n\n\nFemaleLiteracyRate\n115\n0.41\n81.65\n21.95\n13.00\n70.97\n91.60\n98.03\n99.80\n▁▁▂▁▇\n\n\nWaterSourcePrct\n1\n0.99\n84.84\n18.64\n18.30\n74.90\n93.50\n99.07\n100.00\n▁▁▂▂▇\n\n\nLatitude\n0\n1.00\n19.11\n23.93\n-42.00\n4.00\n17.33\n40.00\n65.00\n▁▃▇▆▅\n\n\nLongitude\n0\n1.00\n21.98\n66.52\n-175.00\n-5.75\n21.00\n49.27\n179.14\n▁▃▇▃▂\n\n\npopulation_mill\n0\n1.00\n35.95\n136.87\n0.00\n1.73\n7.57\n24.50\n1370.00\n▇▁▁▁▁"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-study-your-variables",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-study-your-variables",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Study your variables",
    "text": "Pre-step: Exploratory data analysis: Study your variables\n\nStarted this a little bit in previous slide (skim()), but you may want to look at things like:\n\nSample size\nCounts of missing data\nMeans and standard deviations\nIQRs\nMedians\nMinimums and maximums\n\nCan also look at visuals\n\nContinuous variables: histograms (in `skimr() a little)\nCategorical variables: frequency plots"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-study-your-variables-1",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-study-your-variables-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Study your variables",
    "text": "Pre-step: Exploratory data analysis: Study your variables\n\nlibrary(Hmisc)\nhist.data.frame(gapm %&gt;% select(-Longitude, -Latitude, -eight_regions, -six_regions, -geo, -`World bank, 4 income groups 2017`, -country, -population, -`World bank region`, -ElectricityUsePP))"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#poll-everywhere-question-1",
    "href": "slides/13_Purposeful_Selection.html#poll-everywhere-question-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#poll-everywhere-question-2",
    "href": "slides/13_Purposeful_Selection.html#poll-everywhere-question-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-missing-data",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-missing-data",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Missing data",
    "text": "Pre-step: Exploratory data analysis: Missing data\n\nWhy are there missing data?\nWhich variables and observations should be excluded because of missing data?\nWill I impute missing data?\n\n \n\nUnfortunately, we don’t have time to discuss missing data more thoroughly\n\nI will try to cover this topic more thoroughly in BSTA 513\n\n \n\nFor the Gapminder dataset, we chose to use complete cases"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-explore-simple-relationships-and-assumptions",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-explore-simple-relationships-and-assumptions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Explore simple relationships and assumptions",
    "text": "Pre-step: Exploratory data analysis: Explore simple relationships and assumptions\n\nAlso part of Step 1, but I like to start with visuals"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-explore-simple-relationshipsassumptions",
    "href": "slides/13_Purposeful_Selection.html#pre-step-exploratory-data-analysis-explore-simple-relationshipsassumptions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step: Exploratory data analysis: Explore simple relationships/assumptions",
    "text": "Pre-step: Exploratory data analysis: Explore simple relationships/assumptions\n\nAlso part of Step 1, but I like to start with visuals\n\n\ngapm2 %&gt;% ggpairs() # gapm2 is a new dataset with some variables selected"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#pre-step-step-1-explore-simple-relationships-and-assumptions",
    "href": "slides/13_Purposeful_Selection.html#pre-step-step-1-explore-simple-relationships-and-assumptions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Pre-step / Step 1 : Explore simple relationships and assumptions",
    "text": "Pre-step / Step 1 : Explore simple relationships and assumptions\n\ngapm2 %&gt;% ggpairs() # gapm2 is a new dataset with some variables selected"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#poll-everywhere-question-3",
    "href": "slides/13_Purposeful_Selection.html#poll-everywhere-question-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#question-2",
    "href": "slides/Quiz_2_Lab_2.html#question-2",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Question 2",
    "text": "Question 2\n\n\n\n\n\\[\nSSY = SST = \\sum_{i=1}^n (Y_i - \\bar{Y})^2\n\\]"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#question-9",
    "href": "slides/Quiz_2_Lab_2.html#question-9",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Question 9",
    "text": "Question 9"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#question-11",
    "href": "slides/Quiz_2_Lab_2.html#question-11",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Question 11",
    "text": "Question 11\n\n\n\n\n\nBiggest mistake: not including the hat on SBP!"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#question-12",
    "href": "slides/Quiz_2_Lab_2.html#question-12",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Question 12",
    "text": "Question 12\n\n\n\n\n\nBiggest mistakes\n\nNot adjusting for age\nForgetting units"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#notes-on-codingvisualization",
    "href": "slides/Quiz_2_Lab_2.html#notes-on-codingvisualization",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Notes on coding/visualization",
    "text": "Notes on coding/visualization\n\n\n\nDid not take off, but ordering any categorical variables with inherent order is really helpful in our visualizations\n\nAlso, tilting category names is helpful!"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#overall-2",
    "href": "slides/Quiz_2_Lab_2.html#overall-2",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Overall",
    "text": "Overall\n\nOnly 29 names recorded: should be in gradebook now\nAppreciate your feedback\nI will try to address some of these\n\nBalancing bandwidth, equity, different opinions, and things that are just a consequence of me teaching the course for the first time\n\nStill seeing what I can change ASAP"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#in-class",
    "href": "slides/Quiz_2_Lab_2.html#in-class",
    "title": "A word on Quiz 2 and Lab 2",
    "section": "In class",
    "text": "In class\n\nSometimes my own explanations get confusing - would prefer if I revisited topics later rather than going through bad explanation\nLecture notes in PDF: go to github, and I will have a PDF version there by the start of class\nPace of class: mixture of opinions\nExit tickets: some people don’t like them, some do, repetitive\n\nI made all the questions optional now - so fill it out or not, but this is my way of keeping a pulse on how effective lecture was\n\nAnd this will still be my way of making sure we are staying with the course materials\nI view this as an easy way to earn points that take weight away from higher stakes assignments\n\n\nDatasets: some like that we have one dataset, and some want more\n\nNext quarter: have ~2 quarter long datasets (highly dependent on my course load next quarter)\n\nR code: mixture of opinions\n\nLess/more, too much time explaining/too little time explaining\n\nPoll everywheres: mixture of opinions\n\nMy main goal: give you all a “productive” break from me lecturing you"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#homework",
    "href": "slides/Quiz_2_Lab_2.html#homework",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Homework",
    "text": "Homework\n\nFeedback on HW would be great\nPosting earlier: for this quarter - I just don’t have time :(\nReferencing a TB in homework and in class: some confusion if you need to read it on your own\n\nYou don’t!\nI am mostly referencing these things for copyright issues because I am publicly publishing our slides, homeworks, labs, etc."
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#quizzes",
    "href": "slides/Quiz_2_Lab_2.html#quizzes",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Quizzes",
    "text": "Quizzes\n\nSome like the format, some don’t\nWe like the open note\nSome: more of a test if you can search through the slides\nOptions for quizzes: we can brainstorm for next quarter\n\nUltimately, I have to balance time, cheating, equity of take-home"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#labs-project",
    "href": "slides/Quiz_2_Lab_2.html#labs-project",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Labs / Project",
    "text": "Labs / Project\n\nWould be nice to have a lab section\nLabs divided into smaller chunks\n\nBalancing with homework assignments"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#final-words",
    "href": "slides/Quiz_2_Lab_2.html#final-words",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Final words",
    "text": "Final words\n\nSaw a few students discuss how the volume of work this quarter is giving them anxiety, making them feel inadequate, etc.\n\nI understand and have experienced the same feeling - not saying any of us should experience it\nKeep going! Your grades are not your worth! You belong here! I want you to succeed!\nFinally, you are LEARNING!\n\nYou will get things wrong. Hopefully you and your professors are giving you some grace.\nI am still learning! I still reference things from my old classes\n\nIt’s all about setting a foundation so that you know some things and know how to reference materials/internet/books to fill in gaps\n\n\n\nIf I missed anything important to you in your feedback, please share in the anonymous on-going feedback\n\n\n\nQuiz and Lab 2"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#in-class-12",
    "href": "slides/Quiz_2_Lab_2.html#in-class-12",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "In-class (1/2)",
    "text": "In-class (1/2)\n\nSometimes my own explanations get confusing - would prefer if I revisited topics later rather than going through bad explanation\n\nFair enough: a few things over the quarter that I have taken for granted that I can riff a solid explanation, but I cannot\n\nLecture notes in PDF: go to github, and I will have a PDF version there by the start of class\nPace of class: mixture of opinions\nExit tickets: some people don’t like them, some do, repetitive\n\nI made all the questions optional now - so fill it out or not, but this is my way of keeping a pulse on how effective lecture was\n\nAnd this will still be my way of making sure we are staying with the course materials\nI view this as an easy way to earn points that take weight away from higher stakes assignments\n\n\nDatasets: some like that we have one dataset, and some want more\n\nNext quarter: have ~2 quarter long datasets (highly dependent on my course load next quarter)\n\nR code: mixture of opinions\n\nLess/more, too much time explaining/too little time explaining"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#in-class-22",
    "href": "slides/Quiz_2_Lab_2.html#in-class-22",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "In-class (2/2)",
    "text": "In-class (2/2)\n\nPoll everywheres: mixture of opinions\n\nMy main goal: give you all a “productive” break from me lecturing you\n\nMore real world examples to demonstrate concepts\nMore structure to lectures - wrapping up concepts, learning objectives\nLectures: scrollable html option?\nTowards beginning of course: mistakes in slides and due dates"
  },
  {
    "objectID": "slides/Quiz_2_Lab_2.html#things-we-like",
    "href": "slides/Quiz_2_Lab_2.html#things-we-like",
    "title": "Some words on Quiz 2, Lab 2, and Mid-term Feedback",
    "section": "Things we like",
    "text": "Things we like\n\nNotes with annotations and recording\n\nOption for asynchronous viewing, in-person attendance not required\n\nEngaging lectures (for some)\n\n“…slides are my best friend.”\n\nDescriptions in multiple ways, learning types\nConcepts then code\nFlexibility\nEasy to understand terms when explaining\nJudgement free zone"
  },
  {
    "objectID": "slides/12_Model_selection.html#some-important-definitions",
    "href": "slides/12_Model_selection.html#some-important-definitions",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "Some important definitions",
    "text": "Some important definitions\n\nModel selection: picking the “best” model from a set of possible models\n\nModels will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n\n\n \n\nModel selection strategies: a process or framework that helps us pick our “best” model\n\nThese strategies often differ by the approach and criteria used to the determine the “best” model\n\n\n \n\nOverfitting: result of fitting a model so closely to our particular sample data that it cannot be generalized to other samples (or the population)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis",
    "href": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nFor each covariate, we want to see how it relates to the outcome (without adjusting for other covariates)\nWe can partially do this with visualizations\n\nHelps us see the data we throw it into regression that makes assumptions (like our LINE assumptions)\nggpairs() can be a quick way to do it\nggplot() can make each plot\n\n+ geom_boxplot() to make boxplots by groups for categorical covariates\n+ geom_jitter() + stat_summary() to make non-overlaping points with group means for categorical covariates\n+ geom_point() to make scatterplots for continuous covariates\n\n\nWe need to run simple linear regression\n\nWe’re calling regression with multi-level categories “simple” even though there are multiple coefficients"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-1",
    "href": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nLet’s think back to our Gapminder dataset\nAlways good to start with our main relationship: life expectancy vs. female literacy rate\n\nThrowback to Lesson 3 SLR when we first visualized and ran lm() for this relationship\n\n\n\nmodel_FLR = lm(LifeExpectancyYrs ~ FemaleLiteracyRate, data = gapm_sub)\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n51.438\n2.739\n18.782\n0.000\n    FemaleLiteracyRate\n0.230\n0.032\n7.141\n0.000"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#regression-analysis-process",
    "href": "slides/13_Purposeful_Selection.html#regression-analysis-process",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#poll-everywhere-question-4",
    "href": "slides/13_Purposeful_Selection.html#poll-everywhere-question-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-2",
    "href": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nLet’s do this with one other variable before I show you a streamlined version of SLR\n\n\nmodel_WR = lm(LifeExpectancyYrs ~ four_regions, data = gapm_sub)\n\n \n\n\n\n\nCode\nggplot(gapm_sub, aes(x = four_regions, y = LifeExpectancyYrs)) +\n  geom_jitter(size = 1, alpha = .6, width = 0.2) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  labs(x = \"World region\", \n       y = \"Country life expectancy (years)\",\n       title = \"Life expectancy vs. world region\",\n       caption = \"Diamonds = region averages\") +\n  theme(axis.title = element_text(size = 20), \n        axis.text = element_text(size = 20), \n        title = element_text(size = 20))\n\n\n\n\n\n\n\n\n\n\n\nanova(model_WR) %&gt;% tidy() %&gt;% gt() %&gt;%\n   tab_options(table.font.size = 40) %&gt;%\n   fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df\n      sumsq\n      meansq\n      statistic\n      p.value\n    \n  \n  \n    four_regions\n3.000\n2,743.042\n914.347\n33.680\n0.000\n    Residuals\n68.000\n1,846.077\n27.148\nNA\nNA\n  \n  \n  \n\n\n\n\n \n\nRecall from Lesson 5 (SLR: More inference + Evaluation):\n\nanova() with one model name will compare the model (model_WR) to the intercept model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-3",
    "href": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nIf we do a good job visualizing the relationship between our outcome and each covariate, then we can proceed to a streamlined version of the F-test for each relationship\nFirst, I will select the variables that we are considering for model selection:\n\n\ngapm2 = gapm_sub %&gt;% select(LifeExpectancyYrs, CO2emissions, FoodSupplykcPPD, \n                            IncomePP, FemaleLiteracyRate, WaterSourcePrct, \n                            four_regions, members_oecd_g77)\n\n\nWe need to make sure our dataset only contains the variables we are considering for the model:\n\n\ngapm3 = gapm2 %&gt;% select(-LifeExpectancyYrs)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-4",
    "href": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nNow I can run the lapply() function, which allows me to run the same function multiple times over all the columns in gapm3\nFor each covariate I am running: lm(gapm2$LifeExpectancyYrs ~ x) %&gt;% anova()\n\nSo I am fitting the simple linear regression and printing the ANOVA table with F-test (comparing model with a without the covariate)\n\n\n\nlapply( gapm3, function(x) lm(gapm2$LifeExpectancyYrs ~ x) %&gt;% anova() )\n\n$CO2emissions\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx          1  452.3  452.31  7.6536 0.007241 **\nResiduals 70 4136.8   59.10                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$FoodSupplykcPPD\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1893.4 1893.44  49.168 1.188e-09 ***\nResiduals 70 2695.7   38.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$IncomePP\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1220.3 1220.34  25.358 3.557e-06 ***\nResiduals 70 3368.8   48.13                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$FemaleLiteracyRate\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 1934.2 1934.24  50.999 6.895e-10 ***\nResiduals 70 2654.9   37.93                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$WaterSourcePrct\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          1 2988.2 2988.20  130.66 &lt; 2.2e-16 ***\nResiduals 70 1600.9   22.87                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$four_regions\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          3 2743.0  914.35   33.68 1.858e-13 ***\nResiduals 68 1846.1   27.15                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n$members_oecd_g77\nAnalysis of Variance Table\n\nResponse: gapm2$LifeExpectancyYrs\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nx          2 1103.7  551.85  10.925 7.553e-05 ***\nResiduals 69 3485.4   50.51                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWe can scroll through the output to see the ANOVA table for each covariate"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-5",
    "href": "slides/13_Purposeful_Selection.html#step-1-simple-linear-regressions-analysis-5",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 1: Simple linear regressions / analysis",
    "text": "Step 1: Simple linear regressions / analysis\n\nWe can also filter the ANOVA table to just show the p-value for each F-test\n\n\nsapply( gapm3, function(x) anova( lm(gapm2$LifeExpectancyYrs ~ x) )$`Pr(&gt;F)` )\n\n     CO2emissions FoodSupplykcPPD     IncomePP FemaleLiteracyRate\n[1,]  0.007241207    1.187753e-09 3.557341e-06       6.894997e-10\n[2,]           NA              NA           NA                 NA\n     WaterSourcePrct four_regions members_oecd_g77\n[1,]    1.148644e-17 1.857818e-13      7.55261e-05\n[2,]              NA           NA               NA\n\n\n\nRow 1 is the p-value for the F-test\n\nThis will help us in Step 2"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-1",
    "href": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nFrom the previous p-values from the F-test on each covariate’s SLR\n\nDecision: we keep all the covariates since they all have a p-value &lt; 0.25\n\n\n\nsapply( gapm3, function(x) anova( lm(gapm2$LifeExpectancyYrs ~ x) )$`Pr(&gt;F)` )\n\n     CO2emissions FoodSupplykcPPD     IncomePP FemaleLiteracyRate\n[1,]  0.007241207    1.187753e-09 3.557341e-06       6.894997e-10\n[2,]           NA              NA           NA                 NA\n     WaterSourcePrct four_regions members_oecd_g77\n[1,]    1.148644e-17 1.857818e-13      7.55261e-05\n[2,]              NA           NA               NA"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-2",
    "href": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nFit an initial model including any independent variable with p-value &lt; 0.25 and clinically important variables\n\n\ninit_model = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77, \n                 data = gapm2)\ntidy(init_model, conf.int = T) %&gt;% gt() %&gt;% tab_options(table.font.size = 30) %&gt;% \n  fmt_number(decimals = 4)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n37.5560\n4.4083\n8.5194\n0.0000\n28.7410\n46.3710\n    FemaleLiteracyRate\n0.0020\n0.0352\n0.0580\n0.9539\n−0.0684\n0.0725\n    CO2emissions\n−0.2860\n0.1340\n−2.1344\n0.0368\n−0.5539\n−0.0181\n    IncomePP\n0.0002\n0.0001\n2.4133\n0.0188\n0.0000\n0.0003\n    four_regionsAmericas\n9.8963\n2.0031\n4.9405\n0.0000\n5.8909\n13.9017\n    four_regionsAsia\n5.7849\n1.5993\n3.6172\n0.0006\n2.5870\n8.9829\n    four_regionsEurope\n7.1421\n2.6994\n2.6458\n0.0104\n1.7442\n12.5399\n    WaterSourcePrct\n0.1377\n0.0658\n2.0928\n0.0405\n0.0061\n0.2693\n    FoodSupplykcPPD\n0.0052\n0.0021\n2.4961\n0.0153\n0.0010\n0.0093\n    members_oecd_g77oecd\n−0.3317\n2.5476\n−0.1302\n0.8968\n−5.4259\n4.7625\n    members_oecd_g77others\n0.3341\n2.2986\n0.1453\n0.8849\n−4.2622\n4.9304"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-3",
    "href": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\n\n\nThis is where we start identifying covariates that we might remove\n\n \n\nI would start by using the p-value to guide me towards specific variables\n\nFemale literacy rate, but that’s our main covariate\nmembers_oecd_g77\nMaybe water source percent?\n\n\n \n\nSome people will say you can use the p-value alone\n\nI like to double check that those variables do not have a large effect on the other coefficients\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n37.5560\n4.4083\n8.5194\n0.0000\n    FemaleLiteracyRate\n0.0020\n0.0352\n0.0580\n0.9539\n    CO2emissions\n−0.2860\n0.1340\n−2.1344\n0.0368\n    IncomePP\n0.0002\n0.0001\n2.4133\n0.0188\n    four_regionsAmericas\n9.8963\n2.0031\n4.9405\n0.0000\n    four_regionsAsia\n5.7849\n1.5993\n3.6172\n0.0006\n    four_regionsEurope\n7.1421\n2.6994\n2.6458\n0.0104\n    WaterSourcePrct\n0.1377\n0.0658\n2.0928\n0.0405\n    FoodSupplykcPPD\n0.0052\n0.0021\n2.4961\n0.0153\n    members_oecd_g77oecd\n−0.3317\n2.5476\n−0.1302\n0.8968\n    members_oecd_g77others\n0.3341\n2.2986\n0.1453\n0.8849"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient",
    "href": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\n\n\nThis is where we start identifying covariates that we might remove\n\n \n\nI would start by using the p-value to guide me towards specific variables\n\nFemale literacy rate, but that’s our main covariate\nmembers_oecd_g77\nMaybe water source percent?\n\n\n \n\nSome people will say you can use the p-value alone\n\nI like to double check that those variables do not have a large effect on the other coefficients\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n37.5560\n4.4083\n8.5194\n0.0000\n    FemaleLiteracyRate\n0.0020\n0.0352\n0.0580\n0.9539\n    CO2emissions\n−0.2860\n0.1340\n−2.1344\n0.0368\n    IncomePP\n0.0002\n0.0001\n2.4133\n0.0188\n    four_regionsAmericas\n9.8963\n2.0031\n4.9405\n0.0000\n    four_regionsAsia\n5.7849\n1.5993\n3.6172\n0.0006\n    four_regionsEurope\n7.1421\n2.6994\n2.6458\n0.0104\n    WaterSourcePrct\n0.1377\n0.0658\n2.0928\n0.0405\n    FoodSupplykcPPD\n0.0052\n0.0021\n2.4961\n0.0153\n    members_oecd_g77oecd\n−0.3317\n2.5476\n−0.1302\n0.8968\n    members_oecd_g77others\n0.3341\n2.2986\n0.1453\n0.8849"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-4",
    "href": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nVery similar to the process we used when looking at confounders\n\n \n\nOne variable at a time, we run the multivariable model with and without the variable\n\nWe look at the p-value of the F-test for the coefficients of said variable\nWe look at the percent change for the coefficient (\\(\\Delta\\%\\)) of our explanatory variable\n\n\n \n\nGeneral rule: We can remove a variable if…\n\np-value &gt; 0.05 for the F-test of its own coefficients\nAND change in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-5",
    "href": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-5",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nLet’s try this out on members_oecd_g77\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD\n63.000\n1,000.988\n−2.000\n−1.787\n0.055\n0.947\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.0036\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.0036}{0.002} = -74.41\\%\n\\] - Based off the percent change, I would keep this in the model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-6",
    "href": "slides/13_Purposeful_Selection.html#step-2-preliminary-variable-selection-6",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 2: Preliminary variable selection",
    "text": "Step 2: Preliminary variable selection\n\nLet’s try this out on water source percent\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + members_oecd_g77 + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + members_oecd_g77 + FoodSupplykcPPD\n62.000\n1,070.944\n−1.000\n−71.744\n4.380\n0.041\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.034\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.034}{0.002} = -1561.06\\%\n\\] - Based off the percent change, I would keep this in the model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-1",
    "href": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nVery similar to the process we used when looking at confounders\n\n \n\nOne variable at a time, we run the multivariable model with and without the variable\n\nWe look at the p-value of the F-test for the coefficients of said variable\nWe look at the percent change for the coefficient (\\(\\Delta\\%\\)) of our explanatory variable\n\n\n \n\nGeneral rule: We can remove a variable if…\n\np-value &gt; 0.05 for the F-test of its own coefficients\nAND change in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-2",
    "href": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nLet’s try this out on members_oecd_g77\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD\n63.000\n1,000.988\n−2.000\n−1.787\n0.055\n0.947\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.0036\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.0036}{0.002} = -74.41\\%\n\\]\n\nBased off the percent change, I would keep this in the model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-3",
    "href": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nLet’s try this out on water source percent (even though the p-value was &lt; 0.05)\n\n\n\nDisplay the ANOVA table with F-statistic and p-value\nmodel_full = init_model\nmodel_red = lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP +\n               four_regions + members_oecd_g77 + FoodSupplykcPPD, \n                 data = gapm2)\nanova(model_full, model_red) %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n61.000\n999.201\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2emissions + IncomePP + four_regions + members_oecd_g77 + FoodSupplykcPPD\n62.000\n1,070.944\n−1.000\n−71.744\n4.380\n0.041\n  \n  \n  \n\n\n\n\n\n\\(\\widehat\\beta_{FLR, full} = 0.002\\), \\(\\widehat\\beta_{FLR, red} = 0.034\\)\n\n\\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}} = 100\\% \\cdot \\frac{0.002 - 0.034}{0.002} = -1561.06\\%\n\\]\n\nBased off the percent change (and p-value), I would keep this in the model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-4",
    "href": "slides/13_Purposeful_Selection.html#step-3-assess-change-in-coefficient-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 3: Assess change in coefficient",
    "text": "Step 3: Assess change in coefficient\n\nAt the end of this step, we have a preliminary main effects model\nWhere the variables are excluded that met the following criteria:\n\nP-value &gt; 0.05 for the F-test of its own coefficients\nChange in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%\n\nIn our example, the preliminary main effects model (end of Step 3) was the same as the initial model (end of Step 2)\nPreliminary main effects model includes:\n\nFemaleLiteracyRate\nCO2emissions\nIncomePP\nfour_regions\nmembers_oecd_g77\nFoodSupplykcPPD\nWaterSupplePct"
  },
  {
    "objectID": "labs/Lab_04_instructions.html",
    "href": "labs/Lab_04_instructions.html",
    "title": "Lab 4 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nRead to go ! (3/8/2024)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions",
    "href": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nCreate a list of interaction terms from variables in the “main effects model” that has clinical plausibility\n\n \n\nAdd the interaction variables, one at a time, to the main effects model, and assess the significance using a likelihood ratio test or Wald test\n\nMay keep interaction terms with p-value &lt; 0.10 (or 0.05)\n\n\n \n\nKeep the main effects untouched, only simplify the interaction terms\n\n \n\nUse methods from Step 2 (comparing model with all interactions to a smaller model with interactions) to determine which interactions to keep\n\n \n\nThe model by the end of Step 5 is called the preliminary final model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit",
    "href": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit",
    "text": "Step 6: Assess model fit\n\nAssess the adequacy of the model and check its fit\n\n \n\nMethods will be discussed next class\n\nCombination of diagnostics and model fit statistics!\nLook at model fit statistics in this lesson\nLook at diagnostics in Lesson 14: MLR Diagnostics\n\n\n \n\nIf the model is adequate and fits well, then it is the Final model"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4",
    "href": "slides/13_Purposeful_Selection.html#step-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4:",
    "text": "Step 4:"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#next-time",
    "href": "slides/13_Purposeful_Selection.html#next-time",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Next time",
    "text": "Next time\n\nMore details on steps 4-6 on Monday before quiz!\n\n\n\nPurposeful Selection"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\nSmoother scatterplots only check linearity, not addressing linearity issues\n\n \n\nCan also identify extreme observations\n\nAgain, just want to flag these values\nCan influence the assessment of linearity when using fractional polynomials or spline functions\n\n\n \n\nHelps us decide if the continuous variable can stay as is in the model\n\nProblem: if not linear, then we need to represent the variable in a new way (Approaches 1-3)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-1",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\nIn Gapminder dataset, we have 5 continuous variables:\n\nCO2 Emissions\nFood Supply\nIncome\nFemale Literacy Rate\nWater source percent\n\nPlot each of these agains the outcome, life expectancy"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-2",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nWe can quickly look at ggpairs() to identify variables\ngapm2 %&gt;% select(where(is.numeric)) %&gt;% \n  relocate(LifeExpectancyYrs, .after = last_col()) %&gt;% ggpairs()"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-3",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nTake a look at C02, Food Supply, and Income\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\")\n\nFS = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = FoodSupplykcPPD)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Food Supply (kcal PPD)\", y = \"Life Expectancy (yrs)\")\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")\n\ngrid.arrange(CO2, FS, Income, nrow=1)\n\n\n\n\nFood Supply looks admissible\nCO2 Emissions and Income do not look very linear, but I want to zoom into the area of the plots that have most of the data"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-4",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-smoothed-scatterplots-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables: Smoothed scatterplots",
    "text": "Step 4: Assess scale for continuous variables: Smoothed scatterplots\n\n\nZoom into areas on plots with more data\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point() + xlim(0,10) +\n  geom_smooth(se=F) + labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\")\n\nFS = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = FoodSupplykcPPD)) + \n  geom_point() +\n  geom_smooth(se=F) + labs(x = \"Food Supply (kcal PPD)\", y = \"Life Expectancy (yrs)\")\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point() + xlim(0,40000) +\n  geom_smooth(se=F) + labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")\n\ngrid.arrange(CO2, FS, Income, nrow=1)\n\n\n\n\nFood Supply still looks admissible\nCO2 Emissions and Income not linear: will address this!!"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#poll-everywhere-question-5",
    "href": "slides/13_Purposeful_Selection.html#poll-everywhere-question-5",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-1",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\nThree methods/approaches to address the violation of linearity assumption:\n\nApproach 1: Categorize continuous variable\nApproach 2: Fractional Polynomials\nApproach 3: Spline functions"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-1-quantile-methodindicator-variables",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-1-quantile-methodindicator-variables",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Quantile method/Indicator variables",
    "text": "Step 4: Approach 1: Quantile method/Indicator variables\n\n\nTake a look at the quartiles within the scatterplot\nvline_coordinates= data.frame(Quantile_Name=names(quantile(gapm2$CO2emissions)),\n                          quantile_values=as.numeric(quantile(gapm2$CO2emissions)))\n\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point(size = 1) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n  geom_vline(data = vline_coordinates, aes(xintercept = quantile_values), \n             color = \"red\", linetype = \"dashed\", size = .9)\n\nvline_coordinates= data.frame(Quantile_Name=names(quantile(gapm2$IncomePP)),\n                          quantile_values=as.numeric(quantile(gapm2$IncomePP)))\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point(size = 1) +\n  #geom_smooth(se=F) + \n  labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")  +\n  geom_vline(data = vline_coordinates, aes(xintercept = quantile_values), \n             color = \"red\", linetype = \"dashed\", size = .9)\n\ngrid.arrange(CO2, Income, nrow=1)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-2-fractional-polynomials",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-2-fractional-polynomials",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\nMain concepts and transformations presented in Lesson 7 SLR: Model Evaluation and Diagnostics (slide 33 on)\nIdea: test many transformations of a continuous covariate\n\nBased on Royston and Altman, Applied Statistics, 1994\n\n\n \n\nRecall Tukey’s transformation (power) ladder\n\nAnd can use R’s gladder() to see the transformations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n \n\nWe can run through each and test different models, or use the approach from Lesson 7\nThere is also a package we can use!\n\nmfp package in R contains the fp() function"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-3-spline-functions",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-3-spline-functions",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 3: Spline functions",
    "text": "Step 4: Approach 3: Spline functions\n\nSpline function is to fit a series of smooth curves that joined at specific points (called knots)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-1-quantile-methodindicator-variables-1",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-1-quantile-methodindicator-variables-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Quantile method/Indicator variables",
    "text": "Step 4: Approach 1: Quantile method/Indicator variables\n\n\nTake a look at the quartiles within the scatterplot\nvline_coordinates= data.frame(Quantile_Name=names(quantile(gapm2$CO2emissions)),\n                          quantile_values=as.numeric(quantile(gapm2$CO2emissions)))\n\nCO2 = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point(size = 1) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n  geom_vline(data = vline_coordinates, aes(xintercept = quantile_values), \n             color = \"red\", linetype = \"dashed\", size = .9)\n\nvline_coordinates= data.frame(Quantile_Name=names(quantile(gapm2$IncomePP)),\n                          quantile_values=as.numeric(quantile(gapm2$IncomePP)))\n\nIncome = ggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = IncomePP)) + \n  geom_point(size = 1) +\n  #geom_smooth(se=F) + \n  labs(x = \"Income (GDP per capita)\", y = \"Life Expectancy (yrs)\")  +\n  geom_vline(data = vline_coordinates, aes(xintercept = quantile_values), \n             color = \"red\", linetype = \"dashed\", size = .9)\n\ngrid.arrange(CO2, Income, nrow=1)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-2",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables",
    "text": "Step 4: Assess scale for continuous variables\n\nThree methods/approaches to address the violation of linearity assumption:\n \n\nApproach 1: Categorize continuous variable\n\n \n\nApproach 2: Fractional Polynomials\n\n \n\nApproach 3: Spline functions"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#recap-of-steps-1-3",
    "href": "slides/13_Purposeful_Selection.html#recap-of-steps-1-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Recap of Steps 1-3",
    "text": "Recap of Steps 1-3\n\nPre-step: Exploratory data analysis\nStep 1: Simple linear regressions / analysis\n\nLook at each covariate with outcome\nPerform SLR for each covariate\n\nStep 2: Preliminary variable selection\n\nFrom SLR, decide which variables go into the initial model\nUse F-test to see if each covariate (on its own) explains enough variation in outcome\nEnd with initial model\n\nStep 3: Assess change in coefficients\n\nFrom the initial model at end of step 2, we take a variable out of the model if:\n\nP-value &gt; 0.05 for the F-test of its own coefficients\nChange in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%\n\nEnd with preliminary main effects model"
  },
  {
    "objectID": "labs/Lab_04_instructions.html#directions",
    "href": "labs/Lab_04_instructions.html#directions",
    "title": "Lab 4 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nPlease turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n1.1 Purpose\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "labs/Lab_04_instructions.html#lab-activities",
    "href": "labs/Lab_04_instructions.html#lab-activities",
    "title": "Lab 4 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nHow is implicit anti-fat bias, as measured by the IAT score, associated with “insert main independent variable here”?\n\n\n2.2 Step 1: Simple linear regressions / analysis\nWe have done most of this step through visualizations in Lab 2 and 3. Now, we will quickly run a simple linear regression model for each covariate against the IAT score (outcome). Remember, the goal of this is to see if each covariate explains enough variation of the outcome, IAT score. You should have at least 9 simple linear regression models and their results. Results include the F-statistic and p-value from the test if each covariate explains enough variation of the outcome. Please revisit the slides from Lesson 5 (SLR: More inference + Evaluation) for more help with this test.\n\n\n\n\n\n\nVERY IMPORTANT FOR VARIABLES WE ORDERED USING FACTOR!!\n\n\n\nI asked that you order variables to make plots more interpretable. However, for the lm(), R reads the ordered variables in an unexpected way. For these variables to run correctly in R, we need to unorder the variables. We can also set a reference level that makes sense.\nFor example, I may want to unorder my variable iam_001 and set the reference to Neither underweight nor overweight. I can do this with:\n\niat_2021_new = iat_2021_old %&gt;% \n  mutate(iam_unordered = factor( iam_ordered, ordered = FALSE ) %&gt;% \n           relevel( ref = \"Neither underweight nor overweight\"))\n\n\n\nRecall, we mentioned 3 options to running and outputting the results of\n\nWe can run lm() for each covariate in separate lines of code, and use something like summary() or anova() to look at the results of each. (More time consuming to write, but less complicated coding)\nWe can use lapply() to run lm() and display the anova() on each covariate in one line of code. (Less time consuming to write, but more complicated coding, and more prone to errors that may not be apparent from output)\nWe can use sapply() to run lm(), anova(), and display the p-value for each covariate in one line of code. (Less time consuming to write, but more complicated coding, more prone to errors that may not be apparent from output, and no sense of what’s going on in the regression)\n\nPlease take a note for yourself if your dataset contains the original numeric versions of variables that we created factors for. I am not saying that you should take them out. They might be useful if our sample is not big enough to handle all the categorical covariates that we’ve included, but I think our sample is large enough.\n\n\n\n\n\n\nTasks\n\n\n\n\nRun a simple linear regression model for each covariate against the IAT score (outcome).\nDisplay results from the test if each covariate explains enough variation of the outcome. This may be from three options in the instructions: summary()/anova() only, lapply(), or sapply()\n\nInterpretation of the results will be in the next step.\n\n\n\n\n2.3 Step 2: Preliminary variable selection\nUsing the previous p-values from the F-test on each covariate’s SLR, decide which covariates will be included in the initial model. Recall the decision rule: we keep covariates that explain enough variation using p-value &lt; 0.25. Note that because our sample size is so large, the p-values might be really small. For now, that’s okay, but this means we may want to alter our Step 3 a little bit.\nOnce you have decided on the covariates, run the model and display the regression table.\n\n\n\n\n\n\nTasks\n\n\n\n\nDecide which covariates will be included in the initial model and list them.\nRun the initial model and display the regression table.\n\nNo need to write out the model, but you may in addition to the list.\n\n\n\n\n2.4 Step 3: Assess change in coefficient\nNow that all the selected variables are in one initial model, we can start considering the effect of each variable (outside of our main research question).\nRemember our general rule: We can remove a variable if (1) p-value &gt; 0.05 for the F-test to include or exclude the variable and (2) change in coefficient (\\(\\Delta\\%\\)) of our explanatory variable is &lt; 10%. Please remember that the p-values for the F-test for a multi-level categorical variable must be calculated by creating a reduced and full model.\nIt might be helpful to copy your list of covariates here and make note of the ones that you are removing. It was hard for me to keep track of all the variables when our dataset contains sooo many categorical covariates, and the regression table is so long.\nSince our sample size is quite large, most (if not all) of the F-tests will conclude that the variable should be kept in the model. At this point, I advise that you turn to some common sense and the change in coefficients.\n\nFor common sense, you may notice that some of your covariates are essentially measuring the same thing. If there is clinical relevance to having both in the model, then keep them in, but if not, you will have to decide which is more interpretable/relevant/aligned with your research question. For example, if you chose variables involving attitudes and beliefs that are measuring similar things, then you might exclude one. There are measurements like “I am …” with relative weight groups and “Compared to most…” with relative weight groups. These two might capture a lot of the same information, so we may chose one. (Additionally, this might create issues with multicollinearity, which we will discuss on the last day, so just keep that in mind!) Another example is if you used gender identity, this might be a good time to throw out sex assigned at birth. Remember, my reasoning for using SAB was that (1) lab work has been extensive and I wanted to give you an option to avoid multi-selection variables, and (2) it might capture some of the differences around fat attitudes tied with gender. If you included gender identity in your work, then sex assigned at birth could be superfluous.\nFor change in coefficients, focus on the variable of your research question. Does the removal of variables change the coefficients for your explanatory variable? Remember what we discussed with change in coefficients when our explanatory variable is a multi-level categorical variable (Lesson 11.2 Interactions continued slides 26-28). You may find these changes small, which tracks with a lot of our plots in Lab 3. Nothing seemed to have such a big effect on IAT score, and as a consequence it’s hard to see big changes for a potential confounder.\n\nNote that I put common sense first. The change in coefficients may not be very large, and may lead you to think we don’t need a lot of the variables in our model. However, I would let common sense override the change in coefficients if your reasoning is well justified.\npsst… There might be some code in Step 4 that might help you get started in this step.\n\n\n\n\n\n\nTasks\n\n\n\nRemove variables from the initial model based on your common sense, change in coefficient, and/or p-values of the F-tests.\nYou do NOT need to show all your work here. You just need to include:\n\nA brief explanation of what variables were dropped and why (a sentence per variable), and\nAn example of your process with one variable is enough (including code that you ran)\n\n\n\n\n\n2.5 Step 4: Assess scale for continuous variables\nThere is one variable in our model (unless you removed it) that is continuous: age. We need to assess the scale for age. In this step we will have ZERO delivarables. To save you time, I will walk you through my thought process, and why I determined age is fine as is. If you still want to try something else out with age, then you can!\nFirst, we can start with a scatterplot of IAT score and age. Your plot may look a little different than mine.\n\nggplot(data = iat, aes(x = age, y = IAT_score)) +\n  geom_point(size = 0.8) + geom_smooth() + xlim(0, 111)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIn the above scatterplot, it looks like the relationship is mostly linear (and increasing) until we get to approximately 90 years old. At that point IAT score decreases with age. Let’s say we 100% believe there is suddenly more responders around 110 years old than 90-100 year olds. I’m already skeptical of this since we did a quality control in Lab 3. We’ll play it out because it’s not worth making judgement calls on what we consider “admissable” data.\nWe could do quantiles, splines, or polynomials, but those approaches will either make more categorical variables or make the relationship between age and IAT score harder to interpret. We have a pretty linear relationship up until the higher ages!\nI wanted to investigate the linearity a little more so I created an indicator for individuals who are 100 years or older:\n\niat1 = iat %&gt;% mutate(ind_age_100 = ifelse(age &gt; 100, \"TRUE\", \"FALSE\"))\n\nNow I can see if the linearity differs between the two groups of ages:\n\nggplot(iat1, aes(x = age, y = IAT_score, color = ind_age_100)) +\n  geom_point(size = 0.8) + geom_smooth(method = \"lm\")\n\n\n\n\nI am happy to see that both groups’ IAT score are increasing with age. It actually looks like my indicator might be a confounder… In that case, we only need to include the indicator in the model so that the relationship between age and IAT is adjusted for the indicator. I can test to see if the indicator is a big enough confoudner using the change in coefficient of age and my explanatory variable.\nHere’s the model without the indicator:\n\nprelim_model = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + \n                    ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age, data = iat1)\n\nAnd we’ll take a look at the coefficients for the model:\n\nprelim_model$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.061151601                        -0.015513320 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006954580                        -0.023369363 \n iam_unorderedModerately overweight                                 age \n                       -0.056237038                         0.003857134 \n\n                                      # only print certain variables' coefficients\n\nThen we can run the model with the indicator, then look at the coefficients:\n\nprelim_model2 = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age + ind_age_100, \n                 data = iat1)\nprelim_model2$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.060385515                        -0.015352043 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006817642                        -0.023677757 \n iam_unorderedModerately overweight                                 age \n                       -0.056762301                         0.003918822 \n\n                                       # only print certain variables' coefficients\n\nWe can check the % change in the coefficients between the models.\nRecall, \\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}}\n\\] Here’s how I quickly do it with the coefficients:\n\n100 * ( prelim_model2$coefficients[c(2:6, 46)] - prelim_model$coefficients[c(2:6, 46)] ) /\n  prelim_model2$coefficients[c(2:6, 46)]\n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                         -1.2686585                          -1.0505274 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                         -2.0085770                           1.3024639 \n iam_unorderedModerately overweight                                 age \n                          0.9253735                           1.5741387 \n\n\nBased on %’s above, it doesn’t look like the indicator makes much of a difference in my model. It is likely because there are only 29 individuals over the age of 100 and 201,031 individuals under the age of 100 (In my dataset). Those 29 individuals will not have a big impact on the linear relationship between age and IAT, even though the first smoothed scatterplot made it look like it does.\nTo bring this point home, I can plot age and IAT with and without the individuals that are 100 years or older. Let me know if you find a better way to overlay these plots! (I have been a little stressed on time, and couldn’t find a quick answer.)\n\nggplot(iat1, aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"With individuals 100 years or older\")\n\n\n\nggplot(iat1 %&gt;% filter(age &lt; 100), aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"Without individuals 100 years or older\")\n\n\n\n\nI see no difference. Thus, I think it’s okay to leave age as is!!\n\n\n\n\n\n\nTasks\n\n\n\nNo tasks here! If you want to try out what I did above, you can!\n\n\n\n\n2.6 Step 5: Check for interactions\nNow we’re going to check if there are any interactions. I will walk you through a streamlined way to check for interactions between your explanatory variable and all the other variables in the model.\nFirst, I want you to revisit your work in Lab 3. Remind yourself of the variables that you identified as possible effect modifiers.\nAs you check for interactions, don’t forget to make your decisions based on your discussion/hypotheses in Lab 3. Always prioritize investigation of interactions that are justified clinically before investigating interactions only based on statistical significance.\n\n1vars = names(model.frame(prelim_model))[-1]\n\n.env &lt;- environment()\n2interactions &lt;- combn(vars, 2, function(x) paste(x, collapse=\" * \")) %&gt;%\n3    grep(., pattern = \"iam_unordered\", value = T)\n\n\n1\n\nCreate a vector of the variable names that are in your preliminary model. Note I use [-1] to remove IAT_score from my list. Please make sure to change prelim_model to the name of your model at this point.\n\n2\n\nHere we are just combining all our covariates into interactions that R can understand. This makes it so we don’t have to write it all ourselves.\n\n3\n\nMake sure to change the pattern = \"iam_unordered\" to be pattern = to your explanatory variable.\n\n\n\n\nNow that we’ve created the set up for all the possible interactions, we can run them through the lm() function and see the summary of the models. In the following code I use the lappy() function to fit an individual model for the main effects + each interaction listed in interactions.\n\n\n\n\n\n\nNote\n\n\n\nPlease note that this code takes a while to run. Once you run it and take note of the results, you can comment out or add #| eval: false to prevent it from running every time you render. You don’t need to show the results for this in your submitted work, but I want to see the code, and read about your decisions about from results.\n\n\n\nsummary = lapply(interactions,\n             function(int) summary(lm(reformulate(c(vars, int), \"IAT_score\", env=.env),\n                                      data = iat)))\nsummary\n\nYou can alse go straight to using the anova() function to compare the preliminary model.\n\nanova_res = lapply(interactions,\n             function(int) anova(lm(reformulate(c(vars, int), \"IAT_score\", env=.env),\n                                      data = iat),\n1                                 prelim_model))\nanova_res[1]\ng = anova_res[[1]]\ng$F\n\n\n1\n\nYou will to change this name for the preliminary model if you called it something different.\n\n\n\n\n[[1]]\nAnalysis of Variance Table\n\nModel 1: IAT_score ~ iam_unordered + identfat + comptomost + ind_m + ind_f + \n    ind_tmm + ind_twf + ind_gqnc + ind_other + race + ethn + \n    edu_14_f + age + iam_unordered * identfat\nModel 2: IAT_score ~ iam_unordered + identfat + comptomost + ind_m + ind_f + \n    ind_tmm + ind_twf + ind_gqnc + ind_other + race + ethn + \n    edu_14_f + age\n  Res.Df   RSS  Df Sum of Sq     F    Pr(&gt;F)    \n1 200990 31337                                  \n2 201014 31346 -24   -9.4223 2.518 5.558e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n[1]       NA 2.518048\n\n\n\n\n\n\n\n\nTasks\n\n\n\nUsing your discussion in Lab 3 and the results from the F-test on interactions:\n\nCreate a list of the interactions that you will include in your model.\nRun the preliminary final model that includes the main effects and interactions.\n\n\n\n\n\n2.7 Step 6: Assess model fit\nAt this point we may want to compare different models. While Steps 1-5 have been directing us towards a single model, you may have been interested in other models along the way. Maybe there were some interactions that you thought were interesting, but didn’t think of before. Maybe you would like to combine different groups for categorical variables.\nIf you are completely happy with your model, then you don’t have to do this step.\nYou might create a table like such:\n\nsum = summary(prelim_model)\nmodel_fit_stats = data.frame(Model = \"Preliminary main effects model\", Adjusted_R_sq = sum$adj.r.squared, AIC = AIC(prelim_model), BIC = BIC(prelim_model))\n\nmodel_fit_stats\n\n                           Model Adjusted_R_sq      AIC      BIC\n1 Preliminary main effects model    0.04511326 197006.6 197486.5\n\n\n\n\n\n\n\n\nTasks\n\n\n\nOptional: Create a table that displays some fo the model fit statistics to compare preliminary final models.\n\n\n\n\n2.8 Create a forest plot of your coefficient estimates\nIt’s often helpful to have a visualization of coefficient estimates. Forest plots are a nice way to show all the values together. Below I have started a forest plot using my prelim_model. You can make the plot with your final model.\nI used the plot_model() function to make the plot, and here’s a site that discusses some of it’s capabilities. The below plot is just a starting point!! You’ll need to clean up the variables, title, etc.\nYou may use another function to make the plots. I chose this one since it can handle the model as input.\n\nplot_model(prelim_model, show.values = TRUE, value.offset = 0.3) + ylim(-0.25, 0.25)\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nHere are some other packages for forest plots:\n\nhttps://cran.r-project.org/web/packages/forestploter/vignettes/forestploter-intro.html\nhttps://larmarange.github.io/ggstats/articles/ggcoef_model.html\n\n\n\n\n\n\n\nTasks\n\n\n\nCreate a forest plot to visualize the coefficient estimates."
  },
  {
    "objectID": "labs/Lab_04_instructions.html#step-4-assess-scale-for-continuous-variables",
    "href": "labs/Lab_04_instructions.html#step-4-assess-scale-for-continuous-variables",
    "title": "Lab 4 Instructions",
    "section": "3 Step 4: Assess scale for continuous variables",
    "text": "3 Step 4: Assess scale for continuous variables\nThere is one variable in our model (unless you removed it) that is continuous: age. We need to assess the scale for age. In this step we will have ZERO delivarables. To save you time, I will walk you through my thought process, and why I determined age is fine as is. If you still want to try something else out with age, then you can!\nFirst, we can start with a scatterplot of IAT score and age. Your plot may look a little different than mine.\n\nggplot(data = iat, aes(x = age, y = IAT_score)) +\n  geom_point(size = 0.8) + geom_smooth() + xlim(0, 111)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIn the above scatterplot, it looks like the relationship is mostly linear (and increasing) until we get to approximately 90 years old. At that point IAT score decreases with age. Let’s say we 100% believe there is suddenly more responders around 110 years old than 90-100 year olds. I’m already skeptical of this since we did a quality control in Lab 3. We’ll play it out because it’s not worth making judgement calls on what we consider “admissable” data.\nWe could do quantiles, splines, or polynomials, but those approaches will either make more categorical variables or make the relationship between age and IAT score harder to interpret. We have a pretty linear relationship up until the higher ages!\nI wanted to investigate the linearity a little more so I created an indicator for individuals who are 100 years or older:\n\niat1 = iat %&gt;% mutate(ind_age_100 = ifelse(age &gt; 100, \"TRUE\", \"FALSE\"))\n\nNow I can see if the linearity differs between the two groups of ages:\n\nggplot(iat1, aes(x = age, y = IAT_score, color = ind_age_100)) +\n  geom_point(size = 0.8) + geom_smooth(method = \"lm\")\n\n\n\n\nI am happy to see that both groups’ IAT score are increasing with age. It actually looks like my indicator might be a confounder… In that case, we only need to include the indicator in the model so that the relationship between age and IAT is adjusted for the indicator. I can test to see if the indicator is a big enough confoudner using the change in coefficient of age and my explanatory variable.\nHere’s the model without the indicator:\n\nprelim_model = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + \n                    ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age, data = iat1)\n\nAnd we’ll take a look at the coefficients for the model:\n\nprelim_model$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.061151601                        -0.015513320 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006954580                        -0.023369363 \n iam_unorderedModerately overweight                                 age \n                       -0.056237038                         0.003857134 \n\n                                      # only print certain variables' coefficients\n\nThen we can run the model with the indicator, then look at the coefficients:\n\nprelim_model2 = lm(IAT_score ~ iam_unordered + identfat + comptomost + \n                  ind_m + ind_f + ind_tmm + ind_twf + ind_gqnc + ind_other +\n                  race + \n                  ethn +\n                  edu_14_f +\n                  age + ind_age_100, \n                 data = iat1)\nprelim_model2$coefficients[c(2:6, 46)] # by using c(2:6, 46) I am telling R to \n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                       -0.060385515                        -0.015352043 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                        0.006817642                        -0.023677757 \n iam_unorderedModerately overweight                                 age \n                       -0.056762301                         0.003918822 \n\n                                       # only print certain variables' coefficients\n\nWe can check the % change in the coefficients between the models.\nRecall, \\[\n\\Delta\\% = 100\\% \\cdot \\frac{\\widehat\\beta_{FLR, full} - \\widehat\\beta_{FLR, red}}{\\widehat\\beta_{FLR, full}}\n\\] Here’s how I quickly do it with the coefficients:\n\n100 * ( prelim_model2$coefficients[c(2:6, 46)] - prelim_model$coefficients[c(2:6, 46)] ) /\n  prelim_model2$coefficients[c(2:6, 46)]\n\n      iam_unorderedVery underweight iam_unorderedModerately underweight \n                         -1.2686585                          -1.0505274 \n  iam_unorderedSlightly underweight    iam_unorderedSlightly overweight \n                         -2.0085770                           1.3024639 \n iam_unorderedModerately overweight                                 age \n                          0.9253735                           1.5741387 \n\n\nBased on %’s above, it doesn’t look like the indicator makes much of a difference in my model. It is likely because there are only 29 individuals over the age of 100 and 201,031 individuals under the age of 100 (In my dataset). Those 29 individuals will not have a big impact on the linear relationship between age and IAT, even though the first smoothed scatterplot made it look like it does.\nTo bring this point home, I can plot age and IAT with and without the individuals that are 100 years or older.\n\nggplot(iat_3, aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"With individuals 100 years or older\")\n\n\n\nggplot(iat_3 %&gt;% filter(age &lt; 100), aes(x = age, y = IAT_score)) +\n  geom_point() + geom_smooth(method = \"lm\") + xlim(0, 111) +\n  labs(title = \"Without individuals 100 years or older\")"
  },
  {
    "objectID": "labs/Lab_04.html",
    "href": "labs/Lab_04.html",
    "title": "Lab 4",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "labs/Lab_04.html#directions",
    "href": "labs/Lab_04.html#directions",
    "title": "Lab 4",
    "section": "",
    "text": "Please turn in your .html file on Sakai. Please let me know if you greatly prefer to submit a physical copy.\nYou can download the .qmd file for this lab here.\n\n\n\n\n\n\nCaution\n\n\n\nThis is the instructions file. The link above will take you to the editing file where you can add your work and turn it in!! Please do not remove anything from the editing file!!\n\n\n\n\nThe main purpose of this lab is to perform model selection, identify one or more potential final models, and start our interpretation of our main relationship.\n\n\n\nThis lab is graded out of 12 points. Nicky will use the following rubric to assign grades.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "labs/Lab_04.html#lab-activities",
    "href": "labs/Lab_04.html#lab-activities",
    "title": "Lab 4",
    "section": "2 Lab activities",
    "text": "2 Lab activities\nBefore starting this lab, you should go back to Lab 2, save a new .rda file that contains all the new variables from that Lab. Then you can load it here!\n\n2.1 Restate your research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\n\n\n2.2 Step 1: Simple linear regressions / analysis\n\n\n\n\n\n\nTasks\n\n\n\n\nRun a simple linear regression model for each covariate against the IAT score (outcome).\nDisplay results from the test if each covariate explains enough variation of the outcome. This may be from three options in the instructions: summary()/anova() only, lapply(), or sapply()\n\nInterpretation of the results will be in the next step.\n\n\n\n\n2.3 Step 2: Preliminary variable selection\n\n\n\n\n\n\nTasks\n\n\n\n\nDecide which covariates will be included in the initial model and list them.\nRun the initial model and display the regression table.\n\nNo need to write out the model, but you may in addition to the list.\n\n\n\n\n2.4 Step 3: Assess change in coefficient\n\n\n\n\n\n\nTasks\n\n\n\nRemove variables from the initial model based on your common sense, change in coefficient, and/or p-values of the F-tests.\nYou do NOT need to show all your work here. You just need to include:\n\nA brief explanation of what variables were dropped and why (a sentence per variable), and\nAn example of your process with one variable is enough (including code that you ran)\n\n\n\n\n\n2.5 Step 4: Assess scale for continuous variables\n\n\n\n\n\n\nTasks\n\n\n\nNo tasks here! If you want to try out what I did above, you can!\n\n\n\n\n2.6 Step 5: Check for interactions\n\n\n\n\n\n\nTasks\n\n\n\nUsing your discussion in Lab 3 and the results from the F-test on interactions:\n\nCreate a list of the interactions that you will include in your model.\nRun the preliminary final model that includes the main effects and interactions.\n\n\n\n\n\n2.7 Step 6: Assess model fit\n\n\n\n\n\n\nTasks\n\n\n\nOptional: Create a table that displays some fo the model fit statistics to compare preliminary final models.\n\n\n\n\n2.8 Create a forest plot of your coefficient estimates\n\n\n\n\n\n\nTasks\n\n\n\nCreate a forest plot to visualize the coefficient estimates."
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-1",
    "href": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nvars = names(model.frame(main_eff_model))[-1] \n\ninteractions = combn(vars, 2, function(x) paste(x, collapse=\" * \")) %&gt;% \n    grep(., pattern = \"FemaleLiteracyRate\", value = T) \n\n\nMLRs = lapply(interactions, function(int)\n  lm(reformulate(c(vars, int), \"LifeExpectancyYrs\"), data = gapm2))"
  },
  {
    "objectID": "slides/12_Model_selection.html",
    "href": "slides/12_Model_selection.html",
    "title": "Lesson 12: Model/Variable Selection",
    "section": "",
    "text": "Understand the motivation for model selection, including bias-variance trade off and alignment of research goals (association vs. prediction)\nExplain the general process or idea behind different model selection techniques\nRecognize common model fit statistics and understand what they measure"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\nCategorize continuous variables\n\nPercentiles, quartiles, quantiles\n\nCreate indicator variables corresponding to each quartile\n\nMeaningful thresholds\n\nExample: income level groups discussed by Gapminder\n\n\nDisadvantages:\n\nTakes some time to create new variables, especially with multiple continuous covariates\nStart with quartiles, but might be more appropriate to use different splits\n\nNo set rules on this\n\n\nAdvantage: graphical and visually helps"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-1",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n\nFor income, I would use Gapminder’s income level groups\n\nDiscussed in Lesson 10 Categorical Covariates (slide 43)\n\n\n \n\nExperts in the field have developed these income groups\n\nI think this is best solution for income (that was not meeting linearity as a continuous variable)"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-2-fractional-polynomials-1",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-2-fractional-polynomials-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\nlibrary(mfp)\n\nfp_model_CO2 = mfp(LifeExpectancyYrs ~ FemaleLiteracyRate + \n                     fp(CO2emissions, df = 4) + income_levels1 + four_regions +\n                     WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77,\n               data = gapm2, family = \"gaussian\")\n\nfp_model_CO2$fptable %&gt;% gt(rownames_to_stub = T) %&gt;% tab_options(table.font.size = 24)\n\n\n\n\n\n  \n    \n    \n      \n      df.initial\n      select\n      alpha\n      df.final\n      power1\n      power2\n    \n  \n  \n    four_regionsAmericas\n1\n1\n0.05\n1\n1\n.\n    four_regionsAsia\n1\n1\n0.05\n1\n1\n.\n    four_regionsEurope\n1\n1\n0.05\n1\n1\n.\n    WaterSourcePrct\n1\n1\n0.05\n1\n1\n.\n    income_levels1Lower middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1Upper middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1High income\n1\n1\n0.05\n1\n1\n.\n    FoodSupplykcPPD\n1\n1\n0.05\n1\n1\n.\n    FemaleLiteracyRate\n1\n1\n0.05\n1\n1\n.\n    CO2emissions\n4\n1\n0.05\n1\n1\n.\n    members_oecd_g77oecd\n1\n1\n0.05\n1\n1\n.\n    members_oecd_g77others\n1\n1\n0.05\n1\n1\n."
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-conclusion-main-effects-model",
    "href": "slides/13_Purposeful_Selection.html#step-4-conclusion-main-effects-model",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4 Conclusion: main effects model",
    "text": "Step 4 Conclusion: main effects model\n\n\n\nWe concluded that we will use:\n\nIncome levels (categorical) that Gapminder created\nQuartiles for CO2 Emissions\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#overall",
    "href": "slides/Lab_3_feedback.html#overall",
    "title": "Some words on Lab 3",
    "section": "Overall",
    "text": "Overall\n\nGreat work! I know there was a lot of plotting, but you made some great plots!\nI also appreciate your thoughtful approaches to the relationships between variables\nJust a reminder: when visualizing, making tables, displaying information from the data: always keep in the back of your mind:\n\nWhat can a reader get from this if they have never seen the data?\nIs it easy for the reader to understand the plot?\nDoes everything in the plot have a purpose?\nIs the main thing I’m trying to communicate also the thing that stands out?"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#notes-on-coding",
    "href": "slides/Lab_3_feedback.html#notes-on-coding",
    "title": "Some words on Lab 3",
    "section": "Notes on coding",
    "text": "Notes on coding\n\n#| message: false Please use after troubleshooting your code\n\nEspecially when loading libraries! Makes everything neater\n\nWhen assigning category names, capitalization of the first word in each category is customary"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#notes-on-codingvisualization",
    "href": "slides/Lab_3_feedback.html#notes-on-codingvisualization",
    "title": "Some words on Lab 3",
    "section": "Notes on coding/visualization",
    "text": "Notes on coding/visualization\n\n\nQuiz and Lab 2"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#a-few-catches",
    "href": "slides/Lab_3_feedback.html#a-few-catches",
    "title": "Some words on Lab 3",
    "section": "A few catches",
    "text": "A few catches\n\nI said glimpse, but that was a bad choice of words\n\nhead() might be better to show each row with their observations and variables\n\nWhen we were looking into observations that might be suspicious…\n\nWe should look at the intersection of multiple suspicious observations\nA few combinations that people came up with that I thought were good ideas:\n\nSomeone who chose that they felt extremely similar to fat and thin people\nSomeone who reported they were 11-14 years old with education above high school (or other age/education combos)\n\n\nIf the variable is an explanatory variable and originally categorical, it’s good to keep it categorical\n\nLots of lm() functions with variable treated as continuous"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#notes-on-plotting",
    "href": "slides/Lab_3_feedback.html#notes-on-plotting",
    "title": "Some words on Lab 3",
    "section": "Notes on plotting",
    "text": "Notes on plotting\n\nscale_x_discrete(labels = function(x) str_wrap(x, width = 10)): use this to wrap the text on x-axis\n\nStudent used this and I loved it!\n\nKeep your explanatory variable on the x-axis when you are plottinf three variables at once\nhjust and vjust will move your text on the x axis so it does not cover your plot\nPlotting age vs. IAT\n\ngeom_smooth() to show moving mean value\nBoxplots and plotting each mean not exactly right for continuous variables\nSee Lab 4 for how I plot this!\n\nIn geom_smooth(), when to use method = lm\n\nDo not use if trying to see how data look"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#about-confounders-and-effect-modifiers",
    "href": "slides/Lab_3_feedback.html#about-confounders-and-effect-modifiers",
    "title": "Some words on Lab 3",
    "section": "About confounders and effect modifiers",
    "text": "About confounders and effect modifiers\n\nWhen hypothesizing whether a variable a confounder or effect modifier\n\nMake sure to back any claims in your final report with sources\nWe can speculate what’s at play, but we can’t actually know\nOur own identity may bias how we perceive specific dynamics\n\nEducation: I saw many of us speculate that higher education may be associated with lower IAT (as a potential confounder)\n\nData showed the opposite: higher education, higher mean IAT score\nWe can think about people’s perception of controllability of weight: do people assume certain behaviors about fat people?\n\nDoes that align with or go against people’s assumptions about behavior needed for higher education?\nWe also need to think about how education might be linked to socio-economic status, and how that might change what food is affordable\n\nHeavily discussed in Maintenance Phase podcast, but I don’t have direct sources"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#as-we-go-into-lab-4-and-project-report",
    "href": "slides/Lab_3_feedback.html#as-we-go-into-lab-4-and-project-report",
    "title": "Some words on Lab 3",
    "section": "As we go into Lab 4 and Project Report",
    "text": "As we go into Lab 4 and Project Report\n\nIf IAT score ranges from -2 to 2, what changes in mean IAT is a lot?\n\n0.05: about 1.25% change\n0.5: about 12.5% change\n\nA lot of the coefficients may be significant, but are they clinically meaningful?\n\n\n\nQuiz and Lab 2"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#multi-selection-variables",
    "href": "slides/Lab_3_feedback.html#multi-selection-variables",
    "title": "Some words on Lab 3",
    "section": "Multi-selection variables",
    "text": "Multi-selection variables\n\nSplitting gender identities is NOT mutually exclusive\n\nCan identify as all genders!\n\nIf you are trying pinpoint one group and make it mutually exclusive, we need to take extra steps\nSomeone wanted to identify three groups:\n\nIdentifies as trans man, trans woman, genderqueer/non-conforming, and/or other\nIdentifies as man only\nIdentifies as woman only\n\n\n\n\n\niat_new = iat_old %&gt;%\n  mutate(ind_m = grepl(1, genderIdentity), \n         ind_f = grepl(2, genderIdentity),\n         ind_tmm = grepl(3, genderIdentity),\n         ind_twf = grepl(4, genderIdentity),\n         ind_gqnc = grepl(5, genderIdentity),\n         ind_other = grepl(6, genderIdentity))\n\n\n\niat_new = iat_old %&gt;%\n  mutate(ind_m_only = if_else(genderIdentity == '[1]', 1, 0), \n         ind_w_only = if_else(genderIdentity == '[2]', 1, 0), \n         ind_tmm_twf_gqnc_other = if_else(genderIdentity != '[1]' & genderIdentity != '[2]', 1, 0))"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#potential-confounder-vs.-effect-modifier-from-plots",
    "href": "slides/Lab_3_feedback.html#potential-confounder-vs.-effect-modifier-from-plots",
    "title": "Some words on Lab 3",
    "section": "Potential confounder vs. effect modifier from plots",
    "text": "Potential confounder vs. effect modifier from plots"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#multi-selection-variables-1",
    "href": "slides/Lab_3_feedback.html#multi-selection-variables-1",
    "title": "Some words on Lab 3",
    "section": "Multi-selection variables 1",
    "text": "Multi-selection variables 1\n\nSplitting gender identities is NOT mutually exclusive\n\nCan identify as all genders!\n\nIf you are trying pinpoint one group and make it mutually exclusive, we need to take extra steps\nSomeone wanted to identify three groups:\n\nIdentifies as trans man, trans woman, genderqueer/non-conforming, and/or different identity\nIdentifies as man only\nIdentifies as woman only\n\nPoint on data equity:\n\nAlways write out all identities within a grouped category\nI did not call the group something like non-normative genders, instead I said “Identifies as trans man, trans woman, genderqueer/non-conforming, and/or other”\nYour coding names in R can be different, but when you write it out, make sure you define the group\n\nI would maybe call the first group “Trans and non-binary genders” with an exact make up of the group."
  },
  {
    "objectID": "slides/Lab_3_feedback.html#multi-selection-variables-2",
    "href": "slides/Lab_3_feedback.html#multi-selection-variables-2",
    "title": "Some words on Lab 3",
    "section": "Multi-selection variables 2",
    "text": "Multi-selection variables 2\n\nCreate indicators for each identity: don’t any lose information on the individual\n\n\niat_new = iat_old %&gt;%\n  mutate(ind_m = grepl(1, genderIdentity), \n         ind_f = grepl(2, genderIdentity),\n         ind_tmm = grepl(3, genderIdentity),\n         ind_twf = grepl(4, genderIdentity),\n         ind_gqnc = grepl(5, genderIdentity),\n         ind_diff = grepl(6, genderIdentity))\n\n\nCreate mutually exclusive groups: lose some information on the individual\n\n\niat_new = iat_old %&gt;% # iat_old should not have any NA's in genderIdentity\n  mutate(genderID = case_match(genderIdentity,\n                               \"[1]\" ~ \"Male/Man only\",\n                               \"[2]\" ~ \"Female/Woman only\",\n                               .default = \"Trans male/man, trans female/woman, \n                               genderqueer/non-conforming, or differnt identity\"))\n\n\nExample : if someone identifies as a man and a trans man, their data are like:\n\n\\(I(Gender = \\text{Male/Man only}) = 0\\)\n\\(I(Gender = \\text{Female/Woman only}) = 0\\)\n\\(I(Gender = \\text{Trans male/man, trans female/woman, genderqueer/non-conforming, or different identity}) = 1\\)"
  },
  {
    "objectID": "slides/Lab_3_feedback.html#potential-effect-modifier-from-plots",
    "href": "slides/Lab_3_feedback.html#potential-effect-modifier-from-plots",
    "title": "Some words on Lab 3",
    "section": "Potential effect modifier from plots",
    "text": "Potential effect modifier from plots"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-.visibilityhidden",
    "href": "slides/13_Purposeful_Selection.html#step-4-assess-scale-for-continuous-variables-.visibilityhidden",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Assess scale for continuous variables {.visibility=“hidden”}",
    "text": "Step 4: Assess scale for continuous variables {.visibility=“hidden”}\n\n\nResidual plot does not help us with linearity in MLR\nlibrary(ggfortify)\nautoplot(model_full) + theme(text=element_text(size=14))"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-2",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n\nLet’s still try it out with CO2 Emissions (kt)\nI have plotted the quartile lines of food supply with red lines\n\n\n\n\nTake a look at the quartiles within the scatterplot\nvline_coordinates= data.frame(Quantile_Name=names(quantile(gapm2$CO2emissions)),\n                          quantile_values=as.numeric(quantile(gapm2$CO2emissions)))\n\nggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2emissions)) + \n  geom_point(size = 3) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n  geom_vline(data = vline_coordinates, aes(xintercept = quantile_values), \n             color = \"red\", linetype = \"dashed\", size = 2) +\n    theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25))"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-3",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\nLet’s make the quartiles for CO2 emissions:\n\n\nlibrary(dvmisc)\ngapm2 = gapm2 %&gt;% \n  mutate(CO2_q = quant_groups(CO2emissions, groups = 4) %&gt;% factor())\n\n\n\nTake a look at the quartile means within the scatterplot\nggplot(data = gapm2, aes(y = LifeExpectancyYrs, x = CO2_q)) + \n  # geom_point(size = 3, aes(y = LifeExpectancyYrs, x = CO2emissions)) +\n  stat_summary(fun = mean, geom = \"point\", size = 8, shape = 18) +\n  #geom_smooth(se=F) + \n  labs(x = \"CO2 Emissions (kt)\", y = \"Life Expectancy (yrs)\") +\n    theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25))"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-4",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-1-categorize-continuous-variable-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 1: Categorize continuous variable",
    "text": "Step 4: Approach 1: Categorize continuous variable\n\n\n \n\nLet’s fit a new model with the two new representations for income and CO2 emissions\n\n \n\nRemember, this is the main effects model if we decide to make CO2 into quartiles\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-2-fractional-polynomials-2",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-2-fractional-polynomials-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 2: Fractional Polynomials",
    "text": "Step 4: Approach 2: Fractional Polynomials\n\n\n\n\n\n\n\n\n  \n    \n    \n      \n      df.initial\n      select\n      alpha\n      df.final\n      power1\n      power2\n    \n  \n  \n    four_regionsAmericas\n1\n1\n0.05\n1\n1\n.\n    four_regionsAsia\n1\n1\n0.05\n1\n1\n.\n    four_regionsEurope\n1\n1\n0.05\n1\n1\n.\n    WaterSourcePrct\n1\n1\n0.05\n1\n1\n.\n    income_levels1Lower middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1Upper middle income\n1\n1\n0.05\n1\n1\n.\n    income_levels1High income\n1\n1\n0.05\n1\n1\n.\n    FoodSupplykcPPD\n1\n1\n0.05\n1\n1\n.\n    FemaleLiteracyRate\n1\n1\n0.05\n1\n1\n.\n    CO2emissions\n4\n1\n0.05\n1\n1\n.\n    members_oecd_g77oecd\n1\n1\n0.05\n1\n1\n.\n    members_oecd_g77others\n1\n1\n0.05\n1\n1\n.\n  \n  \n  \n\n\n\n\n\n\nConclusion from fractional polynomial is that CO2 does not need to be transformed\nA little counter-intuitive to what we saw in quartiles\nThus, I think leaving CO2 emissions as quartiles is best!"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-4-approach-3-spline-functions-1",
    "href": "slides/13_Purposeful_Selection.html#step-4-approach-3-spline-functions-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 4: Approach 3: Spline functions",
    "text": "Step 4: Approach 3: Spline functions\n\nNeed to specify knots for spline functions\n\nMore knots are flexible, but requires more parameters to estimate\nIn most applications three to five knots are sufficient\n\n\n \n\nWithin our class, fractional polynomials will be sufficient\n\n \n\nIf you think this is cool, I highly suggest you look into Functional Data Analysis (FDA) or Functional Regression\n\nJeffrey Morris is a big name in that field\n\n\n \n\nIn R there are a few options to incorporate splines\n\npspline( ): More information\nsmoothHR(): More information"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-2",
    "href": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\n\n\nMLRs[[1]] %&gt;% tidy() %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 33) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n37.326\n5.463\n6.833\n0.000\n    FemaleLiteracyRate\n−0.035\n0.058\n−0.602\n0.550\n    CO2_q(0.806,2.54]\n9.049\n7.248\n1.249\n0.217\n    CO2_q(2.54,4.66]\n7.843\n16.082\n0.488\n0.628\n    CO2_q(4.66,35.2]\n−5.980\n25.867\n−0.231\n0.818\n    income_levels1Lower middle income\n4.032\n2.661\n1.515\n0.136\n    income_levels1Upper middle income\n4.997\n3.239\n1.543\n0.129\n    income_levels1High income\n6.825\n3.549\n1.923\n0.060\n    four_regionsAmericas\n9.317\n2.193\n4.250\n0.000\n    four_regionsAsia\n5.412\n1.668\n3.246\n0.002\n    four_regionsEurope\n7.267\n2.992\n2.429\n0.018\n    WaterSourcePrct\n0.178\n0.070\n2.529\n0.014\n    FoodSupplykcPPD\n0.004\n0.002\n1.706\n0.094\n    members_oecd_g77oecd\n0.798\n2.731\n0.292\n0.771\n    members_oecd_g77others\n1.121\n2.588\n0.433\n0.667\n    FemaleLiteracyRate:CO2_q(0.806,2.54]\n−0.104\n0.091\n−1.144\n0.258\n    FemaleLiteracyRate:CO2_q(2.54,4.66]\n−0.104\n0.177\n−0.590\n0.557\n    FemaleLiteracyRate:CO2_q(4.66,35.2]\n0.038\n0.268\n0.141\n0.889"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-3",
    "href": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-3",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\nYou can alse go straight to using the anova() function to compare the preliminary model.\n\nanova_res = lapply(interactions,\n            function(int) anova(lm(reformulate(c(vars, int), \"LifeExpectancyYrs\"),\n                                    data = gapm2), main_eff_model)) \nanova_res[[1]] %&gt;% tidy() %&gt;% \n  gt() %&gt;% tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      df.residual\n      rss\n      df\n      sumsq\n      statistic\n      p.value\n    \n  \n  \n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + FemaleLiteracyRate * CO2_q\n54.000\n919.287\nNA\nNA\nNA\nNA\n    LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n57.000\n946.458\n−3.000\n−27.171\n0.532\n0.662"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-4",
    "href": "slides/13_Purposeful_Selection.html#step-5-check-for-interactions-4",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 5: Check for interactions",
    "text": "Step 5: Check for interactions\n\nI went through all the ANOVA tables, and found the following significant interactions:\n\nNone!\n\n\n\nanova_res\n\n[[1]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * CO2_q\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1     54 919.29                          \n2     57 946.46 -3   -27.171 0.532 0.6623\n\n[[2]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * income_levels1\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     54 933.66                           \n2     57 946.46 -3   -12.802 0.2468 0.8633\n\n[[3]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * four_regions\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     54 850.47                           \n2     57 946.46 -3   -95.987 2.0315 0.1203\n\n[[4]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * WaterSourcePrct\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     56 943.42                           \n2     57 946.46 -1   -3.0399 0.1804 0.6726\n\n[[5]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * FoodSupplykcPPD\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     56 915.40                           \n2     57 946.46 -1   -31.063 1.9003 0.1735\n\n[[6]]\nAnalysis of Variance Table\n\nModel 1: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77 + \n    FemaleLiteracyRate * members_oecd_g77\nModel 2: LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 + \n    four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     55 934.95                           \n2     57 946.46 -2   -11.513 0.3386 0.7142\n\n\n \n\nThink about it: does that track with what we saw in our interactions lecture?"
  },
  {
    "objectID": "labs/Project_report_instructions.html#directions",
    "href": "labs/Project_report_instructions.html#directions",
    "title": "Project Report Instructions",
    "section": "1 Directions",
    "text": "1 Directions\n\n\n\n\n\n\nProject template\n\n\n\nYou may use this project template to get started on the report. It is your responsibility to meet the formatting guidelines below!!\nDO NOT USE SITE PAGE (“Project Report Instructions”, current page) as your template!!\n\n\n\n1.1 Purpose\nProject reports serve as a great way to communicate the knowledge learned in a statistics class and connect it to context within research. It is important that we can take a step back from the numbers and analysis to see what questions linear regression can help us answer.\n\n\n1.2 Formatting guide\n\nThe report will be written in Quarto. Turn in both the qmd and html files\n\nNo code should appear in the html document\n\nThis means all R code chunks should have #| echo: false\nThis also means warnings and messages should be turned off\n\n\nThe report should be 10 - 14 paragraphs long\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html - Nicky is able to read all words in figures and tables\n\nWriting, spelling, and grammar should be admissable\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\nSectioning of the report\n\nMain sections that were required: Introduction, Statistical Methods, Results, Discussion, Conclusion, and References\nOther sections that might help group specific methods or results\n\nTitle information at the top of the html\n\nThis includes the title itself, your name, and the date\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n1.3 Examples of reports\nThe following are examples of reports from BSTA 513 with the feedback that I gave them.\nPlease note that 513 uses a different type of outcome than our class. These examples are meant to help guide you with the formatting and some appropriate content.\nAlso note that these were converted to PDFs so I could write in feedback. Some of the tables and figure sizes were distorted. They need to be legible in the html.\n\nReport 1 with my feedback\nReport 2 with my feedback\n\nThe above reports have code showing in their html. Remember that I am asking you to hide all code, warnings, and messages.\n\n\n1.4 Grading\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n1.4.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with very few grammatical or spelling errors. With little editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but are very hard to follow due to grammar mistakes.\nLab not submitted on Sakai (or by email if late) with .html file. Report is not written with complete sentences. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nDiscussion\nThoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nDiscusses limitations and considerations of the results and their consequences, but misses some big considerations.\nDiscusses limitations and considerations of the results, but does not discuss the consequences.\nDiscusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo discussion included.\n\n\nConclusion and References\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but not the focus at all) and statistical caveats are not described. References are not cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is not answered. Or references are not included at all.\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of mean IAT vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)"
  },
  {
    "objectID": "labs/Project_template.html#statistical-methods",
    "href": "labs/Project_template.html#statistical-methods",
    "title": "Project Template: Title here",
    "section": "Statistical Methods",
    "text": "Statistical Methods"
  },
  {
    "objectID": "labs/Project_template.html#results",
    "href": "labs/Project_template.html#results",
    "title": "Project Template: Title here",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "labs/Project_template.html#discussion",
    "href": "labs/Project_template.html#discussion",
    "title": "Project Template: Title here",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "labs/Project_template.html#conclusion",
    "href": "labs/Project_template.html#conclusion",
    "title": "Project Template: Title here",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "labs/Project_template.html#references",
    "href": "labs/Project_template.html#references",
    "title": "Project Template: Title here",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "weeks/week_11_sched.html#monday-318",
    "href": "weeks/week_11_sched.html#monday-318",
    "title": "Week 11",
    "section": "Monday 3/18",
    "text": "Monday 3/18\n\nMeet in 3A001 for in-person project report help!\nSLIDES: Lab 4 feedback and some help on Project Report!!\n\nRecording: I finally turn on my screen at around minute 7!"
  },
  {
    "objectID": "weeks/week_11_sched.html#announcements",
    "href": "weeks/week_11_sched.html#announcements",
    "title": "Week 11",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "href": "slides/14_MLR_Diagnostics.html#lets-remind-ourselves-of-the-model-that-we-have-been-working-with",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Let’s remind ourselves of the model that we have been working with",
    "text": "Let’s remind ourselves of the model that we have been working with\n\nWe have been looking at the association between life expectancy and female literacy rate\nWe used OLS to find the coefficient estimates of our best-fit line\n\n\n\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n50.93\n2.66\n19.14\n0.00\n    female_literacy_rate_2011\n0.23\n0.03\n7.38\n0.00\n  \n  \n  \n\n\n\n\n\\[\\begin{aligned}\n\\widehat{Y} &= \\widehat\\beta_0 + \\widehat\\beta_1 \\cdot X\\\\\n\\widehat{\\text{life expectancy}} &= 50.9 + 0.232 \\cdot \\text{female literacy rate}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "href": "slides/14_MLR_Diagnostics.html#our-residuals-will-help-us-a-lot-in-our-diagnostics",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Our residuals will help us a lot in our diagnostics!",
    "text": "Our residuals will help us a lot in our diagnostics!\n\n\n \n\nThe residuals \\(\\widehat\\epsilon_i\\) are the vertical distances between\n\nthe observed data \\((X_i, Y_i)\\)\nthe fitted values (regression line) \\(\\widehat{Y}_i = \\widehat\\beta_0 + \\widehat\\beta_1 X_i\\)\n\n\n\n\\[\n\\widehat\\epsilon_i =Y_i - \\widehat{Y}_i \\text{,   for } i=1, 2, ..., n\n\\]"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#augment-getting-extra-information-on-the-fitted-model",
    "href": "slides/14_MLR_Diagnostics.html#augment-getting-extra-information-on-the-fitted-model",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "augment(): getting extra information on the fitted model",
    "text": "augment(): getting extra information on the fitted model\n\nRun model1 through augment() (model1 is input)\n\nSo we assigned model1 as the output of the lm() function (model1 is output)\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug1 &lt;- augment(model1) \nglimpse(aug1)\n\nRows: 80\nColumns: 9\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    &lt;dbl&gt; 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     &lt;dbl&gt; 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       &lt;dbl&gt; 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     &lt;dbl&gt; 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    &lt;dbl&gt; 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 &lt;dbl&gt; 0.48238134, 0.58332052, -0.59972251, 0.4757…\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#revisiting-our-line-assumptions",
    "href": "slides/14_MLR_Diagnostics.html#revisiting-our-line-assumptions",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Revisiting our LINE assumptions",
    "text": "Revisiting our LINE assumptions\n\n\n\n\n[L] Linearity of relationship between variables\n\n\nCheck if there is a linear relationship between the mean response (Y) and the explanatory variable (X)\n\n\n\n\n\n[I] Independence of the \\(Y\\) values\n\n\nCheck that the observations are independent\n\n\n\n\n\n[N] Normality of the \\(Y\\)’s given \\(X\\) (residuals)\n\n\nCheck that the responses (at each level X) are normally distributed\n\nUsually measured through the residuals\n\n\n\n\n\n\n[E] Equality of variance of the residuals (homoscedasticity)\n\n\nCheck that the variance (or standard deviation) of the responses is equal for all levels of X\n\nUsually measured through the residuals"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#influential-points",
    "href": "slides/14_MLR_Diagnostics.html#influential-points",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Influential points",
    "text": "Influential points\n\n\n\n\nOutliers\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\n\n\n\n \n \n\n\n\n\n\n\n\n\nHigh leverage observations\n\n\n\nAn observation (\\(X_i, Y_i\\)) whose predictor \\(X_i\\) has an extreme value\n\\(X_i\\) can be an extremely high or low value compared to the rest of the observations"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#outliers",
    "href": "slides/14_MLR_Diagnostics.html#outliers",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Outliers",
    "text": "Outliers\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(Y_i\\) does not follow the general trend of the rest of the data\nHow do we determine if a point is an outlier?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nFollowed by evaluation of its residual (and standardized residual)\n\n\n \n\nUse the internally standardized residual (aka studentized residual) to determine if an observation is an outlier"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-1",
    "href": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#identifying-outliers",
    "href": "slides/14_MLR_Diagnostics.html#identifying-outliers",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Identifying outliers",
    "text": "Identifying outliers\n\n\n\n\nInternally standardized residual\n\n\n\\[\nr_i = \\frac{\\widehat\\epsilon_i}{\\sqrt{\\widehat\\sigma^2(1-h_{ii})}}\n\\]\n\n\n\n\nWe flag an observation if the standardized residual is “large”\n\nDifferent sources will define “large” differently\nPennState site uses \\(|r_i| &gt; 3\\)\nautoplot() shows the 3 observations with the highest standardized residuals\nOther sources use \\(|r_i| &gt; 2\\), which is a little more conservative\n\n\n\n\n\n\n \n\nggplot(data = aug) + \n  geom_histogram(aes(x = .std.resid))"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#countries-that-are-outliers-r_i-2",
    "href": "slides/14_MLR_Diagnostics.html#countries-that-are-outliers-r_i-2",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Countries that are outliers (\\(|r_i| > 2\\))",
    "text": "Countries that are outliers (\\(|r_i| &gt; 2\\))\n\nWe can identify the countries that are outliers\n\n\naug %&gt;% relocate(.std.resid, .after = country) %&gt;%\n  filter(abs(.std.resid) &gt; 2) %&gt;% arrange(desc(abs(.std.resid)))\n\n# A tibble: 6 × 15\n  country   .std.resid LifeExpectancyYrs FemaleLiteracyRate CO2_q income_levels1\n  &lt;chr&gt;          &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         \n1 Swaziland      -2.96              48.9               87.3 (0.8… Lower middle …\n2 South Af…      -2.45              55.8               92.2 (4.6… Upper middle …\n3 Cote d'I…      -2.28              56.9               47.6 [0.0… Lower middle …\n4 Cape Ver…       2.07              72.7               80.3 (0.8… Lower middle …\n5 Sudan           2.05              66.5               63.2 [0.0… Lower middle …\n6 Vanuatu        -2.04              63.2               81.5 [0.0… Lower middle …\n# ℹ 9 more variables: four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;,\n#   FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#high-leverage-observations",
    "href": "slides/14_MLR_Diagnostics.html#high-leverage-observations",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "High leverage observations",
    "text": "High leverage observations\n\nAn observation (\\(X_i, Y_i\\)) whose response \\(X_i\\) is considered “extreme” compared to the other values of \\(X\\)\n\n \n\nHow do we determine if a point has high leverage?\n\nScatterplot of \\(Y\\) vs. \\(X\\)\nCalculating the leverage of each observation"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#leverage-h_i",
    "href": "slides/14_MLR_Diagnostics.html#leverage-h_i",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Leverage \\(h_i\\)",
    "text": "Leverage \\(h_i\\)\n\nValues of leverage are: \\(0 \\leq h_i \\leq 1\\)\nWe flag an observation if the leverage is “high”\n\nOnly good for SLR: Some textbooks use \\(h_i &gt; 4/n\\) where \\(n\\) = sample size\nOnly good for SLR: Some people suggest \\(h_i &gt; 6/n\\)\nWorks for MLR: \\(h_i &gt; 3p/n\\) where \\(p\\) = number of regression coefficients\n\n\n\naug = aug %&gt;% relocate(.hat, .after = FemaleLiteracyRate)\naug %&gt;% arrange(desc(.hat))\n\n# A tibble: 72 × 15\n   country       LifeExpectancyYrs FemaleLiteracyRate  .hat CO2_q income_levels1\n   &lt;chr&gt;                     &lt;dbl&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;         \n 1 Mexico                     75.8               92.3 0.445 (2.5… Upper middle …\n 2 Tajikistan                 69.9               99.6 0.425 [0.0… Lower middle …\n 3 Bosnia and H…              76.9               96.7 0.367 (4.6… Upper middle …\n 4 Uzbekistan                 69                 99.2 0.363 (2.5… Lower middle …\n 5 Bangladesh                 71                 53.4 0.347 [0.0… Lower middle …\n 6 Afghanistan                56.7               13   0.327 [0.0… Low income    \n 7 Zimbabwe                   51.9               80.1 0.321 [0.0… Low income    \n 8 Angola                     60.9               58.6 0.320 (0.8… Lower middle …\n 9 Myanmar                    67.4               90.4 0.304 [0.0… Lower middle …\n10 Yemen                      67.7               48.5 0.296 (0.8… Lower middle …\n# ℹ 62 more rows\n# ℹ 9 more variables: four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;,\n#   FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#countries-with-high-leverage-h_i-4n",
    "href": "slides/14_MLR_Diagnostics.html#countries-with-high-leverage-h_i-4n",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\n\nWe can look at the countries that have high leverage\n\n\naug1 %&gt;% \n  filter(.hat &gt; 4/80) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 6 × 10\n  .rownames country       life_expectancy_years_…¹ female_literacy_rate…²   .hat\n  &lt;chr&gt;     &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n1 1         Afghanistan                       56.7                   13   0.136 \n2 104       Mali                              60                     24.6 0.0980\n3 34        Chad                              57                     25.4 0.0956\n4 146       Sierra Leone                      55.7                   32.6 0.0757\n5 62        Gambia                            66                     41.9 0.0540\n6 70        Guinea-Bissau                     56.2                   42.1 0.0536\n# ℹ abbreviated names: ¹​life_expectancy_years_2011, ²​female_literacy_rate_2011\n# ℹ 5 more variables: .std.resid &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;,\n#   .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-2",
    "href": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#countries-with-high-leverage-h_i-4n-1",
    "href": "slides/14_MLR_Diagnostics.html#countries-with-high-leverage-h_i-4n-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 4/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 4/n\\))\nLabel only countries with large leverage:\n\nggplot(aug1, aes(x = female_literacy_rate_2011, y = life_expectancy_years_2011,\n                 label = country)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", color = \"darkgreen\") +\n  geom_text(aes(label = ifelse(.hat &gt; 0.05, as.character(country), ''))) +\n  geom_vline(xintercept = mean(aug1$female_literacy_rate_2011), color = \"grey\") +\n  geom_hline(yintercept = mean(aug1$life_expectancy_years_2011), color = \"grey\")"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "href": "slides/14_MLR_Diagnostics.html#what-does-the-model-look-like-without-the-high-leverage-points",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "What does the model look like without the high leverage points?",
    "text": "What does the model look like without the high leverage points?\nSensitivity analysis removing countries with high leverage\n\nfinal_model_lowlev &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77, \n                 data = aug_lowlev)\ntidy(final_model_lowlev) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678\n  \n  \n  \n\n\n\ntidy(final_model) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#cooks-distance",
    "href": "slides/14_MLR_Diagnostics.html#cooks-distance",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Cook’s distance",
    "text": "Cook’s distance\n\nMeasures the overall influence of an observation\n\n \n\nAttempts to measure how much influence a single observation has over the fitted model\n\nMeasures how all fitted values change when the \\(ith\\) observation is removed from the model\nCombines leverage and outlier information"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#identifying-points-with-high-cooks-distance",
    "href": "slides/14_MLR_Diagnostics.html#identifying-points-with-high-cooks-distance",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Identifying points with high Cook’s distance",
    "text": "Identifying points with high Cook’s distance\n\n\nThe Cook’s distance for the \\(i^{th}\\) observation is\n\\[d_i = \\frac{h_i}{2(1-h_i)} \\cdot r_i^2\\] where \\(h_i\\) is the leverage and \\(r_i\\) is the studentized residual\n\n\nAnother rule for Cook’s distance that is not strict:\n\nInvestigate observations that have \\(d_i &gt; 1\\)\n\nCook’s distance values are already in the augment tibble: .cooksd\n\n\n\n\nNo countries with high Cook’s distance\n\n\naug = aug %&gt;% relocate(.cooksd, .after = country)\naug %&gt;% arrange(desc(.cooksd)) %&gt;% filter(.cooksd &gt; 1)\n\n# A tibble: 0 × 15\n# ℹ 15 variables: country &lt;chr&gt;, .cooksd &lt;dbl&gt;, LifeExpectancyYrs &lt;dbl&gt;,\n#   FemaleLiteracyRate &lt;dbl&gt;, .hat &lt;dbl&gt;, CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;,\n#   four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;,\n#   members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#plotting-cooks-distance",
    "href": "slides/14_MLR_Diagnostics.html#plotting-cooks-distance",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Plotting Cook’s Distance",
    "text": "Plotting Cook’s Distance\n\n# plot(model) shows figures similar to autoplot()\n# adds on Cook's distance though\nplot(final_model, which = 4)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#model-without-those-4-points-qq-plot-residual-plot",
    "href": "slides/14_MLR_Diagnostics.html#model-without-those-4-points-qq-plot-residual-plot",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\nfinal_model_lowcd &lt;- lm(LifeExpectancyYrs ~ FemaleLiteracyRate + CO2_q + income_levels1 +\n               four_regions + WaterSourcePrct + FoodSupplykcPPD + members_oecd_g77, \n                    data = aug_lowcd)\ntidy(final_model_lowcd) %&gt;% gt() %&gt;% # Without high-leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.435\n3.836\n10.281\n0.000\n    FemaleLiteracyRate\n−0.093\n0.039\n−2.402\n0.020\n    CO2_q(0.806,2.54]\n0.927\n1.795\n0.517\n0.608\n    CO2_q(2.54,4.66]\n−0.309\n2.141\n−0.144\n0.886\n    CO2_q(4.66,35.2]\n0.376\n2.168\n0.173\n0.863\n    income_levels1Lower middle income\n6.275\n2.012\n3.118\n0.003\n    income_levels1Upper middle income\n7.265\n2.434\n2.984\n0.004\n    income_levels1High income\n8.219\n2.624\n3.132\n0.003\n    four_regionsAmericas\n8.735\n1.693\n5.161\n0.000\n    four_regionsAsia\n5.252\n1.474\n3.564\n0.001\n    four_regionsEurope\n7.662\n2.375\n3.226\n0.002\n    WaterSourcePrct\n0.145\n0.051\n2.867\n0.006\n    FoodSupplykcPPD\n0.005\n0.002\n3.231\n0.002\n    members_oecd_g77oecd\n−0.619\n2.062\n−0.300\n0.765\n    members_oecd_g77others\n−0.673\n2.038\n−0.330\n0.742\n  \n  \n  \n\n\n\ntidy(final_model) %&gt;% gt() %&gt;% # With high leverage points\n tab_options(table.font.size = 40) %&gt;%\n fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "href": "slides/14_MLR_Diagnostics.html#model-without-those-4-points-qq-plot-residual-plot-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Model without those 4 points: QQ Plot, Residual plot",
    "text": "Model without those 4 points: QQ Plot, Residual plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am okay with this!\n\nAnd don’t forget: we may want more variables in our model!\nYou do not need to produce plots with the influential points taken out"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#summary-of-how-we-identify-influential-points",
    "href": "slides/14_MLR_Diagnostics.html#summary-of-how-we-identify-influential-points",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Summary of how we identify influential points",
    "text": "Summary of how we identify influential points\n\nUse scatterplot of \\(Y\\) vs. \\(X\\) to see if any points fall outside of range we expect\nUse standardized residuals, leverage, and Cook’s distance to further identify those points\nLook at the models run with and without the identified points to check for drastic changes\n\nLook at QQ plot and residuals to see if assumptions hold without those points\nLook at coefficient estimates to see if they change in sign and large magnitude\n\n\n \n\nNext: how to handle? It’s a little wishy washy"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#how-do-we-deal-with-influential-points",
    "href": "slides/14_MLR_Diagnostics.html#how-do-we-deal-with-influential-points",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "How do we deal with influential points?",
    "text": "How do we deal with influential points?\n\nIf an observation is influential, we can check data errors:\n\nWas there a data entry or collection problem?\nIf you have reason to believe that the observation does not hold within the population (or gives you cause to redefine your population)\n\nIf an observation is influential, we can check our model:\n\nDid you leave out any important predictors?\nShould you consider adding some interaction terms?\nIs there any nonlinearity that needs to be modeled?\n\nBasically, deleting an observation should be justified outside of the numbers!\n\nIf it’s an honest data point, then it’s giving us important information!\n\nMeans we will need to discuss the limitations of our model\n\nFor example: Think about measurements that might help explain life expectancy that are NOT in our model\n\nA really well thought out explanation from StackExchange"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#when-we-have-detected-problems-in-our-model",
    "href": "slides/14_MLR_Diagnostics.html#when-we-have-detected-problems-in-our-model",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "When we have detected problems in our model…",
    "text": "When we have detected problems in our model…\n\nWe have talked about influential points\nWe have talked about identifying issues with our LINE assumptions\n\n \nWhat are our options once we have identified issues in our linear regression model?\n\nAre we missing a crucial measure in our dataset?\nTry a transformation if there is an issue with linearity or normality\n\nAddressed in model selection\n\nTry a weighted least squares approach if unequal variance (oof, not enough time for us to get to)\nTry a robust estimation procedure if we have a lot of outlier issues (outside scope of class)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#transformations",
    "href": "slides/14_MLR_Diagnostics.html#transformations",
    "title": "Model Diagnostics",
    "section": "Transformations",
    "text": "Transformations\n\nWhen we have issues with our LINE (mostly linearity, normality, or equality of variance) assumptions\n\nWe can use transformations to improve the fit of the model\n\nTransformations can…\n\nMake the relationship more linear\nMake the residuals more normal\n“Stabilize” the variance so that it is more constant\nIt can also bring in or reduce outliers\n\nWe can transform the dependent (\\(Y\\)) variable of the independent (\\(X\\)) variable\n\nUsually we want to try transforming the \\(X\\) first\n\n\n \n\nRequires trial and error!!\nMajor drawback: interpreting the model becomes harder!"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#common-transformations",
    "href": "slides/14_MLR_Diagnostics.html#common-transformations",
    "title": "Model Diagnostics",
    "section": "Common transformations",
    "text": "Common transformations\n\nTukey’s transformation (power) ladder\n\nUse R’s gladder() command from the describedata package\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower p\n-3\n-2\n-1\n-1/2\n0\n1/2\n1\n2\n3\n\n\n\n\n\n\\(\\frac{1}{x^3}\\)\n\\(\\frac{1}{x^2}\\)\n\\(\\frac{1}{x}\\)\n\\(\\frac{1}{\\sqrt{x}}\\)\n\\(\\log(x)\\)\n\\(\\sqrt{x}\\)\n\\(x\\)\n\\(x^2\\)\n\\(x^3\\)\n\n\n\n\n\n\nHow to use the power ladder for the general distribution shape\n\nIf data are skewed left, we need to compress smaller values towards the rest of the data\n\nGo “up” ladder to transformations with power &gt; 1\n\nIf data are skewed right, we need to compress larger values towards the rest of the data\n\nGo “down” ladder to transformations with power &lt; 1\n\n\n\n\n\nHow to use the power ladder for heteroscedasticity\n\nIf higher \\(X\\) values have more spread\n\nCompress larger values towards the rest of the data\nGo “down” ladder to transformations with power &lt; 1\n\nIf lower \\(X\\) values have more spread\n\nCompress smaller values towards the rest of the data\nGo “up” ladder to transformations with power &gt; 1"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-3",
    "href": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#transform-dependent-variable",
    "href": "slides/14_MLR_Diagnostics.html#transform-dependent-variable",
    "title": "Model Diagnostics",
    "section": "Transform dependent variable?",
    "text": "Transform dependent variable?\n\nggplot(gapm, aes(x = life_expectancy_years_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#gladder-of-life-expectancy",
    "href": "slides/14_MLR_Diagnostics.html#gladder-of-life-expectancy",
    "title": "Model Diagnostics",
    "section": "gladder() of life expectancy",
    "text": "gladder() of life expectancy\n\ngladder(gapm$life_expectancy_years_2011)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#transform-independent-variable",
    "href": "slides/14_MLR_Diagnostics.html#transform-independent-variable",
    "title": "Model Diagnostics",
    "section": "Transform independent variable?",
    "text": "Transform independent variable?\n\nggplot(gapm, aes(x = female_literacy_rate_2011)) +\n  geom_histogram()"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#gladder-of-female-literacy-rate",
    "href": "slides/14_MLR_Diagnostics.html#gladder-of-female-literacy-rate",
    "title": "Model Diagnostics",
    "section": "gladder() of female literacy rate",
    "text": "gladder() of female literacy rate\n\ngladder(gapm$female_literacy_rate_2011)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#tips",
    "href": "slides/14_MLR_Diagnostics.html#tips",
    "title": "Model Diagnostics",
    "section": "Tips",
    "text": "Tips\n\nRecall, assessing our LINE assumptions are not on \\(Y\\) alone!!\n\nWe can use gladder() to get a sense of what our transformations will do to the data, but we need to check with our residuals again!!\n\nTransformations usually work better if all values are positive (or negative)\nIf observation has a 0, then we cannot perform certain transformations\nLog function only defined for positive values\n\nWe might take the \\(log(X+1)\\) if \\(X\\) includes a 0 value\n\nWhen we make cubic or sqaure transformations, we MUST include the original \\(X\\)\n\nWe do not do this for \\(Y\\) though"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#add-quadratic-and-cubic-transformations-to-dataset",
    "href": "slides/14_MLR_Diagnostics.html#add-quadratic-and-cubic-transformations-to-dataset",
    "title": "Model Diagnostics",
    "section": "Add quadratic and cubic transformations to dataset",
    "text": "Add quadratic and cubic transformations to dataset\n\nHelpful to make a new variable with the transformation in your dataset\n\n\ngapm &lt;- gapm %&gt;% \n  mutate(LE_2 = life_expectancy_years_2011^2,\n         LE_3 = life_expectancy_years_2011^3,\n         FLR_2 = female_literacy_rate_2011^2,\n         FLR_3 = female_literacy_rate_2011^3)\n\nglimpse(gapm)\n\nRows: 188\nColumns: 8\n$ country                    &lt;chr&gt; \"Afghanistan\", \"Albania\", \"Algeria\", \"Andor…\n$ life_expectancy_years_2011 &lt;dbl&gt; 56.7, 76.7, 76.7, 82.6, 60.9, 76.9, 76.0, 7…\n$ female_literacy_rate_2011  &lt;dbl&gt; 13.0, 95.7, NA, NA, 58.6, 99.4, 97.9, 99.5,…\n$ .rownames                  &lt;chr&gt; \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"…\n$ LE_2                       &lt;dbl&gt; 3214.89, 5882.89, 5882.89, 6822.76, 3708.81…\n$ LE_3                       &lt;dbl&gt; 182284.3, 451217.7, 451217.7, 563560.0, 225…\n$ FLR_2                      &lt;dbl&gt; 169.00, 9158.49, NA, NA, 3433.96, 9880.36, …\n$ FLR_3                      &lt;dbl&gt; 2197.0, 876467.5, NA, NA, 201230.1, 982107.…"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "href": "slides/14_MLR_Diagnostics.html#we-are-going-to-compare-a-few-different-models-with-transformations",
    "title": "Model Diagnostics",
    "section": "We are going to compare a few different models with transformations",
    "text": "We are going to compare a few different models with transformations\nWe are going to call life expectancy \\(LE\\) and female literacy rate \\(FLR\\)\n\nModel 1: \\(LE = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 3: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\nModel 4: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\epsilon\\)\nModel 5: \\(LE = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-4",
    "href": "slides/14_MLR_Diagnostics.html#poll-everywhere-question-4",
    "title": "Model Diagnostics",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#compare-scatterplots-does-linearity-improve",
    "href": "slides/14_MLR_Diagnostics.html#compare-scatterplots-does-linearity-improve",
    "title": "Model Diagnostics",
    "section": "Compare Scatterplots: does linearity improve?",
    "text": "Compare Scatterplots: does linearity improve?"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#run-models-with-transformations-examples",
    "href": "slides/14_MLR_Diagnostics.html#run-models-with-transformations-examples",
    "title": "Model Diagnostics",
    "section": "Run models with transformations: examples",
    "text": "Run models with transformations: examples\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2,401.272\n352.070\n6.820\n0.000\n    female_literacy_rate_2011\n31.174\n4.166\n7.484\n0.000\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 = \\beta_0 + \\beta_1 FLR + \\beta_2 FLR^2 +\\beta_3 FLR^3 +\\epsilon\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67,691.796\n149,056.945\n0.454\n0.651\n    female_literacy_rate_2011\n8,092.133\n8,473.154\n0.955\n0.343\n    FLR_2\n−128.596\n147.876\n−0.870\n0.387\n    FLR_3\n0.840\n0.794\n1.059\n0.293"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#normal-q-q-plots-comparison",
    "href": "slides/14_MLR_Diagnostics.html#normal-q-q-plots-comparison",
    "title": "Model Diagnostics",
    "section": "Normal Q-Q plots comparison",
    "text": "Normal Q-Q plots comparison"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#residual-plots-comparison",
    "href": "slides/14_MLR_Diagnostics.html#residual-plots-comparison",
    "title": "Model Diagnostics",
    "section": "Residual plots comparison",
    "text": "Residual plots comparison"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#summary-of-transformations",
    "href": "slides/14_MLR_Diagnostics.html#summary-of-transformations",
    "title": "Model Diagnostics",
    "section": "Summary of transformations",
    "text": "Summary of transformations\n\nIf the model without the transformation is blatantly violating a LINE assumption\n\nThen a transformation is a good idea\n\nIf the model without a transformation is not following the LINE assumptions very well, but is mostly okay\n\nThen try to avoid a transformation\nThink about what predictors might need to be added\nEspecially if you keep seeing the same points as influential\n\nIf interpretability is important in your final work, then transformations are not a great solution"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#reference-all-run-models",
    "href": "slides/14_MLR_Diagnostics.html#reference-all-run-models",
    "title": "Model Diagnostics",
    "section": "Reference: all run models",
    "text": "Reference: all run models\n\n\nModel 2: \\(LE^2 = \\beta_0 + \\beta_1 FLR + \\epsilon\\)\n\nmodel2 &lt;- lm(LE_2 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model2) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n2401.27207\n352.069818\n6.820443\n1.726640e-09\n    female_literacy_rate_2011\n31.17351\n4.165624\n7.483514\n9.352191e-11\n  \n  \n  \n\n\n\n\nModel 3: \\(LE^3 \\sim FLR\\)\n\nmodel3 &lt;- lm(LE_3 ~ female_literacy_rate_2011,\n             data = gapm)\n\ntidy(model3) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n95453.189\n35631.6898\n2.678885\n9.005716e-03\n    female_literacy_rate_2011\n3166.481\n421.5875\n7.510853\n8.285324e-11\n  \n  \n  \n\n\n\n\nModel 4: \\(LE \\sim FLR + FLR^2\\)\n\nmodel4 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2,\n             data = gapm)\n\ntidy(model4) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n57.030875456\n6.282845592\n9.07723652\n8.512585e-14\n    female_literacy_rate_2011\n0.019348795\n0.201021963\n0.09625215\n9.235704e-01\n    FLR_2\n0.001578649\n0.001472592\n1.07202008\n2.870595e-01\n  \n  \n  \n\n\n\n\n\nModel 5: \\(LE \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel5 &lt;- lm(life_expectancy_years_2011 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model5) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n4.732796e+01\n1.117939e+01\n4.2335001\n6.373341e-05\n    female_literacy_rate_2011\n6.517986e-01\n6.354934e-01\n1.0256576\n3.083065e-01\n    FLR_2\n-9.952763e-03\n1.109080e-02\n-0.8973895\n3.723451e-01\n    FLR_3\n6.245016e-05\n5.953283e-05\n1.0490038\n2.975008e-01\n  \n  \n  \n\n\n\n\nModel 6: \\(LE^3 \\sim FLR + FLR^2 + FLR^3\\)\n\nmodel6 &lt;- lm(LE_3 ~ \n               female_literacy_rate_2011 + FLR_2 + FLR_3,\n             data = gapm)\n\ntidy(model6) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n67691.7963283\n1.490569e+05\n0.4541338\n0.6510268\n    female_literacy_rate_2011\n8092.1325988\n8.473154e+03\n0.9550320\n0.3425895\n    FLR_2\n-128.5960879\n1.478757e+02\n-0.8696230\n0.3872447\n    FLR_3\n0.8404736\n7.937625e-01\n1.0588477\n0.2930229\n  \n  \n  \n\n\n\n\n\n\n\n\nLesson 14 MLR Diagnostics"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-1",
    "href": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-1",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit",
    "text": "Step 6: Assess model fit\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\n\nOECD: Organization for Economic Co-operation and Development\nG77: Group of 77\nOther\n\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#lets-remind-ourselves-of-the-final-model",
    "href": "slides/14_MLR_Diagnostics.html#lets-remind-ourselves-of-the-final-model",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s remind ourselves of the final model",
    "text": "Let’s remind ourselves of the final model\n\n\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels1\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct\n\n\n\n\n\nDisplay regression table for final model\ntidy(final_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 32) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-2",
    "href": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-2",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit",
    "text": "Step 6: Assess model fit\n\nModel fit statistics\n\n\nsum_fm = summary(final_model)\nmodel_fit_stats = data.frame(Model = \"Final model\", Adjusted_R_sq = sum_fm$adj.r.squared, AIC = AIC(final_model), BIC = BIC(final_model))\n\nmodel_fit_stats\n\n        Model Adjusted_R_sq      AIC      BIC\n1 Final model      0.743105 421.8035 458.2302\n\nglance(final_model) %&gt;% select(adj.r.squared, AIC, BIC) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      adj.r.squared\n      AIC\n      BIC\n    \n  \n  \n    0.743\n421.804\n458.230\n  \n  \n  \n\n\n\n\n\n\nPurposeful Selection"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-model-fit-statistics",
    "href": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-model-fit-statistics",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit: Model fit statistics",
    "text": "Step 6: Assess model fit: Model fit statistics\n\nWay I did it in the lab instructions\n\n\nsum_fm = summary(final_model)\nmodel_fit_stats = data.frame(Model = \"Final model\", \n                             Adjusted_R_sq = sum_fm$adj.r.squared, \n                             AIC = AIC(final_model), BIC = BIC(final_model))\n\nmodel_fit_stats %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      Model\n      Adjusted_R_sq\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230\n  \n  \n  \n\n\n\n\n\nAnother (maybe faster?) way to do it (glance() in broom package)\n\n\nglance(final_model) %&gt;% mutate(Model = \"Final model\") %&gt;%\n  select(Model, adj.r.squared, AIC, BIC) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      Model\n      adj.r.squared\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230"
  },
  {
    "objectID": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-comparing-model-fits",
    "href": "slides/13_Purposeful_Selection.html#step-6-assess-model-fit-comparing-model-fits",
    "title": "Lesson 13: Purposeful model selection",
    "section": "Step 6: Assess model fit: Comparing model fits",
    "text": "Step 6: Assess model fit: Comparing model fits\n\nRemember the preliminary main effects model (at end of Step 3): same as final model but the continuous varaibles, income and CO2 emissions, were not categorized\nWe can compare model fit statistics of the preliminary main effects model and the final model\n\n\nfm_glance = glance(final_model) %&gt;% mutate(Model = \"Final model\") %&gt;%\n  select(Model, `Adj R-squared` = adj.r.squared, AIC, BIC) \npmem_glance = glance(prelim_me_model) %&gt;% \n  mutate(Model = \"Preliminary main effects model\") %&gt;%\n  select(Model, `Adj R-squared` = adj.r.squared, AIC, BIC) \nrbind(fm_glance, pmem_glance) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      Model\n      Adj R-squared\n      AIC\n      BIC\n    \n  \n  \n    Final model\n0.743\n421.804\n458.230\n    Preliminary main effects model\n0.747\n417.708\n445.028\n  \n  \n  \n\n\n\n\n\nRemember, adjusted \\(R^2\\), AIC, and BIC penalize models for more coefficients\nPreliminary main effects model: better fit statistics, but violates linearity assumption\n\n\n\nPurposeful Selection"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#its-a-lot-to-visualize",
    "href": "slides/14_MLR_Diagnostics.html#its-a-lot-to-visualize",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "It’s a lot to visualize",
    "text": "It’s a lot to visualize\n\nPart of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\nWith 7 variables in out final model, it is hard to visualize outliers and influential points\n\n \n\nI highly encourage you revisit Lesson 6 and 7 (SLR Diagnostics) to help understand these notes"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#remember-our-friend-augment",
    "href": "slides/14_MLR_Diagnostics.html#remember-our-friend-augment",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Remember our friend augment()?",
    "text": "Remember our friend augment()?\n\nRun final_model through augment() (final_model is input)\n\nSo we assigned final_model as the output of the lm() function\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug = augment(final_model)\nhead(aug) %&gt;% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n\n# A tibble: 6 × 14\n  LifeExpectancyYrs .fitted .resid .std.resid  .hat  .cooksd FemaleLiteracyRate\n              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1              56.7    61.5 -4.78      -1.43  0.327 0.0663                 13  \n2              76.7    75.3  1.38       0.387 0.227 0.00293                95.7\n3              60.9    58.6  2.30       0.684 0.320 0.0147                 58.6\n4              76.9    74.7  2.21       0.620 0.238 0.00799                99.4\n5              76      76.9 -0.879     -0.233 0.145 0.000614               97.9\n6              73.8    74.6 -0.796     -0.214 0.168 0.000618               99.5\n# ℹ 7 more variables: CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;, four_regions &lt;fct&gt;,\n#   WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;,\n#   .sigma &lt;dbl&gt;\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "href": "slides/14_MLR_Diagnostics.html#summary-of-the-assumptions-and-their-diagnostic-tool",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Summary of the assumptions and their diagnostic tool",
    "text": "Summary of the assumptions and their diagnostic tool\n\n\n\n\n\n\n\n\nAssumption\nWhat needs to hold?\nDiagnostic tool\n\n\n\n\nLinearity\n\\(\\text{}\\)\n\nRelationship between each \\(X\\) and \\(Y\\) is linear\n\n\nScatterplot of \\(Y\\) vs. \\(X\\)\n\n\\(\\text{}\\)\n\n\nIndependence\n\\(\\text{}\\)\n\nObservations are independent from each other\n\n\nStudy design\n\n\\(\\text{}\\)\n\n\nNormality\n\nResiduals (and thus \\(Y|X_1, X_2, ..., X_p\\))\nare normally distributed\n\n\nQQ plot of residuals\nDistribution of residuals\n\n\n\nEquality of variance\n\nVariance of residuals (and thus \\(Y|X_1, X_2, ..., X_p\\))\nis same across fitted values (homoscedasticity)\n\n\nResidual plot\n\n\\(\\text{}\\)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#autoplot-can-be-a-helpful-tool",
    "href": "slides/14_MLR_Diagnostics.html#autoplot-can-be-a-helpful-tool",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "autoplot() can be a helpful tool",
    "text": "autoplot() can be a helpful tool\n\n\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n\n\n\n\n\n\n\n\n\nLooks like observations 17, 59, 61 have been flagged by autoplot()\n\n17: Cote d’Ivoire\n59: South Africa\n61: Kingdom of Eswatini (formerly Swaziland in 2011)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#countries-with-high-leverage-h_i-3pn",
    "href": "slides/14_MLR_Diagnostics.html#countries-with-high-leverage-h_i-3pn",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Countries with high leverage (\\(h_i > 3p/n\\))",
    "text": "Countries with high leverage (\\(h_i &gt; 3p/n\\))\n\nWe can look at the countries that have high leverage: there are NONE\n\n\nn = nrow(gapm2); p = length(final_model$coefficients) - 1\naug %&gt;% \n  filter(.hat &gt; 3*p/n) %&gt;%\n  arrange(desc(.hat))\n\n# A tibble: 0 × 15\n# ℹ 15 variables: country &lt;chr&gt;, LifeExpectancyYrs &lt;dbl&gt;,\n#   FemaleLiteracyRate &lt;dbl&gt;, .hat &lt;dbl&gt;, CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;,\n#   four_regions &lt;fct&gt;, WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;,\n#   members_oecd_g77 &lt;chr&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt;, .sigma &lt;dbl&gt;,\n#   .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#final-model-qq-plot-residual-plot",
    "href": "slides/14_MLR_Diagnostics.html#final-model-qq-plot-residual-plot",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Final Model: QQ Plot, Residual plot",
    "text": "Final Model: QQ Plot, Residual plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI am okay with this!\n\nAnd don’t forget: we may want more variables in our model!\nYou do not need to produce plots with the influential points taken out"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality",
    "href": "slides/14_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "autoplot() to examine equality of variance and Normality",
    "text": "autoplot() to examine equality of variance and Normality\n\n\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality-1",
    "href": "slides/14_MLR_Diagnostics.html#autoplot-to-examine-equality-of-variance-and-normality-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "autoplot() to examine equality of variance and Normality",
    "text": "autoplot() to examine equality of variance and Normality\n\n\n\nlibrary(ggfortify)\nautoplot(final_model) + theme(text=element_text(size=20))\n\n\n\n\n\n\n\n\n\nLooks like 3 obs are flagged:\n\n17: Cote d’Ivoire\n59: South Africa\n61: Kingdom of Eswatini (formerly Swaziland in 2011)\n\nWithout them, QQ-plot and residual plot look good\n\nPoints on QQ-plot are close to identity line\nResiduals have pretty consistent spread across fitted values\n\nBut don’t take them out!!!\n\nInstead, discuss what may be missing in our regression model that is not capturing the characteristics of these countries"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#what-is-multicollinearity-adapted-from-parts-of-stat-501-page",
    "href": "slides/14_MLR_Diagnostics.html#what-is-multicollinearity-adapted-from-parts-of-stat-501-page",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "What is multicollinearity? (adapted from parts of STAT 501 page)",
    "text": "What is multicollinearity? (adapted from parts of STAT 501 page)\nSo far, we’ve been ignoring something very important: multicollinearity\n\n\n\n\n\n\nMulticollinearity\n\n\nTwo or more covariates in a multivariable regression model are highly correlated\n\n\n\n\n\n\n\nTypes of multicollinearity\n\nStructural multicollinearity\n\nMathematical artifact caused by creating new covariates from other covariates\nFor example: If we have age, and decide to transform age to include age-squared\n\nThen we have age and age-squared in the model: age-squared is perfectly predicted by age!\n\n\nData-based multicollinearity\n\nResult of a poorly designed experiment, reliance on purely observational data, or the inability to manipulate the system on which the data are collected."
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#why-is-multicollinearity-a-problem",
    "href": "slides/14_MLR_Diagnostics.html#why-is-multicollinearity-a-problem",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Why is multicollinearity a problem?",
    "text": "Why is multicollinearity a problem?\nIn linear regression…\n\nEstimated regression coefficient of any one variable depends on other predictors included in the model\n\nNot necessarily bad, but a big change might be an issue\n\nHypothesis tests for any coefficient may yield different conclusions depending on other predictors included in the model\nMarginal contribution of any one predictor variable in reducing the error sum of squares depends on other predictors included in the model\n\n \nWhen there is multicollinearity in our model:\n\nPrecision of the estimated regression coefficients or correlated covariates decreases a lot\n\nBasically, standard error increases and confidence intervals get wider, which means we’re not as confident in our estimate anymore\nBecause highly correlated covariates are not adding much more information, but are constraining our model more"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#how-do-we-detect-multicollinearity",
    "href": "slides/14_MLR_Diagnostics.html#how-do-we-detect-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "How do we detect multicollinearity?",
    "text": "How do we detect multicollinearity?\n\n\n\nVariance inflation factors (VIF): quantifies how much the variance of the estimated coefficient for covariate \\(k\\) increases\n\nIncreases: from SLR with only covariate \\(k\\) to MLR with all other covariates\n\n\n \n\nGeneral rule of thumb\n\n\\(4 &lt; VIF &lt; 10\\): Warrent investigation (but most people aren’t investigating this…)\n\\(VIF &gt; 10\\): Requires correction\n\nInfluencing regression coefficient estimates\n\n\n\n\n\n\n\n\nVIF\n\n\n\\[\nVIF = \\dfrac{1}{1-R_k^2}\n\\]\n\\(R_k^2\\) is the \\(R^2\\)-value obtained by regressing the \\(k^{th}\\) covariate/predictor on the remaining predictors"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#did-you-notice-anything-about-all-the-consequences-of-multicollinearity",
    "href": "slides/14_MLR_Diagnostics.html#did-you-notice-anything-about-all-the-consequences-of-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Did you notice anything about all the consequences of multicollinearity?",
    "text": "Did you notice anything about all the consequences of multicollinearity?\n\nAll consequences relate to estimating a regression coefficient precisely\n\nRecall that precision is linked to analysis goals of association and interpretability\nSee Lesson 12: Model Selection\n\n\n \n\nMulticollinearity is not really an issue when our goal is prediction\n\nHighly correlated covariates/predictors will not hurt our prediction of an outcome"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model",
    "href": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model",
    "text": "Let’s apply it to our final model\n\nNaive way to calculate this:\n\n\nlibrary(rms)\nrms::vif(final_model)\n\n               FemaleLiteracyRate                 CO2_q(0.806,2.54] \n                         4.863139                          2.979224 \n                 CO2_q(2.54,4.66]                  CO2_q(4.66,35.2] \n                         4.758904                          5.180216 \nincome_levels1Lower middle income income_levels1Upper middle income \n                         5.290718                          8.406927 \n        income_levels1High income              four_regionsAmericas \n                         7.293148                          2.531966 \n                 four_regionsAsia                four_regionsEurope \n                         2.096398                          7.771994 \n                  WaterSourcePrct                   FoodSupplykcPPD \n                         4.824266                          3.499250 \n             members_oecd_g77oecd            members_oecd_g77others \n                         2.720955                          5.125196 \n\n\n\nAll \\(VIF &lt; 10\\)\nProblem: multi-level covariates (CO2 Emissions and income level) have different VIF’s even though they should be considered one variable"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly",
    "href": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model correctly",
    "text": "Let’s apply it to our final model correctly\n\nCalculate the GVIF and, more importantly, the \\(GVIF^{1/(2\\cdot df)}\\)\nGVIF is the \\(R^2\\)-value for regressing a covariate’s group indicators on the remaining covariates\n\nCaptures the correlation between covariates better\n\n\\(GVIF^{1/(2\\cdot df)}\\) helps standardize GVIF based on how many levels each categorical covariate has\n\nIf continuous covariate, \\(GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}\\)\n\n\n\nlibrary(car)\ncar::vif(final_model)\n\n                        GVIF Df GVIF^(1/(2*Df))\nFemaleLiteracyRate  4.863139  1        2.205253\nCO2_q               8.223951  3        1.420736\nincome_levels1     11.045885  3        1.492336\nfour_regions       13.935918  3        1.551277\nWaterSourcePrct     4.824266  1        2.196421\nFoodSupplykcPPD     3.499250  1        1.870628\nmembers_oecd_g77    7.430919  2        1.651052\n\n\n\n\nLesson 14: MLR Diagnostics"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-12",
    "href": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-12",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model correctly (1/2)",
    "text": "Let’s apply it to our final model correctly (1/2)\n\nCalculate the GVIF and, more importantly, the \\(GVIF^{1/(2\\cdot df)}\\)\nGVIF is the \\(R^2\\)-value for regressing a covariate’s group indicators on the remaining covariates\n\nCaptures the correlation between covariates better\n\n\\(GVIF^{1/(2\\cdot df)}\\) helps standardize GVIF based on how many levels each categorical covariate has\n\nI’ll refer to this as df-corrected GVIF or standardized GVIF\nIf continuous covariate, \\(GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}\\)\n\n\n\nlibrary(car)\ncar::vif(final_model)\n\n                        GVIF Df GVIF^(1/(2*Df))\nFemaleLiteracyRate  4.863139  1        2.205253\nCO2_q               8.223951  3        1.420736\nincome_levels1     11.045885  3        1.492336\nfour_regions       13.935918  3        1.551277\nWaterSourcePrct     4.824266  1        2.196421\nFoodSupplykcPPD     3.499250  1        1.870628\nmembers_oecd_g77    7.430919  2        1.651052"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-22",
    "href": "slides/14_MLR_Diagnostics.html#lets-apply-it-to-our-final-model-correctly-22",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Let’s apply it to our final model correctly (2/2)",
    "text": "Let’s apply it to our final model correctly (2/2)\n\nIf continuous covariate, \\(GVIF^{1/(2\\cdot df)} = \\sqrt{GVIF}\\)\nSo we can square \\(GVIF^{1/(2\\cdot df)}\\) and set VIF rules\nOR: we can correct any \\(GVIF^{1/(2\\cdot df)} &gt; \\sqrt{10} = 3.162\\)\n\n\ncar::vif(final_model)\n\n                        GVIF Df GVIF^(1/(2*Df))\nFemaleLiteracyRate  4.863139  1        2.205253\nCO2_q               8.223951  3        1.420736\nincome_levels1     11.045885  3        1.492336\nfour_regions       13.935918  3        1.551277\nWaterSourcePrct     4.824266  1        2.196421\nFoodSupplykcPPD     3.499250  1        1.870628\nmembers_oecd_g77    7.430919  2        1.651052\n\n\n\nAll of these covariates are okay! No multicollinearity to correct in this dataset!"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#poll-everywhere-question",
    "href": "slides/14_MLR_Diagnostics.html#poll-everywhere-question",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#but-what-if-we-do-need-to-make-corrections-for-multicollinearity",
    "href": "slides/14_MLR_Diagnostics.html#but-what-if-we-do-need-to-make-corrections-for-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "But what if we do need to make corrections for multicollinearity?",
    "text": "But what if we do need to make corrections for multicollinearity?\n\nWe have been dealing with data-based multicollinearity in our example\nIf we had issues with multicollinearity, then what are our options?\n\nRemove the variable(s) with large VIF\nUse expert knowledge in the field to decide\n\nIf one variable has a large VIF, then there is usually another one or more variables with large VIFs\n\nBasically, all the covariates that are correlated will have large VIFs\n\nExample: our two largest GVIFs were for world region and income levels\n\nHypothetical: their \\(GVIF^{1/(2\\cdot df)} &gt; 3.162\\)\nRemove one of them\nI’m no expert, but from more of a data equity lens, there’s a lot of generalizations made about world regions\n\nI think relying on the income level of a country might give us more information as well"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#what-about-structural-multicollinearity",
    "href": "slides/14_MLR_Diagnostics.html#what-about-structural-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "What about structural multicollinearity?",
    "text": "What about structural multicollinearity?\n\nStructural multicollinearity\n\nMathematical artifact caused by creating new covariates from other covariates\n\n\n \n\nFor example: If we have age, and decide to transform age to include age-squared\n\nThen we have age and age-squared in the model: age-squared is perfectly predicted by age!\nBy having the untransformed and transformed covariate in the model, they are inherently correlated!\n\n\n \n\nBest practice to reduce the correlation: center you covariate\n\nBy centering age, we no longer have a one-to-one connection between age and age-squared\nIf centered at 40yo: a 35 yo and a 45 yo will both have centered age of 5, and age-squared of 25\n\n\n \n\nCheck out the Penn State site for a work through of an example with VIFs"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#sumamry-of-multicollinearity",
    "href": "slides/14_MLR_Diagnostics.html#sumamry-of-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Sumamry of multicollinearity",
    "text": "Sumamry of multicollinearity\n\n\nLesson 14: MLR Diagnostics"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#fit-a-model-with-only-co2-and-life-expectancy",
    "href": "slides/14_MLR_Diagnostics.html#fit-a-model-with-only-co2-and-life-expectancy",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Fit a model with only CO2 and life expectancy",
    "text": "Fit a model with only CO2 and life expectancy\n\ngapm3 = gapm2 %&gt;% mutate(CO2_sq = CO2emissions^2)\nmod = lm(LifeExpectancyYrs ~ CO2emissions + CO2_sq, data = gapm3)\ntidy(mod, conf.int = T) %&gt;% gt()\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n65.53834729\n1.30862362\n50.081892\n5.525529e-56\n62.9277146\n68.14897998\n    CO2emissions\n1.70140174\n0.37393698\n4.549969\n2.235912e-05\n0.9554179\n2.44738561\n    CO2_sq\n-0.04633628\n0.01268783\n-3.652027\n5.023859e-04\n-0.0716478\n-0.02102476\n  \n  \n  \n\n\n\nvif(mod)\n\nCO2emissions       CO2_sq \n    6.191072     6.191072"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#summary-of-multicollinearity",
    "href": "slides/14_MLR_Diagnostics.html#summary-of-multicollinearity",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Summary of multicollinearity",
    "text": "Summary of multicollinearity\n\nCorrelated covariates/predictors will hurt our model’s precision and interpretations of coefficients\n\n \n\nWe need to check for multicollinearity by using VIFs or GVIFs\n\n \n\nIf \\(VIF &gt; 10\\) or \\(GVIF^{1/(2\\cdot df)} &gt; 3.162\\), we need to do something about the covariates\n\nData based: remove one the of correlated variables\nStructural based: centering usually fixes it"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#regression-analysis-process",
    "href": "slides/14_MLR_Diagnostics.html#regression-analysis-process",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html#regression-analysis-process-1",
    "href": "slides/14_MLR_Diagnostics.html#regression-analysis-process-1",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "Regression analysis process",
    "text": "Regression analysis process\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\nLesson 14: MLR Diagnostics"
  },
  {
    "objectID": "slides/14_MLR_Diagnostics.html",
    "href": "slides/14_MLR_Diagnostics.html",
    "title": "Lesson 14: MLR Model Diagnostics",
    "section": "",
    "text": "Apply tools from SLR (Lesson 6: SLR Diagnostics) in MLR to evaluate LINE assumptions, including residual plots and QQ-plots\nApply tools involving standardized residuals, leverage, and Cook’s distance from SLR (Lesson 7: SLR Diagnostics 2) in MLR to flag potentially influential points\nUse Variance Inflation Factor (VIF) and it’s general form to detect and correct multicollinearity\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\nBuilding a model\nSelecting variables\nPrediction vs interpretation\nComparing potential models\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\nFind best fit line\nUsing OLS in this class\nParameter estimation\nCategorical covariates\nInteractions\n\n\n\n\n\n\n\n\nModel Evaluation\n\n\n\nEvaluation of model fit\nTesting model assumptions\nResiduals\nTransformations\nInfluential points\nMulticollinearity\n\n\n\n\n\n\n\n\nModel Use (Inference)\n\n\n\n\n\nInference for coefficients\nHypothesis testing for coefficients\n\n\n\nInference for expected \\(Y\\) given \\(X\\)\nPrediction of new \\(Y\\) given \\(X\\)\n\n\n\n\n\n\n\n\n\n\n\nOur final model contains\n\nFemale Literacy Rate FLR\nCO2 Emissions in quartiles CO2_q\nIncome levels in groups assigned by Gapminder income_levels1\nWorld regions four_regions\nMembership of global and economic groups members_oecd_g77\nFood Supply FoodSupplykcPPD\nClean Water Supply WaterSupplePct\n\n\n\n\n\nDisplay regression table for final model\ntidy(final_model) %&gt;% gt() %&gt;% tab_options(table.font.size = 32) %&gt;%  \n  fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n39.877\n4.889\n8.157\n0.000\n    FemaleLiteracyRate\n−0.073\n0.047\n−1.555\n0.125\n    CO2_q(0.806,2.54]\n1.099\n1.914\n0.574\n0.568\n    CO2_q(2.54,4.66]\n−0.292\n2.419\n−0.121\n0.904\n    CO2_q(4.66,35.2]\n−0.595\n2.524\n−0.236\n0.814\n    income_levels1Lower middle income\n5.441\n2.343\n2.322\n0.024\n    income_levels1Upper middle income\n6.111\n2.954\n2.069\n0.043\n    income_levels1High income\n7.959\n3.277\n2.429\n0.018\n    four_regionsAmericas\n9.003\n2.050\n4.391\n0.000\n    four_regionsAsia\n5.260\n1.637\n3.213\n0.002\n    four_regionsEurope\n6.855\n2.871\n2.387\n0.020\n    WaterSourcePrct\n0.166\n0.066\n2.496\n0.015\n    FoodSupplykcPPD\n0.004\n0.002\n1.825\n0.073\n    members_oecd_g77oecd\n1.119\n2.674\n0.418\n0.677\n    members_oecd_g77others\n1.047\n2.511\n0.417\n0.678\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nPart of the reason why we discussed model diagnostics in SLR was so that we could have accompanying visuals to help us understand\n\n \n\nWith 7 variables in out final model, it is hard to visualize outliers and influential points\n\n \n\nI highly encourage you revisit Lesson 6 and 7 (SLR Diagnostics) to help understand these notes\n\n\n\n\n\nRun final_model through augment() (final_model is input)\n\nSo we assigned final_model as the output of the lm() function\n\nWill give us values about each observation in the context of the fitted regression model\n\ncook’s distance (.cooksd), fitted value (.fitted, \\(\\widehat{Y}_i\\)), leverage (.hat), residual (.resid), standardized residuals (.std.resid)\n\n\n\naug = augment(final_model)\nhead(aug) %&gt;% relocate(.fitted, .resid, .std.resid, .hat, .cooksd, .after = LifeExpectancyYrs)\n\n# A tibble: 6 × 14\n  LifeExpectancyYrs .fitted .resid .std.resid  .hat  .cooksd FemaleLiteracyRate\n              &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;              &lt;dbl&gt;\n1              56.7    61.5 -4.78      -1.43  0.327 0.0663                 13  \n2              76.7    75.3  1.38       0.387 0.227 0.00293                95.7\n3              60.9    58.6  2.30       0.684 0.320 0.0147                 58.6\n4              76.9    74.7  2.21       0.620 0.238 0.00799                99.4\n5              76      76.9 -0.879     -0.233 0.145 0.000614               97.9\n6              73.8    74.6 -0.796     -0.214 0.168 0.000618               99.5\n# ℹ 7 more variables: CO2_q &lt;fct&gt;, income_levels1 &lt;fct&gt;, four_regions &lt;fct&gt;,\n#   WaterSourcePrct &lt;dbl&gt;, FoodSupplykcPPD &lt;dbl&gt;, members_oecd_g77 &lt;chr&gt;,\n#   .sigma &lt;dbl&gt;\n\n\nRDocumentation on the augment() function."
  },
  {
    "objectID": "slides/Lab4_PR.html#overall",
    "href": "slides/Lab4_PR.html#overall",
    "title": "Some words on Lab 4 + help with coefficients",
    "section": "Overall",
    "text": "Overall\n\nInteractions: Limit to 1-2 interactions!!\n\nWith large sample size, many will come up significant\nMake choice based on your visualizations and previous research\n\nWhen deciding on what covariates to leave in or take out of a model:\n\nIf the covariate is categorical with more than 2 levels, you MUST use F-test!!\n\nCheck you HTML document in the report!!\n\nMake sure things look good: proportions and font size should make sense\nTables and figures should look good!\n\nWhen looking at % change:\n\nSome only looked at model with explanatory variable and variable in question\nNeed to use the model with all other variables as well!!"
  },
  {
    "objectID": "slides/Lab4_PR.html#table-1",
    "href": "slides/Lab4_PR.html#table-1",
    "title": "Some words on Lab 4 + help with coefficients",
    "section": "Table 1",
    "text": "Table 1\nGood page on creating a Table 1"
  },
  {
    "objectID": "slides/Lab4_PR.html#displaying-coefficients",
    "href": "slides/Lab4_PR.html#displaying-coefficients",
    "title": "Some words on Lab 4 + help with coefficients",
    "section": "Displaying coefficients",
    "text": "Displaying coefficients\nHelpful source for plotting predictions for variables that are part of interactions!\n\nI would make a table or forest plot for the main effects of my model\n\nThen make a plot showing the interactions\n\nLet’s look at it together?\n\n\nlibrary(ggeffects)\ndat &lt;- predict_response(final_model, terms = c(\"identfat\", \"comptomost\"))\nplot(dat)\n\nSee recording for results!\n\n\nQuiz and Lab 2"
  }
]