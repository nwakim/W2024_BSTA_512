---
title: "Week 6"
description: "Common families of discrete distributions cont."
date: "10/30/2023"
date-modified: "9/15/2023"
categories: [""]
format: 
  html:
    link-external-newwindow: true
    toc: true
editor_options: 
  chunk_output_type: console
---

```{css, echo=FALSE}
.title{
  font-size: 40px;
  color: #006a4e;
  background-color: #fff;
  padding: 10px;
}

.description{
  font-size: 20px;
  color: #fff;
  background-color: #006a4e;
  padding: 10px;
}
```

## Resources

| Chapter | Topic                |                                Slides                                 |                                  Annotated Slides                                  |                           Recording                           |
|---------------|---------------|:-------------:|:-------------:|:-------------:|
| 17      | Negative Binomial RV | [{{< iconify ri slideshow-fill size=30px >}}]{style="color:#f8f5f0;"} | [{{< iconify pepicons-pop pen-circle-filled size=29px >}}]{style="color:#f8f5f0;"} | [{{< iconify mdi video size=34px >}}]{style="color:#f8f5f0;"} |
| 18      | Poisson RV           | [{{< iconify ri slideshow-fill size=30px >}}]{style="color:#f8f5f0;"} | [{{< iconify pepicons-pop pen-circle-filled size=29px >}}]{style="color:#f8f5f0;"} | [{{< iconify mdi video size=34px >}}]{style="color:#f8f5f0;"} |

For the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:

1.  Click on the icon with three horizontal bars on the bottom left of the browser.
2.  Click on "Tools" with the gear icon at the top of the sidebar.
3.  Click on "PDF Export Mode."
4.  From there, you can print or save the PDF as you would normally from your internet browser.

## On the Horizon

## Class Exit Tickets

[{{< iconify healthicons health-worker-form-negative size=30px >}}]{style="color:#f8f5f0;"} Monday (10/30)

## Additional Information

## Statistician of the Week: Lester Mackey

::: {.grid}

::: {.g-col-4}
```{r fig.cap = "Lester Mackey", fig.alt = "Image credit: Dana J Quigley", preview = TRUE, echo = FALSE}
knitr::include_graphics("../images/stat_of_week/mackey.jpeg")
```
:::

:::{.g-col-8}

Dr. Mackey is a machine learning researcher at Microsoft Research New England and an adjunct professor at Stanford University.  His PhD (Computer Science 2012) and MA (Statistics 2011) are both from University of California, Berkeley, while his undergraduate degree (Computer Science 2007) is from Princeton University. 

He is involved in Stanford's initiative of <a href = "https://stats-for-good.stanford.edu/" target = "_blank">Statistics for Social Good</a> and has the following quote on his website:

>Quixotic though it may sound, I hope to use computer science and statistics to change the world for the better.

:::

:::


#### Topics covered

From Dr. Mackey's <a href = "http://stanford.edu/~lmackey/" target = "_blank">personal website</a> his areas of research are:

* statistical machine learning
* scalable algorithms
* high-dimensional statistics
* approximate inference
* probability

#### Relevant work

* Koulik Khamaru, Yash Deshpande, Lester Mackey, and Martin J. Wainwright, <a href = "https://arxiv.org/pdf/2107.02266.pdf" target = "_blank">Near-optimal inference in adaptive linear regression</a>

> When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals...

* Pierre Bayle, Alexandre Bayle, Lucas Janson, and Lester Mackey, <a href = "https://proceedings.neurips.cc/paper/2020/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html" target = "_blank">Cross-validation Confidence Intervals for Test Error</a> **Advances in Neural Information Processing Systems (NeurIPS)**, December 2020.

> This work develops central limit theorems for cross-validation and consistent estimators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our real-data experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature...


#### Outside links

* [Mathematically Gifted & Black](https://mathematicallygiftedandblack.com/honorees/lester-mackey/)
* [Linkedin](https://www.linkedin.com/in/lester-mackey-5902909/)
* [personal](http://stanford.edu/~lmackey/)


####  Other

The precursor to <a href = "https://www.kaggle.com/" target = "_blank">kaggle</a> was a <a href = "https://en.wikipedia.org/wiki/Netflix_Prize" target = "_blank">$1 million prize given by Netflix</a> to the most accurate prediction of ratings that people give to the movies they watch.  As undergraduates, Dr. Mackey and two friends led the competition for a few hours in its first year.  Later, groups merged and Dr. Mackey's group merged with a few others, forming The Ensemble.  Their final analysis came in second with the **exact same** error rates as the winning entry.  The winning entry, however, had been submitted 20 minutes prior.  Sigh.

## Muddiest Points

This will be filled in with your Exit Ticket responses.

## Clearest Points

This will be filled in with your Exit Ticket responses.
